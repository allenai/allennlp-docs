

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>allennlp.data.token_indexers &mdash; AllenNLP 0.9.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="allennlp.data.tokenizers" href="allennlp.data.tokenizers.html" />
    <link rel="prev" title="allennlp.data.iterators" href="allennlp.data.iterators.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/allennlp-logo-dark.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.9.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Package Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="allennlp.commands.html">allennlp.commands</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.subcommand.html">allennlp.commands.subcommand</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.configure.html">allennlp.commands.configure</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.evaluate.html">allennlp.commands.evaluate</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.make_vocab.html">allennlp.commands.make_vocab</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.predict.html">allennlp.commands.predict</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.train.html">allennlp.commands.train</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.fine_tune.html">allennlp.commands.fine_tune</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.elmo.html">allennlp.commands.elmo</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.dry_run.html">allennlp.commands.dry_run</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.find_learning_rate.html">allennlp.commands.find_learning_rate</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.test_install.html">allennlp.commands.test_install</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.print_results.html">allennlp.commands.print_results</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.common.html">allennlp.common</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.checks.html">allennlp.common.checks</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.configuration.html">allennlp.common.configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.file_utils.html">allennlp.common.file_utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.from_params.html">allennlp.common.from_params</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.params.html">allennlp.common.params</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.registrable.html">allennlp.common.registrable</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.tee_logger.html">allennlp.common.tee_logger</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.testing.html">allennlp.common.testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.tqdm.html">allennlp.common.checks</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.util.html">allennlp.common.util</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="allennlp.data.html">allennlp.data</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.dataset.html">allennlp.data.dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.dataset_readers.html">allennlp.data.dataset_readers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.dataset_reader.html">allennlp.data.dataset_readers.dataset_reader</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.dataset_utils.html">allennlp.data.dataset_readers.dataset_utils</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.babi.html">allennlp.data.dataset_readers.babi</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.ccgbank.html">allennlp.data.dataset_readers.ccgbank</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.conll2000.html">allennlp.data.dataset_readers.conll2000</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.conll2003.html">allennlp.data.dataset_readers.conll2003</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.coreference_resolution.html">allennlp.data.dataset_readers.coreference_resolution</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.event2mind.html">allennlp.data.dataset_readers.event2mind</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.interleaving_dataset_reader.html">allennlp.data.dataset_readers.interleaving_dataset_reader</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.language_modeling.html">allennlp.data.dataset_readers.language_modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.masked_language_modeling.html">allennlp.data.dataset_readers.masked_language_modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.multiprocess_dataset_reader.html">allennlp.data.dataset_readers.multiprocess_dataset_reader</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.next_token_lm.html">allennlp.data.dataset_readers.next_token_lm</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.ontonotes_ner.html">allennlp.data.dataset_readers.ontonotes_ner</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.penn_tree_bank.html">allennlp.data.dataset_readers.penn_tree_bank</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.quora_paraphrase.html">allennlp.data.dataset_readers.quora_paraphrase</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.reading_comprehension.html">allennlp.data.dataset_readers.reading_comprehension</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.semantic_dependency_parsing.html">allennlp.data.dataset_readers.semantic_dependency_parsing</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.semantic_parsing.html">allennlp.data.dataset_readers.semantic_parsing</a><ul>
<li class="toctree-l4"><a class="reference internal" href="allennlp.data.dataset_readers.semantic_parsing.wikitables.html">allennlp.data.dataset_readers.semantic_parsing.wikitables</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.semantic_role_labeling.html">allennlp.data.dataset_readers.semantic_role_labeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.seq2seq.html">allennlp.data.dataset_readers.seq2seq</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.sequence_tagging.html">allennlp.data.dataset_readers.sequence_tagging</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.simple_language_modeling.html">allennlp.data.dataset_readers.simple_language_modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.snli.html">allennlp.data.dataset_readers.snli</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.stanford_sentiment_tree_bank.html">allennlp.data.dataset_readers.stanford_sentiment_tree_bank</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.universal_dependencies.html">allennlp.data.dataset_readers.universal_dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.universal_dependencies_multilang.html">allennlp.data.dataset_readers.universal_dependencies_multilang</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.quora_paraphrase.html">allennlp.data.dataset_readers.quora_paraphrase</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.copynet_seq2seq.html">allennlp.data.dataset_readers.copynet_seq2seq</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.text_classification_json.html">allennlp.data.dataset_readers.text_classification_json</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.fields.html">allennlp.data.fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.instance.html">allennlp.data.instance</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.iterators.html">allennlp.data.iterators</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">allennlp.data.token_indexers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.tokenizers.html">allennlp.data.tokenizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.vocabulary.html">allennlp.data.vocabulary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.interpret.html">allennlp.interpret</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.interpret.attackers.html">allennlp.interpret.attackers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.interpret.saliency_interpreters.html">allennlp.interpret.saliency_interpreters</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.models.html">allennlp.models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.model.html">allennlp.models.model</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.archival.html">allennlp.models.archival</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.basic_classifier.html">allennlp.models.basic_classifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.bert_for_classification.html">allennlp.models.bert_for_classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.biaffine_dependency_parser.html">allennlp.models.biaffine_dependency_parser</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.biaffine_dependency_parser_multilang.html">allennlp.models.biaffine_dependency_parser_multilang</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.biattentive_classification_network.html">allennlp.models.biattentive_classification_network</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.bimpm.html">allennlp.models.bimpm</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.constituency_parser.html">allennlp.models.constituency_parser</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.coreference_resolution.html">allennlp.models.coreference_resolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.crf_tagger.html">allennlp.models.crf_tagger</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.decomposable_attention.html">allennlp.models.decomposable_attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.encoder_decoders.html">allennlp.models.encoder_decoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.ensemble.html">allennlp.models.ensemble</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.esim.html">allennlp.models.esim</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.event2mind.html">allennlp.models.event2mind</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.graph_parser.html">allennlp.models.graph_parser</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.language_model.html">allennlp.models.language_model</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.masked_language_model.html">allennlp.models.masked_language_model</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.next_token_lm.html">allennlp.models.next_token_lm</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.reading_comprehension.html">allennlp.models.reading_comprehension</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.semantic_parsing.html">allennlp.models.semantic_parsing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="allennlp.models.semantic_parsing.nlvr.html">allennlp.models.semantic_parsing.nlvr</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.models.semantic_parsing.wikitables.html">allennlp.models.semantic_parsing.wikitables</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.models.semantic_parsing.atis.html">allennlp.models.semantic_parsing.atis</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.models.semantic_parsing.quarel.html">allennlp.models.semantic_parsing.quarel</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.semantic_role_labeler.html">allennlp.models.semantic_role_labeler</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.simple_tagger.html">allennlp.models.simple_tagger</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.srl_bert.html">allennlp.models.srl_bert</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.srl_util.html">allennlp.models.srl_util</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.predictors.html">allennlp.predictors</a></li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.modules.html">allennlp.modules</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.attention.html">allennlp.modules.attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.matrix_attention.html">allennlp.modules.matrix_attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.augmented_lstm.html">allennlp.modules.augmented_lstm</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.lstm_cell_with_projection.html">allennlp.modules.lstm_cell_with_projection</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.elmo.html">allennlp.modules.elmo</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.elmo_lstm.html">allennlp.modules.elmo_lstm</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.language_model_heads.html">allennlp.modules.language_model_heads</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.conditional_random_field.html">allennlp.modules.conditional_random_field</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.feedforward.html">allennlp.modules.feedforward</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.highway.html">allennlp.modules.highway</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.matrix_attention.html">allennlp.modules.matrix_attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.openai_transformer.html">allennlp.modules.openai_transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.seq2seq_encoders.html">allennlp.modules.seq2seq_encoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.seq2seq_decoders.html">allennlp.modules.seq2seq_decoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.seq2vec_encoders.html">allennlp.modules.seq2vec_encoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.span_extractors.html">allennlp.modules.span_extractors</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.similarity_functions.html">allennlp.modules.similarity_functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.stacked_alternating_lstm.html">allennlp.modules.stacked_alternating_lstm</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.stacked_bidirectional_lstm.html">allennlp.modules.stacked_bidirectional_lstm</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.text_field_embedders.html">allennlp.modules.text_field_embedders</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.time_distributed.html">allennlp.modules.time_distributed</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.token_embedders.html">allennlp.modules.token_embedders</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.scalar_mix.html">allennlp.modules.scalar_mix</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.layer_norm.html">allennlp.modules.layer_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.pruner.html">allennlp.modules.pruner</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.maxout.html">allennlp.modules.maxout</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.input_variational_dropout.html">allennlp.modules.input_variational_dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.bimpm_matching.html">allennlp.modules.bimpm_matching</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.masked_layer_norm.html">allennlp.modules.masked_layer_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.sampled_softmax_loss.html">allennlp.modules.sampled_softmax_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.residual_with_layer_dropout.html">allennlp.modules.residual_with_layer_dropout</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.nn.html">allennlp.nn</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.nn.activations.html">allennlp.nn.activations</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.nn.chu_liu_edmonds.html">allennlp.nn.chu_liu_edmonds</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.nn.initializers.html">allennlp.nn.initializers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.nn.regularizers.html">allennlp.nn.regularizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.nn.util.html">allennlp.nn.util</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.nn.beam_search.html">allennlp.nn.beam_search</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.semparse.html">allennlp.semparse</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.semparse.common.html">allennlp.semparse.common</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.semparse.contexts.html">allennlp.semparse.contexts</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.semparse.executors.html">allennlp.semparse.executors</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.semparse.type_declarations.html">allennlp.semparse.type_declarations</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.semparse.worlds.html">allennlp.semparse.worlds</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.semparse.executors.html">allennlp.semparse.executors</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.semparse.domain_languages.html">allennlp.semparse.domain_languages</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.semparse.util.html">allennlp.semparse.util</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.service.html">allennlp.service</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.service.server_simple.html">allennlp.service.server_simple</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.service.config_explorer.html">allennlp.service.config_explorer</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.state_machines.html">allennlp.state_machines</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.state_machines.states.html">allennlp.state_machines.states</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.state_machines.trainers.html">allennlp.state_machines.trainers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.state_machines.transition_functions.html">allennlp.state_machines.transition_functions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.tools.html">allennlp.tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.training.html">allennlp.training</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.callbacks.html">allennlp.training.callbacks</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.callback_trainer.html">allennlp.training.callback_trainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.checkpointer.html">allennlp.training.checkpointer</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.scheduler.html">allennlp.training.scheduler</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.learning_rate_schedulers.html">allennlp.training.learning_rate_schedulers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.momentum_schedulers.html">allennlp.training.momentum_schedulers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.metric_tracker.html">allennlp.training.metric_tracker</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.metrics.html">allennlp.training.metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.moving_average.html">allennlp.training.moving_average</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.no_op_trainer.html">allennlp.training.no_op_trainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.optimizers.html">allennlp.training.optimizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.tensorboard_writer.html">allennlp.training.tensorboard_writer</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.trainer.html">allennlp.training.trainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.trainer_base.html">allennlp.training.trainer_base</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.trainer_pieces.html">allennlp.training.trainer_pieces</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.util.html">allennlp.training.util</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.pretrained.html">allennlp.pretrained</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">AllenNLP</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="allennlp.data.html">allennlp.data</a> &raquo;</li>
        
      <li>allennlp.data.token_indexers</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/api/allennlp.data.token_indexers.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-allennlp.data.token_indexers">
<span id="allennlp-data-token-indexers"></span><h1>allennlp.data.token_indexers<a class="headerlink" href="#module-allennlp.data.token_indexers" title="Permalink to this headline">¶</a></h1>
<p>A <code class="docutils literal notranslate"><span class="pre">TokenIndexer</span></code> determines how string tokens get represented as arrays of indices in a model.</p>
<ul class="simple">
<li><p><a class="reference internal" href="#token-indexer"><span class="std std-ref">TokenIndexer</span></a></p></li>
<li><p><a class="reference internal" href="#dep-label-indexer"><span class="std std-ref">DepLabelIndexer</span></a></p></li>
<li><p><a class="reference internal" href="#ner-tag-indexer"><span class="std std-ref">NerTagIndexer</span></a></p></li>
<li><p><a class="reference internal" href="#pos-tag-indexer"><span class="std std-ref">PosTagIndexer</span></a></p></li>
<li><p><a class="reference internal" href="#single-id-token-indexer"><span class="std std-ref">SingleIdTokenIndexer</span></a></p></li>
<li><p><a class="reference internal" href="#token-characters-indexer"><span class="std std-ref">TokenCharactersIndexer</span></a></p></li>
<li><p><a class="reference internal" href="#elmo-indexer"><span class="std std-ref">ELMoTokenCharactersIndexer</span></a></p></li>
<li><p><a class="reference internal" href="#openai-transformer-byte-pair-indexer"><span class="std std-ref">OpenaiTransformerBytePairIndexer</span></a></p></li>
<li><p><a class="reference internal" href="#wordpiece-indexer"><span class="std std-ref">WordpieceIndexer</span></a></p></li>
<li><p><a class="reference internal" href="#pretrained-transformer-indexer"><span class="std std-ref">PretrainedTransformerIndexer</span></a></p></li>
<li><p><a class="reference internal" href="#spacy-token-indexer"><span class="std std-ref">SpacyTokenIndexer</span></a></p></li>
</ul>
<span class="target" id="module-allennlp.data.token_indexers.token_indexer"><span id="token-indexer"></span></span><dl class="class">
<dt id="allennlp.data.token_indexers.token_indexer.TokenIndexer">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.token_indexers.token_indexer.</code><code class="sig-name descname">TokenIndexer</code><span class="sig-paren">(</span><em class="sig-param">token_min_padding_length: int = 0</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/token_indexer.py#L14-L139"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.token_indexer.TokenIndexer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">typing.Generic</span></code>, <a class="reference internal" href="allennlp.common.registrable.html#allennlp.common.registrable.Registrable" title="allennlp.common.registrable.Registrable"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.common.registrable.Registrable</span></code></a></p>
<p>A <code class="docutils literal notranslate"><span class="pre">TokenIndexer</span></code> determines how string tokens get represented as arrays of indices in a model.
This class both converts strings into numerical values, with the help of a
<a class="reference internal" href="allennlp.data.vocabulary.html#allennlp.data.vocabulary.Vocabulary" title="allennlp.data.vocabulary.Vocabulary"><code class="xref py py-class docutils literal notranslate"><span class="pre">Vocabulary</span></code></a>, and it produces actual arrays.</p>
<p>Tokens can be represented as single IDs (e.g., the word “cat” gets represented by the number
34), or as lists of character IDs (e.g., “cat” gets represented by the numbers [23, 10, 18]),
or in some other way that you can come up with (e.g., if you have some structured input you
want to represent in a special way in your data arrays, you can do that here).</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>token_min_padding_length</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code>, optional (default=``0``)</span></dt><dd><p>The minimum padding length required for the <a class="reference internal" href="#allennlp.data.token_indexers.token_indexer.TokenIndexer" title="allennlp.data.token_indexers.token_indexer.TokenIndexer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TokenIndexer</span></code></a>. For example,
the minimum padding length of <code class="xref py py-class docutils literal notranslate"><span class="pre">SingleIdTokenIndexer</span></code> is the largest size of
filter when using <code class="xref py py-class docutils literal notranslate"><span class="pre">CnnEncoder</span></code>.
Note that if you set this for one TokenIndexer, you likely have to set it for all
<a class="reference internal" href="#allennlp.data.token_indexers.token_indexer.TokenIndexer" title="allennlp.data.token_indexers.token_indexer.TokenIndexer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TokenIndexer</span></code></a> for the same field, otherwise you’ll get mismatched tensor sizes.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="allennlp.data.token_indexers.token_indexer.TokenIndexer.as_padded_tensor">
<code class="sig-name descname">as_padded_tensor</code><span class="sig-paren">(</span><em class="sig-param">self, tokens: Dict[str, List[~TokenType]], desired_num_tokens: Dict[str, int], padding_lengths: Dict[str, int]</em><span class="sig-paren">)</span> &#x2192; Dict[str, torch.Tensor]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/token_indexer.py#L96-L117"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.token_indexer.TokenIndexer.as_padded_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>This method pads a list of tokens to <code class="docutils literal notranslate"><span class="pre">desired_num_tokens</span></code> and returns that padded list
of input tokens as a torch Tensor. If the input token list is longer than <code class="docutils literal notranslate"><span class="pre">desired_num_tokens</span></code>
then it will be truncated.</p>
<p><code class="docutils literal notranslate"><span class="pre">padding_lengths</span></code> is used to provide supplemental padding parameters which are needed
in some cases.  For example, it contains the widths to pad characters to when doing
character-level padding.</p>
<p>Note that this method should be abstract, but it is implemented to allow backward compatability.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.token_indexer.TokenIndexer.count_vocab_items">
<code class="sig-name descname">count_vocab_items</code><span class="sig-paren">(</span><em class="sig-param">self, token: allennlp.data.tokenizers.token.Token, counter: Dict[str, Dict[str, int]]</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/token_indexer.py#L41-L50"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.token_indexer.TokenIndexer.count_vocab_items" title="Permalink to this definition">¶</a></dt>
<dd><p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">Vocabulary</span></code> needs to assign indices to whatever strings we see in the training
data (possibly doing some frequency filtering and using an OOV, or out of vocabulary,
token).  This method takes a token and a dictionary of counts and increments counts for
whatever vocabulary items are present in the token.  If this is a single token ID
representation, the vocabulary item is likely the token itself.  If this is a token
characters representation, the vocabulary items are all of the characters in the token.</p>
</dd></dl>

<dl class="attribute">
<dt id="allennlp.data.token_indexers.token_indexer.TokenIndexer.default_implementation">
<code class="sig-name descname">default_implementation</code><em class="property">: str</em><em class="property"> = 'single_id'</em><a class="headerlink" href="#allennlp.data.token_indexers.token_indexer.TokenIndexer.default_implementation" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.token_indexer.TokenIndexer.get_keys">
<code class="sig-name descname">get_keys</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">index_name: str</em><span class="sig-paren">)</span> &#x2192; List[str]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/token_indexer.py#L129-L134"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.token_indexer.TokenIndexer.get_keys" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a list of the keys this indexer return from <code class="docutils literal notranslate"><span class="pre">tokens_to_indices</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.token_indexer.TokenIndexer.get_padding_lengths">
<code class="sig-name descname">get_padding_lengths</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">token: ~TokenType</em><span class="sig-paren">)</span> &#x2192; Dict[str, int]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/token_indexer.py#L79-L86"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.token_indexer.TokenIndexer.get_padding_lengths" title="Permalink to this definition">¶</a></dt>
<dd><p>This method returns a padding dictionary for the given token that specifies lengths for
all arrays that need padding.  For example, for single ID tokens the returned dictionary
will be empty, but for a token characters representation, this will return the number
of characters in the token.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.token_indexer.TokenIndexer.get_padding_token">
<code class="sig-name descname">get_padding_token</code><span class="sig-paren">(</span><em class="sig-param">self</em><span class="sig-paren">)</span> &#x2192; ~TokenType<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/token_indexer.py#L65-L77"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.token_indexer.TokenIndexer.get_padding_token" title="Permalink to this definition">¶</a></dt>
<dd><p>Deprecated. Please just implement the padding token in <cite>as_padded_tensor</cite> instead.
TODO(Mark): remove in 1.0 release. This is only a concrete implementation to preserve
backward compatability, otherwise it would be abstract.</p>
<p>When we need to add padding tokens, what should they look like?  This method returns a
“blank” token of whatever type is returned by <a class="reference internal" href="#allennlp.data.token_indexers.token_indexer.TokenIndexer.tokens_to_indices" title="allennlp.data.token_indexers.token_indexer.TokenIndexer.tokens_to_indices"><code class="xref py py-func docutils literal notranslate"><span class="pre">tokens_to_indices()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.token_indexer.TokenIndexer.get_token_min_padding_length">
<code class="sig-name descname">get_token_min_padding_length</code><span class="sig-paren">(</span><em class="sig-param">self</em><span class="sig-paren">)</span> &#x2192; int<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/token_indexer.py#L88-L94"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.token_indexer.TokenIndexer.get_token_min_padding_length" title="Permalink to this definition">¶</a></dt>
<dd><p>This method returns the minimum padding length required for this TokenIndexer.
For example, the minimum padding length of <cite>SingleIdTokenIndexer</cite> is the largest
size of filter when using <cite>CnnEncoder</cite>.</p>
</dd></dl>

<dl class="attribute">
<dt id="allennlp.data.token_indexers.token_indexer.TokenIndexer.has_warned_for_as_padded_tensor">
<code class="sig-name descname">has_warned_for_as_padded_tensor</code><em class="property"> = False</em><a class="headerlink" href="#allennlp.data.token_indexers.token_indexer.TokenIndexer.has_warned_for_as_padded_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.token_indexer.TokenIndexer.pad_token_sequence">
<code class="sig-name descname">pad_token_sequence</code><span class="sig-paren">(</span><em class="sig-param">self, tokens: Dict[str, List[~TokenType]], desired_num_tokens: Dict[str, int], padding_lengths: Dict[str, int]</em><span class="sig-paren">)</span> &#x2192; Dict[str, ~TokenType]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/token_indexer.py#L119-L127"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.token_indexer.TokenIndexer.pad_token_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Deprecated. Please use <cite>as_padded_tensor</cite> instead.
TODO(Mark): remove in 1.0 release.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.token_indexer.TokenIndexer.tokens_to_indices">
<code class="sig-name descname">tokens_to_indices</code><span class="sig-paren">(</span><em class="sig-param">self, tokens: List[allennlp.data.tokenizers.token.Token], vocabulary: allennlp.data.vocabulary.Vocabulary, index_name: str</em><span class="sig-paren">)</span> &#x2192; Dict[str, List[~TokenType]]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/token_indexer.py#L52-L63"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.token_indexer.TokenIndexer.tokens_to_indices" title="Permalink to this definition">¶</a></dt>
<dd><p>Takes a list of tokens and converts them to one or more sets of indices.
This could be just an ID for each token from the vocabulary.
Or it could split each token into characters and return one ID per character.
Or (for instance, in the case of byte-pair encoding) there might not be a clean
mapping from individual tokens to indices.</p>
</dd></dl>

</dd></dl>

<span class="target" id="module-allennlp.data.token_indexers.dep_label_indexer"><span id="dep-label-indexer"></span></span><dl class="class">
<dt id="allennlp.data.token_indexers.dep_label_indexer.DepLabelIndexer">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.token_indexers.dep_label_indexer.</code><code class="sig-name descname">DepLabelIndexer</code><span class="sig-paren">(</span><em class="sig-param">namespace: str = 'dep_labels'</em>, <em class="sig-param">token_min_padding_length: int = 0</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/dep_label_indexer.py#L16-L64"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.dep_label_indexer.DepLabelIndexer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.token_indexers.token_indexer.TokenIndexer" title="allennlp.data.token_indexers.token_indexer.TokenIndexer"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.data.token_indexers.token_indexer.TokenIndexer</span></code></a></p>
<p>This <code class="xref py py-class docutils literal notranslate"><span class="pre">TokenIndexer</span></code> represents tokens by their syntactic dependency label, as determined
by the <code class="docutils literal notranslate"><span class="pre">dep_</span></code> field on <code class="docutils literal notranslate"><span class="pre">Token</span></code>.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>namespace</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">str</span></code>, optional (default=``dep_labels``)</span></dt><dd><p>We will use this namespace in the <code class="xref py py-class docutils literal notranslate"><span class="pre">Vocabulary</span></code> to map strings to indices.</p>
</dd>
<dt><strong>token_min_padding_length</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code>, optional (default=``0``)</span></dt><dd><p>See <code class="xref py py-class docutils literal notranslate"><span class="pre">TokenIndexer</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="allennlp.data.token_indexers.dep_label_indexer.DepLabelIndexer.as_padded_tensor">
<code class="sig-name descname">as_padded_tensor</code><span class="sig-paren">(</span><em class="sig-param">self, tokens: Dict[str, List[int]], desired_num_tokens: Dict[str, int], padding_lengths: Dict[str, int]</em><span class="sig-paren">)</span> &#x2192; Dict[str, torch.Tensor]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/dep_label_indexer.py#L58-L64"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.dep_label_indexer.DepLabelIndexer.as_padded_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>This method pads a list of tokens to <code class="docutils literal notranslate"><span class="pre">desired_num_tokens</span></code> and returns that padded list
of input tokens as a torch Tensor. If the input token list is longer than <code class="docutils literal notranslate"><span class="pre">desired_num_tokens</span></code>
then it will be truncated.</p>
<p><code class="docutils literal notranslate"><span class="pre">padding_lengths</span></code> is used to provide supplemental padding parameters which are needed
in some cases.  For example, it contains the widths to pad characters to when doing
character-level padding.</p>
<p>Note that this method should be abstract, but it is implemented to allow backward compatability.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.dep_label_indexer.DepLabelIndexer.count_vocab_items">
<code class="sig-name descname">count_vocab_items</code><span class="sig-paren">(</span><em class="sig-param">self, token: allennlp.data.tokenizers.token.Token, counter: Dict[str, Dict[str, int]]</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/dep_label_indexer.py#L35-L43"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.dep_label_indexer.DepLabelIndexer.count_vocab_items" title="Permalink to this definition">¶</a></dt>
<dd><p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">Vocabulary</span></code> needs to assign indices to whatever strings we see in the training
data (possibly doing some frequency filtering and using an OOV, or out of vocabulary,
token).  This method takes a token and a dictionary of counts and increments counts for
whatever vocabulary items are present in the token.  If this is a single token ID
representation, the vocabulary item is likely the token itself.  If this is a token
characters representation, the vocabulary items are all of the characters in the token.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.dep_label_indexer.DepLabelIndexer.get_padding_lengths">
<code class="sig-name descname">get_padding_lengths</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">token: int</em><span class="sig-paren">)</span> &#x2192; Dict[str, int]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/dep_label_indexer.py#L54-L56"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.dep_label_indexer.DepLabelIndexer.get_padding_lengths" title="Permalink to this definition">¶</a></dt>
<dd><p>This method returns a padding dictionary for the given token that specifies lengths for
all arrays that need padding.  For example, for single ID tokens the returned dictionary
will be empty, but for a token characters representation, this will return the number
of characters in the token.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.dep_label_indexer.DepLabelIndexer.tokens_to_indices">
<code class="sig-name descname">tokens_to_indices</code><span class="sig-paren">(</span><em class="sig-param">self, tokens: List[allennlp.data.tokenizers.token.Token], vocabulary: allennlp.data.vocabulary.Vocabulary, index_name: str</em><span class="sig-paren">)</span> &#x2192; Dict[str, List[int]]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/dep_label_indexer.py#L45-L52"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.dep_label_indexer.DepLabelIndexer.tokens_to_indices" title="Permalink to this definition">¶</a></dt>
<dd><p>Takes a list of tokens and converts them to one or more sets of indices.
This could be just an ID for each token from the vocabulary.
Or it could split each token into characters and return one ID per character.
Or (for instance, in the case of byte-pair encoding) there might not be a clean
mapping from individual tokens to indices.</p>
</dd></dl>

</dd></dl>

<span class="target" id="module-allennlp.data.token_indexers.ner_tag_indexer"><span id="ner-tag-indexer"></span></span><dl class="class">
<dt id="allennlp.data.token_indexers.ner_tag_indexer.NerTagIndexer">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.token_indexers.ner_tag_indexer.</code><code class="sig-name descname">NerTagIndexer</code><span class="sig-paren">(</span><em class="sig-param">namespace: str = 'ner_tokens'</em>, <em class="sig-param">token_min_padding_length: int = 0</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/ner_tag_indexer.py#L16-L60"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.ner_tag_indexer.NerTagIndexer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.token_indexers.token_indexer.TokenIndexer" title="allennlp.data.token_indexers.token_indexer.TokenIndexer"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.data.token_indexers.token_indexer.TokenIndexer</span></code></a></p>
<p>This <code class="xref py py-class docutils literal notranslate"><span class="pre">TokenIndexer</span></code> represents tokens by their entity type (i.e., their NER tag), as
determined by the <code class="docutils literal notranslate"><span class="pre">ent_type_</span></code> field on <code class="docutils literal notranslate"><span class="pre">Token</span></code>.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>namespace</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">str</span></code>, optional (default=``ner_tokens``)</span></dt><dd><p>We will use this namespace in the <code class="xref py py-class docutils literal notranslate"><span class="pre">Vocabulary</span></code> to map strings to indices.</p>
</dd>
<dt><strong>token_min_padding_length</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code>, optional (default=``0``)</span></dt><dd><p>See <code class="xref py py-class docutils literal notranslate"><span class="pre">TokenIndexer</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="allennlp.data.token_indexers.ner_tag_indexer.NerTagIndexer.as_padded_tensor">
<code class="sig-name descname">as_padded_tensor</code><span class="sig-paren">(</span><em class="sig-param">self, tokens: Dict[str, List[int]], desired_num_tokens: Dict[str, int], padding_lengths: Dict[str, int]</em><span class="sig-paren">)</span> &#x2192; Dict[str, torch.Tensor]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/ner_tag_indexer.py#L54-L60"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.ner_tag_indexer.NerTagIndexer.as_padded_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>This method pads a list of tokens to <code class="docutils literal notranslate"><span class="pre">desired_num_tokens</span></code> and returns that padded list
of input tokens as a torch Tensor. If the input token list is longer than <code class="docutils literal notranslate"><span class="pre">desired_num_tokens</span></code>
then it will be truncated.</p>
<p><code class="docutils literal notranslate"><span class="pre">padding_lengths</span></code> is used to provide supplemental padding parameters which are needed
in some cases.  For example, it contains the widths to pad characters to when doing
character-level padding.</p>
<p>Note that this method should be abstract, but it is implemented to allow backward compatability.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.ner_tag_indexer.NerTagIndexer.count_vocab_items">
<code class="sig-name descname">count_vocab_items</code><span class="sig-paren">(</span><em class="sig-param">self, token: allennlp.data.tokenizers.token.Token, counter: Dict[str, Dict[str, int]]</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/ner_tag_indexer.py#L34-L39"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.ner_tag_indexer.NerTagIndexer.count_vocab_items" title="Permalink to this definition">¶</a></dt>
<dd><p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">Vocabulary</span></code> needs to assign indices to whatever strings we see in the training
data (possibly doing some frequency filtering and using an OOV, or out of vocabulary,
token).  This method takes a token and a dictionary of counts and increments counts for
whatever vocabulary items are present in the token.  If this is a single token ID
representation, the vocabulary item is likely the token itself.  If this is a token
characters representation, the vocabulary items are all of the characters in the token.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.ner_tag_indexer.NerTagIndexer.get_padding_lengths">
<code class="sig-name descname">get_padding_lengths</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">token: int</em><span class="sig-paren">)</span> &#x2192; Dict[str, int]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/ner_tag_indexer.py#L50-L52"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.ner_tag_indexer.NerTagIndexer.get_padding_lengths" title="Permalink to this definition">¶</a></dt>
<dd><p>This method returns a padding dictionary for the given token that specifies lengths for
all arrays that need padding.  For example, for single ID tokens the returned dictionary
will be empty, but for a token characters representation, this will return the number
of characters in the token.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.ner_tag_indexer.NerTagIndexer.tokens_to_indices">
<code class="sig-name descname">tokens_to_indices</code><span class="sig-paren">(</span><em class="sig-param">self, tokens: List[allennlp.data.tokenizers.token.Token], vocabulary: allennlp.data.vocabulary.Vocabulary, index_name: str</em><span class="sig-paren">)</span> &#x2192; Dict[str, List[int]]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/ner_tag_indexer.py#L41-L48"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.ner_tag_indexer.NerTagIndexer.tokens_to_indices" title="Permalink to this definition">¶</a></dt>
<dd><p>Takes a list of tokens and converts them to one or more sets of indices.
This could be just an ID for each token from the vocabulary.
Or it could split each token into characters and return one ID per character.
Or (for instance, in the case of byte-pair encoding) there might not be a clean
mapping from individual tokens to indices.</p>
</dd></dl>

</dd></dl>

<span class="target" id="module-allennlp.data.token_indexers.pos_tag_indexer"><span id="pos-tag-indexer"></span></span><dl class="class">
<dt id="allennlp.data.token_indexers.pos_tag_indexer.PosTagIndexer">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.token_indexers.pos_tag_indexer.</code><code class="sig-name descname">PosTagIndexer</code><span class="sig-paren">(</span><em class="sig-param">namespace: str = 'pos_tokens'</em>, <em class="sig-param">coarse_tags: bool = False</em>, <em class="sig-param">token_min_padding_length: int = 0</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/pos_tag_indexer.py#L16-L82"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.pos_tag_indexer.PosTagIndexer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.token_indexers.token_indexer.TokenIndexer" title="allennlp.data.token_indexers.token_indexer.TokenIndexer"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.data.token_indexers.token_indexer.TokenIndexer</span></code></a></p>
<p>This <code class="xref py py-class docutils literal notranslate"><span class="pre">TokenIndexer</span></code> represents tokens by their part of speech tag, as determined by
the <code class="docutils literal notranslate"><span class="pre">pos_</span></code> or <code class="docutils literal notranslate"><span class="pre">tag_</span></code> fields on <code class="docutils literal notranslate"><span class="pre">Token</span></code> (corresponding to spacy’s coarse-grained and
fine-grained POS tags, respectively).</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>namespace</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">str</span></code>, optional (default=``pos_tokens``)</span></dt><dd><p>We will use this namespace in the <code class="xref py py-class docutils literal notranslate"><span class="pre">Vocabulary</span></code> to map strings to indices.</p>
</dd>
<dt><strong>coarse_tags</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">bool</span></code>, optional (default=``False``)</span></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, we will use coarse POS tags instead of the default fine-grained POS tags.</p>
</dd>
<dt><strong>token_min_padding_length</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code>, optional (default=``0``)</span></dt><dd><p>See <code class="xref py py-class docutils literal notranslate"><span class="pre">TokenIndexer</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="allennlp.data.token_indexers.pos_tag_indexer.PosTagIndexer.as_padded_tensor">
<code class="sig-name descname">as_padded_tensor</code><span class="sig-paren">(</span><em class="sig-param">self, tokens: Dict[str, List[int]], desired_num_tokens: Dict[str, int], padding_lengths: Dict[str, int]</em><span class="sig-paren">)</span> &#x2192; Dict[str, torch.Tensor]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/pos_tag_indexer.py#L76-L82"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.pos_tag_indexer.PosTagIndexer.as_padded_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>This method pads a list of tokens to <code class="docutils literal notranslate"><span class="pre">desired_num_tokens</span></code> and returns that padded list
of input tokens as a torch Tensor. If the input token list is longer than <code class="docutils literal notranslate"><span class="pre">desired_num_tokens</span></code>
then it will be truncated.</p>
<p><code class="docutils literal notranslate"><span class="pre">padding_lengths</span></code> is used to provide supplemental padding parameters which are needed
in some cases.  For example, it contains the widths to pad characters to when doing
character-level padding.</p>
<p>Note that this method should be abstract, but it is implemented to allow backward compatability.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.pos_tag_indexer.PosTagIndexer.count_vocab_items">
<code class="sig-name descname">count_vocab_items</code><span class="sig-paren">(</span><em class="sig-param">self, token: allennlp.data.tokenizers.token.Token, counter: Dict[str, Dict[str, int]]</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/pos_tag_indexer.py#L40-L51"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.pos_tag_indexer.PosTagIndexer.count_vocab_items" title="Permalink to this definition">¶</a></dt>
<dd><p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">Vocabulary</span></code> needs to assign indices to whatever strings we see in the training
data (possibly doing some frequency filtering and using an OOV, or out of vocabulary,
token).  This method takes a token and a dictionary of counts and increments counts for
whatever vocabulary items are present in the token.  If this is a single token ID
representation, the vocabulary item is likely the token itself.  If this is a token
characters representation, the vocabulary items are all of the characters in the token.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.pos_tag_indexer.PosTagIndexer.get_padding_lengths">
<code class="sig-name descname">get_padding_lengths</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">token: int</em><span class="sig-paren">)</span> &#x2192; Dict[str, int]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/pos_tag_indexer.py#L72-L74"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.pos_tag_indexer.PosTagIndexer.get_padding_lengths" title="Permalink to this definition">¶</a></dt>
<dd><p>This method returns a padding dictionary for the given token that specifies lengths for
all arrays that need padding.  For example, for single ID tokens the returned dictionary
will be empty, but for a token characters representation, this will return the number
of characters in the token.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.pos_tag_indexer.PosTagIndexer.tokens_to_indices">
<code class="sig-name descname">tokens_to_indices</code><span class="sig-paren">(</span><em class="sig-param">self, tokens: List[allennlp.data.tokenizers.token.Token], vocabulary: allennlp.data.vocabulary.Vocabulary, index_name: str</em><span class="sig-paren">)</span> &#x2192; Dict[str, List[int]]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/pos_tag_indexer.py#L53-L70"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.pos_tag_indexer.PosTagIndexer.tokens_to_indices" title="Permalink to this definition">¶</a></dt>
<dd><p>Takes a list of tokens and converts them to one or more sets of indices.
This could be just an ID for each token from the vocabulary.
Or it could split each token into characters and return one ID per character.
Or (for instance, in the case of byte-pair encoding) there might not be a clean
mapping from individual tokens to indices.</p>
</dd></dl>

</dd></dl>

<span class="target" id="module-allennlp.data.token_indexers.single_id_token_indexer"><span id="single-id-token-indexer"></span></span><dl class="class">
<dt id="allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.token_indexers.single_id_token_indexer.</code><code class="sig-name descname">SingleIdTokenIndexer</code><span class="sig-paren">(</span><em class="sig-param">namespace: str = 'tokens'</em>, <em class="sig-param">lowercase_tokens: bool = False</em>, <em class="sig-param">start_tokens: List[str] = None</em>, <em class="sig-param">end_tokens: List[str] = None</em>, <em class="sig-param">token_min_padding_length: int = 0</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/single_id_token_indexer.py#L14-L86"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.token_indexers.token_indexer.TokenIndexer" title="allennlp.data.token_indexers.token_indexer.TokenIndexer"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.data.token_indexers.token_indexer.TokenIndexer</span></code></a></p>
<p>This <code class="xref py py-class docutils literal notranslate"><span class="pre">TokenIndexer</span></code> represents tokens as single integers.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>namespace</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">str</span></code>, optional (default=``tokens``)</span></dt><dd><p>We will use this namespace in the <code class="xref py py-class docutils literal notranslate"><span class="pre">Vocabulary</span></code> to map strings to indices.</p>
</dd>
<dt><strong>lowercase_tokens</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">bool</span></code>, optional (default=``False``)</span></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, we will call <code class="docutils literal notranslate"><span class="pre">token.lower()</span></code> before getting an index for the token from the
vocabulary.</p>
</dd>
<dt><strong>start_tokens</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">List[str]</span></code>, optional (default=``None``)</span></dt><dd><p>These are prepended to the tokens provided to <code class="docutils literal notranslate"><span class="pre">tokens_to_indices</span></code>.</p>
</dd>
<dt><strong>end_tokens</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">List[str]</span></code>, optional (default=``None``)</span></dt><dd><p>These are appended to the tokens provided to <code class="docutils literal notranslate"><span class="pre">tokens_to_indices</span></code>.</p>
</dd>
<dt><strong>token_min_padding_length</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code>, optional (default=``0``)</span></dt><dd><p>See <code class="xref py py-class docutils literal notranslate"><span class="pre">TokenIndexer</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer.as_padded_tensor">
<code class="sig-name descname">as_padded_tensor</code><span class="sig-paren">(</span><em class="sig-param">self, tokens: Dict[str, List[int]], desired_num_tokens: Dict[str, int], padding_lengths: Dict[str, int]</em><span class="sig-paren">)</span> &#x2192; Dict[str, torch.Tensor]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/single_id_token_indexer.py#L80-L86"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer.as_padded_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>This method pads a list of tokens to <code class="docutils literal notranslate"><span class="pre">desired_num_tokens</span></code> and returns that padded list
of input tokens as a torch Tensor. If the input token list is longer than <code class="docutils literal notranslate"><span class="pre">desired_num_tokens</span></code>
then it will be truncated.</p>
<p><code class="docutils literal notranslate"><span class="pre">padding_lengths</span></code> is used to provide supplemental padding parameters which are needed
in some cases.  For example, it contains the widths to pad characters to when doing
character-level padding.</p>
<p>Note that this method should be abstract, but it is implemented to allow backward compatability.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer.count_vocab_items">
<code class="sig-name descname">count_vocab_items</code><span class="sig-paren">(</span><em class="sig-param">self, token: allennlp.data.tokenizers.token.Token, counter: Dict[str, Dict[str, int]]</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/single_id_token_indexer.py#L46-L54"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer.count_vocab_items" title="Permalink to this definition">¶</a></dt>
<dd><p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">Vocabulary</span></code> needs to assign indices to whatever strings we see in the training
data (possibly doing some frequency filtering and using an OOV, or out of vocabulary,
token).  This method takes a token and a dictionary of counts and increments counts for
whatever vocabulary items are present in the token.  If this is a single token ID
representation, the vocabulary item is likely the token itself.  If this is a token
characters representation, the vocabulary items are all of the characters in the token.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer.get_padding_lengths">
<code class="sig-name descname">get_padding_lengths</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">token: int</em><span class="sig-paren">)</span> &#x2192; Dict[str, int]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/single_id_token_indexer.py#L76-L78"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer.get_padding_lengths" title="Permalink to this definition">¶</a></dt>
<dd><p>This method returns a padding dictionary for the given token that specifies lengths for
all arrays that need padding.  For example, for single ID tokens the returned dictionary
will be empty, but for a token characters representation, this will return the number
of characters in the token.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer.tokens_to_indices">
<code class="sig-name descname">tokens_to_indices</code><span class="sig-paren">(</span><em class="sig-param">self, tokens: List[allennlp.data.tokenizers.token.Token], vocabulary: allennlp.data.vocabulary.Vocabulary, index_name: str</em><span class="sig-paren">)</span> &#x2192; Dict[str, List[int]]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/single_id_token_indexer.py#L56-L74"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer.tokens_to_indices" title="Permalink to this definition">¶</a></dt>
<dd><p>Takes a list of tokens and converts them to one or more sets of indices.
This could be just an ID for each token from the vocabulary.
Or it could split each token into characters and return one ID per character.
Or (for instance, in the case of byte-pair encoding) there might not be a clean
mapping from individual tokens to indices.</p>
</dd></dl>

</dd></dl>

<span class="target" id="module-allennlp.data.token_indexers.token_characters_indexer"><span id="token-characters-indexer"></span></span><dl class="class">
<dt id="allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.token_indexers.token_characters_indexer.</code><code class="sig-name descname">TokenCharactersIndexer</code><span class="sig-paren">(</span><em class="sig-param">namespace: str = 'token_characters'</em>, <em class="sig-param">character_tokenizer: allennlp.data.tokenizers.character_tokenizer.CharacterTokenizer = &lt;allennlp.data.tokenizers.character_tokenizer.CharacterTokenizer object&gt;</em>, <em class="sig-param">start_tokens: List[str] = None</em>, <em class="sig-param">end_tokens: List[str] = None</em>, <em class="sig-param">min_padding_length: int = 0</em>, <em class="sig-param">token_min_padding_length: int = 0</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/token_characters_indexer.py#L17-L132"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.token_indexers.token_indexer.TokenIndexer" title="allennlp.data.token_indexers.token_indexer.TokenIndexer"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.data.token_indexers.token_indexer.TokenIndexer</span></code></a></p>
<p>This <code class="xref py py-class docutils literal notranslate"><span class="pre">TokenIndexer</span></code> represents tokens as lists of character indices.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>namespace</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">str</span></code>, optional (default=``token_characters``)</span></dt><dd><p>We will use this namespace in the <code class="xref py py-class docutils literal notranslate"><span class="pre">Vocabulary</span></code> to map the characters in each token
to indices.</p>
</dd>
<dt><strong>character_tokenizer</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">CharacterTokenizer</span></code>, optional (default=``CharacterTokenizer()``)</span></dt><dd><p>We use a <code class="xref py py-class docutils literal notranslate"><span class="pre">CharacterTokenizer</span></code> to handle splitting tokens into characters, as it has
options for byte encoding and other things.  The default here is to instantiate a
<code class="docutils literal notranslate"><span class="pre">CharacterTokenizer</span></code> with its default parameters, which uses unicode characters and
retains casing.</p>
</dd>
<dt><strong>start_tokens</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">List[str]</span></code>, optional (default=``None``)</span></dt><dd><p>These are prepended to the tokens provided to <code class="docutils literal notranslate"><span class="pre">tokens_to_indices</span></code>.</p>
</dd>
<dt><strong>end_tokens</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">List[str]</span></code>, optional (default=``None``)</span></dt><dd><p>These are appended to the tokens provided to <code class="docutils literal notranslate"><span class="pre">tokens_to_indices</span></code>.</p>
</dd>
<dt><strong>min_padding_length: ``int``, optional (default=``0``)</strong></dt><dd><p>We use this value as the minimum length of padding. Usually used with :class:<code class="docutils literal notranslate"><span class="pre">CnnEncoder</span></code>, its
value should be set to the maximum value of <code class="docutils literal notranslate"><span class="pre">ngram_filter_sizes</span></code> correspondingly.</p>
</dd>
<dt><strong>token_min_padding_length</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code>, optional (default=``0``)</span></dt><dd><p>See <code class="xref py py-class docutils literal notranslate"><span class="pre">TokenIndexer</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer.as_padded_tensor">
<code class="sig-name descname">as_padded_tensor</code><span class="sig-paren">(</span><em class="sig-param">self, tokens: Dict[str, List[List[int]]], desired_num_tokens: Dict[str, int], padding_lengths: Dict[str, int]</em><span class="sig-paren">)</span> &#x2192; Dict[str, torch.Tensor]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/token_characters_indexer.py#L103-L132"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer.as_padded_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>This method pads a list of tokens to <code class="docutils literal notranslate"><span class="pre">desired_num_tokens</span></code> and returns that padded list
of input tokens as a torch Tensor. If the input token list is longer than <code class="docutils literal notranslate"><span class="pre">desired_num_tokens</span></code>
then it will be truncated.</p>
<p><code class="docutils literal notranslate"><span class="pre">padding_lengths</span></code> is used to provide supplemental padding parameters which are needed
in some cases.  For example, it contains the widths to pad characters to when doing
character-level padding.</p>
<p>Note that this method should be abstract, but it is implemented to allow backward compatability.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer.count_vocab_items">
<code class="sig-name descname">count_vocab_items</code><span class="sig-paren">(</span><em class="sig-param">self, token: allennlp.data.tokenizers.token.Token, counter: Dict[str, Dict[str, int]]</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/token_characters_indexer.py#L64-L72"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer.count_vocab_items" title="Permalink to this definition">¶</a></dt>
<dd><p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">Vocabulary</span></code> needs to assign indices to whatever strings we see in the training
data (possibly doing some frequency filtering and using an OOV, or out of vocabulary,
token).  This method takes a token and a dictionary of counts and increments counts for
whatever vocabulary items are present in the token.  If this is a single token ID
representation, the vocabulary item is likely the token itself.  If this is a token
characters representation, the vocabulary items are all of the characters in the token.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer.get_padding_lengths">
<code class="sig-name descname">get_padding_lengths</code><span class="sig-paren">(</span><em class="sig-param">self, token: List[int]</em><span class="sig-paren">)</span> &#x2192; Dict[str, int]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/token_characters_indexer.py#L95-L97"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer.get_padding_lengths" title="Permalink to this definition">¶</a></dt>
<dd><p>This method returns a padding dictionary for the given token that specifies lengths for
all arrays that need padding.  For example, for single ID tokens the returned dictionary
will be empty, but for a token characters representation, this will return the number
of characters in the token.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer.get_padding_token">
<code class="sig-name descname">get_padding_token</code><span class="sig-paren">(</span><em class="sig-param">self</em><span class="sig-paren">)</span> &#x2192; List[int]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/token_characters_indexer.py#L99-L101"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer.get_padding_token" title="Permalink to this definition">¶</a></dt>
<dd><p>Deprecated. Please just implement the padding token in <cite>as_padded_tensor</cite> instead.
TODO(Mark): remove in 1.0 release. This is only a concrete implementation to preserve
backward compatability, otherwise it would be abstract.</p>
<p>When we need to add padding tokens, what should they look like?  This method returns a
“blank” token of whatever type is returned by <a class="reference internal" href="#allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer.tokens_to_indices" title="allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer.tokens_to_indices"><code class="xref py py-func docutils literal notranslate"><span class="pre">tokens_to_indices()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer.tokens_to_indices">
<code class="sig-name descname">tokens_to_indices</code><span class="sig-paren">(</span><em class="sig-param">self, tokens: List[allennlp.data.tokenizers.token.Token], vocabulary: allennlp.data.vocabulary.Vocabulary, index_name: str</em><span class="sig-paren">)</span> &#x2192; Dict[str, List[List[int]]]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/token_characters_indexer.py#L74-L93"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer.tokens_to_indices" title="Permalink to this definition">¶</a></dt>
<dd><p>Takes a list of tokens and converts them to one or more sets of indices.
This could be just an ID for each token from the vocabulary.
Or it could split each token into characters and return one ID per character.
Or (for instance, in the case of byte-pair encoding) there might not be a clean
mapping from individual tokens to indices.</p>
</dd></dl>

</dd></dl>

<span class="target" id="module-allennlp.data.token_indexers.elmo_indexer"><span id="elmo-indexer"></span></span><dl class="class">
<dt id="allennlp.data.token_indexers.elmo_indexer.ELMoCharacterMapper">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.token_indexers.elmo_indexer.</code><code class="sig-name descname">ELMoCharacterMapper</code><span class="sig-paren">(</span><em class="sig-param">tokens_to_add: Dict[str</em>, <em class="sig-param">int] = None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/elmo_indexer.py#L27-L91"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.elmo_indexer.ELMoCharacterMapper" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Maps individual tokens to sequences of character ids, compatible with ELMo.
To be consistent with previously trained models, we include it here as special of existing
character indexers.</p>
<p>We allow to add optional additional special tokens with designated
character ids with <code class="docutils literal notranslate"><span class="pre">tokens_to_add</span></code>.</p>
<dl class="attribute">
<dt id="allennlp.data.token_indexers.elmo_indexer.ELMoCharacterMapper.beginning_of_sentence_character">
<code class="sig-name descname">beginning_of_sentence_character</code><em class="property"> = 256</em><a class="headerlink" href="#allennlp.data.token_indexers.elmo_indexer.ELMoCharacterMapper.beginning_of_sentence_character" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="allennlp.data.token_indexers.elmo_indexer.ELMoCharacterMapper.beginning_of_sentence_characters">
<code class="sig-name descname">beginning_of_sentence_characters</code><em class="property"> = [258, 256, 259, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260]</em><a class="headerlink" href="#allennlp.data.token_indexers.elmo_indexer.ELMoCharacterMapper.beginning_of_sentence_characters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="allennlp.data.token_indexers.elmo_indexer.ELMoCharacterMapper.beginning_of_word_character">
<code class="sig-name descname">beginning_of_word_character</code><em class="property"> = 258</em><a class="headerlink" href="#allennlp.data.token_indexers.elmo_indexer.ELMoCharacterMapper.beginning_of_word_character" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="allennlp.data.token_indexers.elmo_indexer.ELMoCharacterMapper.bos_token">
<code class="sig-name descname">bos_token</code><em class="property"> = '&lt;S&gt;'</em><a class="headerlink" href="#allennlp.data.token_indexers.elmo_indexer.ELMoCharacterMapper.bos_token" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.elmo_indexer.ELMoCharacterMapper.convert_word_to_char_ids">
<code class="sig-name descname">convert_word_to_char_ids</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">word: str</em><span class="sig-paren">)</span> &#x2192; List[int]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/elmo_indexer.py#L67-L86"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.elmo_indexer.ELMoCharacterMapper.convert_word_to_char_ids" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="allennlp.data.token_indexers.elmo_indexer.ELMoCharacterMapper.end_of_sentence_character">
<code class="sig-name descname">end_of_sentence_character</code><em class="property"> = 257</em><a class="headerlink" href="#allennlp.data.token_indexers.elmo_indexer.ELMoCharacterMapper.end_of_sentence_character" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="allennlp.data.token_indexers.elmo_indexer.ELMoCharacterMapper.end_of_sentence_characters">
<code class="sig-name descname">end_of_sentence_characters</code><em class="property"> = [258, 257, 259, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260]</em><a class="headerlink" href="#allennlp.data.token_indexers.elmo_indexer.ELMoCharacterMapper.end_of_sentence_characters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="allennlp.data.token_indexers.elmo_indexer.ELMoCharacterMapper.end_of_word_character">
<code class="sig-name descname">end_of_word_character</code><em class="property"> = 259</em><a class="headerlink" href="#allennlp.data.token_indexers.elmo_indexer.ELMoCharacterMapper.end_of_word_character" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="allennlp.data.token_indexers.elmo_indexer.ELMoCharacterMapper.eos_token">
<code class="sig-name descname">eos_token</code><em class="property"> = '&lt;/S&gt;'</em><a class="headerlink" href="#allennlp.data.token_indexers.elmo_indexer.ELMoCharacterMapper.eos_token" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="allennlp.data.token_indexers.elmo_indexer.ELMoCharacterMapper.max_word_length">
<code class="sig-name descname">max_word_length</code><em class="property"> = 50</em><a class="headerlink" href="#allennlp.data.token_indexers.elmo_indexer.ELMoCharacterMapper.max_word_length" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="allennlp.data.token_indexers.elmo_indexer.ELMoCharacterMapper.padding_character">
<code class="sig-name descname">padding_character</code><em class="property"> = 260</em><a class="headerlink" href="#allennlp.data.token_indexers.elmo_indexer.ELMoCharacterMapper.padding_character" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="allennlp.data.token_indexers.elmo_indexer.ELMoTokenCharactersIndexer">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.token_indexers.elmo_indexer.</code><code class="sig-name descname">ELMoTokenCharactersIndexer</code><span class="sig-paren">(</span><em class="sig-param">namespace: str = 'elmo_characters'</em>, <em class="sig-param">tokens_to_add: Dict[str</em>, <em class="sig-param">int] = None</em>, <em class="sig-param">token_min_padding_length: int = 0</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/elmo_indexer.py#L95-L156"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.elmo_indexer.ELMoTokenCharactersIndexer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.token_indexers.token_indexer.TokenIndexer" title="allennlp.data.token_indexers.token_indexer.TokenIndexer"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.data.token_indexers.token_indexer.TokenIndexer</span></code></a></p>
<p>Convert a token to an array of character ids to compute ELMo representations.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>namespace</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">str</span></code>, optional (default=``elmo_characters``)</span></dt><dd></dd>
<dt><strong>tokens_to_add</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">int]</span></code>, optional (default=``None``)</span></dt><dd><p>If not None, then provides a mapping of special tokens to character
ids. When using pre-trained models, then the character id must be
less then 261, and we recommend using un-used ids (e.g. 1-32).</p>
</dd>
<dt><strong>token_min_padding_length</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code>, optional (default=``0``)</span></dt><dd><p>See <code class="xref py py-class docutils literal notranslate"><span class="pre">TokenIndexer</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="allennlp.data.token_indexers.elmo_indexer.ELMoTokenCharactersIndexer.as_padded_tensor">
<code class="sig-name descname">as_padded_tensor</code><span class="sig-paren">(</span><em class="sig-param">self, tokens: Dict[str, List[List[int]]], desired_num_tokens: Dict[str, int], padding_lengths: Dict[str, int]</em><span class="sig-paren">)</span> &#x2192; Dict[str, torch.Tensor]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/elmo_indexer.py#L148-L156"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.elmo_indexer.ELMoTokenCharactersIndexer.as_padded_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>This method pads a list of tokens to <code class="docutils literal notranslate"><span class="pre">desired_num_tokens</span></code> and returns that padded list
of input tokens as a torch Tensor. If the input token list is longer than <code class="docutils literal notranslate"><span class="pre">desired_num_tokens</span></code>
then it will be truncated.</p>
<p><code class="docutils literal notranslate"><span class="pre">padding_lengths</span></code> is used to provide supplemental padding parameters which are needed
in some cases.  For example, it contains the widths to pad characters to when doing
character-level padding.</p>
<p>Note that this method should be abstract, but it is implemented to allow backward compatability.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.elmo_indexer.ELMoTokenCharactersIndexer.count_vocab_items">
<code class="sig-name descname">count_vocab_items</code><span class="sig-paren">(</span><em class="sig-param">self, token: allennlp.data.tokenizers.token.Token, counter: Dict[str, Dict[str, int]]</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/elmo_indexer.py#L118-L120"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.elmo_indexer.ELMoTokenCharactersIndexer.count_vocab_items" title="Permalink to this definition">¶</a></dt>
<dd><p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">Vocabulary</span></code> needs to assign indices to whatever strings we see in the training
data (possibly doing some frequency filtering and using an OOV, or out of vocabulary,
token).  This method takes a token and a dictionary of counts and increments counts for
whatever vocabulary items are present in the token.  If this is a single token ID
representation, the vocabulary item is likely the token itself.  If this is a token
characters representation, the vocabulary items are all of the characters in the token.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.elmo_indexer.ELMoTokenCharactersIndexer.get_padding_lengths">
<code class="sig-name descname">get_padding_lengths</code><span class="sig-paren">(</span><em class="sig-param">self, token: List[int]</em><span class="sig-paren">)</span> &#x2192; Dict[str, int]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/elmo_indexer.py#L139-L142"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.elmo_indexer.ELMoTokenCharactersIndexer.get_padding_lengths" title="Permalink to this definition">¶</a></dt>
<dd><p>This method returns a padding dictionary for the given token that specifies lengths for
all arrays that need padding.  For example, for single ID tokens the returned dictionary
will be empty, but for a token characters representation, this will return the number
of characters in the token.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.elmo_indexer.ELMoTokenCharactersIndexer.tokens_to_indices">
<code class="sig-name descname">tokens_to_indices</code><span class="sig-paren">(</span><em class="sig-param">self, tokens: List[allennlp.data.tokenizers.token.Token], vocabulary: allennlp.data.vocabulary.Vocabulary, index_name: str</em><span class="sig-paren">)</span> &#x2192; Dict[str, List[List[int]]]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/elmo_indexer.py#L122-L137"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.elmo_indexer.ELMoTokenCharactersIndexer.tokens_to_indices" title="Permalink to this definition">¶</a></dt>
<dd><p>Takes a list of tokens and converts them to one or more sets of indices.
This could be just an ID for each token from the vocabulary.
Or it could split each token into characters and return one ID per character.
Or (for instance, in the case of byte-pair encoding) there might not be a clean
mapping from individual tokens to indices.</p>
</dd></dl>

</dd></dl>

<span class="target" id="module-allennlp.data.token_indexers.openai_transformer_byte_pair_indexer"><span id="openai-transformer-byte-pair-indexer"></span></span><dl class="class">
<dt id="allennlp.data.token_indexers.openai_transformer_byte_pair_indexer.OpenaiTransformerBytePairIndexer">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.token_indexers.openai_transformer_byte_pair_indexer.</code><code class="sig-name descname">OpenaiTransformerBytePairIndexer</code><span class="sig-paren">(</span><em class="sig-param">encoder: Dict[str</em>, <em class="sig-param">int] = None</em>, <em class="sig-param">byte_pairs: List[Tuple[str</em>, <em class="sig-param">str]] = None</em>, <em class="sig-param">n_ctx: int = 512</em>, <em class="sig-param">model_path: str = None</em>, <em class="sig-param">namespace: str = 'openai_transformer'</em>, <em class="sig-param">tokens_to_add: List[str] = None</em>, <em class="sig-param">token_min_padding_length: int = 0</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/openai_transformer_byte_pair_indexer.py#L33-L244"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.openai_transformer_byte_pair_indexer.OpenaiTransformerBytePairIndexer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.token_indexers.token_indexer.TokenIndexer" title="allennlp.data.token_indexers.token_indexer.TokenIndexer"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.data.token_indexers.token_indexer.TokenIndexer</span></code></a></p>
<p>Generates the indices for the byte-pair encoding used by
the OpenAI transformer language model: <a class="reference external" href="https://blog.openai.com/language-unsupervised/">https://blog.openai.com/language-unsupervised/</a></p>
<p>This is unlike most of our TokenIndexers in that its
indexing is not based on a <cite>Vocabulary</cite> but on a fixed
set of mappings that are loaded by the constructor.</p>
<p>Note: recommend using <code class="docutils literal notranslate"><span class="pre">OpenAISplitter</span></code> tokenizer with this indexer,
as it applies the same text normalization as the original implementation.</p>
<p>Note 2: when <code class="docutils literal notranslate"><span class="pre">tokens_to_add</span></code> is not None, be sure to set
<code class="docutils literal notranslate"><span class="pre">n_special=len(tokens_to_add)</span></code> in <code class="docutils literal notranslate"><span class="pre">OpenaiTransformer</span></code>, otherwise
behavior is undefined.</p>
<dl class="method">
<dt id="allennlp.data.token_indexers.openai_transformer_byte_pair_indexer.OpenaiTransformerBytePairIndexer.as_padded_tensor">
<code class="sig-name descname">as_padded_tensor</code><span class="sig-paren">(</span><em class="sig-param">self, tokens: Dict[str, List[int]], desired_num_tokens: Dict[str, int], padding_lengths: Dict[str, int]</em><span class="sig-paren">)</span> &#x2192; Dict[str, torch.Tensor]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/openai_transformer_byte_pair_indexer.py#L238-L244"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.openai_transformer_byte_pair_indexer.OpenaiTransformerBytePairIndexer.as_padded_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>This method pads a list of tokens to <code class="docutils literal notranslate"><span class="pre">desired_num_tokens</span></code> and returns that padded list
of input tokens as a torch Tensor. If the input token list is longer than <code class="docutils literal notranslate"><span class="pre">desired_num_tokens</span></code>
then it will be truncated.</p>
<p><code class="docutils literal notranslate"><span class="pre">padding_lengths</span></code> is used to provide supplemental padding parameters which are needed
in some cases.  For example, it contains the widths to pad characters to when doing
character-level padding.</p>
<p>Note that this method should be abstract, but it is implemented to allow backward compatability.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.openai_transformer_byte_pair_indexer.OpenaiTransformerBytePairIndexer.byte_pair_encode">
<code class="sig-name descname">byte_pair_encode</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">token: allennlp.data.tokenizers.token.Token</em>, <em class="sig-param">lowercase: bool = True</em><span class="sig-paren">)</span> &#x2192; List[str]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/openai_transformer_byte_pair_indexer.py#L113-L187"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.openai_transformer_byte_pair_indexer.OpenaiTransformerBytePairIndexer.byte_pair_encode" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.openai_transformer_byte_pair_indexer.OpenaiTransformerBytePairIndexer.count_vocab_items">
<code class="sig-name descname">count_vocab_items</code><span class="sig-paren">(</span><em class="sig-param">self, token: allennlp.data.tokenizers.token.Token, counter: Dict[str, Dict[str, int]]</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/openai_transformer_byte_pair_indexer.py#L108-L111"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.openai_transformer_byte_pair_indexer.OpenaiTransformerBytePairIndexer.count_vocab_items" title="Permalink to this definition">¶</a></dt>
<dd><p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">Vocabulary</span></code> needs to assign indices to whatever strings we see in the training
data (possibly doing some frequency filtering and using an OOV, or out of vocabulary,
token).  This method takes a token and a dictionary of counts and increments counts for
whatever vocabulary items are present in the token.  If this is a single token ID
representation, the vocabulary item is likely the token itself.  If this is a token
characters representation, the vocabulary items are all of the characters in the token.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.openai_transformer_byte_pair_indexer.OpenaiTransformerBytePairIndexer.get_padding_lengths">
<code class="sig-name descname">get_padding_lengths</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">token: int</em><span class="sig-paren">)</span> &#x2192; Dict[str, int]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/openai_transformer_byte_pair_indexer.py#L234-L236"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.openai_transformer_byte_pair_indexer.OpenaiTransformerBytePairIndexer.get_padding_lengths" title="Permalink to this definition">¶</a></dt>
<dd><p>This method returns a padding dictionary for the given token that specifies lengths for
all arrays that need padding.  For example, for single ID tokens the returned dictionary
will be empty, but for a token characters representation, this will return the number
of characters in the token.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.openai_transformer_byte_pair_indexer.OpenaiTransformerBytePairIndexer.tokens_to_indices">
<code class="sig-name descname">tokens_to_indices</code><span class="sig-paren">(</span><em class="sig-param">self, tokens: List[allennlp.data.tokenizers.token.Token], vocabulary: allennlp.data.vocabulary.Vocabulary, index_name: str</em><span class="sig-paren">)</span> &#x2192; Dict[str, List[int]]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/openai_transformer_byte_pair_indexer.py#L195-L232"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.openai_transformer_byte_pair_indexer.OpenaiTransformerBytePairIndexer.tokens_to_indices" title="Permalink to this definition">¶</a></dt>
<dd><p>Takes a list of tokens and converts them to one or more sets of indices.
This could be just an ID for each token from the vocabulary.
Or it could split each token into characters and return one ID per character.
Or (for instance, in the case of byte-pair encoding) there might not be a clean
mapping from individual tokens to indices.</p>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="allennlp.data.token_indexers.openai_transformer_byte_pair_indexer.text_standardize">
<code class="sig-prename descclassname">allennlp.data.token_indexers.openai_transformer_byte_pair_indexer.</code><code class="sig-name descname">text_standardize</code><span class="sig-paren">(</span><em class="sig-param">text</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/openai_transformer_byte_pair_indexer.py#L17-L29"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.openai_transformer_byte_pair_indexer.text_standardize" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply text standardization following original implementation.</p>
</dd></dl>

<span class="target" id="module-allennlp.data.token_indexers.wordpiece_indexer"><span id="wordpiece-indexer"></span></span><dl class="class">
<dt id="allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.token_indexers.wordpiece_indexer.</code><code class="sig-name descname">PretrainedBertIndexer</code><span class="sig-paren">(</span><em class="sig-param">pretrained_model: str</em>, <em class="sig-param">use_starting_offsets: bool = False</em>, <em class="sig-param">do_lowercase: bool = True</em>, <em class="sig-param">never_lowercase: List[str] = None</em>, <em class="sig-param">max_pieces: int = 512</em>, <em class="sig-param">truncate_long_sequences: bool = True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/wordpiece_indexer.py#L296-L367"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.token_indexers.wordpiece_indexer.WordpieceIndexer" title="allennlp.data.token_indexers.wordpiece_indexer.WordpieceIndexer"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.data.token_indexers.wordpiece_indexer.WordpieceIndexer</span></code></a></p>
<p>A <code class="docutils literal notranslate"><span class="pre">TokenIndexer</span></code> corresponding to a pretrained BERT model.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>pretrained_model: ``str``</strong></dt><dd><p>Either the name of the pretrained model to use (e.g. ‘bert-base-uncased’),
or the path to the .txt file with its vocabulary.</p>
<p>If the name is a key in the list of pretrained models at
<a class="reference external" href="https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/pytorch_pretrained_bert/tokenization.py#L33">https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/pytorch_pretrained_bert/tokenization.py#L33</a>
the corresponding path will be used; otherwise it will be interpreted as a path or URL.</p>
</dd>
<dt><strong>use_starting_offsets: bool, optional (default: False)</strong></dt><dd><p>By default, the “offsets” created by the token indexer correspond to the
last wordpiece in each word. If <code class="docutils literal notranslate"><span class="pre">use_starting_offsets</span></code> is specified,
they will instead correspond to the first wordpiece in each word.</p>
</dd>
<dt><strong>do_lowercase: ``bool``, optional (default = True)</strong></dt><dd><p>Whether to lowercase the tokens before converting to wordpiece ids.</p>
</dd>
<dt><strong>never_lowercase: ``List[str]``, optional</strong></dt><dd><p>Tokens that should never be lowercased. Default is
[‘[UNK]’, ‘[SEP]’, ‘[PAD]’, ‘[CLS]’, ‘[MASK]’].</p>
</dd>
<dt><strong>max_pieces: int, optional (default: 512)</strong></dt><dd><p>The BERT embedder uses positional embeddings and so has a corresponding
maximum length for its input ids. Any inputs longer than this will
either be truncated (default), or be split apart and batched using a
sliding window.</p>
</dd>
<dt><strong>truncate_long_sequences</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">bool</span></code>, optional (default=``True``)</span></dt><dd><p>By default, long sequences will be truncated to the maximum sequence
length. Otherwise, they will be split apart and batched using a
sliding window.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="allennlp.data.token_indexers.wordpiece_indexer.WordpieceIndexer">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.token_indexers.wordpiece_indexer.</code><code class="sig-name descname">WordpieceIndexer</code><span class="sig-paren">(</span><em class="sig-param">vocab: Dict[str, int], wordpiece_tokenizer: Callable[[str], List[str]], namespace: str = 'wordpiece', use_starting_offsets: bool = False, max_pieces: int = 512, do_lowercase: bool = False, never_lowercase: List[str] = None, start_tokens: List[str] = None, end_tokens: List[str] = None, separator_token: str = '[SEP]', truncate_long_sequences: bool = True, token_min_padding_length: int = 0</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/wordpiece_indexer.py#L22-L292"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.wordpiece_indexer.WordpieceIndexer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.token_indexers.token_indexer.TokenIndexer" title="allennlp.data.token_indexers.token_indexer.TokenIndexer"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.data.token_indexers.token_indexer.TokenIndexer</span></code></a></p>
<p>A token indexer that does the wordpiece-tokenization (e.g. for BERT embeddings).
If you are using one of the pretrained BERT models, you’ll want to use the <code class="docutils literal notranslate"><span class="pre">PretrainedBertIndexer</span></code>
subclass rather than this base class.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>vocab</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">int]</span></code></span></dt><dd><p>The mapping {wordpiece -&gt; id}.  Note this is not an AllenNLP <code class="docutils literal notranslate"><span class="pre">Vocabulary</span></code>.</p>
</dd>
<dt><strong>wordpiece_tokenizer</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">Callable[[str],</span> <span class="pre">List[str]]</span></code></span></dt><dd><p>A function that does the actual tokenization.</p>
</dd>
<dt><strong>namespace</strong><span class="classifier">str, optional (default: “wordpiece”)</span></dt><dd><p>The namespace in the AllenNLP <code class="docutils literal notranslate"><span class="pre">Vocabulary</span></code> into which the wordpieces
will be loaded.</p>
</dd>
<dt><strong>use_starting_offsets</strong><span class="classifier">bool, optional (default: False)</span></dt><dd><p>By default, the “offsets” created by the token indexer correspond to the
last wordpiece in each word. If <code class="docutils literal notranslate"><span class="pre">use_starting_offsets</span></code> is specified,
they will instead correspond to the first wordpiece in each word.</p>
</dd>
<dt><strong>max_pieces</strong><span class="classifier">int, optional (default: 512)</span></dt><dd><p>The BERT embedder uses positional embeddings and so has a corresponding
maximum length for its input ids. Any inputs longer than this will
either be truncated (default), or be split apart and batched using a
sliding window.</p>
</dd>
<dt><strong>do_lowercase</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">bool</span></code>, optional (default=``False``)</span></dt><dd><p>Should we lowercase the provided tokens before getting the indices?
You would need to do this if you are using an -uncased BERT model
but your DatasetReader is not lowercasing tokens (which might be the
case if you’re also using other embeddings based on cased tokens).</p>
</dd>
<dt><strong>never_lowercase: ``List[str]``, optional</strong></dt><dd><p>Tokens that should never be lowercased. Default is
[‘[UNK]’, ‘[SEP]’, ‘[PAD]’, ‘[CLS]’, ‘[MASK]’].</p>
</dd>
<dt><strong>start_tokens</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">List[str]</span></code>, optional (default=``None``)</span></dt><dd><p>These are prepended to the tokens provided to <code class="docutils literal notranslate"><span class="pre">tokens_to_indices</span></code>.</p>
</dd>
<dt><strong>end_tokens</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">List[str]</span></code>, optional (default=``None``)</span></dt><dd><p>These are appended to the tokens provided to <code class="docutils literal notranslate"><span class="pre">tokens_to_indices</span></code>.</p>
</dd>
<dt><strong>separator_token</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">str</span></code>, optional (default=``[SEP]``)</span></dt><dd><p>This token indicates the segments in the sequence.</p>
</dd>
<dt><strong>truncate_long_sequences</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">bool</span></code>, optional (default=``True``)</span></dt><dd><p>By default, long sequences will be truncated to the maximum sequence
length. Otherwise, they will be split apart and batched using a
sliding window.</p>
</dd>
<dt><strong>token_min_padding_length</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code>, optional (default=``0``)</span></dt><dd><p>See <code class="xref py py-class docutils literal notranslate"><span class="pre">TokenIndexer</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="allennlp.data.token_indexers.wordpiece_indexer.WordpieceIndexer.as_padded_tensor">
<code class="sig-name descname">as_padded_tensor</code><span class="sig-paren">(</span><em class="sig-param">self, tokens: Dict[str, List[int]], desired_num_tokens: Dict[str, int], padding_lengths: Dict[str, int]</em><span class="sig-paren">)</span> &#x2192; Dict[str, torch.Tensor]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/wordpiece_indexer.py#L278-L284"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.wordpiece_indexer.WordpieceIndexer.as_padded_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>This method pads a list of tokens to <code class="docutils literal notranslate"><span class="pre">desired_num_tokens</span></code> and returns that padded list
of input tokens as a torch Tensor. If the input token list is longer than <code class="docutils literal notranslate"><span class="pre">desired_num_tokens</span></code>
then it will be truncated.</p>
<p><code class="docutils literal notranslate"><span class="pre">padding_lengths</span></code> is used to provide supplemental padding parameters which are needed
in some cases.  For example, it contains the widths to pad characters to when doing
character-level padding.</p>
<p>Note that this method should be abstract, but it is implemented to allow backward compatability.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.wordpiece_indexer.WordpieceIndexer.count_vocab_items">
<code class="sig-name descname">count_vocab_items</code><span class="sig-paren">(</span><em class="sig-param">self, token: allennlp.data.tokenizers.token.Token, counter: Dict[str, Dict[str, int]]</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/wordpiece_indexer.py#L116-L119"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.wordpiece_indexer.WordpieceIndexer.count_vocab_items" title="Permalink to this definition">¶</a></dt>
<dd><p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">Vocabulary</span></code> needs to assign indices to whatever strings we see in the training
data (possibly doing some frequency filtering and using an OOV, or out of vocabulary,
token).  This method takes a token and a dictionary of counts and increments counts for
whatever vocabulary items are present in the token.  If this is a single token ID
representation, the vocabulary item is likely the token itself.  If this is a token
characters representation, the vocabulary items are all of the characters in the token.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.wordpiece_indexer.WordpieceIndexer.get_keys">
<code class="sig-name descname">get_keys</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">index_name: str</em><span class="sig-paren">)</span> &#x2192; List[str]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/wordpiece_indexer.py#L286-L292"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.wordpiece_indexer.WordpieceIndexer.get_keys" title="Permalink to this definition">¶</a></dt>
<dd><p>We need to override this because the indexer generates multiple keys.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.wordpiece_indexer.WordpieceIndexer.get_padding_lengths">
<code class="sig-name descname">get_padding_lengths</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">token: int</em><span class="sig-paren">)</span> &#x2192; Dict[str, int]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/wordpiece_indexer.py#L274-L276"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.wordpiece_indexer.WordpieceIndexer.get_padding_lengths" title="Permalink to this definition">¶</a></dt>
<dd><p>This method returns a padding dictionary for the given token that specifies lengths for
all arrays that need padding.  For example, for single ID tokens the returned dictionary
will be empty, but for a token characters representation, this will return the number
of characters in the token.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.wordpiece_indexer.WordpieceIndexer.tokens_to_indices">
<code class="sig-name descname">tokens_to_indices</code><span class="sig-paren">(</span><em class="sig-param">self, tokens: List[allennlp.data.tokenizers.token.Token], vocabulary: allennlp.data.vocabulary.Vocabulary, index_name: str</em><span class="sig-paren">)</span> &#x2192; Dict[str, List[int]]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/wordpiece_indexer.py#L135-L258"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.wordpiece_indexer.WordpieceIndexer.tokens_to_indices" title="Permalink to this definition">¶</a></dt>
<dd><p>Takes a list of tokens and converts them to one or more sets of indices.
This could be just an ID for each token from the vocabulary.
Or it could split each token into characters and return one ID per character.
Or (for instance, in the case of byte-pair encoding) there might not be a clean
mapping from individual tokens to indices.</p>
</dd></dl>

</dd></dl>

<span class="target" id="module-allennlp.data.token_indexers.pretrained_transformer_indexer"><span id="pretrained-transformer-indexer"></span></span><dl class="class">
<dt id="allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.token_indexers.pretrained_transformer_indexer.</code><code class="sig-name descname">PretrainedTransformerIndexer</code><span class="sig-paren">(</span><em class="sig-param">model_name: str</em>, <em class="sig-param">do_lowercase: bool</em>, <em class="sig-param">namespace: str = 'tags'</em>, <em class="sig-param">token_min_padding_length: int = 0</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/pretrained_transformer_indexer.py#L18-L106"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.token_indexers.token_indexer.TokenIndexer" title="allennlp.data.token_indexers.token_indexer.TokenIndexer"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.data.token_indexers.token_indexer.TokenIndexer</span></code></a></p>
<p>This <code class="xref py py-class docutils literal notranslate"><span class="pre">TokenIndexer</span></code> uses a tokenizer from the <code class="docutils literal notranslate"><span class="pre">pytorch_transformers</span></code> repository to
index tokens.  This <code class="docutils literal notranslate"><span class="pre">Indexer</span></code> is only really appropriate to use if you’ve also used a
corresponding <code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedTransformerTokenizer</span></code> to tokenize your input.  Otherwise you’ll
have a mismatch between your tokens and your vocabulary, and you’ll get a lot of UNK tokens.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>model_name</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">str</span></code></span></dt><dd><p>The name of the <code class="docutils literal notranslate"><span class="pre">pytorch_transformers</span></code> model to use.</p>
</dd>
<dt><strong>do_lowercase</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">str</span></code></span></dt><dd><p>Whether to lowercase the tokens (this should match the casing of the model name that you
pass)</p>
</dd>
<dt><strong>namespace</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">str</span></code>, optional (default=``tags``)</span></dt><dd><p>We will add the tokens in the pytorch_transformer vocabulary to this vocabulary namespace.
We use a somewhat confusing default value of <code class="docutils literal notranslate"><span class="pre">tags</span></code> so that we do not add padding or UNK
tokens to this namespace, which would break on loading because we wouldn’t find our default
OOV token.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer.as_padded_tensor">
<code class="sig-name descname">as_padded_tensor</code><span class="sig-paren">(</span><em class="sig-param">self, tokens: Dict[str, List[int]], desired_num_tokens: Dict[str, int], padding_lengths: Dict[str, int]</em><span class="sig-paren">)</span> &#x2192; Dict[str, torch.Tensor]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/pretrained_transformer_indexer.py#L86-L94"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer.as_padded_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>This method pads a list of tokens to <code class="docutils literal notranslate"><span class="pre">desired_num_tokens</span></code> and returns that padded list
of input tokens as a torch Tensor. If the input token list is longer than <code class="docutils literal notranslate"><span class="pre">desired_num_tokens</span></code>
then it will be truncated.</p>
<p><code class="docutils literal notranslate"><span class="pre">padding_lengths</span></code> is used to provide supplemental padding parameters which are needed
in some cases.  For example, it contains the widths to pad characters to when doing
character-level padding.</p>
<p>Note that this method should be abstract, but it is implemented to allow backward compatability.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer.count_vocab_items">
<code class="sig-name descname">count_vocab_items</code><span class="sig-paren">(</span><em class="sig-param">self, token: allennlp.data.tokenizers.token.Token, counter: Dict[str, Dict[str, int]]</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/pretrained_transformer_indexer.py#L58-L61"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer.count_vocab_items" title="Permalink to this definition">¶</a></dt>
<dd><p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">Vocabulary</span></code> needs to assign indices to whatever strings we see in the training
data (possibly doing some frequency filtering and using an OOV, or out of vocabulary,
token).  This method takes a token and a dictionary of counts and increments counts for
whatever vocabulary items are present in the token.  If this is a single token ID
representation, the vocabulary item is likely the token itself.  If this is a token
characters representation, the vocabulary items are all of the characters in the token.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer.get_padding_lengths">
<code class="sig-name descname">get_padding_lengths</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">token: int</em><span class="sig-paren">)</span> &#x2192; Dict[str, int]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/pretrained_transformer_indexer.py#L82-L84"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer.get_padding_lengths" title="Permalink to this definition">¶</a></dt>
<dd><p>This method returns a padding dictionary for the given token that specifies lengths for
all arrays that need padding.  For example, for single ID tokens the returned dictionary
will be empty, but for a token characters representation, this will return the number
of characters in the token.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer.tokens_to_indices">
<code class="sig-name descname">tokens_to_indices</code><span class="sig-paren">(</span><em class="sig-param">self, tokens: List[allennlp.data.tokenizers.token.Token], vocabulary: allennlp.data.vocabulary.Vocabulary, index_name: str</em><span class="sig-paren">)</span> &#x2192; Dict[str, List[int]]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/pretrained_transformer_indexer.py#L69-L80"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer.tokens_to_indices" title="Permalink to this definition">¶</a></dt>
<dd><p>Takes a list of tokens and converts them to one or more sets of indices.
This could be just an ID for each token from the vocabulary.
Or it could split each token into characters and return one ID per character.
Or (for instance, in the case of byte-pair encoding) there might not be a clean
mapping from individual tokens to indices.</p>
</dd></dl>

</dd></dl>

<span class="target" id="module-allennlp.data.token_indexers.spacy_indexer"><span id="spacy-token-indexer"></span></span><dl class="class">
<dt id="allennlp.data.token_indexers.spacy_indexer.SpacyTokenIndexer">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.token_indexers.spacy_indexer.</code><code class="sig-name descname">SpacyTokenIndexer</code><span class="sig-paren">(</span><em class="sig-param">hidden_dim: int = 96</em>, <em class="sig-param">token_min_padding_length: int = 0</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/spacy_indexer.py#L15-L75"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.spacy_indexer.SpacyTokenIndexer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.token_indexers.token_indexer.TokenIndexer" title="allennlp.data.token_indexers.token_indexer.TokenIndexer"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.data.token_indexers.token_indexer.TokenIndexer</span></code></a></p>
<p>This <a class="reference internal" href="#allennlp.data.token_indexers.spacy_indexer.SpacyTokenIndexer" title="allennlp.data.token_indexers.spacy_indexer.SpacyTokenIndexer"><code class="xref py py-class docutils literal notranslate"><span class="pre">SpacyTokenIndexer</span></code></a> represents tokens as word vectors
from a spacy model. You might want to do this for two main reasons;
easier integration with a spacy pipeline and no out of vocabulary
tokens.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>hidden_dim</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code>, optional (default=``96``)</span></dt><dd><p>The dimension of the vectors that spacy generates for
representing words.</p>
</dd>
<dt><strong>token_min_padding_length</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code>, optional (default=``0``)</span></dt><dd><p>See <code class="xref py py-class docutils literal notranslate"><span class="pre">TokenIndexer</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="allennlp.data.token_indexers.spacy_indexer.SpacyTokenIndexer.as_padded_tensor">
<code class="sig-name descname">as_padded_tensor</code><span class="sig-paren">(</span><em class="sig-param">self, tokens: Dict[str, List[numpy.ndarray]], desired_num_tokens: Dict[str, int], padding_lengths: Dict[str, int]</em><span class="sig-paren">)</span> &#x2192; Dict[str, torch.Tensor]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/spacy_indexer.py#L66-L75"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.spacy_indexer.SpacyTokenIndexer.as_padded_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>This method pads a list of tokens to <code class="docutils literal notranslate"><span class="pre">desired_num_tokens</span></code> and returns that padded list
of input tokens as a torch Tensor. If the input token list is longer than <code class="docutils literal notranslate"><span class="pre">desired_num_tokens</span></code>
then it will be truncated.</p>
<p><code class="docutils literal notranslate"><span class="pre">padding_lengths</span></code> is used to provide supplemental padding parameters which are needed
in some cases.  For example, it contains the widths to pad characters to when doing
character-level padding.</p>
<p>Note that this method should be abstract, but it is implemented to allow backward compatability.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.spacy_indexer.SpacyTokenIndexer.count_vocab_items">
<code class="sig-name descname">count_vocab_items</code><span class="sig-paren">(</span><em class="sig-param">self, token: allennlp.data.tokenizers.token.Token, counter: Dict[str, Dict[str, int]]</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/spacy_indexer.py#L37-L42"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.spacy_indexer.SpacyTokenIndexer.count_vocab_items" title="Permalink to this definition">¶</a></dt>
<dd><p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">Vocabulary</span></code> needs to assign indices to whatever strings we see in the training
data (possibly doing some frequency filtering and using an OOV, or out of vocabulary,
token).  This method takes a token and a dictionary of counts and increments counts for
whatever vocabulary items are present in the token.  If this is a single token ID
representation, the vocabulary item is likely the token itself.  If this is a token
characters representation, the vocabulary items are all of the characters in the token.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.spacy_indexer.SpacyTokenIndexer.get_padding_lengths">
<code class="sig-name descname">get_padding_lengths</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">token: numpy.ndarray</em><span class="sig-paren">)</span> &#x2192; Dict[str, numpy.ndarray]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/spacy_indexer.py#L61-L64"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.spacy_indexer.SpacyTokenIndexer.get_padding_lengths" title="Permalink to this definition">¶</a></dt>
<dd><p>This method returns a padding dictionary for the given token that specifies lengths for
all arrays that need padding.  For example, for single ID tokens the returned dictionary
will be empty, but for a token characters representation, this will return the number
of characters in the token.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.spacy_indexer.SpacyTokenIndexer.get_padding_token">
<code class="sig-name descname">get_padding_token</code><span class="sig-paren">(</span><em class="sig-param">self</em><span class="sig-paren">)</span> &#x2192; numpy.ndarray<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/spacy_indexer.py#L58-L59"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.spacy_indexer.SpacyTokenIndexer.get_padding_token" title="Permalink to this definition">¶</a></dt>
<dd><p>Deprecated. Please just implement the padding token in <cite>as_padded_tensor</cite> instead.
TODO(Mark): remove in 1.0 release. This is only a concrete implementation to preserve
backward compatability, otherwise it would be abstract.</p>
<p>When we need to add padding tokens, what should they look like?  This method returns a
“blank” token of whatever type is returned by <a class="reference internal" href="#allennlp.data.token_indexers.spacy_indexer.SpacyTokenIndexer.tokens_to_indices" title="allennlp.data.token_indexers.spacy_indexer.SpacyTokenIndexer.tokens_to_indices"><code class="xref py py-func docutils literal notranslate"><span class="pre">tokens_to_indices()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.token_indexers.spacy_indexer.SpacyTokenIndexer.tokens_to_indices">
<code class="sig-name descname">tokens_to_indices</code><span class="sig-paren">(</span><em class="sig-param">self, tokens: List[spacy.tokens.token.Token], vocabulary: allennlp.data.vocabulary.Vocabulary, index_name: str</em><span class="sig-paren">)</span> &#x2192; Dict[str, List[numpy.ndarray]]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/spacy_indexer.py#L44-L56"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.token_indexers.spacy_indexer.SpacyTokenIndexer.tokens_to_indices" title="Permalink to this definition">¶</a></dt>
<dd><p>Takes a list of tokens and converts them to one or more sets of indices.
This could be just an ID for each token from the vocabulary.
Or it could split each token into characters and return one ID per character.
Or (for instance, in the case of byte-pair encoding) there might not be a clean
mapping from individual tokens to indices.</p>
</dd></dl>

</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="allennlp.data.tokenizers.html" class="btn btn-neutral float-right" title="allennlp.data.tokenizers" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="allennlp.data.iterators.html" class="btn btn-neutral float-left" title="allennlp.data.iterators" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Allen Institute for Artificial Intelligence

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
  
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(400);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>


</body>
</html>