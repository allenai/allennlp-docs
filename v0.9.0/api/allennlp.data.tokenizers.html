

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>allennlp.data.tokenizers &mdash; AllenNLP 0.9.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="allennlp.data.vocabulary" href="allennlp.data.vocabulary.html" />
    <link rel="prev" title="allennlp.data.token_indexers" href="allennlp.data.token_indexers.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/allennlp-logo-dark.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.9.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Package Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="allennlp.commands.html">allennlp.commands</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.subcommand.html">allennlp.commands.subcommand</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.configure.html">allennlp.commands.configure</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.evaluate.html">allennlp.commands.evaluate</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.make_vocab.html">allennlp.commands.make_vocab</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.predict.html">allennlp.commands.predict</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.train.html">allennlp.commands.train</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.fine_tune.html">allennlp.commands.fine_tune</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.elmo.html">allennlp.commands.elmo</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.dry_run.html">allennlp.commands.dry_run</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.find_learning_rate.html">allennlp.commands.find_learning_rate</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.test_install.html">allennlp.commands.test_install</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.print_results.html">allennlp.commands.print_results</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.common.html">allennlp.common</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.checks.html">allennlp.common.checks</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.configuration.html">allennlp.common.configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.file_utils.html">allennlp.common.file_utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.from_params.html">allennlp.common.from_params</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.params.html">allennlp.common.params</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.registrable.html">allennlp.common.registrable</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.tee_logger.html">allennlp.common.tee_logger</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.testing.html">allennlp.common.testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.tqdm.html">allennlp.common.checks</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.util.html">allennlp.common.util</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="allennlp.data.html">allennlp.data</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.dataset.html">allennlp.data.dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.dataset_readers.html">allennlp.data.dataset_readers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.dataset_reader.html">allennlp.data.dataset_readers.dataset_reader</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.dataset_utils.html">allennlp.data.dataset_readers.dataset_utils</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.babi.html">allennlp.data.dataset_readers.babi</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.ccgbank.html">allennlp.data.dataset_readers.ccgbank</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.conll2000.html">allennlp.data.dataset_readers.conll2000</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.conll2003.html">allennlp.data.dataset_readers.conll2003</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.coreference_resolution.html">allennlp.data.dataset_readers.coreference_resolution</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.event2mind.html">allennlp.data.dataset_readers.event2mind</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.interleaving_dataset_reader.html">allennlp.data.dataset_readers.interleaving_dataset_reader</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.language_modeling.html">allennlp.data.dataset_readers.language_modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.masked_language_modeling.html">allennlp.data.dataset_readers.masked_language_modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.multiprocess_dataset_reader.html">allennlp.data.dataset_readers.multiprocess_dataset_reader</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.next_token_lm.html">allennlp.data.dataset_readers.next_token_lm</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.ontonotes_ner.html">allennlp.data.dataset_readers.ontonotes_ner</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.penn_tree_bank.html">allennlp.data.dataset_readers.penn_tree_bank</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.quora_paraphrase.html">allennlp.data.dataset_readers.quora_paraphrase</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.reading_comprehension.html">allennlp.data.dataset_readers.reading_comprehension</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.semantic_dependency_parsing.html">allennlp.data.dataset_readers.semantic_dependency_parsing</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.semantic_parsing.html">allennlp.data.dataset_readers.semantic_parsing</a><ul>
<li class="toctree-l4"><a class="reference internal" href="allennlp.data.dataset_readers.semantic_parsing.wikitables.html">allennlp.data.dataset_readers.semantic_parsing.wikitables</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.semantic_role_labeling.html">allennlp.data.dataset_readers.semantic_role_labeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.seq2seq.html">allennlp.data.dataset_readers.seq2seq</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.sequence_tagging.html">allennlp.data.dataset_readers.sequence_tagging</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.simple_language_modeling.html">allennlp.data.dataset_readers.simple_language_modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.snli.html">allennlp.data.dataset_readers.snli</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.stanford_sentiment_tree_bank.html">allennlp.data.dataset_readers.stanford_sentiment_tree_bank</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.universal_dependencies.html">allennlp.data.dataset_readers.universal_dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.universal_dependencies_multilang.html">allennlp.data.dataset_readers.universal_dependencies_multilang</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.quora_paraphrase.html">allennlp.data.dataset_readers.quora_paraphrase</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.copynet_seq2seq.html">allennlp.data.dataset_readers.copynet_seq2seq</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.text_classification_json.html">allennlp.data.dataset_readers.text_classification_json</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.fields.html">allennlp.data.fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.instance.html">allennlp.data.instance</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.iterators.html">allennlp.data.iterators</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.token_indexers.html">allennlp.data.token_indexers</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">allennlp.data.tokenizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.vocabulary.html">allennlp.data.vocabulary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.interpret.html">allennlp.interpret</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.interpret.attackers.html">allennlp.interpret.attackers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.interpret.saliency_interpreters.html">allennlp.interpret.saliency_interpreters</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.models.html">allennlp.models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.model.html">allennlp.models.model</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.archival.html">allennlp.models.archival</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.basic_classifier.html">allennlp.models.basic_classifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.bert_for_classification.html">allennlp.models.bert_for_classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.biaffine_dependency_parser.html">allennlp.models.biaffine_dependency_parser</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.biaffine_dependency_parser_multilang.html">allennlp.models.biaffine_dependency_parser_multilang</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.biattentive_classification_network.html">allennlp.models.biattentive_classification_network</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.bimpm.html">allennlp.models.bimpm</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.constituency_parser.html">allennlp.models.constituency_parser</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.coreference_resolution.html">allennlp.models.coreference_resolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.crf_tagger.html">allennlp.models.crf_tagger</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.decomposable_attention.html">allennlp.models.decomposable_attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.encoder_decoders.html">allennlp.models.encoder_decoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.ensemble.html">allennlp.models.ensemble</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.esim.html">allennlp.models.esim</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.event2mind.html">allennlp.models.event2mind</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.graph_parser.html">allennlp.models.graph_parser</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.language_model.html">allennlp.models.language_model</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.masked_language_model.html">allennlp.models.masked_language_model</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.next_token_lm.html">allennlp.models.next_token_lm</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.reading_comprehension.html">allennlp.models.reading_comprehension</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.semantic_parsing.html">allennlp.models.semantic_parsing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="allennlp.models.semantic_parsing.nlvr.html">allennlp.models.semantic_parsing.nlvr</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.models.semantic_parsing.wikitables.html">allennlp.models.semantic_parsing.wikitables</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.models.semantic_parsing.atis.html">allennlp.models.semantic_parsing.atis</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.models.semantic_parsing.quarel.html">allennlp.models.semantic_parsing.quarel</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.semantic_role_labeler.html">allennlp.models.semantic_role_labeler</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.simple_tagger.html">allennlp.models.simple_tagger</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.srl_bert.html">allennlp.models.srl_bert</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.srl_util.html">allennlp.models.srl_util</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.predictors.html">allennlp.predictors</a></li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.modules.html">allennlp.modules</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.attention.html">allennlp.modules.attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.matrix_attention.html">allennlp.modules.matrix_attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.augmented_lstm.html">allennlp.modules.augmented_lstm</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.lstm_cell_with_projection.html">allennlp.modules.lstm_cell_with_projection</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.elmo.html">allennlp.modules.elmo</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.elmo_lstm.html">allennlp.modules.elmo_lstm</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.language_model_heads.html">allennlp.modules.language_model_heads</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.conditional_random_field.html">allennlp.modules.conditional_random_field</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.feedforward.html">allennlp.modules.feedforward</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.highway.html">allennlp.modules.highway</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.matrix_attention.html">allennlp.modules.matrix_attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.openai_transformer.html">allennlp.modules.openai_transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.seq2seq_encoders.html">allennlp.modules.seq2seq_encoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.seq2seq_decoders.html">allennlp.modules.seq2seq_decoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.seq2vec_encoders.html">allennlp.modules.seq2vec_encoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.span_extractors.html">allennlp.modules.span_extractors</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.similarity_functions.html">allennlp.modules.similarity_functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.stacked_alternating_lstm.html">allennlp.modules.stacked_alternating_lstm</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.stacked_bidirectional_lstm.html">allennlp.modules.stacked_bidirectional_lstm</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.text_field_embedders.html">allennlp.modules.text_field_embedders</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.time_distributed.html">allennlp.modules.time_distributed</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.token_embedders.html">allennlp.modules.token_embedders</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.scalar_mix.html">allennlp.modules.scalar_mix</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.layer_norm.html">allennlp.modules.layer_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.pruner.html">allennlp.modules.pruner</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.maxout.html">allennlp.modules.maxout</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.input_variational_dropout.html">allennlp.modules.input_variational_dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.bimpm_matching.html">allennlp.modules.bimpm_matching</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.masked_layer_norm.html">allennlp.modules.masked_layer_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.sampled_softmax_loss.html">allennlp.modules.sampled_softmax_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.residual_with_layer_dropout.html">allennlp.modules.residual_with_layer_dropout</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.nn.html">allennlp.nn</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.nn.activations.html">allennlp.nn.activations</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.nn.chu_liu_edmonds.html">allennlp.nn.chu_liu_edmonds</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.nn.initializers.html">allennlp.nn.initializers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.nn.regularizers.html">allennlp.nn.regularizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.nn.util.html">allennlp.nn.util</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.nn.beam_search.html">allennlp.nn.beam_search</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.semparse.html">allennlp.semparse</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.semparse.common.html">allennlp.semparse.common</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.semparse.contexts.html">allennlp.semparse.contexts</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.semparse.executors.html">allennlp.semparse.executors</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.semparse.type_declarations.html">allennlp.semparse.type_declarations</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.semparse.worlds.html">allennlp.semparse.worlds</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.semparse.executors.html">allennlp.semparse.executors</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.semparse.domain_languages.html">allennlp.semparse.domain_languages</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.semparse.util.html">allennlp.semparse.util</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.service.html">allennlp.service</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.service.server_simple.html">allennlp.service.server_simple</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.service.config_explorer.html">allennlp.service.config_explorer</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.state_machines.html">allennlp.state_machines</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.state_machines.states.html">allennlp.state_machines.states</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.state_machines.trainers.html">allennlp.state_machines.trainers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.state_machines.transition_functions.html">allennlp.state_machines.transition_functions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.tools.html">allennlp.tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.training.html">allennlp.training</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.callbacks.html">allennlp.training.callbacks</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.callback_trainer.html">allennlp.training.callback_trainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.checkpointer.html">allennlp.training.checkpointer</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.scheduler.html">allennlp.training.scheduler</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.learning_rate_schedulers.html">allennlp.training.learning_rate_schedulers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.momentum_schedulers.html">allennlp.training.momentum_schedulers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.metric_tracker.html">allennlp.training.metric_tracker</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.metrics.html">allennlp.training.metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.moving_average.html">allennlp.training.moving_average</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.no_op_trainer.html">allennlp.training.no_op_trainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.optimizers.html">allennlp.training.optimizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.tensorboard_writer.html">allennlp.training.tensorboard_writer</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.trainer.html">allennlp.training.trainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.trainer_base.html">allennlp.training.trainer_base</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.trainer_pieces.html">allennlp.training.trainer_pieces</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.util.html">allennlp.training.util</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.pretrained.html">allennlp.pretrained</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">AllenNLP</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="allennlp.data.html">allennlp.data</a> &raquo;</li>
        
      <li>allennlp.data.tokenizers</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/api/allennlp.data.tokenizers.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-allennlp.data.tokenizers.token">
<span id="allennlp-data-tokenizers"></span><h1>allennlp.data.tokenizers<a class="headerlink" href="#module-allennlp.data.tokenizers.token" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="allennlp.data.tokenizers.token.Token">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.tokenizers.token.</code><code class="sig-name descname">Token</code><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/token.py#L3-L48"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.token.Token" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p>
<p>A simple token representation, keeping track of the token’s text, offset in the passage it was
taken from, POS tag, dependency relation, and similar information.  These fields match spacy’s
exactly, so we can just use a spacy token for this.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>text</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">str</span></code>, optional</span></dt><dd><p>The original text represented by this token.</p>
</dd>
<dt><strong>idx</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code>, optional</span></dt><dd><p>The character offset of this token into the tokenized passage.</p>
</dd>
<dt><strong>lemma_</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">str</span></code>, optional</span></dt><dd><p>The lemma of this token.</p>
</dd>
<dt><strong>pos_</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">str</span></code>, optional</span></dt><dd><p>The coarse-grained part of speech of this token.</p>
</dd>
<dt><strong>tag_</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">str</span></code>, optional</span></dt><dd><p>The fine-grained part of speech of this token.</p>
</dd>
<dt><strong>dep_</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">str</span></code>, optional</span></dt><dd><p>The dependency relation for this token.</p>
</dd>
<dt><strong>ent_type_</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">str</span></code>, optional</span></dt><dd><p>The entity type (i.e., the NER tag) for this token.</p>
</dd>
<dt><strong>text_id</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code>, optional</span></dt><dd><p>If your tokenizer returns integers instead of strings (e.g., because you’re doing byte
encoding, or some hash-based embedding), set this with the integer.  If this is set, we
will bypass the vocabulary when indexing this token, regardless of whether <code class="docutils literal notranslate"><span class="pre">text</span></code> is also
set.  You can <cite>also</cite> set <code class="docutils literal notranslate"><span class="pre">text</span></code> with the original text, if you want, so that you can
still use a character-level representation in addition to a hash-based word embedding.</p>
<p>The other fields on <code class="docutils literal notranslate"><span class="pre">Token</span></code> follow the fields on spacy’s <code class="docutils literal notranslate"><span class="pre">Token</span></code> object; this is one we
added, similar to spacy’s <code class="docutils literal notranslate"><span class="pre">lex_id</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="allennlp.data.tokenizers.token.Token.dep_">
<em class="property">property </em><code class="sig-name descname">dep_</code><a class="headerlink" href="#allennlp.data.tokenizers.token.Token.dep_" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 5</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.tokenizers.token.Token.ent_type_">
<em class="property">property </em><code class="sig-name descname">ent_type_</code><a class="headerlink" href="#allennlp.data.tokenizers.token.Token.ent_type_" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 6</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.tokenizers.token.Token.idx">
<em class="property">property </em><code class="sig-name descname">idx</code><a class="headerlink" href="#allennlp.data.tokenizers.token.Token.idx" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.tokenizers.token.Token.lemma_">
<em class="property">property </em><code class="sig-name descname">lemma_</code><a class="headerlink" href="#allennlp.data.tokenizers.token.Token.lemma_" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 2</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.tokenizers.token.Token.pos_">
<em class="property">property </em><code class="sig-name descname">pos_</code><a class="headerlink" href="#allennlp.data.tokenizers.token.Token.pos_" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 3</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.tokenizers.token.Token.tag_">
<em class="property">property </em><code class="sig-name descname">tag_</code><a class="headerlink" href="#allennlp.data.tokenizers.token.Token.tag_" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 4</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.tokenizers.token.Token.text">
<em class="property">property </em><code class="sig-name descname">text</code><a class="headerlink" href="#allennlp.data.tokenizers.token.Token.text" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.tokenizers.token.Token.text_id">
<em class="property">property </em><code class="sig-name descname">text_id</code><a class="headerlink" href="#allennlp.data.tokenizers.token.Token.text_id" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 7</p>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="allennlp.data.tokenizers.token.show_token">
<code class="sig-prename descclassname">allennlp.data.tokenizers.token.</code><code class="sig-name descname">show_token</code><span class="sig-paren">(</span><em class="sig-param">token: allennlp.data.tokenizers.token.Token</em><span class="sig-paren">)</span> &#x2192; str<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/token.py#L51-L58"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.token.show_token" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<span class="target" id="module-allennlp.data.tokenizers"></span><p>This module contains various classes for performing
tokenization, stemming, and filtering.</p>
<ul class="simple">
<li><p><a class="reference internal" href="#tokenizer"><span class="std std-ref">Tokenizer</span></a></p></li>
<li><p><a class="reference internal" href="#word-tokenizer"><span class="std std-ref">WordTokenizer</span></a></p></li>
<li><p><a class="reference internal" href="#character-tokenizer"><span class="std std-ref">CharacterTokenizer</span></a></p></li>
<li><p><a class="reference internal" href="#pretrained-transformer-tokenizer"><span class="std std-ref">PretrainedTransformerTokenizer</span></a></p></li>
<li><p><a class="reference internal" href="#word-filter"><span class="std std-ref">WordFilter</span></a></p></li>
<li><p><a class="reference internal" href="#word-splitter"><span class="std std-ref">WordSplitter</span></a></p></li>
<li><p><a class="reference internal" href="#word-stemmer"><span class="std std-ref">WordStemmer</span></a></p></li>
</ul>
<span class="target" id="module-allennlp.data.tokenizers.tokenizer"><span id="tokenizer"></span></span><dl class="class">
<dt id="allennlp.data.tokenizers.tokenizer.Tokenizer">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.tokenizers.tokenizer.</code><code class="sig-name descname">Tokenizer</code><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/tokenizer.py#L7-L44"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.tokenizer.Tokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="allennlp.common.registrable.html#allennlp.common.registrable.Registrable" title="allennlp.common.registrable.Registrable"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.common.registrable.Registrable</span></code></a></p>
<p>A <code class="docutils literal notranslate"><span class="pre">Tokenizer</span></code> splits strings of text into tokens.  Typically, this either splits text into
word tokens or character tokens, and those are the two tokenizer subclasses we have implemented
here, though you could imagine wanting to do other kinds of tokenization for structured or
other inputs.</p>
<p>As part of tokenization, concrete implementations of this API will also handle stemming,
stopword filtering, adding start and end tokens, or other kinds of things you might want to do
to your tokens.  See the parameters to, e.g., <a class="reference internal" href="#allennlp.data.tokenizers.word_tokenizer.WordTokenizer" title="allennlp.data.tokenizers.word_tokenizer.WordTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">WordTokenizer</span></code></a>, or whichever tokenizer
you want to use.</p>
<p>If the base input to your model is words, you should use a <a class="reference internal" href="#allennlp.data.tokenizers.word_tokenizer.WordTokenizer" title="allennlp.data.tokenizers.word_tokenizer.WordTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">WordTokenizer</span></code></a>, even if
you also want to have a character-level encoder to get an additional vector for each word
token.  Splitting word tokens into character arrays is handled separately, in the
<code class="xref py py-class docutils literal notranslate"><span class="pre">token_representations.TokenRepresentation</span></code> class.</p>
<dl class="method">
<dt id="allennlp.data.tokenizers.tokenizer.Tokenizer.batch_tokenize">
<code class="sig-name descname">batch_tokenize</code><span class="sig-paren">(</span><em class="sig-param">self, texts: List[str]</em><span class="sig-paren">)</span> &#x2192; List[List[allennlp.data.tokenizers.token.Token]]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/tokenizer.py#L26-L34"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.tokenizer.Tokenizer.batch_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Batches together tokenization of several texts, in case that is faster for particular
tokenizers.</p>
<p>By default we just do this without batching.  Override this in your tokenizer if you have a
good way of doing batched computation.</p>
</dd></dl>

<dl class="attribute">
<dt id="allennlp.data.tokenizers.tokenizer.Tokenizer.default_implementation">
<code class="sig-name descname">default_implementation</code><em class="property">: str</em><em class="property"> = 'word'</em><a class="headerlink" href="#allennlp.data.tokenizers.tokenizer.Tokenizer.default_implementation" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="allennlp.data.tokenizers.tokenizer.Tokenizer.tokenize">
<code class="sig-name descname">tokenize</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">text: str</em><span class="sig-paren">)</span> &#x2192; List[allennlp.data.tokenizers.token.Token]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/tokenizer.py#L36-L44"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.tokenizer.Tokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Actually implements splitting words into tokens.</p>
<dl class="field-list">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><dl>
<dt><strong>tokens</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">List[Token]</span></code></span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<span class="target" id="module-allennlp.data.tokenizers.word_tokenizer"><span id="word-tokenizer"></span></span><dl class="class">
<dt id="allennlp.data.tokenizers.word_tokenizer.WordTokenizer">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.tokenizers.word_tokenizer.</code><code class="sig-name descname">WordTokenizer</code><span class="sig-paren">(</span><em class="sig-param">word_splitter: allennlp.data.tokenizers.word_splitter.WordSplitter = None</em>, <em class="sig-param">word_filter: allennlp.data.tokenizers.word_filter.WordFilter = &lt;allennlp.data.tokenizers.word_filter.PassThroughWordFilter object&gt;</em>, <em class="sig-param">word_stemmer: allennlp.data.tokenizers.word_stemmer.WordStemmer = &lt;allennlp.data.tokenizers.word_stemmer.PassThroughWordStemmer object&gt;</em>, <em class="sig-param">start_tokens: List[str] = None</em>, <em class="sig-param">end_tokens: List[str] = None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_tokenizer.py#L13-L76"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_tokenizer.WordTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.tokenizers.tokenizer.Tokenizer" title="allennlp.data.tokenizers.tokenizer.Tokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.data.tokenizers.tokenizer.Tokenizer</span></code></a></p>
<p>A <code class="docutils literal notranslate"><span class="pre">WordTokenizer</span></code> handles the splitting of strings into words as well as any desired
post-processing (e.g., stemming, filtering, etc.).  Note that we leave one particular piece of
post-processing for later: the decision of whether or not to lowercase the token.  This is for
two reasons: (1) if you want to make two different casing decisions for whatever reason, you
won’t have to run the tokenizer twice, and more importantly (2) if you want to lowercase words
for your word embedding, but retain capitalization in a character-level representation, we need
to retain the capitalization here.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>word_splitter</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">WordSplitter</span></code>, optional</span></dt><dd><p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">WordSplitter</span></code> to use for splitting text strings into word tokens.  The default
is to use the <code class="docutils literal notranslate"><span class="pre">SpacyWordSplitter</span></code> with default parameters.</p>
</dd>
<dt><strong>word_filter</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">WordFilter</span></code>, optional</span></dt><dd><p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">WordFilter</span></code> to use for, e.g., removing stopwords.  Default is to do no
filtering.</p>
</dd>
<dt><strong>word_stemmer</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">WordStemmer</span></code>, optional</span></dt><dd><p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">WordStemmer</span></code> to use.  Default is no stemming.</p>
</dd>
<dt><strong>start_tokens</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">List[str]</span></code>, optional</span></dt><dd><p>If given, these tokens will be added to the beginning of every string we tokenize.</p>
</dd>
<dt><strong>end_tokens</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">List[str]</span></code>, optional</span></dt><dd><p>If given, these tokens will be added to the end of every string we tokenize.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="allennlp.data.tokenizers.word_tokenizer.WordTokenizer.batch_tokenize">
<code class="sig-name descname">batch_tokenize</code><span class="sig-paren">(</span><em class="sig-param">self, texts: List[str]</em><span class="sig-paren">)</span> &#x2192; List[List[allennlp.data.tokenizers.token.Token]]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_tokenizer.py#L64-L67"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_tokenizer.WordTokenizer.batch_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Batches together tokenization of several texts, in case that is faster for particular
tokenizers.</p>
<p>By default we just do this without batching.  Override this in your tokenizer if you have a
good way of doing batched computation.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.tokenizers.word_tokenizer.WordTokenizer.tokenize">
<code class="sig-name descname">tokenize</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">text: str</em><span class="sig-paren">)</span> &#x2192; List[allennlp.data.tokenizers.token.Token]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_tokenizer.py#L53-L62"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_tokenizer.WordTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Does whatever processing is required to convert a string of text into a sequence of tokens.</p>
<p>At a minimum, this uses a <code class="docutils literal notranslate"><span class="pre">WordSplitter</span></code> to split words into text.  It may also do
stemming or stopword removal, depending on the parameters given to the constructor.</p>
</dd></dl>

</dd></dl>

<span class="target" id="module-allennlp.data.tokenizers.character_tokenizer"><span id="character-tokenizer"></span></span><dl class="class">
<dt id="allennlp.data.tokenizers.character_tokenizer.CharacterTokenizer">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.tokenizers.character_tokenizer.</code><code class="sig-name descname">CharacterTokenizer</code><span class="sig-paren">(</span><em class="sig-param">byte_encoding: str = None</em>, <em class="sig-param">lowercase_characters: bool = False</em>, <em class="sig-param">start_tokens: List[str] = None</em>, <em class="sig-param">end_tokens: List[str] = None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/character_tokenizer.py#L10-L76"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.character_tokenizer.CharacterTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.tokenizers.tokenizer.Tokenizer" title="allennlp.data.tokenizers.tokenizer.Tokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.data.tokenizers.tokenizer.Tokenizer</span></code></a></p>
<p>A <code class="docutils literal notranslate"><span class="pre">CharacterTokenizer</span></code> splits strings into character tokens.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>byte_encoding</strong><span class="classifier">str, optional (default=``None``)</span></dt><dd><p>If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, we will use this encoding to encode the string as bytes, and use the byte
sequence as characters, instead of the unicode characters in the python string.  E.g., the
character ‘á’ would be a single token if this option is <code class="docutils literal notranslate"><span class="pre">None</span></code>, but it would be two
tokens if this option is set to <code class="docutils literal notranslate"><span class="pre">&quot;utf-8&quot;</span></code>.</p>
<p>If this is not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">tokenize</span></code> will return a <code class="docutils literal notranslate"><span class="pre">List[int]</span></code> instead of a
<code class="docutils literal notranslate"><span class="pre">List[str]</span></code>, and we will bypass the vocabulary in the <code class="docutils literal notranslate"><span class="pre">TokenIndexer</span></code>.</p>
</dd>
<dt><strong>lowercase_characters</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">bool</span></code>, optional (default=``False``)</span></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, we will lowercase all of the characters in the text before doing any other
operation.  You probably do not want to do this, as character vocabularies are generally
not very large to begin with, but it’s an option if you really want it.</p>
</dd>
<dt><strong>start_tokens</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">List[str]</span></code>, optional</span></dt><dd><p>If given, these tokens will be added to the beginning of every string we tokenize.  If
using byte encoding, this should actually be a <code class="docutils literal notranslate"><span class="pre">List[int]</span></code>, not a <code class="docutils literal notranslate"><span class="pre">List[str]</span></code>.</p>
</dd>
<dt><strong>end_tokens</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">List[str]</span></code>, optional</span></dt><dd><p>If given, these tokens will be added to the end of every string we tokenize.  If using byte
encoding, this should actually be a <code class="docutils literal notranslate"><span class="pre">List[int]</span></code>, not a <code class="docutils literal notranslate"><span class="pre">List[str]</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="allennlp.data.tokenizers.character_tokenizer.CharacterTokenizer.tokenize">
<code class="sig-name descname">tokenize</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">text: str</em><span class="sig-paren">)</span> &#x2192; List[allennlp.data.tokenizers.token.Token]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/character_tokenizer.py#L49-L71"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.character_tokenizer.CharacterTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Actually implements splitting words into tokens.</p>
<dl class="field-list">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><dl>
<dt><strong>tokens</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">List[Token]</span></code></span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<span class="target" id="module-allennlp.data.tokenizers.pretrained_transformer_tokenizer"><span id="pretrained-transformer-tokenizer"></span></span><dl class="class">
<dt id="allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.tokenizers.pretrained_transformer_tokenizer.</code><code class="sig-name descname">PretrainedTransformerTokenizer</code><span class="sig-paren">(</span><em class="sig-param">model_name: str</em>, <em class="sig-param">do_lowercase: bool</em>, <em class="sig-param">start_tokens: List[str] = None</em>, <em class="sig-param">end_tokens: List[str] = None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/pretrained_transformer_tokenizer.py#L14-L57"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.tokenizers.tokenizer.Tokenizer" title="allennlp.data.tokenizers.tokenizer.Tokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.data.tokenizers.tokenizer.Tokenizer</span></code></a></p>
<p>A <code class="docutils literal notranslate"><span class="pre">PretrainedTransformerTokenizer</span></code> uses a model from HuggingFace’s
<code class="docutils literal notranslate"><span class="pre">pytorch_transformers</span></code> library to tokenize some input text.  This often means wordpieces
(where <code class="docutils literal notranslate"><span class="pre">'AllenNLP</span> <span class="pre">is</span> <span class="pre">awesome'</span></code> might get split into <code class="docutils literal notranslate"><span class="pre">['Allen',</span> <span class="pre">'##NL',</span> <span class="pre">'##P',</span> <span class="pre">'is',</span>
<span class="pre">'awesome']</span></code>), but it could also use byte-pair encoding, or some other tokenization, depending
on the pretrained model that you’re using.</p>
<p>We take a model name as an input parameter, which we will pass to
<code class="docutils literal notranslate"><span class="pre">AutoTokenizer.from_pretrained</span></code>.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>model_name</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">str</span></code></span></dt><dd><p>The name of the pretrained wordpiece tokenizer to use.</p>
</dd>
<dt><strong>start_tokens</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">List[str]</span></code>, optional</span></dt><dd><p>If given, these tokens will be added to the beginning of every string we tokenize.  We try
to be a little bit smart about defaults here - e.g., if your model name contains <code class="docutils literal notranslate"><span class="pre">bert</span></code>,
we by default add <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> at the beginning and <code class="docutils literal notranslate"><span class="pre">[SEP]</span></code> at the end.</p>
</dd>
<dt><strong>end_tokens</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">List[str]</span></code>, optional</span></dt><dd><p>If given, these tokens will be added to the end of every string we tokenize.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer.tokenize">
<code class="sig-name descname">tokenize</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">text: str</em><span class="sig-paren">)</span> &#x2192; List[allennlp.data.tokenizers.token.Token]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/pretrained_transformer_tokenizer.py#L52-L57"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Actually implements splitting words into tokens.</p>
<dl class="field-list">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><dl>
<dt><strong>tokens</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">List[Token]</span></code></span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<span class="target" id="module-allennlp.data.tokenizers.word_filter"><span id="word-filter"></span></span><dl class="class">
<dt id="allennlp.data.tokenizers.word_filter.PassThroughWordFilter">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.tokenizers.word_filter.</code><code class="sig-name descname">PassThroughWordFilter</code><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_filter.py#L30-L36"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_filter.PassThroughWordFilter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.tokenizers.word_filter.WordFilter" title="allennlp.data.tokenizers.word_filter.WordFilter"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.data.tokenizers.word_filter.WordFilter</span></code></a></p>
<p>Does not filter words; it’s a no-op.  This is the default word filter.</p>
<dl class="method">
<dt id="allennlp.data.tokenizers.word_filter.PassThroughWordFilter.filter_words">
<code class="sig-name descname">filter_words</code><span class="sig-paren">(</span><em class="sig-param">self, words: List[allennlp.data.tokenizers.token.Token]</em><span class="sig-paren">)</span> &#x2192; List[allennlp.data.tokenizers.token.Token]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_filter.py#L34-L36"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_filter.PassThroughWordFilter.filter_words" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a filtered list of words.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="allennlp.data.tokenizers.word_filter.RegexFilter">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.tokenizers.word_filter.</code><code class="sig-name descname">RegexFilter</code><span class="sig-paren">(</span><em class="sig-param">patterns: List[str]</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_filter.py#L40-L58"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_filter.RegexFilter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.tokenizers.word_filter.WordFilter" title="allennlp.data.tokenizers.word_filter.WordFilter"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.data.tokenizers.word_filter.WordFilter</span></code></a></p>
<p>A <code class="docutils literal notranslate"><span class="pre">RegexFilter</span></code> removes words according to supplied regex patterns.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>patterns</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">List[str]</span></code></span></dt><dd><p>Words matching these regex patterns will be removed as stopwords.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="allennlp.data.tokenizers.word_filter.RegexFilter.filter_words">
<code class="sig-name descname">filter_words</code><span class="sig-paren">(</span><em class="sig-param">self, words: List[allennlp.data.tokenizers.token.Token]</em><span class="sig-paren">)</span> &#x2192; List[allennlp.data.tokenizers.token.Token]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_filter.py#L54-L58"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_filter.RegexFilter.filter_words" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a filtered list of words.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="allennlp.data.tokenizers.word_filter.StopwordFilter">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.tokenizers.word_filter.</code><code class="sig-name descname">StopwordFilter</code><span class="sig-paren">(</span><em class="sig-param">stopword_file: str = None</em>, <em class="sig-param">tokens_to_add: List[str] = None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_filter.py#L62-L88"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_filter.StopwordFilter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.tokenizers.word_filter.WordFilter" title="allennlp.data.tokenizers.word_filter.WordFilter"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.data.tokenizers.word_filter.WordFilter</span></code></a></p>
<p>A <code class="docutils literal notranslate"><span class="pre">StopwordFilter</span></code> uses a list of stopwords to filter.
If no file is specified, spaCy’s default list of English stopwords is used.
Words and stopwords are lowercased for comparison.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>stopword_file</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">str</span></code>, optional</span></dt><dd><p>A filename containing stopwords to filter out (file format is one stopword per line).</p>
</dd>
<dt><strong>tokens_to_add</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">List[str]</span></code>, optional</span></dt><dd><p>A list of tokens to additionally filter out.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="allennlp.data.tokenizers.word_filter.StopwordFilter.filter_words">
<code class="sig-name descname">filter_words</code><span class="sig-paren">(</span><em class="sig-param">self, words: List[allennlp.data.tokenizers.token.Token]</em><span class="sig-paren">)</span> &#x2192; List[allennlp.data.tokenizers.token.Token]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_filter.py#L86-L88"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_filter.StopwordFilter.filter_words" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a filtered list of words.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="allennlp.data.tokenizers.word_filter.WordFilter">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.tokenizers.word_filter.</code><code class="sig-name descname">WordFilter</code><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_filter.py#L12-L26"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_filter.WordFilter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="allennlp.common.registrable.html#allennlp.common.registrable.Registrable" title="allennlp.common.registrable.Registrable"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.common.registrable.Registrable</span></code></a></p>
<p>A <code class="docutils literal notranslate"><span class="pre">WordFilter</span></code> removes words from a token list.  Typically, this is for stopword removal,
though you could feasibly use it for more domain-specific removal if you want.</p>
<p>Word removal happens <cite>before</cite> stemming, so keep that in mind if you’re designing a list of
words to be removed.</p>
<dl class="attribute">
<dt id="allennlp.data.tokenizers.word_filter.WordFilter.default_implementation">
<code class="sig-name descname">default_implementation</code><em class="property">: str</em><em class="property"> = 'pass_through'</em><a class="headerlink" href="#allennlp.data.tokenizers.word_filter.WordFilter.default_implementation" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="allennlp.data.tokenizers.word_filter.WordFilter.filter_words">
<code class="sig-name descname">filter_words</code><span class="sig-paren">(</span><em class="sig-param">self, words: List[allennlp.data.tokenizers.token.Token]</em><span class="sig-paren">)</span> &#x2192; List[allennlp.data.tokenizers.token.Token]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_filter.py#L22-L26"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_filter.WordFilter.filter_words" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a filtered list of words.</p>
</dd></dl>

</dd></dl>

<span class="target" id="module-allennlp.data.tokenizers.word_splitter"><span id="word-splitter"></span></span><dl class="class">
<dt id="allennlp.data.tokenizers.word_splitter.BertBasicWordSplitter">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.tokenizers.word_splitter.</code><code class="sig-name descname">BertBasicWordSplitter</code><span class="sig-paren">(</span><em class="sig-param">do_lower_case: bool = True</em>, <em class="sig-param">never_split: Optional[List[str]] = None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_splitter.py#L231-L248"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_splitter.BertBasicWordSplitter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.tokenizers.word_splitter.WordSplitter" title="allennlp.data.tokenizers.word_splitter.WordSplitter"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.data.tokenizers.word_splitter.WordSplitter</span></code></a></p>
<p>The <code class="docutils literal notranslate"><span class="pre">BasicWordSplitter</span></code> from the BERT implementation.
This is used to split a sentence into words.
Then the <code class="docutils literal notranslate"><span class="pre">BertTokenIndexer</span></code> converts each word into wordpieces.</p>
<dl class="method">
<dt id="allennlp.data.tokenizers.word_splitter.BertBasicWordSplitter.split_words">
<code class="sig-name descname">split_words</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">sentence: str</em><span class="sig-paren">)</span> &#x2192; List[allennlp.data.tokenizers.token.Token]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_splitter.py#L246-L248"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_splitter.BertBasicWordSplitter.split_words" title="Permalink to this definition">¶</a></dt>
<dd><p>Splits <code class="docutils literal notranslate"><span class="pre">sentence</span></code> into a list of <code class="xref py py-class docutils literal notranslate"><span class="pre">Token</span></code> objects.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="allennlp.data.tokenizers.word_splitter.JustSpacesWordSplitter">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.tokenizers.word_splitter.</code><code class="sig-name descname">JustSpacesWordSplitter</code><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_splitter.py#L119-L131"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_splitter.JustSpacesWordSplitter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.tokenizers.word_splitter.WordSplitter" title="allennlp.data.tokenizers.word_splitter.WordSplitter"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.data.tokenizers.word_splitter.WordSplitter</span></code></a></p>
<p>A <code class="docutils literal notranslate"><span class="pre">WordSplitter</span></code> that assumes you’ve already done your own tokenization somehow and have
separated the tokens by spaces.  We just split the input string on whitespace and return the
resulting list.  We use a somewhat odd name here to avoid coming too close to the more
commonly used <code class="docutils literal notranslate"><span class="pre">SpacyWordSplitter</span></code>.</p>
<p>Note that we use <code class="docutils literal notranslate"><span class="pre">sentence.split()</span></code>, which means that the amount of whitespace between the
tokens does not matter.  This will never result in spaces being included as tokens.</p>
<dl class="method">
<dt id="allennlp.data.tokenizers.word_splitter.JustSpacesWordSplitter.split_words">
<code class="sig-name descname">split_words</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">sentence: str</em><span class="sig-paren">)</span> &#x2192; List[allennlp.data.tokenizers.token.Token]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_splitter.py#L129-L131"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_splitter.JustSpacesWordSplitter.split_words" title="Permalink to this definition">¶</a></dt>
<dd><p>Splits <code class="docutils literal notranslate"><span class="pre">sentence</span></code> into a list of <code class="xref py py-class docutils literal notranslate"><span class="pre">Token</span></code> objects.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="allennlp.data.tokenizers.word_splitter.LettersDigitsWordSplitter">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.tokenizers.word_splitter.</code><code class="sig-name descname">LettersDigitsWordSplitter</code><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_splitter.py#L105-L115"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_splitter.LettersDigitsWordSplitter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.tokenizers.word_splitter.WordSplitter" title="allennlp.data.tokenizers.word_splitter.WordSplitter"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.data.tokenizers.word_splitter.WordSplitter</span></code></a></p>
<p>A <code class="docutils literal notranslate"><span class="pre">WordSplitter</span></code> which keeps runs of (unicode) letters and runs of digits together, while
every other non-whitespace character becomes a separate word.</p>
<dl class="method">
<dt id="allennlp.data.tokenizers.word_splitter.LettersDigitsWordSplitter.split_words">
<code class="sig-name descname">split_words</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">sentence: str</em><span class="sig-paren">)</span> &#x2192; List[allennlp.data.tokenizers.token.Token]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_splitter.py#L110-L115"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_splitter.LettersDigitsWordSplitter.split_words" title="Permalink to this definition">¶</a></dt>
<dd><p>Splits <code class="docutils literal notranslate"><span class="pre">sentence</span></code> into a list of <code class="xref py py-class docutils literal notranslate"><span class="pre">Token</span></code> objects.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="allennlp.data.tokenizers.word_splitter.OpenAISplitter">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.tokenizers.word_splitter.</code><code class="sig-name descname">OpenAISplitter</code><span class="sig-paren">(</span><em class="sig-param">language: str = 'en_core_web_sm'</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_splitter.py#L207-L227"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_splitter.OpenAISplitter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.tokenizers.word_splitter.WordSplitter" title="allennlp.data.tokenizers.word_splitter.WordSplitter"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.data.tokenizers.word_splitter.WordSplitter</span></code></a></p>
<p>For OpenAI transformer</p>
<dl class="method">
<dt id="allennlp.data.tokenizers.word_splitter.OpenAISplitter.batch_split_words">
<code class="sig-name descname">batch_split_words</code><span class="sig-paren">(</span><em class="sig-param">self, sentences: List[str]</em><span class="sig-paren">)</span> &#x2192; List[List[allennlp.data.tokenizers.token.Token]]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_splitter.py#L218-L222"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_splitter.OpenAISplitter.batch_split_words" title="Permalink to this definition">¶</a></dt>
<dd><p>Spacy needs to do batch processing, or it can be really slow.  This method lets you take
advantage of that if you want.  Default implementation is to just iterate of the sentences
and call <code class="docutils literal notranslate"><span class="pre">split_words</span></code>, but the <code class="docutils literal notranslate"><span class="pre">SpacyWordSplitter</span></code> will actually do batched
processing.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.tokenizers.word_splitter.OpenAISplitter.split_words">
<code class="sig-name descname">split_words</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">sentence: str</em><span class="sig-paren">)</span> &#x2192; List[allennlp.data.tokenizers.token.Token]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_splitter.py#L224-L227"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_splitter.OpenAISplitter.split_words" title="Permalink to this definition">¶</a></dt>
<dd><p>Splits <code class="docutils literal notranslate"><span class="pre">sentence</span></code> into a list of <code class="xref py py-class docutils literal notranslate"><span class="pre">Token</span></code> objects.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="allennlp.data.tokenizers.word_splitter.SimpleWordSplitter">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.tokenizers.word_splitter.</code><code class="sig-name descname">SimpleWordSplitter</code><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_splitter.py#L43-L101"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_splitter.SimpleWordSplitter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.tokenizers.word_splitter.WordSplitter" title="allennlp.data.tokenizers.word_splitter.WordSplitter"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.data.tokenizers.word_splitter.WordSplitter</span></code></a></p>
<p>Does really simple tokenization.  NLTK was too slow, so we wrote our own simple tokenizer
instead.  This just does an initial split(), followed by some heuristic filtering of each
whitespace-delimited token, separating contractions and punctuation.  We assume lower-cased,
reasonably well-formed English sentences as input.</p>
<dl class="method">
<dt id="allennlp.data.tokenizers.word_splitter.SimpleWordSplitter.split_words">
<code class="sig-name descname">split_words</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">sentence: str</em><span class="sig-paren">)</span> &#x2192; List[allennlp.data.tokenizers.token.Token]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_splitter.py#L58-L98"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_splitter.SimpleWordSplitter.split_words" title="Permalink to this definition">¶</a></dt>
<dd><p>Splits a sentence into word tokens.  We handle four kinds of things: words with punctuation
that should be ignored as a special case (Mr. Mrs., etc.), contractions/genitives (isn’t,
don’t, Matt’s), and beginning and ending punctuation (“antennagate”, (parentheticals), and
such.).</p>
<p>The basic outline is to split on whitespace, then check each of these cases.  First, we
strip off beginning punctuation, then strip off ending punctuation, then strip off
contractions.  When we strip something off the beginning of a word, we can add it to the
list of tokens immediately.  When we strip it off the end, we have to save it to be added
to after the word itself has been added.  Before stripping off any part of a token, we
first check to be sure the token isn’t in our list of special cases.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="allennlp.data.tokenizers.word_splitter.SpacyWordSplitter">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.tokenizers.word_splitter.</code><code class="sig-name descname">SpacyWordSplitter</code><span class="sig-paren">(</span><em class="sig-param">language: str = 'en_core_web_sm'</em>, <em class="sig-param">pos_tags: bool = False</em>, <em class="sig-param">parse: bool = False</em>, <em class="sig-param">ner: bool = False</em>, <em class="sig-param">keep_spacy_tokens: bool = False</em>, <em class="sig-param">split_on_spaces: bool = False</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_splitter.py#L159-L203"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_splitter.SpacyWordSplitter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.tokenizers.word_splitter.WordSplitter" title="allennlp.data.tokenizers.word_splitter.WordSplitter"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.data.tokenizers.word_splitter.WordSplitter</span></code></a></p>
<p>A <code class="docutils literal notranslate"><span class="pre">WordSplitter</span></code> that uses spaCy’s tokenizer.  It’s fast and reasonable - this is the
recommended <code class="docutils literal notranslate"><span class="pre">WordSplitter</span></code>. By default it will return allennlp Tokens,
which are small, efficient NamedTuples (and are serializable). If you want
to keep the original spaCy tokens, pass keep_spacy_tokens=True.</p>
<dl class="method">
<dt id="allennlp.data.tokenizers.word_splitter.SpacyWordSplitter.batch_split_words">
<code class="sig-name descname">batch_split_words</code><span class="sig-paren">(</span><em class="sig-param">self, sentences: List[str]</em><span class="sig-paren">)</span> &#x2192; List[List[allennlp.data.tokenizers.token.Token]]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_splitter.py#L195-L198"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_splitter.SpacyWordSplitter.batch_split_words" title="Permalink to this definition">¶</a></dt>
<dd><p>Spacy needs to do batch processing, or it can be really slow.  This method lets you take
advantage of that if you want.  Default implementation is to just iterate of the sentences
and call <code class="docutils literal notranslate"><span class="pre">split_words</span></code>, but the <code class="docutils literal notranslate"><span class="pre">SpacyWordSplitter</span></code> will actually do batched
processing.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.tokenizers.word_splitter.SpacyWordSplitter.split_words">
<code class="sig-name descname">split_words</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">sentence: str</em><span class="sig-paren">)</span> &#x2192; List[allennlp.data.tokenizers.token.Token]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_splitter.py#L200-L203"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_splitter.SpacyWordSplitter.split_words" title="Permalink to this definition">¶</a></dt>
<dd><p>Splits <code class="docutils literal notranslate"><span class="pre">sentence</span></code> into a list of <code class="xref py py-class docutils literal notranslate"><span class="pre">Token</span></code> objects.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="allennlp.data.tokenizers.word_splitter.WhitespaceTokenizer">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.tokenizers.word_splitter.</code><code class="sig-name descname">WhitespaceTokenizer</code><span class="sig-paren">(</span><em class="sig-param">vocab</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_splitter.py#L138-L155"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_splitter.WhitespaceTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Spacy doesn’t assume that text is tokenised. Sometimes this
is annoying, like when you have gold data which is pre-tokenised,
but Spacy’s tokenisation doesn’t match the gold. This can be used
as follows:
nlp = spacy.load(“en_core_web_md”)
# hack to replace tokenizer with a whitespace tokenizer
nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)
… use nlp(“here is some text”) as normal.</p>
</dd></dl>

<dl class="class">
<dt id="allennlp.data.tokenizers.word_splitter.WordSplitter">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.tokenizers.word_splitter.</code><code class="sig-name descname">WordSplitter</code><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_splitter.py#L17-L39"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_splitter.WordSplitter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="allennlp.common.registrable.html#allennlp.common.registrable.Registrable" title="allennlp.common.registrable.Registrable"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.common.registrable.Registrable</span></code></a></p>
<p>A <code class="docutils literal notranslate"><span class="pre">WordSplitter</span></code> splits strings into words.  This is typically called a “tokenizer” in NLP,
because splitting strings into characters is trivial, but we use <code class="docutils literal notranslate"><span class="pre">Tokenizer</span></code> to refer to the
higher-level object that splits strings into tokens (which could just be character tokens).
So, we’re using “word splitter” here for this.</p>
<dl class="method">
<dt id="allennlp.data.tokenizers.word_splitter.WordSplitter.batch_split_words">
<code class="sig-name descname">batch_split_words</code><span class="sig-paren">(</span><em class="sig-param">self, sentences: List[str]</em><span class="sig-paren">)</span> &#x2192; List[List[allennlp.data.tokenizers.token.Token]]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_splitter.py#L26-L33"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_splitter.WordSplitter.batch_split_words" title="Permalink to this definition">¶</a></dt>
<dd><p>Spacy needs to do batch processing, or it can be really slow.  This method lets you take
advantage of that if you want.  Default implementation is to just iterate of the sentences
and call <code class="docutils literal notranslate"><span class="pre">split_words</span></code>, but the <code class="docutils literal notranslate"><span class="pre">SpacyWordSplitter</span></code> will actually do batched
processing.</p>
</dd></dl>

<dl class="attribute">
<dt id="allennlp.data.tokenizers.word_splitter.WordSplitter.default_implementation">
<code class="sig-name descname">default_implementation</code><em class="property">: str</em><em class="property"> = 'spacy'</em><a class="headerlink" href="#allennlp.data.tokenizers.word_splitter.WordSplitter.default_implementation" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="allennlp.data.tokenizers.word_splitter.WordSplitter.split_words">
<code class="sig-name descname">split_words</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">sentence: str</em><span class="sig-paren">)</span> &#x2192; List[allennlp.data.tokenizers.token.Token]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_splitter.py#L35-L39"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_splitter.WordSplitter.split_words" title="Permalink to this definition">¶</a></dt>
<dd><p>Splits <code class="docutils literal notranslate"><span class="pre">sentence</span></code> into a list of <code class="xref py py-class docutils literal notranslate"><span class="pre">Token</span></code> objects.</p>
</dd></dl>

</dd></dl>

<span class="target" id="module-allennlp.data.tokenizers.word_stemmer"><span id="word-stemmer"></span></span><dl class="class">
<dt id="allennlp.data.tokenizers.word_stemmer.PassThroughWordStemmer">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.tokenizers.word_stemmer.</code><code class="sig-name descname">PassThroughWordStemmer</code><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_stemmer.py#L28-L34"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_stemmer.PassThroughWordStemmer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.tokenizers.word_stemmer.WordStemmer" title="allennlp.data.tokenizers.word_stemmer.WordStemmer"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.data.tokenizers.word_stemmer.WordStemmer</span></code></a></p>
<p>Does not stem words; it’s a no-op.  This is the default word stemmer.</p>
<dl class="method">
<dt id="allennlp.data.tokenizers.word_stemmer.PassThroughWordStemmer.stem_word">
<code class="sig-name descname">stem_word</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">word: allennlp.data.tokenizers.token.Token</em><span class="sig-paren">)</span> &#x2192; allennlp.data.tokenizers.token.Token<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_stemmer.py#L32-L34"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_stemmer.PassThroughWordStemmer.stem_word" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <code class="docutils literal notranslate"><span class="pre">Token</span></code> with <code class="docutils literal notranslate"><span class="pre">word.text</span></code> replaced by a stemmed word.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="allennlp.data.tokenizers.word_stemmer.PorterStemmer">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.tokenizers.word_stemmer.</code><code class="sig-name descname">PorterStemmer</code><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_stemmer.py#L38-L55"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_stemmer.PorterStemmer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.tokenizers.word_stemmer.WordStemmer" title="allennlp.data.tokenizers.word_stemmer.WordStemmer"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.data.tokenizers.word_stemmer.WordStemmer</span></code></a></p>
<p>Uses NLTK’s PorterStemmer to stem words.</p>
<dl class="method">
<dt id="allennlp.data.tokenizers.word_stemmer.PorterStemmer.stem_word">
<code class="sig-name descname">stem_word</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">word: allennlp.data.tokenizers.token.Token</em><span class="sig-paren">)</span> &#x2192; allennlp.data.tokenizers.token.Token<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_stemmer.py#L45-L55"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_stemmer.PorterStemmer.stem_word" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <code class="docutils literal notranslate"><span class="pre">Token</span></code> with <code class="docutils literal notranslate"><span class="pre">word.text</span></code> replaced by a stemmed word.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="allennlp.data.tokenizers.word_stemmer.WordStemmer">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.tokenizers.word_stemmer.</code><code class="sig-name descname">WordStemmer</code><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_stemmer.py#L8-L24"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_stemmer.WordStemmer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="allennlp.common.registrable.html#allennlp.common.registrable.Registrable" title="allennlp.common.registrable.Registrable"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.common.registrable.Registrable</span></code></a></p>
<p>A <code class="docutils literal notranslate"><span class="pre">WordStemmer</span></code> lemmatizes words.  This means that we map words to their root form, so that,
e.g., “have”, “has”, and “had” all have the same internal representation.</p>
<p>You should think carefully about whether and how much stemming you want in your model.  Kind of
the whole point of using word embeddings is so that you don’t have to do this, but in a highly
inflected language, or in a low-data setting, you might need it anyway.  The default
<code class="docutils literal notranslate"><span class="pre">WordStemmer</span></code> does nothing, just returning the work token as-is.</p>
<dl class="attribute">
<dt id="allennlp.data.tokenizers.word_stemmer.WordStemmer.default_implementation">
<code class="sig-name descname">default_implementation</code><em class="property">: str</em><em class="property"> = 'pass_through'</em><a class="headerlink" href="#allennlp.data.tokenizers.word_stemmer.WordStemmer.default_implementation" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="allennlp.data.tokenizers.word_stemmer.WordStemmer.stem_word">
<code class="sig-name descname">stem_word</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">word: allennlp.data.tokenizers.token.Token</em><span class="sig-paren">)</span> &#x2192; allennlp.data.tokenizers.token.Token<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_stemmer.py#L20-L24"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.word_stemmer.WordStemmer.stem_word" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <code class="docutils literal notranslate"><span class="pre">Token</span></code> with <code class="docutils literal notranslate"><span class="pre">word.text</span></code> replaced by a stemmed word.</p>
</dd></dl>

</dd></dl>

<span class="target" id="module-allennlp.data.tokenizers.sentence_splitter"><span id="sentence-splitter"></span></span><dl class="class">
<dt id="allennlp.data.tokenizers.sentence_splitter.SentenceSplitter">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.tokenizers.sentence_splitter.</code><code class="sig-name descname">SentenceSplitter</code><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/sentence_splitter.py#L10-L26"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.sentence_splitter.SentenceSplitter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="allennlp.common.registrable.html#allennlp.common.registrable.Registrable" title="allennlp.common.registrable.Registrable"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.common.registrable.Registrable</span></code></a></p>
<p>A <code class="docutils literal notranslate"><span class="pre">SentenceSplitter</span></code> splits strings into sentences.</p>
<dl class="method">
<dt id="allennlp.data.tokenizers.sentence_splitter.SentenceSplitter.batch_split_sentences">
<code class="sig-name descname">batch_split_sentences</code><span class="sig-paren">(</span><em class="sig-param">self, texts: List[str]</em><span class="sig-paren">)</span> &#x2192; List[List[str]]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/sentence_splitter.py#L22-L26"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.sentence_splitter.SentenceSplitter.batch_split_sentences" title="Permalink to this definition">¶</a></dt>
<dd><p>Default implementation is to just iterate over the texts and call <code class="docutils literal notranslate"><span class="pre">split_sentences</span></code>.</p>
</dd></dl>

<dl class="attribute">
<dt id="allennlp.data.tokenizers.sentence_splitter.SentenceSplitter.default_implementation">
<code class="sig-name descname">default_implementation</code><em class="property">: str</em><em class="property"> = 'spacy'</em><a class="headerlink" href="#allennlp.data.tokenizers.sentence_splitter.SentenceSplitter.default_implementation" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="allennlp.data.tokenizers.sentence_splitter.SentenceSplitter.split_sentences">
<code class="sig-name descname">split_sentences</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">text: str</em><span class="sig-paren">)</span> &#x2192; List[str]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/sentence_splitter.py#L16-L20"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.sentence_splitter.SentenceSplitter.split_sentences" title="Permalink to this definition">¶</a></dt>
<dd><p>Splits a <code class="docutils literal notranslate"><span class="pre">text</span></code> <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code> paragraph into a list of <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, where each is a sentence.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="allennlp.data.tokenizers.sentence_splitter.SpacySentenceSplitter">
<em class="property">class </em><code class="sig-prename descclassname">allennlp.data.tokenizers.sentence_splitter.</code><code class="sig-name descname">SpacySentenceSplitter</code><span class="sig-paren">(</span><em class="sig-param">language: str = 'en_core_web_sm'</em>, <em class="sig-param">rule_based: bool = False</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/sentence_splitter.py#L30-L64"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.sentence_splitter.SpacySentenceSplitter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.tokenizers.sentence_splitter.SentenceSplitter" title="allennlp.data.tokenizers.sentence_splitter.SentenceSplitter"><code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.data.tokenizers.sentence_splitter.SentenceSplitter</span></code></a></p>
<p>A <code class="docutils literal notranslate"><span class="pre">SentenceSplitter</span></code> that uses spaCy’s built-in sentence boundary detection.</p>
<p>Spacy’s default sentence splitter uses a dependency parse to detect sentence boundaries, so
it is slow, but accurate.</p>
<p>Another option is to use rule-based sentence boundary detection. It’s fast and has a small memory footprint,
since it uses punctuation to detect sentence boundaries. This can be activated with the <cite>rule_based</cite> flag.</p>
<p>By default, <code class="docutils literal notranslate"><span class="pre">SpacySentenceSplitter</span></code> calls the default spacy boundary detector.</p>
<dl class="method">
<dt id="allennlp.data.tokenizers.sentence_splitter.SpacySentenceSplitter.batch_split_sentences">
<code class="sig-name descname">batch_split_sentences</code><span class="sig-paren">(</span><em class="sig-param">self, texts: List[str]</em><span class="sig-paren">)</span> &#x2192; List[List[str]]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/sentence_splitter.py#L59-L64"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.sentence_splitter.SpacySentenceSplitter.batch_split_sentences" title="Permalink to this definition">¶</a></dt>
<dd><p>This method lets you take advantage of spacy’s batch processing.</p>
</dd></dl>

<dl class="method">
<dt id="allennlp.data.tokenizers.sentence_splitter.SpacySentenceSplitter.split_sentences">
<code class="sig-name descname">split_sentences</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">text: str</em><span class="sig-paren">)</span> &#x2192; List[str]<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/sentence_splitter.py#L55-L57"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.tokenizers.sentence_splitter.SpacySentenceSplitter.split_sentences" title="Permalink to this definition">¶</a></dt>
<dd><p>Splits a <code class="docutils literal notranslate"><span class="pre">text</span></code> <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code> paragraph into a list of <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, where each is a sentence.</p>
</dd></dl>

</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="allennlp.data.vocabulary.html" class="btn btn-neutral float-right" title="allennlp.data.vocabulary" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="allennlp.data.token_indexers.html" class="btn btn-neutral float-left" title="allennlp.data.token_indexers" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Allen Institute for Artificial Intelligence

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
  
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(400);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>


</body>
</html>