{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Officially supported AllenNLP models. \u2757\ufe0f To file an issue, please open a ticket on allenai/allennlp and tag it with \"Models\". \u2757\ufe0f In this README # About Tasks and components Pre-trained models Installing From PyPI From source Using Docker About # This repository contains the components - such as DatasetReader , Model , and Predictor classes - for applying AllenNLP to a wide variety of NLP tasks . It also provides an easy way to download and use pre-trained models that were trained with these components. Tasks and components # This is an overview of the tasks supported by the AllenNLP Models library along with the corresponding components provided, organized by category. For a more comprehensive overview, see the AllenNLP Models documentation or the Paperswithcode page . Classification Classification tasks involve predicting one or more labels from a predefined set to assign to each input. Examples include Sentiment Analysis, where the labels might be {\"positive\", \"negative\", \"neutral\"} , and Binary Question Answering, where the labels are {True, False} . \ud83d\udee0 Components provided: Dataset readers for various datasets, including BoolQ and SST , as well as a Biattentive Classification Network model. Coreference Resolution Coreference resolution tasks require finding all of the expressions in a text that refer to common entities. See nlp.stanford.edu/projects/coref for more details. \ud83d\udee0 Components provided: A general Coref model and several dataset readers. Generation This is a broad category for tasks such as Summarization that involve generating unstructered and often variable-length text. \ud83d\udee0 Components provided: Several Seq2Seq models such a Bart , CopyNet , and a general Composed Seq2Seq , along with corresponding dataset readers. Language Modeling Language modeling tasks involve learning a probability distribution over sequences of tokens. \ud83d\udee0 Components provided: Several language model implementations, such as a Masked LM and a Next Token LM . Multiple Choice Multiple choice tasks require selecting a correct choice among alternatives, where the set of choices may be different for each input. This differs from classification where the set of choices is predefined and fixed across all inputs. \ud83d\udee0 Components provided: A transformer-based multiple choice model and a handful of dataset readers for specific datasets. Pair Classification Pair classification is another broad category that contains tasks such as Textual Entailment, which is to determine whether, for a pair of sentences, the facts in the first sentence imply the facts in the second. \ud83d\udee0 Components provided: Dataset readers for several datasets, including SNLI and Quora Paraphrase . Reading Comprehension Reading comprehension tasks involve answering questions about a passage of text to show that the system understands the passage. \ud83d\udee0 Components provided: Models such as BiDAF and a transformer-based QA model , as well as readers for datasets such as DROP , QuAC , and SQuAD . Structured Prediction Structured prediction includes tasks such as Semantic Role Labeling (SRL), which is for determining the latent predicate argument structure of a sentence and providing representations that can answer basic questions about sentence meaning, including who did what to whom, etc. \ud83d\udee0 Components provided: Dataset readers for Penn Tree Bank , OntoNotes , etc., and several models including one for SRL and a very general graph parser . Sequence Tagging Sequence tagging tasks include Named Entity Recognition (NER) and Fine-grained NER. \ud83d\udee0 Components provided: A Conditional Random Field model and dataset readers for datasets such as CoNLL-2000 , CoNLL-2003 , CCGbank , and OntoNotes . Text + Vision This is a catch-all category for any text + vision multi-modal tasks such Visual Question Answering (VQA), the task of generating a answer in response to a natural language question about the contents of an image. \ud83d\udee0 Components provided: Several models such as a ViLBERT model for VQA and one for Visual Entailment , along with corresponding dataset readers. Pre-trained models # Every pretrained model in AllenNLP Models has a corresponding ModelCard in the allennlp_models/modelcards/ folder. Many of these models are also hosted on the AllenNLP Demo and the AllenNLP Project Gallery . To programmatically list the available models, you can run the following from a Python session: >>> from allennlp_models import pretrained >>> print ( pretrained . get_pretrained_models ()) The output is a dictionary that maps the model IDs to their ModelCard : {'structured-prediction-srl-bert': <allennlp.common.model_card.ModelCard object at 0x14a705a30>, ...} You can load a Predictor for any of these models with the pretrained.load_predictor() helper. For example: >>> pretrained . load_predictor ( \"mc-roberta-swag\" ) Here is a list of pre-trained models currently available. coref-spanbert - Higher-order coref with coarse-to-fine inference (with SpanBERT embeddings). evaluate_rc-lerc - A BERT model that scores candidate answers from 0 to 1. generation-bart - BART with a language model head for generation. glove-sst - LSTM binary classifier with GloVe embeddings. lm-masked-language-model - BERT-based masked language model lm-next-token-lm-gpt2 - OpenAI's GPT-2 language model that generates the next token. mc-roberta-commonsenseqa - RoBERTa-based multiple choice model for CommonSenseQA. mc-roberta-piqa - RoBERTa-based multiple choice model for PIQA. mc-roberta-swag - RoBERTa-based multiple choice model for SWAG. pair-classification-decomposable-attention-elmo - The decomposable attention model (Parikh et al, 2017) combined with ELMo embeddings trained on SNLI. pair-classification-esim - Enhanced LSTM trained on SNLI. pair-classification-roberta-mnli - RoBERTa finetuned on MNLI. pair-classification-roberta-rte - A pair classification model patterned after the proposed model in Devlin et al, fine-tuned on the SuperGLUE RTE corpus pair-classification-roberta-snli - RoBERTa finetuned on SNLI. rc-bidaf-elmo - BiDAF model with ELMo embeddings instead of GloVe. rc-bidaf - BiDAF model with GloVe embeddings. rc-naqanet - An augmented version of QANet that adds rudimentary numerical reasoning ability, trained on DROP (Dua et al., 2019), as published in the original DROP paper. rc-nmn - A neural module network trained on DROP. rc-transformer-qa - A reading comprehension model patterned after the proposed model in Devlin et al, with improvements borrowed from the SQuAD model in the transformers project roberta-sst - RoBERTa-based binary classifier for Stanford Sentiment Treebank semparse-nlvr - The model is a semantic parser trained on Cornell NLVR. semparse-text-to-sql - This model is an implementation of an encoder-decoder architecture with LSTMs and constrained type decoding trained on the ATIS dataset. semparse-wikitables - The model is a semantic parser trained on WikiTableQuestions. structured-prediction-biaffine-parser - A neural model for dependency parsing using biaffine classifiers on top of a bidirectional LSTM. structured-prediction-constituency-parser - Constituency parser with character-based ELMo embeddings structured-prediction-srl-bert - A BERT based model (Shi et al, 2019) with some modifications (no additional parameters apart from a linear classification layer) structured-prediction-srl - A reimplementation of a deep BiLSTM sequence prediction model (Stanovsky et al., 2018) tagging-elmo-crf-tagger - NER tagger using a Gated Recurrent Unit (GRU) character encoder as well as a GRU phrase encoder, with GloVe embeddings. tagging-fine-grained-crf-tagger - This model identifies a broad range of 16 semantic types in the input text. It is a reimplementation of Lample (2016) and uses a biLSTM with a CRF layer, character embeddings and ELMo embeddings. tagging-fine-grained-transformer-crf-tagger - Fine-grained NER model ve-vilbert - ViLBERT-based model for Visual Entailment. vqa-vilbert - ViLBERT (short for Vision-and-Language BERT), is a model for learning task-agnostic joint representations of image content and natural language. Installing # From PyPI # allennlp-models is available on PyPI. To install with pip , just run pip install allennlp-models Note that the allennlp-models package is tied to the allennlp core package . Therefore when you install the models package you will get the corresponding version of allennlp (if you haven't already installed allennlp ). For example, pip install allennlp-models == 2 .2.0 pip freeze | grep allennlp # > allennlp==2.2.0 # > allennlp-models==2.2.0 From source # If you intend to install the models package from source, then you probably also want to install allennlp from source . Once you have allennlp installed, run the following within the same Python environment: git clone https://github.com/allenai/allennlp-models.git cd allennlp-models ALLENNLP_VERSION_OVERRIDE = 'allennlp' pip install -e . pip install -r dev-requirements.txt The ALLENNLP_VERSION_OVERRIDE environment variable ensures that the allennlp dependency is unpinned so that your local install of allennlp will be sufficient. If, however, you haven't installed allennlp yet and don't want to manage a local install, just omit this environment variable and allennlp will be installed from the main branch on GitHub. Both allennlp and allennlp-models are developed and tested side-by-side, so they should be kept up-to-date with each other. If you look at the GitHub Actions workflow for allennlp-models , it's always tested against the main branch of allennlp . Similarly, allennlp is always tested against the main branch of allennlp-models . Using Docker # Docker provides a virtual machine with everything set up to run AllenNLP-- whether you will leverage a GPU or just run on a CPU. Docker provides more isolation and consistency, and also makes it easy to distribute your environment to a compute cluster. Once you have installed Docker you can either use a prebuilt image from a release or build an image locally with any version of allennlp and allennlp-models . If you have GPUs available, you also need to install the nvidia-docker runtime. To build an image locally from a specific release, run docker build \\ --build-arg RELEASE = 1 .2.2 \\ --build-arg CUDA = 10 .2 \\ -t allennlp/models - < Dockerfile.release Just replace the RELEASE and CUDA build args with what you need. You can check the available tags on Docker Hub to see which CUDA versions are available for a given RELEASE . Alternatively, you can build against specific commits of allennlp and allennlp-models with docker build \\ --build-arg ALLENNLP_COMMIT = d823a2591e94912a6315e429d0fe0ee2efb4b3ee \\ --build-arg ALLENNLP_MODELS_COMMIT = 01bc777e0d89387f03037d398cd967390716daf1 \\ --build-arg CUDA = 10 .2 \\ -t allennlp/models - < Dockerfile.commit Just change the ALLENNLP_COMMIT / ALLENNLP_MODELS_COMMIT and CUDA build args to the desired commit SHAs and CUDA versions, respectively. Once you've built your image, you can run it like this: mkdir -p $HOME /.allennlp/ docker run --rm --gpus all -v $HOME /.allennlp:/root/.allennlp allennlp/models Note: the --gpus all is only valid if you've installed the nvidia-docker runtime.","title":"Home"},{"location":"#in-this-readme","text":"About Tasks and components Pre-trained models Installing From PyPI From source Using Docker","title":"In this README"},{"location":"#about","text":"This repository contains the components - such as DatasetReader , Model , and Predictor classes - for applying AllenNLP to a wide variety of NLP tasks . It also provides an easy way to download and use pre-trained models that were trained with these components.","title":"About"},{"location":"#tasks-and-components","text":"This is an overview of the tasks supported by the AllenNLP Models library along with the corresponding components provided, organized by category. For a more comprehensive overview, see the AllenNLP Models documentation or the Paperswithcode page . Classification Classification tasks involve predicting one or more labels from a predefined set to assign to each input. Examples include Sentiment Analysis, where the labels might be {\"positive\", \"negative\", \"neutral\"} , and Binary Question Answering, where the labels are {True, False} . \ud83d\udee0 Components provided: Dataset readers for various datasets, including BoolQ and SST , as well as a Biattentive Classification Network model. Coreference Resolution Coreference resolution tasks require finding all of the expressions in a text that refer to common entities. See nlp.stanford.edu/projects/coref for more details. \ud83d\udee0 Components provided: A general Coref model and several dataset readers. Generation This is a broad category for tasks such as Summarization that involve generating unstructered and often variable-length text. \ud83d\udee0 Components provided: Several Seq2Seq models such a Bart , CopyNet , and a general Composed Seq2Seq , along with corresponding dataset readers. Language Modeling Language modeling tasks involve learning a probability distribution over sequences of tokens. \ud83d\udee0 Components provided: Several language model implementations, such as a Masked LM and a Next Token LM . Multiple Choice Multiple choice tasks require selecting a correct choice among alternatives, where the set of choices may be different for each input. This differs from classification where the set of choices is predefined and fixed across all inputs. \ud83d\udee0 Components provided: A transformer-based multiple choice model and a handful of dataset readers for specific datasets. Pair Classification Pair classification is another broad category that contains tasks such as Textual Entailment, which is to determine whether, for a pair of sentences, the facts in the first sentence imply the facts in the second. \ud83d\udee0 Components provided: Dataset readers for several datasets, including SNLI and Quora Paraphrase . Reading Comprehension Reading comprehension tasks involve answering questions about a passage of text to show that the system understands the passage. \ud83d\udee0 Components provided: Models such as BiDAF and a transformer-based QA model , as well as readers for datasets such as DROP , QuAC , and SQuAD . Structured Prediction Structured prediction includes tasks such as Semantic Role Labeling (SRL), which is for determining the latent predicate argument structure of a sentence and providing representations that can answer basic questions about sentence meaning, including who did what to whom, etc. \ud83d\udee0 Components provided: Dataset readers for Penn Tree Bank , OntoNotes , etc., and several models including one for SRL and a very general graph parser . Sequence Tagging Sequence tagging tasks include Named Entity Recognition (NER) and Fine-grained NER. \ud83d\udee0 Components provided: A Conditional Random Field model and dataset readers for datasets such as CoNLL-2000 , CoNLL-2003 , CCGbank , and OntoNotes . Text + Vision This is a catch-all category for any text + vision multi-modal tasks such Visual Question Answering (VQA), the task of generating a answer in response to a natural language question about the contents of an image. \ud83d\udee0 Components provided: Several models such as a ViLBERT model for VQA and one for Visual Entailment , along with corresponding dataset readers.","title":"Tasks and components"},{"location":"#pre-trained-models","text":"Every pretrained model in AllenNLP Models has a corresponding ModelCard in the allennlp_models/modelcards/ folder. Many of these models are also hosted on the AllenNLP Demo and the AllenNLP Project Gallery . To programmatically list the available models, you can run the following from a Python session: >>> from allennlp_models import pretrained >>> print ( pretrained . get_pretrained_models ()) The output is a dictionary that maps the model IDs to their ModelCard : {'structured-prediction-srl-bert': <allennlp.common.model_card.ModelCard object at 0x14a705a30>, ...} You can load a Predictor for any of these models with the pretrained.load_predictor() helper. For example: >>> pretrained . load_predictor ( \"mc-roberta-swag\" ) Here is a list of pre-trained models currently available. coref-spanbert - Higher-order coref with coarse-to-fine inference (with SpanBERT embeddings). evaluate_rc-lerc - A BERT model that scores candidate answers from 0 to 1. generation-bart - BART with a language model head for generation. glove-sst - LSTM binary classifier with GloVe embeddings. lm-masked-language-model - BERT-based masked language model lm-next-token-lm-gpt2 - OpenAI's GPT-2 language model that generates the next token. mc-roberta-commonsenseqa - RoBERTa-based multiple choice model for CommonSenseQA. mc-roberta-piqa - RoBERTa-based multiple choice model for PIQA. mc-roberta-swag - RoBERTa-based multiple choice model for SWAG. pair-classification-decomposable-attention-elmo - The decomposable attention model (Parikh et al, 2017) combined with ELMo embeddings trained on SNLI. pair-classification-esim - Enhanced LSTM trained on SNLI. pair-classification-roberta-mnli - RoBERTa finetuned on MNLI. pair-classification-roberta-rte - A pair classification model patterned after the proposed model in Devlin et al, fine-tuned on the SuperGLUE RTE corpus pair-classification-roberta-snli - RoBERTa finetuned on SNLI. rc-bidaf-elmo - BiDAF model with ELMo embeddings instead of GloVe. rc-bidaf - BiDAF model with GloVe embeddings. rc-naqanet - An augmented version of QANet that adds rudimentary numerical reasoning ability, trained on DROP (Dua et al., 2019), as published in the original DROP paper. rc-nmn - A neural module network trained on DROP. rc-transformer-qa - A reading comprehension model patterned after the proposed model in Devlin et al, with improvements borrowed from the SQuAD model in the transformers project roberta-sst - RoBERTa-based binary classifier for Stanford Sentiment Treebank semparse-nlvr - The model is a semantic parser trained on Cornell NLVR. semparse-text-to-sql - This model is an implementation of an encoder-decoder architecture with LSTMs and constrained type decoding trained on the ATIS dataset. semparse-wikitables - The model is a semantic parser trained on WikiTableQuestions. structured-prediction-biaffine-parser - A neural model for dependency parsing using biaffine classifiers on top of a bidirectional LSTM. structured-prediction-constituency-parser - Constituency parser with character-based ELMo embeddings structured-prediction-srl-bert - A BERT based model (Shi et al, 2019) with some modifications (no additional parameters apart from a linear classification layer) structured-prediction-srl - A reimplementation of a deep BiLSTM sequence prediction model (Stanovsky et al., 2018) tagging-elmo-crf-tagger - NER tagger using a Gated Recurrent Unit (GRU) character encoder as well as a GRU phrase encoder, with GloVe embeddings. tagging-fine-grained-crf-tagger - This model identifies a broad range of 16 semantic types in the input text. It is a reimplementation of Lample (2016) and uses a biLSTM with a CRF layer, character embeddings and ELMo embeddings. tagging-fine-grained-transformer-crf-tagger - Fine-grained NER model ve-vilbert - ViLBERT-based model for Visual Entailment. vqa-vilbert - ViLBERT (short for Vision-and-Language BERT), is a model for learning task-agnostic joint representations of image content and natural language.","title":"Pre-trained models"},{"location":"#installing","text":"","title":"Installing"},{"location":"#from-pypi","text":"allennlp-models is available on PyPI. To install with pip , just run pip install allennlp-models Note that the allennlp-models package is tied to the allennlp core package . Therefore when you install the models package you will get the corresponding version of allennlp (if you haven't already installed allennlp ). For example, pip install allennlp-models == 2 .2.0 pip freeze | grep allennlp # > allennlp==2.2.0 # > allennlp-models==2.2.0","title":"From PyPI"},{"location":"#from-source","text":"If you intend to install the models package from source, then you probably also want to install allennlp from source . Once you have allennlp installed, run the following within the same Python environment: git clone https://github.com/allenai/allennlp-models.git cd allennlp-models ALLENNLP_VERSION_OVERRIDE = 'allennlp' pip install -e . pip install -r dev-requirements.txt The ALLENNLP_VERSION_OVERRIDE environment variable ensures that the allennlp dependency is unpinned so that your local install of allennlp will be sufficient. If, however, you haven't installed allennlp yet and don't want to manage a local install, just omit this environment variable and allennlp will be installed from the main branch on GitHub. Both allennlp and allennlp-models are developed and tested side-by-side, so they should be kept up-to-date with each other. If you look at the GitHub Actions workflow for allennlp-models , it's always tested against the main branch of allennlp . Similarly, allennlp is always tested against the main branch of allennlp-models .","title":"From source"},{"location":"#using-docker","text":"Docker provides a virtual machine with everything set up to run AllenNLP-- whether you will leverage a GPU or just run on a CPU. Docker provides more isolation and consistency, and also makes it easy to distribute your environment to a compute cluster. Once you have installed Docker you can either use a prebuilt image from a release or build an image locally with any version of allennlp and allennlp-models . If you have GPUs available, you also need to install the nvidia-docker runtime. To build an image locally from a specific release, run docker build \\ --build-arg RELEASE = 1 .2.2 \\ --build-arg CUDA = 10 .2 \\ -t allennlp/models - < Dockerfile.release Just replace the RELEASE and CUDA build args with what you need. You can check the available tags on Docker Hub to see which CUDA versions are available for a given RELEASE . Alternatively, you can build against specific commits of allennlp and allennlp-models with docker build \\ --build-arg ALLENNLP_COMMIT = d823a2591e94912a6315e429d0fe0ee2efb4b3ee \\ --build-arg ALLENNLP_MODELS_COMMIT = 01bc777e0d89387f03037d398cd967390716daf1 \\ --build-arg CUDA = 10 .2 \\ -t allennlp/models - < Dockerfile.commit Just change the ALLENNLP_COMMIT / ALLENNLP_MODELS_COMMIT and CUDA build args to the desired commit SHAs and CUDA versions, respectively. Once you've built your image, you can run it like this: mkdir -p $HOME /.allennlp/ docker run --rm --gpus all -v $HOME /.allennlp:/root/.allennlp allennlp/models Note: the --gpus all is only valid if you've installed the nvidia-docker runtime.","title":"Using Docker"},{"location":"CHANGELOG/","text":"Changelog # All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . v2.4.0 - 2021-04-22 # Added # Added T5 model for generation. Added a classmethod constructor on Seq2SeqPredictor : .pretrained_t5_for_generation() . Added a parameter called source_prefix to CNNDailyMailDatasetReader . This is useful with T5, for example, by setting source_prefix to \"summarization: \". Tests for VqaMeasure . Distributed tests for ConllCorefScores and SrlEvalScorer metrics. Fixed # VqaMeasure now calculates correctly in the distributed case. ConllCorefScores now calculates correctly in the distributed case. SrlEvalScorer raises an appropriate error if run in the distributed setting. Changed # Updated registered_predictor_name to null in model cards for the models where it was the same as the default predictor. v2.3.0 - 2021-04-14 # Fixed # Fixed bug in experiment_from_huggingface.jsonnet and experiment.jsonnet by changing min_count to have key labels instead of answers . Resolves failure of model checks that involve calling _extend in vocabulary.py TransformerQA now outputs span probabilities as well as scores. TransformerQAPredictor now implements predictions_to_labeled_instances , which is required for the interpret module. Added # Added script that produces the coref training data. Added tests for using allennlp predict on multitask models. Added reader and training config for RoBERTa on SuperGLUE's Recognizing Textual Entailment task v2.2.0 - 2021-03-26 # Added # Evaluating RC task card and associated LERC model card Compatibility with PyTorch 1.8 Allows the order of examples in the task cards to be specified explicitly Dataset reader for SuperGLUE BoolQ Changed # Add option combine_input_fields in SnliDatasetReader to support only having \"non-entailment\" and \"entailment\" as output labels. Made all the models run on AllenNLP 2.1 Add option ignore_loss_on_o_tags in CrfTagger to set the flag outside its forward function. Add make_output_human_readable for pair classification models ( BiMpm , DecomposableAttention , and ESIM ). Fixed # Fixed https://github.com/allenai/allennlp/issues/4745. Updated QaNet and NumericallyAugmentedQaNet models to remove bias for layers that are followed by normalization layers. Updated the model cards for rc-naqanet , vqa-vilbert and ve-vilbert . Predictors now work for the vilbert-multitask model. Support unlabeled instances in SnliDatasetReader . v2.1.0 - 2021-02-24 # Changed # coding_scheme parameter is now deprecated in Conll2000DatasetReader , please use convert_to_coding_scheme instead. Added # BART model now adds a predicted_text field in make_output_human_readable that has the cleaned text corresponding to predicted_tokens . Fixed # Made label parameter in TransformerMCReader.text_to_instance optional with default of None . Updated many of the models for version 2.1.0. Fixed and re-trained many of the models. v2.0.1 - 2021-02-01 # Fixed # Fixed OpenIePredictor.predict_json so it treats auxiliary verbs as verbs when the language is English. v2.0.0 - 2021-01-27 # Fixed # Made the training configs compatible with the tensorboard logging changes in the main repo v2.0.0rc1 - 2021-01-21 # Added # Dataset readers, models, metrics, and training configs for VQAv2, GQA, and Visual Entailment Fixed # Fixed training_configs/pair_classification/bimpm.jsonnet and training_configs/rc/dialog_qa.jsonnet to work with new data loading API. Fixed the potential for a dead-lock when training the TransformerQA model on multiple GPUs when nodes receive different sized batches. Fixed BART. This implementation had some major bugs in it that caused poor performance during prediction. Removed # Moving ModelCard and TaskCard abstractions out of the models repository. Changed # master branch renamed to main SquadEmAndF1 metric can now also accept a batch of predictions and corresponding answers (instead of a single one) in the form of list (for each). v1.3.0 - 2020-12-15 # Fixed # Fix an index bug in BART prediction. Add None check in PrecoReader 's text_to_instance() method. Fixed SemanticRoleLabelerPredictor.tokens_to_instances so it treats auxiliary verbs as verbs when the language is English Added # Added link to source code to API docs. Information updates for remaining model cards (also includes the ones in demo, but not in the repository). Changed # Updated Dockerfile.release and Dockerfile.commit to work with different CUDA versions. Changes required for the transformers dependency update to version 4.0.1. Fixed # Added missing folder for taskcards in setup.py v1.2.2 - 2020-11-17 # Changed # Changed AllenNLP dependency for releases to allow for a range of versions, instead of being pinned to an exact version. There will now be multiple Docker images pushed to Docker Hub for releases, each corresponding to a different supported CUDA version (currently just 10.2 and 11.0). Fixed # Fixed pair-classification-esim pretrained model. Fixed ValueError error message in Seq2SeqDatasetReader . Better check for start and end symbols in Seq2SeqDatasetReader that doesn't fail for BPE-based tokenizers. Added # Added short_description field to ModelCard . Information updates for all model cards. v1.2.1 - 2020-11-10 # Added # Added the TaskCard class and task cards for common tasks. Added a test for the interpret functionality Changed # Added more information to model cards for pair classification models ( pair-classification-decomposable-attention-elmo , pair-classification-roberta-snli , pair-classification-roberta-mnli , pair-classification-esim ). Fixed # Fixed TransformerElmo config to work with the new AllenNLP Pinned the version of torch more tightly to make AMP work Fixed the somewhat fragile Bidaf test v1.2.0 - 2020-10-29 # Changed # Updated docstring for Transformer MC. Added more information to model cards for multiple choice models ( mc-roberta-commonsenseqa , mc-roberta-piqa , and mc-roberta-swag ). Fixed # Fixed many training configs to work out-of-the box. These include the configs for bart_cnn_dm , swag , bidaf , bidaf_elmo , naqanet , and qanet . Fixed minor bug in MaskedLanguageModel, where getting token ids used hard-coded assumptions (that could be wrong) instead of our standard utility function. v1.2.0rc1 - 2020-10-22 # Added # Added dataset reader support for SQuAD 2.0 with both the SquadReader and TransformerSquadReader . Updated the SQuAD v1.1 metric to work with SQuAD 2.0 as well. Updated the TransformerQA model to work for SQuAD 2.0. Added official support for Python 3.8. Added a json template for model cards. Added training_config as a field in model cards. Added a BeamSearchGenerator registrable class which can be provided to a NextTokenLM model to utilize beam search for predicting a sequence of tokens, instead of a single next token. BeamSearchGenerator is an abstract class, so a concrete registered implementation needs to be used. One implementation is provided so far: TransformerBeamSearchGenerator , registered as transformer , which will work with any NextTokenLM that uses a PretrainedTransformerEmbedder . Added an overrides parameter to pretrained.load_predictor() . Changed # rc-transformer-qa pretrained model is now an updated version trained on SQuAD v2.0. skip_invalid_examples parameter in SQuAD dataset readers has been deprecated. Please use skip_impossible_questions instead. Fixed # Fixed lm-masked-language-model pretrained model. Fixed BART for latest transformers version. Fixed BiDAF predictor and BiDAF predictor tests. Fixed a bug with Seq2SeqDatasetReader that would cause an exception when the desired behavior is to not add start or end symbols to either the source or the target and the default start_symbol or end_symbol are not part of the tokenizer's vocabulary. v1.1.0 - 2020-09-08 # Fixed # Updated LanguageModelTokenEmbedder to allow allow multiple token embedders, but only use first with non-empty type Fixed evaluation of metrics when using distributed setting. Fixed a bug introduced in 1.0 where the SRL model did not reproduce the original result. v1.1.0rc4 - 2020-08-21 # Added # Added regression tests for training configs that run on a scheduled workflow. Added a test for the pretrained sentiment analysis model. Added way for questions from quora dataset to be concatenated like the sequences in the SNLI dataset. v1.1.0rc3 - 2020-08-12 # Fixed # Fixed GraphParser.get_metrics so that it expects a dict from F1Measure.get_metric . CopyNet and SimpleSeq2Seq models now work with AMP. Made the SST reader a little more strict in the kinds of input it accepts. v1.1.0rc2 - 2020-07-31 # Changed # Updated to PyTorch 1.6. Fixed # Updated the RoBERTa SST config to make proper use of the CLS token Updated RoBERTa SNLI and MNLI pretrained models for latest transformers version Added # Added BART model Added ModelCard and related classes. Added model cards for all the pretrained models. Added a field registered_predictor_name to ModelCard . Added a method load_predictor to allennlp_models.pretrained . Added support to multi-layer decoder in simple seq2seq model. v1.1.0rc1 - 2020-07-14 # Fixed # Updated the BERT SRL model to be compatible with the new huggingface tokenizers. CopyNetSeq2Seq model now works with pretrained transformers. A bug with NextTokenLM that caused simple gradient interpreters to fail. A bug in training_config of qanet and bimpm that used the old version of regularizer and initializer . The fine-grained NER transformer model did not survive an upgrade of the transformers library, but it is now fixed. Fixed many minor formatting issues in docstrings. Docs are now published at https://docs.allennlp.org/models/ . Changed # CopyNetDatasetReader no longer automatically adds START_TOKEN and END_TOKEN to the tokenized source. If you want these in the tokenized source, it's up to the source tokenizer. Added # Added two models for fine-grained NER Added a category for multiple choice models, including a few reference implementations Implemented manual distributed sharding for SNLI dataset reader. v1.0.0 - 2020-06-16 # No additional note-worthy changes since rc6. v1.0.0rc6 - 2020-06-11 # Changed # Removed deprecated \"simple_seq2seq\" predictor Fixed # Replaced deepcopy of Instance s with new Instance.duplicate() method. A bug where pretrained sentence taggers would fail to be initialized because some of the models were not imported. A bug in some RC models that would cause mixed precision training to crash when using NVIDIA apex. Predictor names were inconsistently switching between dashes and underscores. Now they all use underscores. Added # Added option to SemanticDependenciesDatasetReader to not skip instances that have no arcs, for validation data Added a default predictors to several models Added sentiment analysis models to pretrained.py Added NLI models to pretrained.py v1.0.0rc5 - 2020-05-14 # Changed # Moved the models into categories based on their format Fixed # Made transformer_qa predictor accept JSON input with the keys \"question\" and \"passage\" to be consistent with the reading_comprehension predictor. Added # conllu dependency (previously part of allennlp 's dependencies) v1.0.0rc4 - 2020-05-14 # We first introduced this CHANGELOG after release v1.0.0rc4 , so please refer to the GitHub release notes for this and earlier releases.","title":"CHANGELOG"},{"location":"CHANGELOG/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"CHANGELOG/#v240-2021-04-22","text":"","title":"v2.4.0 - 2021-04-22"},{"location":"CHANGELOG/#added","text":"Added T5 model for generation. Added a classmethod constructor on Seq2SeqPredictor : .pretrained_t5_for_generation() . Added a parameter called source_prefix to CNNDailyMailDatasetReader . This is useful with T5, for example, by setting source_prefix to \"summarization: \". Tests for VqaMeasure . Distributed tests for ConllCorefScores and SrlEvalScorer metrics.","title":"Added"},{"location":"CHANGELOG/#fixed","text":"VqaMeasure now calculates correctly in the distributed case. ConllCorefScores now calculates correctly in the distributed case. SrlEvalScorer raises an appropriate error if run in the distributed setting.","title":"Fixed"},{"location":"CHANGELOG/#changed","text":"Updated registered_predictor_name to null in model cards for the models where it was the same as the default predictor.","title":"Changed"},{"location":"CHANGELOG/#v230-2021-04-14","text":"","title":"v2.3.0 - 2021-04-14"},{"location":"CHANGELOG/#fixed_1","text":"Fixed bug in experiment_from_huggingface.jsonnet and experiment.jsonnet by changing min_count to have key labels instead of answers . Resolves failure of model checks that involve calling _extend in vocabulary.py TransformerQA now outputs span probabilities as well as scores. TransformerQAPredictor now implements predictions_to_labeled_instances , which is required for the interpret module.","title":"Fixed"},{"location":"CHANGELOG/#added_1","text":"Added script that produces the coref training data. Added tests for using allennlp predict on multitask models. Added reader and training config for RoBERTa on SuperGLUE's Recognizing Textual Entailment task","title":"Added"},{"location":"CHANGELOG/#v220-2021-03-26","text":"","title":"v2.2.0 - 2021-03-26"},{"location":"CHANGELOG/#added_2","text":"Evaluating RC task card and associated LERC model card Compatibility with PyTorch 1.8 Allows the order of examples in the task cards to be specified explicitly Dataset reader for SuperGLUE BoolQ","title":"Added"},{"location":"CHANGELOG/#changed_1","text":"Add option combine_input_fields in SnliDatasetReader to support only having \"non-entailment\" and \"entailment\" as output labels. Made all the models run on AllenNLP 2.1 Add option ignore_loss_on_o_tags in CrfTagger to set the flag outside its forward function. Add make_output_human_readable for pair classification models ( BiMpm , DecomposableAttention , and ESIM ).","title":"Changed"},{"location":"CHANGELOG/#fixed_2","text":"Fixed https://github.com/allenai/allennlp/issues/4745. Updated QaNet and NumericallyAugmentedQaNet models to remove bias for layers that are followed by normalization layers. Updated the model cards for rc-naqanet , vqa-vilbert and ve-vilbert . Predictors now work for the vilbert-multitask model. Support unlabeled instances in SnliDatasetReader .","title":"Fixed"},{"location":"CHANGELOG/#v210-2021-02-24","text":"","title":"v2.1.0 - 2021-02-24"},{"location":"CHANGELOG/#changed_2","text":"coding_scheme parameter is now deprecated in Conll2000DatasetReader , please use convert_to_coding_scheme instead.","title":"Changed"},{"location":"CHANGELOG/#added_3","text":"BART model now adds a predicted_text field in make_output_human_readable that has the cleaned text corresponding to predicted_tokens .","title":"Added"},{"location":"CHANGELOG/#fixed_3","text":"Made label parameter in TransformerMCReader.text_to_instance optional with default of None . Updated many of the models for version 2.1.0. Fixed and re-trained many of the models.","title":"Fixed"},{"location":"CHANGELOG/#v201-2021-02-01","text":"","title":"v2.0.1 - 2021-02-01"},{"location":"CHANGELOG/#fixed_4","text":"Fixed OpenIePredictor.predict_json so it treats auxiliary verbs as verbs when the language is English.","title":"Fixed"},{"location":"CHANGELOG/#v200-2021-01-27","text":"","title":"v2.0.0 - 2021-01-27"},{"location":"CHANGELOG/#fixed_5","text":"Made the training configs compatible with the tensorboard logging changes in the main repo","title":"Fixed"},{"location":"CHANGELOG/#v200rc1-2021-01-21","text":"","title":"v2.0.0rc1 - 2021-01-21"},{"location":"CHANGELOG/#added_4","text":"Dataset readers, models, metrics, and training configs for VQAv2, GQA, and Visual Entailment","title":"Added"},{"location":"CHANGELOG/#fixed_6","text":"Fixed training_configs/pair_classification/bimpm.jsonnet and training_configs/rc/dialog_qa.jsonnet to work with new data loading API. Fixed the potential for a dead-lock when training the TransformerQA model on multiple GPUs when nodes receive different sized batches. Fixed BART. This implementation had some major bugs in it that caused poor performance during prediction.","title":"Fixed"},{"location":"CHANGELOG/#removed","text":"Moving ModelCard and TaskCard abstractions out of the models repository.","title":"Removed"},{"location":"CHANGELOG/#changed_3","text":"master branch renamed to main SquadEmAndF1 metric can now also accept a batch of predictions and corresponding answers (instead of a single one) in the form of list (for each).","title":"Changed"},{"location":"CHANGELOG/#v130-2020-12-15","text":"","title":"v1.3.0 - 2020-12-15"},{"location":"CHANGELOG/#fixed_7","text":"Fix an index bug in BART prediction. Add None check in PrecoReader 's text_to_instance() method. Fixed SemanticRoleLabelerPredictor.tokens_to_instances so it treats auxiliary verbs as verbs when the language is English","title":"Fixed"},{"location":"CHANGELOG/#added_5","text":"Added link to source code to API docs. Information updates for remaining model cards (also includes the ones in demo, but not in the repository).","title":"Added"},{"location":"CHANGELOG/#changed_4","text":"Updated Dockerfile.release and Dockerfile.commit to work with different CUDA versions. Changes required for the transformers dependency update to version 4.0.1.","title":"Changed"},{"location":"CHANGELOG/#fixed_8","text":"Added missing folder for taskcards in setup.py","title":"Fixed"},{"location":"CHANGELOG/#v122-2020-11-17","text":"","title":"v1.2.2 - 2020-11-17"},{"location":"CHANGELOG/#changed_5","text":"Changed AllenNLP dependency for releases to allow for a range of versions, instead of being pinned to an exact version. There will now be multiple Docker images pushed to Docker Hub for releases, each corresponding to a different supported CUDA version (currently just 10.2 and 11.0).","title":"Changed"},{"location":"CHANGELOG/#fixed_9","text":"Fixed pair-classification-esim pretrained model. Fixed ValueError error message in Seq2SeqDatasetReader . Better check for start and end symbols in Seq2SeqDatasetReader that doesn't fail for BPE-based tokenizers.","title":"Fixed"},{"location":"CHANGELOG/#added_6","text":"Added short_description field to ModelCard . Information updates for all model cards.","title":"Added"},{"location":"CHANGELOG/#v121-2020-11-10","text":"","title":"v1.2.1 - 2020-11-10"},{"location":"CHANGELOG/#added_7","text":"Added the TaskCard class and task cards for common tasks. Added a test for the interpret functionality","title":"Added"},{"location":"CHANGELOG/#changed_6","text":"Added more information to model cards for pair classification models ( pair-classification-decomposable-attention-elmo , pair-classification-roberta-snli , pair-classification-roberta-mnli , pair-classification-esim ).","title":"Changed"},{"location":"CHANGELOG/#fixed_10","text":"Fixed TransformerElmo config to work with the new AllenNLP Pinned the version of torch more tightly to make AMP work Fixed the somewhat fragile Bidaf test","title":"Fixed"},{"location":"CHANGELOG/#v120-2020-10-29","text":"","title":"v1.2.0 - 2020-10-29"},{"location":"CHANGELOG/#changed_7","text":"Updated docstring for Transformer MC. Added more information to model cards for multiple choice models ( mc-roberta-commonsenseqa , mc-roberta-piqa , and mc-roberta-swag ).","title":"Changed"},{"location":"CHANGELOG/#fixed_11","text":"Fixed many training configs to work out-of-the box. These include the configs for bart_cnn_dm , swag , bidaf , bidaf_elmo , naqanet , and qanet . Fixed minor bug in MaskedLanguageModel, where getting token ids used hard-coded assumptions (that could be wrong) instead of our standard utility function.","title":"Fixed"},{"location":"CHANGELOG/#v120rc1-2020-10-22","text":"","title":"v1.2.0rc1 - 2020-10-22"},{"location":"CHANGELOG/#added_8","text":"Added dataset reader support for SQuAD 2.0 with both the SquadReader and TransformerSquadReader . Updated the SQuAD v1.1 metric to work with SQuAD 2.0 as well. Updated the TransformerQA model to work for SQuAD 2.0. Added official support for Python 3.8. Added a json template for model cards. Added training_config as a field in model cards. Added a BeamSearchGenerator registrable class which can be provided to a NextTokenLM model to utilize beam search for predicting a sequence of tokens, instead of a single next token. BeamSearchGenerator is an abstract class, so a concrete registered implementation needs to be used. One implementation is provided so far: TransformerBeamSearchGenerator , registered as transformer , which will work with any NextTokenLM that uses a PretrainedTransformerEmbedder . Added an overrides parameter to pretrained.load_predictor() .","title":"Added"},{"location":"CHANGELOG/#changed_8","text":"rc-transformer-qa pretrained model is now an updated version trained on SQuAD v2.0. skip_invalid_examples parameter in SQuAD dataset readers has been deprecated. Please use skip_impossible_questions instead.","title":"Changed"},{"location":"CHANGELOG/#fixed_12","text":"Fixed lm-masked-language-model pretrained model. Fixed BART for latest transformers version. Fixed BiDAF predictor and BiDAF predictor tests. Fixed a bug with Seq2SeqDatasetReader that would cause an exception when the desired behavior is to not add start or end symbols to either the source or the target and the default start_symbol or end_symbol are not part of the tokenizer's vocabulary.","title":"Fixed"},{"location":"CHANGELOG/#v110-2020-09-08","text":"","title":"v1.1.0 - 2020-09-08"},{"location":"CHANGELOG/#fixed_13","text":"Updated LanguageModelTokenEmbedder to allow allow multiple token embedders, but only use first with non-empty type Fixed evaluation of metrics when using distributed setting. Fixed a bug introduced in 1.0 where the SRL model did not reproduce the original result.","title":"Fixed"},{"location":"CHANGELOG/#v110rc4-2020-08-21","text":"","title":"v1.1.0rc4 - 2020-08-21"},{"location":"CHANGELOG/#added_9","text":"Added regression tests for training configs that run on a scheduled workflow. Added a test for the pretrained sentiment analysis model. Added way for questions from quora dataset to be concatenated like the sequences in the SNLI dataset.","title":"Added"},{"location":"CHANGELOG/#v110rc3-2020-08-12","text":"","title":"v1.1.0rc3 - 2020-08-12"},{"location":"CHANGELOG/#fixed_14","text":"Fixed GraphParser.get_metrics so that it expects a dict from F1Measure.get_metric . CopyNet and SimpleSeq2Seq models now work with AMP. Made the SST reader a little more strict in the kinds of input it accepts.","title":"Fixed"},{"location":"CHANGELOG/#v110rc2-2020-07-31","text":"","title":"v1.1.0rc2 - 2020-07-31"},{"location":"CHANGELOG/#changed_9","text":"Updated to PyTorch 1.6.","title":"Changed"},{"location":"CHANGELOG/#fixed_15","text":"Updated the RoBERTa SST config to make proper use of the CLS token Updated RoBERTa SNLI and MNLI pretrained models for latest transformers version","title":"Fixed"},{"location":"CHANGELOG/#added_10","text":"Added BART model Added ModelCard and related classes. Added model cards for all the pretrained models. Added a field registered_predictor_name to ModelCard . Added a method load_predictor to allennlp_models.pretrained . Added support to multi-layer decoder in simple seq2seq model.","title":"Added"},{"location":"CHANGELOG/#v110rc1-2020-07-14","text":"","title":"v1.1.0rc1 - 2020-07-14"},{"location":"CHANGELOG/#fixed_16","text":"Updated the BERT SRL model to be compatible with the new huggingface tokenizers. CopyNetSeq2Seq model now works with pretrained transformers. A bug with NextTokenLM that caused simple gradient interpreters to fail. A bug in training_config of qanet and bimpm that used the old version of regularizer and initializer . The fine-grained NER transformer model did not survive an upgrade of the transformers library, but it is now fixed. Fixed many minor formatting issues in docstrings. Docs are now published at https://docs.allennlp.org/models/ .","title":"Fixed"},{"location":"CHANGELOG/#changed_10","text":"CopyNetDatasetReader no longer automatically adds START_TOKEN and END_TOKEN to the tokenized source. If you want these in the tokenized source, it's up to the source tokenizer.","title":"Changed"},{"location":"CHANGELOG/#added_11","text":"Added two models for fine-grained NER Added a category for multiple choice models, including a few reference implementations Implemented manual distributed sharding for SNLI dataset reader.","title":"Added"},{"location":"CHANGELOG/#v100-2020-06-16","text":"No additional note-worthy changes since rc6.","title":"v1.0.0 - 2020-06-16"},{"location":"CHANGELOG/#v100rc6-2020-06-11","text":"","title":"v1.0.0rc6 - 2020-06-11"},{"location":"CHANGELOG/#changed_11","text":"Removed deprecated \"simple_seq2seq\" predictor","title":"Changed"},{"location":"CHANGELOG/#fixed_17","text":"Replaced deepcopy of Instance s with new Instance.duplicate() method. A bug where pretrained sentence taggers would fail to be initialized because some of the models were not imported. A bug in some RC models that would cause mixed precision training to crash when using NVIDIA apex. Predictor names were inconsistently switching between dashes and underscores. Now they all use underscores.","title":"Fixed"},{"location":"CHANGELOG/#added_12","text":"Added option to SemanticDependenciesDatasetReader to not skip instances that have no arcs, for validation data Added a default predictors to several models Added sentiment analysis models to pretrained.py Added NLI models to pretrained.py","title":"Added"},{"location":"CHANGELOG/#v100rc5-2020-05-14","text":"","title":"v1.0.0rc5 - 2020-05-14"},{"location":"CHANGELOG/#changed_12","text":"Moved the models into categories based on their format","title":"Changed"},{"location":"CHANGELOG/#fixed_18","text":"Made transformer_qa predictor accept JSON input with the keys \"question\" and \"passage\" to be consistent with the reading_comprehension predictor.","title":"Fixed"},{"location":"CHANGELOG/#added_13","text":"conllu dependency (previously part of allennlp 's dependencies)","title":"Added"},{"location":"CHANGELOG/#v100rc4-2020-05-14","text":"We first introduced this CHANGELOG after release v1.0.0rc4 , so please refer to the GitHub release notes for this and earlier releases.","title":"v1.0.0rc4 - 2020-05-14"},{"location":"models/pretrained/","text":"allennlp_models .pretrained [SOURCE] get_tasks # def get_tasks () -> Dict [ str , TaskCard ] Returns a mapping of TaskCard s for all tasks. get_pretrained_models # def get_pretrained_models () -> Dict [ str , ModelCard ] Returns a mapping of ModelCard s for all available pretrained models. load_predictor # def load_predictor ( model_id : str , pretrained_models : Dict [ str , ModelCard ] = None , overrides : Union [ str , Dict [ str , Any ]] = None ) -> Predictor Returns the Predictor corresponding to the given model_id . The model_id should be key present in the mapping returned by get_pretrained_models .","title":"pretrained"},{"location":"models/pretrained/#get_tasks","text":"def get_tasks () -> Dict [ str , TaskCard ] Returns a mapping of TaskCard s for all tasks.","title":"get_tasks"},{"location":"models/pretrained/#get_pretrained_models","text":"def get_pretrained_models () -> Dict [ str , ModelCard ] Returns a mapping of ModelCard s for all available pretrained models.","title":"get_pretrained_models"},{"location":"models/pretrained/#load_predictor","text":"def load_predictor ( model_id : str , pretrained_models : Dict [ str , ModelCard ] = None , overrides : Union [ str , Dict [ str , Any ]] = None ) -> Predictor Returns the Predictor corresponding to the given model_id . The model_id should be key present in the mapping returned by get_pretrained_models .","title":"load_predictor"},{"location":"models/classification/dataset_readers/boolq/","text":"allennlp_models .classification .dataset_readers .boolq [SOURCE] BoolQDatasetReader # @DatasetReader . register ( \"boolq\" ) class BoolQDatasetReader ( DatasetReader ): | def __init__ ( | self , | tokenizer : Tokenizer = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | ** kwargs | ) This DatasetReader is designed to read in the BoolQ data for binary QA task. It returns a dataset of instances with the following fields: The output of read is a list of Instance s with the fields: tokens : TextField and label : LabelField Registered as a DatasetReader with name \"boolq\". Parameters \u00b6 tokenizer : Tokenizer , optional (default = WhitespaceTokenizer() ) Tokenizer to use to split the input sequences into words or other kinds of tokens. token_indexers : Dict[str, TokenIndexer] , optional (default = {\"tokens\": SingleIdTokenIndexer()} ) We use this to define the input representation for the text. See TokenIndexer . text_to_instance # class BoolQDatasetReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | passage : str , | question : str , | label : Optional [ bool ] = None | ) -> Instance We take the passage and the question as input, tokenize and concat them. Parameters \u00b6 passage : str The passage in a given BoolQ record. question : str The passage in a given BoolQ record. label : bool , optional (default = None ) The label for the passage and the question. Returns \u00b6 An Instance containing the following fields: tokens : TextField The tokens in the concatenation of the passage and the question. label : LabelField The answer to the question. apply_token_indexers # class BoolQDatasetReader ( DatasetReader ): | ... | def apply_token_indexers ( self , instance : Instance ) -> None","title":"boolq"},{"location":"models/classification/dataset_readers/boolq/#boolqdatasetreader","text":"@DatasetReader . register ( \"boolq\" ) class BoolQDatasetReader ( DatasetReader ): | def __init__ ( | self , | tokenizer : Tokenizer = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | ** kwargs | ) This DatasetReader is designed to read in the BoolQ data for binary QA task. It returns a dataset of instances with the following fields: The output of read is a list of Instance s with the fields: tokens : TextField and label : LabelField Registered as a DatasetReader with name \"boolq\".","title":"BoolQDatasetReader"},{"location":"models/classification/dataset_readers/boolq/#text_to_instance","text":"class BoolQDatasetReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | passage : str , | question : str , | label : Optional [ bool ] = None | ) -> Instance We take the passage and the question as input, tokenize and concat them.","title":"text_to_instance"},{"location":"models/classification/dataset_readers/boolq/#apply_token_indexers","text":"class BoolQDatasetReader ( DatasetReader ): | ... | def apply_token_indexers ( self , instance : Instance ) -> None","title":"apply_token_indexers"},{"location":"models/classification/dataset_readers/stanford_sentiment_tree_bank/","text":"allennlp_models .classification .dataset_readers .stanford_sentiment_tree_bank [SOURCE] StanfordSentimentTreeBankDatasetReader # @DatasetReader . register ( \"sst_tokens\" ) class StanfordSentimentTreeBankDatasetReader ( DatasetReader ): | def __init__ ( | self , | token_indexers : Dict [ str , TokenIndexer ] = None , | tokenizer : Optional [ Tokenizer ] = None , | use_subtrees : bool = False , | granularity : str = \"5-class\" , | ** kwargs | ) -> None Reads tokens and their sentiment labels from the Stanford Sentiment Treebank. The Stanford Sentiment Treebank comes with labels from 0 to 4. \"5-class\" uses these labels as is. \"3-class\" converts the problem into one of identifying whether a sentence is negative, positive, or neutral sentiment. In this case, 0 and 1 are grouped as label 0 (negative sentiment), 2 is converted to label 1 (neutral sentiment) and 3 and 4 are grouped as label 2 (positive sentiment). \"2-class\" turns it into a binary classification problem between positive and negative sentiment. 0 and 1 are grouped as the label 0 (negative sentiment), 2 (neutral) is discarded, and 3 and 4 are grouped as the label 1 (positive sentiment). Expected format for each input line: a linearized tree, where nodes are labeled by their sentiment. The output of read is a list of Instance s with the fields: tokens : TextField and label : LabelField Registered as a DatasetReader with name \"sst_tokens\". Parameters \u00b6 token_indexers : Dict[str, TokenIndexer] , optional (default = {\"tokens\": SingleIdTokenIndexer()} ) We use this to define the input representation for the text. See TokenIndexer . use_subtrees : bool , optional (default = False ) Whether or not to use sentiment-tagged subtrees. granularity : str , optional (default = \"5-class\" ) One of \"5-class\" , \"3-class\" , or \"2-class\" , indicating the number of sentiment labels to use. text_to_instance # class StanfordSentimentTreeBankDatasetReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | tokens : List [ str ], | sentiment : str = None | ) -> Optional [ Instance ] We take pre-tokenized input here, because we might not have a tokenizer in this class. Parameters \u00b6 tokens : List[str] The tokens in a given sentence. sentiment : str , optional (default = None ) The sentiment for this sentence. Returns \u00b6 An Instance containing the following fields: tokens : TextField The tokens in the sentence or phrase. label : LabelField The sentiment label of the sentence or phrase.","title":"stanford_sentiment_tree_bank"},{"location":"models/classification/dataset_readers/stanford_sentiment_tree_bank/#stanfordsentimenttreebankdatasetreader","text":"@DatasetReader . register ( \"sst_tokens\" ) class StanfordSentimentTreeBankDatasetReader ( DatasetReader ): | def __init__ ( | self , | token_indexers : Dict [ str , TokenIndexer ] = None , | tokenizer : Optional [ Tokenizer ] = None , | use_subtrees : bool = False , | granularity : str = \"5-class\" , | ** kwargs | ) -> None Reads tokens and their sentiment labels from the Stanford Sentiment Treebank. The Stanford Sentiment Treebank comes with labels from 0 to 4. \"5-class\" uses these labels as is. \"3-class\" converts the problem into one of identifying whether a sentence is negative, positive, or neutral sentiment. In this case, 0 and 1 are grouped as label 0 (negative sentiment), 2 is converted to label 1 (neutral sentiment) and 3 and 4 are grouped as label 2 (positive sentiment). \"2-class\" turns it into a binary classification problem between positive and negative sentiment. 0 and 1 are grouped as the label 0 (negative sentiment), 2 (neutral) is discarded, and 3 and 4 are grouped as the label 1 (positive sentiment). Expected format for each input line: a linearized tree, where nodes are labeled by their sentiment. The output of read is a list of Instance s with the fields: tokens : TextField and label : LabelField Registered as a DatasetReader with name \"sst_tokens\".","title":"StanfordSentimentTreeBankDatasetReader"},{"location":"models/classification/dataset_readers/stanford_sentiment_tree_bank/#text_to_instance","text":"class StanfordSentimentTreeBankDatasetReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | tokens : List [ str ], | sentiment : str = None | ) -> Optional [ Instance ] We take pre-tokenized input here, because we might not have a tokenizer in this class.","title":"text_to_instance"},{"location":"models/classification/models/biattentive_classification_network/","text":"allennlp_models .classification .models .biattentive_classification_network [SOURCE] BiattentiveClassificationNetwork # @Model . register ( \"bcn\" ) class BiattentiveClassificationNetwork ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | embedding_dropout : float , | pre_encode_feedforward : FeedForward , | encoder : Seq2SeqEncoder , | integrator : Seq2SeqEncoder , | integrator_dropout : float , | output_layer : Union [ FeedForward , Maxout ], | elmo : Elmo = None , | use_input_elmo : bool = False , | use_integrator_output_elmo : bool = False , | initializer : InitializerApplicator = InitializerApplicator (), | ** kwargs | ) -> None This class implements the Biattentive Classification Network model described in section 5 of Learned in Translation: Contextualized Word Vectors (NIPS 2017) for text classification. We assume we're given a piece of text, and we predict some output label. At a high level, the model starts by embedding the tokens and running them through a feed-forward neural net ( pre_encode_feedforward ). Then, we encode these representations with a Seq2SeqEncoder ( encoder ). We run biattention on the encoder output representations (self-attention in this case, since the two representations that typically go into biattention are identical) and get out an attentive vector representation of the text. We combine this text representation with the encoder outputs computed earlier, and then run this through yet another Seq2SeqEncoder (the integrator ). Lastly, we take the output of the integrator and max, min, mean, and self-attention pool to create a final representation, which is passed through a maxout network or some feed-forward layers to output a classification ( output_layer ). Registered as a Model with name \"bcn\". Parameters \u00b6 vocab : Vocabulary A Vocabulary, required in order to compute sizes for input/output projections. text_field_embedder : TextFieldEmbedder Used to embed the tokens TextField we get as input to the model. embedding_dropout : float The amount of dropout to apply on the embeddings. pre_encode_feedforward : FeedForward A feedforward network that is run on the embedded tokens before they are passed to the encoder. encoder : Seq2SeqEncoder The encoder to use on the tokens. integrator : Seq2SeqEncoder The encoder to use when integrating the attentive text encoding with the token encodings. integrator_dropout : float The amount of dropout to apply on integrator output. output_layer : Union[Maxout, FeedForward] The maxout or feed forward network that takes the final representations and produces a classification prediction. elmo : Elmo , optional (default = None ) If provided, will be used to concatenate pretrained ELMo representations to either the integrator output ( use_integrator_output_elmo ) or the input ( use_input_elmo ). use_input_elmo : bool , optional (default = False ) If true, concatenate pretrained ELMo representations to the input vectors. use_integrator_output_elmo : bool , optional (default = False ) If true, concatenate pretrained ELMo representations to the integrator output. initializer : InitializerApplicator , optional (default = InitializerApplicator() ) Used to initialize the model parameters. forward # class BiattentiveClassificationNetwork ( Model ): | ... | @overrides | def forward ( | self , | tokens : TextFieldTensors , | label : torch . LongTensor = None | ) -> Dict [ str , torch . Tensor ] Parameters \u00b6 tokens : TextFieldTensors The output of TextField.as_array() . label : torch.LongTensor , optional (default = None ) A variable representing the label for each instance in the batch. Returns \u00b6 An output dictionary consisting of: class_probabilities ( torch.FloatTensor ) : A tensor of shape (batch_size, num_classes) representing a distribution over the label classes for each instance. loss ( torch.FloatTensor , optional) : A scalar loss to be optimised. make_output_human_readable # class BiattentiveClassificationNetwork ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Does a simple argmax over the class probabilities, converts indices to string labels, and adds a \"label\" key to the dictionary with the result. get_metrics # class BiattentiveClassificationNetwork ( Model ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"biattentive_classification_network"},{"location":"models/classification/models/biattentive_classification_network/#biattentiveclassificationnetwork","text":"@Model . register ( \"bcn\" ) class BiattentiveClassificationNetwork ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | embedding_dropout : float , | pre_encode_feedforward : FeedForward , | encoder : Seq2SeqEncoder , | integrator : Seq2SeqEncoder , | integrator_dropout : float , | output_layer : Union [ FeedForward , Maxout ], | elmo : Elmo = None , | use_input_elmo : bool = False , | use_integrator_output_elmo : bool = False , | initializer : InitializerApplicator = InitializerApplicator (), | ** kwargs | ) -> None This class implements the Biattentive Classification Network model described in section 5 of Learned in Translation: Contextualized Word Vectors (NIPS 2017) for text classification. We assume we're given a piece of text, and we predict some output label. At a high level, the model starts by embedding the tokens and running them through a feed-forward neural net ( pre_encode_feedforward ). Then, we encode these representations with a Seq2SeqEncoder ( encoder ). We run biattention on the encoder output representations (self-attention in this case, since the two representations that typically go into biattention are identical) and get out an attentive vector representation of the text. We combine this text representation with the encoder outputs computed earlier, and then run this through yet another Seq2SeqEncoder (the integrator ). Lastly, we take the output of the integrator and max, min, mean, and self-attention pool to create a final representation, which is passed through a maxout network or some feed-forward layers to output a classification ( output_layer ). Registered as a Model with name \"bcn\".","title":"BiattentiveClassificationNetwork"},{"location":"models/classification/models/biattentive_classification_network/#forward","text":"class BiattentiveClassificationNetwork ( Model ): | ... | @overrides | def forward ( | self , | tokens : TextFieldTensors , | label : torch . LongTensor = None | ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"models/classification/models/biattentive_classification_network/#make_output_human_readable","text":"class BiattentiveClassificationNetwork ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Does a simple argmax over the class probabilities, converts indices to string labels, and adds a \"label\" key to the dictionary with the result.","title":"make_output_human_readable"},{"location":"models/classification/models/biattentive_classification_network/#get_metrics","text":"class BiattentiveClassificationNetwork ( Model ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"models/common/ontonotes/","text":"allennlp_models .common .ontonotes [SOURCE] OntonotesSentence # class OntonotesSentence : | def __init__ ( | self , | document_id : str , | sentence_id : int , | words : List [ str ], | pos_tags : List [ str ], | parse_tree : Optional [ Tree ], | predicate_lemmas : List [ Optional [ str ]], | predicate_framenet_ids : List [ Optional [ str ]], | word_senses : List [ Optional [ float ]], | speakers : List [ Optional [ str ]], | named_entities : List [ str ], | srl_frames : List [ Tuple [ str , List [ str ]]], | coref_spans : Set [ TypedSpan ] | ) -> None A class representing the annotations available for a single CONLL formatted sentence. Parameters \u00b6 document_id : str This is a variation on the document filename sentence_id : int The integer ID of the sentence within a document. words : List[str] This is the tokens as segmented/tokenized in the Treebank. pos_tags : List[str] This is the Penn-Treebank-style part of speech. When parse information is missing, all parts of speech except the one for which there is some sense or proposition annotation are marked with a XX tag. The verb is marked with just a VERB tag. parse_tree : nltk.Tree An nltk Tree representing the parse. It includes POS tags as pre-terminal nodes. When the parse information is missing, the parse will be None . predicate_lemmas : List[Optional[str]] The predicate lemma of the words for which we have semantic role information or word sense information. All other indices are None . predicate_framenet_ids : List[Optional[int]] The PropBank frameset ID of the lemmas in predicate_lemmas , or None . word_senses : List[Optional[float]] The word senses for the words in the sentence, or None . These are floats because the word sense can have values after the decimal, like 1.1 . speakers : List[Optional[str]] The speaker information for the words in the sentence, if present, or None This is the speaker or author name where available. Mostly in Broadcast Conversation and Web Log data. When not available the rows are marked with an \"-\". named_entities : List[str] The BIO tags for named entities in the sentence. srl_frames : List[Tuple[str, List[str]]] A dictionary keyed by the verb in the sentence for the given Propbank frame labels, in a BIO format. coref_spans : Set[TypedSpan] The spans for entity mentions involved in coreference resolution within the sentence. Each element is a tuple composed of (cluster_id, (start_index, end_index)). Indices are inclusive . Ontonotes # class Ontonotes This DatasetReader is designed to read in the English OntoNotes v5.0 data in the format used by the CoNLL 2011/2012 shared tasks. In order to use this Reader, you must follow the instructions provided here (v12 release): , which will allow you to download the CoNLL style annotations for the OntoNotes v5.0 release -- LDC2013T19.tgz obtained from LDC. Once you have run the scripts on the extracted data, you will have a folder structured as follows: conll-formatted-ontonotes-5.0/ \u2500\u2500 data \u251c\u2500\u2500 development \u2514\u2500\u2500 data \u2514\u2500\u2500 english \u2514\u2500\u2500 annotations \u251c\u2500\u2500 bc \u251c\u2500\u2500 bn \u251c\u2500\u2500 mz \u251c\u2500\u2500 nw \u251c\u2500\u2500 pt \u251c\u2500\u2500 tc \u2514\u2500\u2500 wb \u251c\u2500\u2500 test \u2514\u2500\u2500 data \u2514\u2500\u2500 english \u2514\u2500\u2500 annotations \u251c\u2500\u2500 bc \u251c\u2500\u2500 bn \u251c\u2500\u2500 mz \u251c\u2500\u2500 nw \u251c\u2500\u2500 pt \u251c\u2500\u2500 tc \u2514\u2500\u2500 wb \u2514\u2500\u2500 train \u2514\u2500\u2500 data \u2514\u2500\u2500 english \u2514\u2500\u2500 annotations \u251c\u2500\u2500 bc \u251c\u2500\u2500 bn \u251c\u2500\u2500 mz \u251c\u2500\u2500 nw \u251c\u2500\u2500 pt \u251c\u2500\u2500 tc \u2514\u2500\u2500 wb The file path provided to this class can then be any of the train, test or development directories(or the top level data directory, if you are not utilizing the splits). The data has the following format, ordered by column. Document ID : str This is a variation on the document filename Part number : int Some files are divided into multiple parts numbered as 000, 001, 002, ... etc. Word number : int This is the word index of the word in that sentence. Word : str This is the token as segmented/tokenized in the Treebank. Initially the *_skel file contain the placeholder [WORD] which gets replaced by the actual token from the Treebank which is part of the OntoNotes release. POS Tag : str This is the Penn Treebank style part of speech. When parse information is missing, all part of speeches except the one for which there is some sense or proposition annotation are marked with a XX tag. The verb is marked with just a VERB tag. Parse bit : str This is the bracketed structure broken before the first open parenthesis in the parse, and the word/part-of-speech leaf replaced with a * . When the parse information is missing, the first word of a sentence is tagged as (TOP* and the last word is tagged as *) and all intermediate words are tagged with a * . Predicate lemma : str The predicate lemma is mentioned for the rows for which we have semantic role information or word sense information. All other rows are marked with a \"-\". Predicate Frameset ID : int The PropBank frameset ID of the predicate in Column 7. Word sense : float This is the word sense of the word in Column 3. Speaker/Author : str This is the speaker or author name where available. Mostly in Broadcast Conversation and Web Log data. When not available the rows are marked with an \"-\". Named Entities : str These columns identifies the spans representing various named entities. For documents which do not have named entity annotation, each line is represented with an * . Predicate Arguments : str There is one column each of predicate argument structure information for the predicate mentioned in Column 7. If there are no predicates tagged in a sentence this is a single column with all rows marked with an * . -1. Co-reference : str Co-reference chain information encoded in a parenthesis structure. For documents that do not have co-reference annotations, each line is represented with a \"-\". dataset_iterator # class Ontonotes : | ... | def dataset_iterator ( | self , | file_path : str | ) -> Iterator [ OntonotesSentence ] An iterator over the entire dataset, yielding all sentences processed. dataset_path_iterator # class Ontonotes : | ... | @staticmethod | def dataset_path_iterator ( file_path : str ) -> Iterator [ str ] An iterator returning file_paths in a directory containing CONLL-formatted files. dataset_document_iterator # class Ontonotes : | ... | def dataset_document_iterator ( | self , | file_path : str | ) -> Iterator [ List [ OntonotesSentence ]] An iterator over CONLL formatted files which yields documents, regardless of the number of document annotations in a particular file. This is useful for conll data which has been preprocessed, such as the preprocessing which takes place for the 2012 CONLL Coreference Resolution task. sentence_iterator # class Ontonotes : | ... | def sentence_iterator ( | self , | file_path : str | ) -> Iterator [ OntonotesSentence ] An iterator over the sentences in an individual CONLL formatted file.","title":"ontonotes"},{"location":"models/common/ontonotes/#ontonotessentence","text":"class OntonotesSentence : | def __init__ ( | self , | document_id : str , | sentence_id : int , | words : List [ str ], | pos_tags : List [ str ], | parse_tree : Optional [ Tree ], | predicate_lemmas : List [ Optional [ str ]], | predicate_framenet_ids : List [ Optional [ str ]], | word_senses : List [ Optional [ float ]], | speakers : List [ Optional [ str ]], | named_entities : List [ str ], | srl_frames : List [ Tuple [ str , List [ str ]]], | coref_spans : Set [ TypedSpan ] | ) -> None A class representing the annotations available for a single CONLL formatted sentence.","title":"OntonotesSentence"},{"location":"models/common/ontonotes/#ontonotes","text":"class Ontonotes This DatasetReader is designed to read in the English OntoNotes v5.0 data in the format used by the CoNLL 2011/2012 shared tasks. In order to use this Reader, you must follow the instructions provided here (v12 release): , which will allow you to download the CoNLL style annotations for the OntoNotes v5.0 release -- LDC2013T19.tgz obtained from LDC. Once you have run the scripts on the extracted data, you will have a folder structured as follows: conll-formatted-ontonotes-5.0/ \u2500\u2500 data \u251c\u2500\u2500 development \u2514\u2500\u2500 data \u2514\u2500\u2500 english \u2514\u2500\u2500 annotations \u251c\u2500\u2500 bc \u251c\u2500\u2500 bn \u251c\u2500\u2500 mz \u251c\u2500\u2500 nw \u251c\u2500\u2500 pt \u251c\u2500\u2500 tc \u2514\u2500\u2500 wb \u251c\u2500\u2500 test \u2514\u2500\u2500 data \u2514\u2500\u2500 english \u2514\u2500\u2500 annotations \u251c\u2500\u2500 bc \u251c\u2500\u2500 bn \u251c\u2500\u2500 mz \u251c\u2500\u2500 nw \u251c\u2500\u2500 pt \u251c\u2500\u2500 tc \u2514\u2500\u2500 wb \u2514\u2500\u2500 train \u2514\u2500\u2500 data \u2514\u2500\u2500 english \u2514\u2500\u2500 annotations \u251c\u2500\u2500 bc \u251c\u2500\u2500 bn \u251c\u2500\u2500 mz \u251c\u2500\u2500 nw \u251c\u2500\u2500 pt \u251c\u2500\u2500 tc \u2514\u2500\u2500 wb The file path provided to this class can then be any of the train, test or development directories(or the top level data directory, if you are not utilizing the splits). The data has the following format, ordered by column. Document ID : str This is a variation on the document filename Part number : int Some files are divided into multiple parts numbered as 000, 001, 002, ... etc. Word number : int This is the word index of the word in that sentence. Word : str This is the token as segmented/tokenized in the Treebank. Initially the *_skel file contain the placeholder [WORD] which gets replaced by the actual token from the Treebank which is part of the OntoNotes release. POS Tag : str This is the Penn Treebank style part of speech. When parse information is missing, all part of speeches except the one for which there is some sense or proposition annotation are marked with a XX tag. The verb is marked with just a VERB tag. Parse bit : str This is the bracketed structure broken before the first open parenthesis in the parse, and the word/part-of-speech leaf replaced with a * . When the parse information is missing, the first word of a sentence is tagged as (TOP* and the last word is tagged as *) and all intermediate words are tagged with a * . Predicate lemma : str The predicate lemma is mentioned for the rows for which we have semantic role information or word sense information. All other rows are marked with a \"-\". Predicate Frameset ID : int The PropBank frameset ID of the predicate in Column 7. Word sense : float This is the word sense of the word in Column 3. Speaker/Author : str This is the speaker or author name where available. Mostly in Broadcast Conversation and Web Log data. When not available the rows are marked with an \"-\". Named Entities : str These columns identifies the spans representing various named entities. For documents which do not have named entity annotation, each line is represented with an * . Predicate Arguments : str There is one column each of predicate argument structure information for the predicate mentioned in Column 7. If there are no predicates tagged in a sentence this is a single column with all rows marked with an * . -1. Co-reference : str Co-reference chain information encoded in a parenthesis structure. For documents that do not have co-reference annotations, each line is represented with a \"-\".","title":"Ontonotes"},{"location":"models/common/ontonotes/#dataset_iterator","text":"class Ontonotes : | ... | def dataset_iterator ( | self , | file_path : str | ) -> Iterator [ OntonotesSentence ] An iterator over the entire dataset, yielding all sentences processed.","title":"dataset_iterator"},{"location":"models/common/ontonotes/#dataset_path_iterator","text":"class Ontonotes : | ... | @staticmethod | def dataset_path_iterator ( file_path : str ) -> Iterator [ str ] An iterator returning file_paths in a directory containing CONLL-formatted files.","title":"dataset_path_iterator"},{"location":"models/common/ontonotes/#dataset_document_iterator","text":"class Ontonotes : | ... | def dataset_document_iterator ( | self , | file_path : str | ) -> Iterator [ List [ OntonotesSentence ]] An iterator over CONLL formatted files which yields documents, regardless of the number of document annotations in a particular file. This is useful for conll data which has been preprocessed, such as the preprocessing which takes place for the 2012 CONLL Coreference Resolution task.","title":"dataset_document_iterator"},{"location":"models/common/ontonotes/#sentence_iterator","text":"class Ontonotes : | ... | def sentence_iterator ( | self , | file_path : str | ) -> Iterator [ OntonotesSentence ] An iterator over the sentences in an individual CONLL formatted file.","title":"sentence_iterator"},{"location":"models/coref/util/","text":"allennlp_models .coref .util [SOURCE] make_coref_instance # def make_coref_instance ( sentences : List [ List [ str ]], token_indexers : Dict [ str , TokenIndexer ], max_span_width : int , gold_clusters : Optional [ List [ List [ Tuple [ int , int ]]]] = None , wordpiece_modeling_tokenizer : PretrainedTransformerTokenizer = None , max_sentences : int = None , remove_singleton_clusters : bool = True ) -> Instance Parameters \u00b6 sentences : List[List[str]] A list of lists representing the tokenised words and sentences in the document. token_indexers : Dict[str, TokenIndexer] This is used to index the words in the document. See TokenIndexer . max_span_width : int The maximum width of candidate spans to consider. gold_clusters : Optional[List[List[Tuple[int, int]]]] , optional (default = None ) A list of all clusters in the document, represented as word spans with absolute indices in the entire document. Each cluster contains some number of spans, which can be nested and overlap. If there are exact matches between clusters, they will be resolved using _canonicalize_clusters . wordpiece_modeling_tokenizer : PretrainedTransformerTokenizer , optional (default = None ) If not None, this dataset reader does subword tokenization using the supplied tokenizer and distribute the labels to the resulting wordpieces. All the modeling will be based on wordpieces. If this is set to False (default), the user is expected to use PretrainedTransformerMismatchedIndexer and PretrainedTransformerMismatchedEmbedder , and the modeling will be on the word-level. max_sentences : int , optional (default = None ) The maximum number of sentences in each document to keep. By default keeps all sentences. remove_singleton_clusters : bool , optional (default = True ) Some datasets contain clusters that are singletons (i.e. no coreferents). This option allows the removal of them. Returns \u00b6 An Instance containing the following Fields : text : TextField The text of the full document. spans : ListField[SpanField] A ListField containing the spans represented as SpanFields with respect to the document text. span_labels : SequenceLabelField , optional The id of the cluster which each possible span belongs to, or -1 if it does not belong to a cluster. As these labels have variable length (it depends on how many spans we are considering), we represent this a as a SequenceLabelField with respect to the spans ListField .","title":"util"},{"location":"models/coref/util/#make_coref_instance","text":"def make_coref_instance ( sentences : List [ List [ str ]], token_indexers : Dict [ str , TokenIndexer ], max_span_width : int , gold_clusters : Optional [ List [ List [ Tuple [ int , int ]]]] = None , wordpiece_modeling_tokenizer : PretrainedTransformerTokenizer = None , max_sentences : int = None , remove_singleton_clusters : bool = True ) -> Instance","title":"make_coref_instance"},{"location":"models/coref/dataset_readers/conll/","text":"allennlp_models .coref .dataset_readers .conll [SOURCE] ConllCorefReader # @DatasetReader . register ( \"coref\" ) class ConllCorefReader ( DatasetReader ): | def __init__ ( | self , | max_span_width : int , | token_indexers : Dict [ str , TokenIndexer ] = None , | wordpiece_modeling_tokenizer : Optional [ PretrainedTransformerTokenizer ] = None , | max_sentences : int = None , | remove_singleton_clusters : bool = False , | ** kwargs | ) -> None Reads a single CoNLL-formatted file. This is the same file format as used in the allennlp.data.dataset_readers.semantic_role_labelling.SrlReader , but is preprocessed to dump all documents into a single file per train, dev and test split. See scripts/compile_coref_data.sh for more details of how to pre-process the Ontonotes 5.0 data into the correct format. Returns a Dataset where the Instances have four fields : text , a TextField containing the full document text, spans , a ListField[SpanField] of inclusive start and end indices for span candidates, and metadata , a MetadataField that stores the instance's original text. For data with gold cluster labels, we also include the original clusters (a list of list of index pairs) and a SequenceLabelField of cluster ids for every span candidate. Parameters \u00b6 max_span_width : int The maximum width of candidate spans to consider. token_indexers : Dict[str, TokenIndexer] , optional This is used to index the words in the document. See TokenIndexer . Default is {\"tokens\": SingleIdTokenIndexer()} . wordpiece_modeling_tokenizer : PretrainedTransformerTokenizer , optional (default = None ) If not None, this dataset reader does subword tokenization using the supplied tokenizer and distribute the labels to the resulting wordpieces. All the modeling will be based on wordpieces. If this is set to False (default), the user is expected to use PretrainedTransformerMismatchedIndexer and PretrainedTransformerMismatchedEmbedder , and the modeling will be on the word-level. max_sentences : int , optional (default = None ) The maximum number of sentences in each document to keep. By default keeps all sentences. remove_singleton_clusters : bool , optional (default = False ) Some datasets contain clusters that are singletons (i.e. no coreferents). This option allows the removal of them. Ontonotes shouldn't have these, and this option should be used for testing only. text_to_instance # class ConllCorefReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | sentences : List [ List [ str ]], | gold_clusters : Optional [ List [ List [ Tuple [ int , int ]]]] = None | ) -> Instance","title":"conll"},{"location":"models/coref/dataset_readers/conll/#conllcorefreader","text":"@DatasetReader . register ( \"coref\" ) class ConllCorefReader ( DatasetReader ): | def __init__ ( | self , | max_span_width : int , | token_indexers : Dict [ str , TokenIndexer ] = None , | wordpiece_modeling_tokenizer : Optional [ PretrainedTransformerTokenizer ] = None , | max_sentences : int = None , | remove_singleton_clusters : bool = False , | ** kwargs | ) -> None Reads a single CoNLL-formatted file. This is the same file format as used in the allennlp.data.dataset_readers.semantic_role_labelling.SrlReader , but is preprocessed to dump all documents into a single file per train, dev and test split. See scripts/compile_coref_data.sh for more details of how to pre-process the Ontonotes 5.0 data into the correct format. Returns a Dataset where the Instances have four fields : text , a TextField containing the full document text, spans , a ListField[SpanField] of inclusive start and end indices for span candidates, and metadata , a MetadataField that stores the instance's original text. For data with gold cluster labels, we also include the original clusters (a list of list of index pairs) and a SequenceLabelField of cluster ids for every span candidate.","title":"ConllCorefReader"},{"location":"models/coref/dataset_readers/conll/#text_to_instance","text":"class ConllCorefReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | sentences : List [ List [ str ]], | gold_clusters : Optional [ List [ List [ Tuple [ int , int ]]]] = None | ) -> Instance","title":"text_to_instance"},{"location":"models/coref/dataset_readers/preco/","text":"allennlp_models .coref .dataset_readers .preco [SOURCE] PrecoReader # @DatasetReader . register ( \"preco\" ) class PrecoReader ( DatasetReader ): | def __init__ ( | self , | max_span_width : int , | token_indexers : Dict [ str , TokenIndexer ] = None , | wordpiece_modeling_tokenizer : Optional [ PretrainedTransformerTokenizer ] = None , | max_sentences : int = None , | remove_singleton_clusters : bool = True , | ** kwargs | ) -> None Reads a single JSON-lines file for the PreCo dataset . Each line contains a \"sentences\" key for a list of sentences and a \"mention_clusters\" key for the clusters. Returns a Dataset where the Instances have four fields : text , a TextField containing the full document text, spans , a ListField[SpanField] of inclusive start and end indices for span candidates, and metadata , a MetadataField that stores the instance's original text. For data with gold cluster labels, we also include the original clusters (a list of list of index pairs) and a SequenceLabelField of cluster ids for every span candidate. Parameters \u00b6 max_span_width : int The maximum width of candidate spans to consider. token_indexers : Dict[str, TokenIndexer] , optional This is used to index the words in the document. See TokenIndexer . Default is {\"tokens\": SingleIdTokenIndexer()} . wordpiece_modeling_tokenizer : PretrainedTransformerTokenizer , optional (default = None ) If not None, this dataset reader does subword tokenization using the supplied tokenizer and distribute the labels to the resulting wordpieces. All the modeling will be based on wordpieces. If this is set to False (default), the user is expected to use PretrainedTransformerMismatchedIndexer and PretrainedTransformerMismatchedEmbedder , and the modeling will be on the word-level. max_sentences : int , optional (default = None ) The maximum number of sentences in each document to keep. By default keeps all sentences. remove_singleton_clusters : bool , optional (default = False ) Some datasets contain clusters that are singletons (i.e. no coreferents). This option allows the removal of them. text_to_instance # class PrecoReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | sentences : List [ List [ str ]], | gold_clusters : Optional [ List [ List [ Tuple [ int , int , int ]]]] = None | ) -> Instance","title":"preco"},{"location":"models/coref/dataset_readers/preco/#precoreader","text":"@DatasetReader . register ( \"preco\" ) class PrecoReader ( DatasetReader ): | def __init__ ( | self , | max_span_width : int , | token_indexers : Dict [ str , TokenIndexer ] = None , | wordpiece_modeling_tokenizer : Optional [ PretrainedTransformerTokenizer ] = None , | max_sentences : int = None , | remove_singleton_clusters : bool = True , | ** kwargs | ) -> None Reads a single JSON-lines file for the PreCo dataset . Each line contains a \"sentences\" key for a list of sentences and a \"mention_clusters\" key for the clusters. Returns a Dataset where the Instances have four fields : text , a TextField containing the full document text, spans , a ListField[SpanField] of inclusive start and end indices for span candidates, and metadata , a MetadataField that stores the instance's original text. For data with gold cluster labels, we also include the original clusters (a list of list of index pairs) and a SequenceLabelField of cluster ids for every span candidate.","title":"PrecoReader"},{"location":"models/coref/dataset_readers/preco/#text_to_instance","text":"class PrecoReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | sentences : List [ List [ str ]], | gold_clusters : Optional [ List [ List [ Tuple [ int , int , int ]]]] = None | ) -> Instance","title":"text_to_instance"},{"location":"models/coref/dataset_readers/winobias/","text":"allennlp_models .coref .dataset_readers .winobias [SOURCE] WinobiasReader # @DatasetReader . register ( \"winobias\" ) class WinobiasReader ( DatasetReader ): | def __init__ ( | self , | max_span_width : int , | token_indexers : Dict [ str , TokenIndexer ] = None , | ** kwargs | ) -> None A dataset reader for the dataset described in Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods Winobias is a dataset to analyse the issue of gender bias in co-reference resolution. It contains simple sentences with pro/anti stereotypical gender associations with which to measure the bias of a coreference system trained on another corpus. It is effectively a toy dataset and as such, uses very simplistic language; it has little use outside of evaluating a model for bias. The dataset is formatted with a single sentence per line, with a maximum of 2 non-nested coreference clusters annotated using either square or round brackets. For example: [The salesperson] sold (some books) to the librarian because [she] was trying to sell (them). Returns a list of Instances which have four fields : text , a TextField containing the full sentence text, spans , a ListField[SpanField] of inclusive start and end indices for span candidates, and metadata , a MetadataField that stores the instance's original text. For data with gold cluster labels, we also include the original clusters (a list of list of index pairs) and a SequenceLabelField of cluster ids for every span candidate in the metadata also. Parameters \u00b6 max_span_width : int The maximum width of candidate spans to consider. token_indexers : Dict[str, TokenIndexer] , optional This is used to index the words in the sentence. See TokenIndexer . Default is {\"tokens\": SingleIdTokenIndexer()} . text_to_instance # class WinobiasReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | sentence : List [ Token ], | gold_clusters : Optional [ List [ List [ Tuple [ int , int ]]]] = None | ) -> Instance Parameters \u00b6 sentence : List[Token] The already tokenised sentence to analyse. gold_clusters : Optional[List[List[Tuple[int, int]]]] , optional (default = None ) A list of all clusters in the sentence, represented as word spans. Each cluster contains some number of spans, which can be nested and overlap, but will never exactly match between clusters. Returns \u00b6 An Instance containing the following Fields : text : TextField The text of the full sentence. spans : ListField[SpanField] A ListField containing the spans represented as SpanFields with respect to the sentence text. span_labels : SequenceLabelField , optional The id of the cluster which each possible span belongs to, or -1 if it does not belong to a cluster. As these labels have variable length (it depends on how many spans we are considering), we represent this a as a SequenceLabelField with respect to the spans ListField .","title":"winobias"},{"location":"models/coref/dataset_readers/winobias/#winobiasreader","text":"@DatasetReader . register ( \"winobias\" ) class WinobiasReader ( DatasetReader ): | def __init__ ( | self , | max_span_width : int , | token_indexers : Dict [ str , TokenIndexer ] = None , | ** kwargs | ) -> None A dataset reader for the dataset described in Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods Winobias is a dataset to analyse the issue of gender bias in co-reference resolution. It contains simple sentences with pro/anti stereotypical gender associations with which to measure the bias of a coreference system trained on another corpus. It is effectively a toy dataset and as such, uses very simplistic language; it has little use outside of evaluating a model for bias. The dataset is formatted with a single sentence per line, with a maximum of 2 non-nested coreference clusters annotated using either square or round brackets. For example: [The salesperson] sold (some books) to the librarian because [she] was trying to sell (them). Returns a list of Instances which have four fields : text , a TextField containing the full sentence text, spans , a ListField[SpanField] of inclusive start and end indices for span candidates, and metadata , a MetadataField that stores the instance's original text. For data with gold cluster labels, we also include the original clusters (a list of list of index pairs) and a SequenceLabelField of cluster ids for every span candidate in the metadata also.","title":"WinobiasReader"},{"location":"models/coref/dataset_readers/winobias/#text_to_instance","text":"class WinobiasReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | sentence : List [ Token ], | gold_clusters : Optional [ List [ List [ Tuple [ int , int ]]]] = None | ) -> Instance","title":"text_to_instance"},{"location":"models/coref/metrics/conll_coref_scores/","text":"allennlp_models .coref .metrics .conll_coref_scores [SOURCE] ConllCorefScores # @Metric . register ( \"conll_coref_scores\" ) class ConllCorefScores ( Metric ): | def __init__ ( self ) -> None supports_distributed # class ConllCorefScores ( Metric ): | ... | supports_distributed = True __call__ # class ConllCorefScores ( Metric ): | ... | @overrides | def __call__ ( | self , | top_spans : torch . Tensor , | antecedent_indices : torch . Tensor , | predicted_antecedents : torch . Tensor , | metadata_list : List [ Dict [ str , Any ]] | ) Parameters \u00b6 top_spans : torch.Tensor (start, end) indices for all spans kept after span pruning in the model. Expected shape: (batch_size, num_spans, 2) antecedent_indices : torch.Tensor For each span, the indices of all allowed antecedents for that span. Expected shape: (batch_size, num_spans, num_antecedents) predicted_antecedents : torch.Tensor For each span, this contains the index (into antecedent_indices) of the most likely antecedent for that span. Expected shape: (batch_size, num_spans) metadata_list : List[Dict[str, Any]] A metadata dictionary for each instance in the batch. We use the \"clusters\" key from this dictionary, which has the annotated gold coreference clusters for that instance. get_metric # class ConllCorefScores ( Metric ): | ... | @overrides | def get_metric ( | self , | reset : bool = False | ) -> Tuple [ float , float , float ] reset # class ConllCorefScores ( Metric ): | ... | @overrides | def reset ( self ) get_gold_clusters # class ConllCorefScores ( Metric ): | ... | @staticmethod | def get_gold_clusters ( gold_clusters ) get_predicted_clusters # class ConllCorefScores ( Metric ): | ... | @staticmethod | def get_predicted_clusters ( | top_spans : torch . Tensor , | antecedent_indices : torch . Tensor , | predicted_antecedents : torch . Tensor | ) -> Tuple [ | List [ Tuple [ Tuple [ int , int ], ... ]], Dict [ Tuple [ int , int ], Tuple [ Tuple [ int , int ], ... ]] | ] Scorer # class Scorer : | def __init__ ( self , metric ) Mostly borrowed from https://github.com/clarkkev/deep-coref/blob/master/evaluation.py update # class Scorer : | ... | def update ( | self , | predicted , | gold , | mention_to_predicted , | mention_to_gold | ) get_f1 # class Scorer : | ... | def get_f1 ( self ) get_recall # class Scorer : | ... | def get_recall ( self ) get_precision # class Scorer : | ... | def get_precision ( self ) get_prf # class Scorer : | ... | def get_prf ( self ) b_cubed # class Scorer : | ... | @staticmethod | def b_cubed ( clusters , mention_to_gold ) Averaged per-mention precision and recall. https://pdfs.semanticscholar.org/cfe3/c24695f1c14b78a5b8e95bcbd1c666140fd1.pdf muc # class Scorer : | ... | @staticmethod | def muc ( clusters , mention_to_gold ) Counts the mentions in each predicted cluster which need to be re-allocated in order for each predicted cluster to be contained by the respective gold cluster. https://aclweb.org/anthology/M/M95/M95-1005.pdf phi4 # class Scorer : | ... | @staticmethod | def phi4 ( gold_clustering , predicted_clustering ) Subroutine for ceafe. Computes the mention F measure between gold and predicted mentions in a cluster. ceafe # class Scorer : | ... | @staticmethod | def ceafe ( clusters , gold_clusters ) Computes the Constrained Entity-Alignment F-Measure (CEAF) for evaluating coreference. Gold and predicted mentions are aligned into clusterings which maximise a metric - in this case, the F measure between gold and predicted clusters. https://www.semanticscholar.org/paper/On-Coreference-Resolution-Performance-Metrics-Luo/de133c1f22d0dfe12539e25dda70f28672459b99","title":"conll_coref_scores"},{"location":"models/coref/metrics/conll_coref_scores/#conllcorefscores","text":"@Metric . register ( \"conll_coref_scores\" ) class ConllCorefScores ( Metric ): | def __init__ ( self ) -> None","title":"ConllCorefScores"},{"location":"models/coref/metrics/conll_coref_scores/#supports_distributed","text":"class ConllCorefScores ( Metric ): | ... | supports_distributed = True","title":"supports_distributed"},{"location":"models/coref/metrics/conll_coref_scores/#__call__","text":"class ConllCorefScores ( Metric ): | ... | @overrides | def __call__ ( | self , | top_spans : torch . Tensor , | antecedent_indices : torch . Tensor , | predicted_antecedents : torch . Tensor , | metadata_list : List [ Dict [ str , Any ]] | )","title":"__call__"},{"location":"models/coref/metrics/conll_coref_scores/#get_metric","text":"class ConllCorefScores ( Metric ): | ... | @overrides | def get_metric ( | self , | reset : bool = False | ) -> Tuple [ float , float , float ]","title":"get_metric"},{"location":"models/coref/metrics/conll_coref_scores/#reset","text":"class ConllCorefScores ( Metric ): | ... | @overrides | def reset ( self )","title":"reset"},{"location":"models/coref/metrics/conll_coref_scores/#get_gold_clusters","text":"class ConllCorefScores ( Metric ): | ... | @staticmethod | def get_gold_clusters ( gold_clusters )","title":"get_gold_clusters"},{"location":"models/coref/metrics/conll_coref_scores/#get_predicted_clusters","text":"class ConllCorefScores ( Metric ): | ... | @staticmethod | def get_predicted_clusters ( | top_spans : torch . Tensor , | antecedent_indices : torch . Tensor , | predicted_antecedents : torch . Tensor | ) -> Tuple [ | List [ Tuple [ Tuple [ int , int ], ... ]], Dict [ Tuple [ int , int ], Tuple [ Tuple [ int , int ], ... ]] | ]","title":"get_predicted_clusters"},{"location":"models/coref/metrics/conll_coref_scores/#scorer","text":"class Scorer : | def __init__ ( self , metric ) Mostly borrowed from https://github.com/clarkkev/deep-coref/blob/master/evaluation.py","title":"Scorer"},{"location":"models/coref/metrics/conll_coref_scores/#update","text":"class Scorer : | ... | def update ( | self , | predicted , | gold , | mention_to_predicted , | mention_to_gold | )","title":"update"},{"location":"models/coref/metrics/conll_coref_scores/#get_f1","text":"class Scorer : | ... | def get_f1 ( self )","title":"get_f1"},{"location":"models/coref/metrics/conll_coref_scores/#get_recall","text":"class Scorer : | ... | def get_recall ( self )","title":"get_recall"},{"location":"models/coref/metrics/conll_coref_scores/#get_precision","text":"class Scorer : | ... | def get_precision ( self )","title":"get_precision"},{"location":"models/coref/metrics/conll_coref_scores/#get_prf","text":"class Scorer : | ... | def get_prf ( self )","title":"get_prf"},{"location":"models/coref/metrics/conll_coref_scores/#b_cubed","text":"class Scorer : | ... | @staticmethod | def b_cubed ( clusters , mention_to_gold ) Averaged per-mention precision and recall. https://pdfs.semanticscholar.org/cfe3/c24695f1c14b78a5b8e95bcbd1c666140fd1.pdf","title":"b_cubed"},{"location":"models/coref/metrics/conll_coref_scores/#muc","text":"class Scorer : | ... | @staticmethod | def muc ( clusters , mention_to_gold ) Counts the mentions in each predicted cluster which need to be re-allocated in order for each predicted cluster to be contained by the respective gold cluster. https://aclweb.org/anthology/M/M95/M95-1005.pdf","title":"muc"},{"location":"models/coref/metrics/conll_coref_scores/#phi4","text":"class Scorer : | ... | @staticmethod | def phi4 ( gold_clustering , predicted_clustering ) Subroutine for ceafe. Computes the mention F measure between gold and predicted mentions in a cluster.","title":"phi4"},{"location":"models/coref/metrics/conll_coref_scores/#ceafe","text":"class Scorer : | ... | @staticmethod | def ceafe ( clusters , gold_clusters ) Computes the Constrained Entity-Alignment F-Measure (CEAF) for evaluating coreference. Gold and predicted mentions are aligned into clusterings which maximise a metric - in this case, the F measure between gold and predicted clusters. https://www.semanticscholar.org/paper/On-Coreference-Resolution-Performance-Metrics-Luo/de133c1f22d0dfe12539e25dda70f28672459b99","title":"ceafe"},{"location":"models/coref/metrics/mention_recall/","text":"allennlp_models .coref .metrics .mention_recall [SOURCE] MentionRecall # @Metric . register ( \"mention_recall\" ) class MentionRecall ( Metric ): | def __init__ ( self ) -> None __call__ # class MentionRecall ( Metric ): | ... | @overrides | def __call__ ( | self , | batched_top_spans : torch . Tensor , | batched_metadata : List [ Dict [ str , Any ]] | ) get_metric # class MentionRecall ( Metric ): | ... | @overrides | def get_metric ( self , reset : bool = False ) -> float reset # class MentionRecall ( Metric ): | ... | @overrides | def reset ( self )","title":"mention_recall"},{"location":"models/coref/metrics/mention_recall/#mentionrecall","text":"@Metric . register ( \"mention_recall\" ) class MentionRecall ( Metric ): | def __init__ ( self ) -> None","title":"MentionRecall"},{"location":"models/coref/metrics/mention_recall/#__call__","text":"class MentionRecall ( Metric ): | ... | @overrides | def __call__ ( | self , | batched_top_spans : torch . Tensor , | batched_metadata : List [ Dict [ str , Any ]] | )","title":"__call__"},{"location":"models/coref/metrics/mention_recall/#get_metric","text":"class MentionRecall ( Metric ): | ... | @overrides | def get_metric ( self , reset : bool = False ) -> float","title":"get_metric"},{"location":"models/coref/metrics/mention_recall/#reset","text":"class MentionRecall ( Metric ): | ... | @overrides | def reset ( self )","title":"reset"},{"location":"models/coref/models/coref/","text":"allennlp_models .coref .models .coref [SOURCE] CoreferenceResolver # @Model . register ( \"coref\" ) class CoreferenceResolver ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | context_layer : Seq2SeqEncoder , | mention_feedforward : FeedForward , | antecedent_feedforward : FeedForward , | feature_size : int , | max_span_width : int , | spans_per_word : float , | max_antecedents : int , | coarse_to_fine : bool = False , | inference_order : int = 1 , | lexical_dropout : float = 0.2 , | initializer : InitializerApplicator = InitializerApplicator (), | ** kwargs | ) -> None This Model implements the coreference resolution model described in Higher-order Coreference Resolution with Coarse-to-fine Inference by Lee et al., 2018. The basic outline of this model is to get an embedded representation of each span in the document. These span representations are scored and used to prune away spans that are unlikely to occur in a coreference cluster. For the remaining spans, the model decides which antecedent span (if any) they are coreferent with. The resulting coreference links, after applying transitivity, imply a clustering of the spans in the document. Parameters \u00b6 vocab : Vocabulary text_field_embedder : TextFieldEmbedder Used to embed the text TextField we get as input to the model. context_layer : Seq2SeqEncoder This layer incorporates contextual information for each word in the document. mention_feedforward : FeedForward This feedforward network is applied to the span representations which is then scored by a linear layer. antecedent_feedforward : FeedForward This feedforward network is applied to pairs of span representation, along with any pairwise features, which is then scored by a linear layer. feature_size : int The embedding size for all the embedded features, such as distances or span widths. max_span_width : int The maximum width of candidate spans. spans_per_word : float A multiplier between zero and one which controls what percentage of candidate mention spans we retain with respect to the number of words in the document. max_antecedents : int For each mention which survives the pruning stage, we consider this many antecedents. coarse_to_fine : bool , optional (default = False ) Whether or not to apply the coarse-to-fine filtering. inference_order : int , optional (default = 1 ) The number of inference orders. When greater than 1, the span representations are updated and coreference scores re-computed. lexical_dropout : int The probability of dropping out dimensions of the embedded text. initializer : InitializerApplicator , optional (default = InitializerApplicator() ) Used to initialize the model parameters. forward # class CoreferenceResolver ( Model ): | ... | @overrides | def forward ( | self , | text : TextFieldTensors , | spans : torch . IntTensor , | span_labels : torch . IntTensor = None , | metadata : List [ Dict [ str , Any ]] = None | ) -> Dict [ str , torch . Tensor ] Parameters \u00b6 text : TextFieldTensors The output of a TextField representing the text of the document. spans : torch.IntTensor A tensor of shape (batch_size, num_spans, 2), representing the inclusive start and end indices of candidate spans for mentions. Comes from a ListField[SpanField] of indices into the text of the document. span_labels : torch.IntTensor , optional (default = None ) A tensor of shape (batch_size, num_spans), representing the cluster ids of each span, or -1 for those which do not appear in any clusters. metadata : List[Dict[str, Any]] , optional (default = None ) A metadata dictionary for each instance in the batch. We use the \"original_text\" and \"clusters\" keys from this dictionary, which respectively have the original text and the annotated gold coreference clusters for that instance. Returns \u00b6 An output dictionary consisting of: top_spans : torch.IntTensor A tensor of shape (batch_size, num_spans_to_keep, 2) representing the start and end word indices of the top spans that survived the pruning stage. antecedent_indices : torch.IntTensor A tensor of shape (num_spans_to_keep, max_antecedents) representing for each top span the index (with respect to top_spans) of the possible antecedents the model considered. predicted_antecedents : torch.IntTensor A tensor of shape (batch_size, num_spans_to_keep) representing, for each top span, the index (with respect to antecedent_indices) of the most likely antecedent. -1 means there was no predicted link. loss : torch.FloatTensor , optional A scalar loss to be optimised. make_output_human_readable # class CoreferenceResolver ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) Converts the list of spans and predicted antecedent indices into clusters of spans for each element in the batch. Parameters \u00b6 output_dict : Dict[str, torch.Tensor] The result of calling forward on an instance or batch of instances. Returns \u00b6 The same output dictionary, but with an additional clusters key: clusters : List[List[List[Tuple[int, int]]]] A nested list, representing, for each instance in the batch, the list of clusters, which are in turn comprised of a list of (start, end) inclusive spans into the original document. get_metrics # class CoreferenceResolver ( Model ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] default_predictor # class CoreferenceResolver ( Model ): | ... | default_predictor = \"coreference_resolution\"","title":"coref"},{"location":"models/coref/models/coref/#coreferenceresolver","text":"@Model . register ( \"coref\" ) class CoreferenceResolver ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | context_layer : Seq2SeqEncoder , | mention_feedforward : FeedForward , | antecedent_feedforward : FeedForward , | feature_size : int , | max_span_width : int , | spans_per_word : float , | max_antecedents : int , | coarse_to_fine : bool = False , | inference_order : int = 1 , | lexical_dropout : float = 0.2 , | initializer : InitializerApplicator = InitializerApplicator (), | ** kwargs | ) -> None This Model implements the coreference resolution model described in Higher-order Coreference Resolution with Coarse-to-fine Inference by Lee et al., 2018. The basic outline of this model is to get an embedded representation of each span in the document. These span representations are scored and used to prune away spans that are unlikely to occur in a coreference cluster. For the remaining spans, the model decides which antecedent span (if any) they are coreferent with. The resulting coreference links, after applying transitivity, imply a clustering of the spans in the document.","title":"CoreferenceResolver"},{"location":"models/coref/models/coref/#forward","text":"class CoreferenceResolver ( Model ): | ... | @overrides | def forward ( | self , | text : TextFieldTensors , | spans : torch . IntTensor , | span_labels : torch . IntTensor = None , | metadata : List [ Dict [ str , Any ]] = None | ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"models/coref/models/coref/#make_output_human_readable","text":"class CoreferenceResolver ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) Converts the list of spans and predicted antecedent indices into clusters of spans for each element in the batch.","title":"make_output_human_readable"},{"location":"models/coref/models/coref/#get_metrics","text":"class CoreferenceResolver ( Model ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"models/coref/models/coref/#default_predictor","text":"class CoreferenceResolver ( Model ): | ... | default_predictor = \"coreference_resolution\"","title":"default_predictor"},{"location":"models/coref/predictors/coref/","text":"allennlp_models .coref .predictors .coref [SOURCE] CorefPredictor # @Predictor . register ( \"coreference_resolution\" ) class CorefPredictor ( Predictor ): | def __init__ ( | self , | model : Model , | dataset_reader : DatasetReader , | language : str = \"en_core_web_sm\" | ) -> None Predictor for the CoreferenceResolver model. Registered as a Predictor with name \"coreference_resolution\". predict # class CorefPredictor ( Predictor ): | ... | def predict ( self , document : str ) -> JsonDict Predict the coreference clusters in the given document. { \"document\": [tokenized document text] \"clusters\": [ [ [start_index, end_index], [start_index, end_index] ], [ [start_index, end_index], [start_index, end_index], [start_index, end_index], ], .... ] } Parameters \u00b6 document : str A string representation of a document. Returns \u00b6 A dictionary representation of the predicted coreference clusters. predict_tokenized # class CorefPredictor ( Predictor ): | ... | def predict_tokenized ( self , tokenized_document : List [ str ]) -> JsonDict Predict the coreference clusters in the given document. Parameters \u00b6 tokenized_document : List[str] A list of words representation of a tokenized document. Returns \u00b6 A dictionary representation of the predicted coreference clusters. predictions_to_labeled_instances # class CorefPredictor ( Predictor ): | ... | @overrides | def predictions_to_labeled_instances ( | self , | instance : Instance , | outputs : Dict [ str , numpy . ndarray ] | ) -> List [ Instance ] Takes each predicted cluster and makes it into a labeled Instance with only that cluster labeled, so we can compute gradients of the loss on the model's prediction of that cluster . This lets us run interpretation methods using those gradients. See superclass docstring for more info. replace_corefs # class CorefPredictor ( Predictor ): | ... | @staticmethod | def replace_corefs ( | document : Doc , | clusters : List [ List [ List [ int ]]] | ) -> str Uses a list of coreference clusters to convert a spacy document into a string, where each coreference is replaced by its main mention. coref_resolved # class CorefPredictor ( Predictor ): | ... | def coref_resolved ( self , document : str ) -> str Produce a document where each coreference is replaced by its main mention Parameters \u00b6 document : str A string representation of a document. Returns \u00b6 A string with each coreference replaced by its main mention","title":"coref"},{"location":"models/coref/predictors/coref/#corefpredictor","text":"@Predictor . register ( \"coreference_resolution\" ) class CorefPredictor ( Predictor ): | def __init__ ( | self , | model : Model , | dataset_reader : DatasetReader , | language : str = \"en_core_web_sm\" | ) -> None Predictor for the CoreferenceResolver model. Registered as a Predictor with name \"coreference_resolution\".","title":"CorefPredictor"},{"location":"models/coref/predictors/coref/#predict","text":"class CorefPredictor ( Predictor ): | ... | def predict ( self , document : str ) -> JsonDict Predict the coreference clusters in the given document. { \"document\": [tokenized document text] \"clusters\": [ [ [start_index, end_index], [start_index, end_index] ], [ [start_index, end_index], [start_index, end_index], [start_index, end_index], ], .... ] }","title":"predict"},{"location":"models/coref/predictors/coref/#predict_tokenized","text":"class CorefPredictor ( Predictor ): | ... | def predict_tokenized ( self , tokenized_document : List [ str ]) -> JsonDict Predict the coreference clusters in the given document.","title":"predict_tokenized"},{"location":"models/coref/predictors/coref/#predictions_to_labeled_instances","text":"class CorefPredictor ( Predictor ): | ... | @overrides | def predictions_to_labeled_instances ( | self , | instance : Instance , | outputs : Dict [ str , numpy . ndarray ] | ) -> List [ Instance ] Takes each predicted cluster and makes it into a labeled Instance with only that cluster labeled, so we can compute gradients of the loss on the model's prediction of that cluster . This lets us run interpretation methods using those gradients. See superclass docstring for more info.","title":"predictions_to_labeled_instances"},{"location":"models/coref/predictors/coref/#replace_corefs","text":"class CorefPredictor ( Predictor ): | ... | @staticmethod | def replace_corefs ( | document : Doc , | clusters : List [ List [ List [ int ]]] | ) -> str Uses a list of coreference clusters to convert a spacy document into a string, where each coreference is replaced by its main mention.","title":"replace_corefs"},{"location":"models/coref/predictors/coref/#coref_resolved","text":"class CorefPredictor ( Predictor ): | ... | def coref_resolved ( self , document : str ) -> str Produce a document where each coreference is replaced by its main mention","title":"coref_resolved"},{"location":"models/generation/dataset_readers/cnn_dm/","text":"allennlp_models .generation .dataset_readers .cnn_dm [SOURCE] CNNDailyMailDatasetReader # @DatasetReader . register ( \"cnn_dm\" ) class CNNDailyMailDatasetReader ( DatasetReader ): | def __init__ ( | self , | source_tokenizer : Tokenizer = None , | target_tokenizer : Tokenizer = None , | source_token_indexers : Dict [ str , TokenIndexer ] = None , | target_token_indexers : Dict [ str , TokenIndexer ] = None , | source_max_tokens : Optional [ int ] = None , | target_max_tokens : Optional [ int ] = None , | source_prefix : Optional [ str ] = None , | ** kwargs | ) -> None Reads the CNN/DailyMail dataset for text summarization. The output of read is a list of Instance s with the fields: source_tokens : TextField and target_tokens : TextField Parameters \u00b6 source_tokenizer : Tokenizer , optional Tokenizer to use to split the input sequences into words or other kinds of tokens. Defaults to SpacyTokenizer() . target_tokenizer : Tokenizer , optional Tokenizer to use to split the output sequences (during training) into words or other kinds of tokens. Defaults to source_tokenizer . source_token_indexers : Dict[str, TokenIndexer] , optional Indexers used to define input (source side) token representations. Defaults to {\"tokens\": SingleIdTokenIndexer()} . target_token_indexers : Dict[str, TokenIndexer] , optional Indexers used to define output (target side) token representations. Defaults to source_token_indexers . source_max_tokens : int , optional Maximum number of tokens in source sequence. target_max_tokens : int , optional Maximum number of tokens in target sequence. source_prefix : str , optional An optional prefix to prepend to source strings. For example, with a T5 model, you want to set the source_prefix to \"summarize: \". text_to_instance # class CNNDailyMailDatasetReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | source_sequence : str , | target_sequence : str = None | ) -> Instance apply_token_indexers # class CNNDailyMailDatasetReader ( DatasetReader ): | ... | @overrides | def apply_token_indexers ( self , instance : Instance ) -> None","title":"cnn_dm"},{"location":"models/generation/dataset_readers/cnn_dm/#cnndailymaildatasetreader","text":"@DatasetReader . register ( \"cnn_dm\" ) class CNNDailyMailDatasetReader ( DatasetReader ): | def __init__ ( | self , | source_tokenizer : Tokenizer = None , | target_tokenizer : Tokenizer = None , | source_token_indexers : Dict [ str , TokenIndexer ] = None , | target_token_indexers : Dict [ str , TokenIndexer ] = None , | source_max_tokens : Optional [ int ] = None , | target_max_tokens : Optional [ int ] = None , | source_prefix : Optional [ str ] = None , | ** kwargs | ) -> None Reads the CNN/DailyMail dataset for text summarization. The output of read is a list of Instance s with the fields: source_tokens : TextField and target_tokens : TextField","title":"CNNDailyMailDatasetReader"},{"location":"models/generation/dataset_readers/cnn_dm/#text_to_instance","text":"class CNNDailyMailDatasetReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | source_sequence : str , | target_sequence : str = None | ) -> Instance","title":"text_to_instance"},{"location":"models/generation/dataset_readers/cnn_dm/#apply_token_indexers","text":"class CNNDailyMailDatasetReader ( DatasetReader ): | ... | @overrides | def apply_token_indexers ( self , instance : Instance ) -> None","title":"apply_token_indexers"},{"location":"models/generation/dataset_readers/copynet_seq2seq/","text":"allennlp_models .generation .dataset_readers .copynet_seq2seq [SOURCE] CopyNetDatasetReader # @DatasetReader . register ( \"copynet_seq2seq\" ) class CopyNetDatasetReader ( DatasetReader ): | def __init__ ( | self , | target_namespace : str , | source_tokenizer : Tokenizer = None , | target_tokenizer : Tokenizer = None , | source_token_indexers : Dict [ str , TokenIndexer ] = None , | ** kwargs | ) -> None Read a tsv file containing paired sequences, and create a dataset suitable for a CopyNet model, or any model with a matching API. The expected format for each input line is: . An instance produced by CopyNetDatasetReader will containing at least the following fields: source_tokens : a TextField containing the tokenized source sentence. This will result in a tensor of shape (batch_size, source_length) . source_token_ids : an ArrayField of size (batch_size, source_length) that contains an ID for each token in the source sentence. Tokens that match at the lowercase level will share the same ID. If target_tokens is passed as well, these IDs will also correspond to the target_token_ids field, i.e. any tokens that match at the lowercase level in both the source and target sentences will share the same ID. Note that these IDs have no correlation with the token indices from the corresponding vocabulary namespaces. source_to_target : a NamespaceSwappingField that keeps track of the index of the target token that matches each token in the source sentence. When there is no matching target token, the OOV index is used. This will result in a tensor of shape (batch_size, source_length) . metadata : a MetadataField which contains the source tokens and potentially target tokens as lists of strings. When target_string is passed, the instance will also contain these fields: target_tokens : a TextField containing the tokenized target sentence, including the START_SYMBOL and END_SYMBOL . This will result in a tensor of shape (batch_size, target_length) . target_token_ids : an ArrayField of size (batch_size, target_length) . This is calculated in the same way as source_token_ids . See the \"Notes\" section below for a description of how these fields are used. Parameters \u00b6 target_namespace : str The vocab namespace for the targets. This needs to be passed to the dataset reader in order to construct the NamespaceSwappingField. source_tokenizer : Tokenizer , optional Tokenizer to use to split the input sequences into words or other kinds of tokens. Defaults to SpacyTokenizer() . target_tokenizer : Tokenizer , optional Tokenizer to use to split the output sequences (during training) into words or other kinds of tokens. Defaults to source_tokenizer . source_token_indexers : Dict[str, TokenIndexer] , optional Indexers used to define input (source side) token representations. Defaults to {\"tokens\": SingleIdTokenIndexer()} . Notes \u00b6 In regards to the fields in an Instance produced by this dataset reader, source_token_ids and target_token_ids are primarily used during training to determine whether a target token is copied from a source token (or multiple matching source tokens), while source_to_target is primarily used during prediction to combine the copy scores of source tokens with the generation scores for matching tokens in the target namespace. text_to_instance # class CopyNetDatasetReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | source_string : str , | target_string : str = None | ) -> Instance Turn raw source string and target string into an Instance . Parameters \u00b6 source_string : str target_string : str , optional (default = None ) Returns \u00b6 Instance See the above for a description of the fields that the instance will contain. apply_token_indexers # class CopyNetDatasetReader ( DatasetReader ): | ... | @overrides | def apply_token_indexers ( self , instance : Instance ) -> None","title":"copynet_seq2seq"},{"location":"models/generation/dataset_readers/copynet_seq2seq/#copynetdatasetreader","text":"@DatasetReader . register ( \"copynet_seq2seq\" ) class CopyNetDatasetReader ( DatasetReader ): | def __init__ ( | self , | target_namespace : str , | source_tokenizer : Tokenizer = None , | target_tokenizer : Tokenizer = None , | source_token_indexers : Dict [ str , TokenIndexer ] = None , | ** kwargs | ) -> None Read a tsv file containing paired sequences, and create a dataset suitable for a CopyNet model, or any model with a matching API. The expected format for each input line is: . An instance produced by CopyNetDatasetReader will containing at least the following fields: source_tokens : a TextField containing the tokenized source sentence. This will result in a tensor of shape (batch_size, source_length) . source_token_ids : an ArrayField of size (batch_size, source_length) that contains an ID for each token in the source sentence. Tokens that match at the lowercase level will share the same ID. If target_tokens is passed as well, these IDs will also correspond to the target_token_ids field, i.e. any tokens that match at the lowercase level in both the source and target sentences will share the same ID. Note that these IDs have no correlation with the token indices from the corresponding vocabulary namespaces. source_to_target : a NamespaceSwappingField that keeps track of the index of the target token that matches each token in the source sentence. When there is no matching target token, the OOV index is used. This will result in a tensor of shape (batch_size, source_length) . metadata : a MetadataField which contains the source tokens and potentially target tokens as lists of strings. When target_string is passed, the instance will also contain these fields: target_tokens : a TextField containing the tokenized target sentence, including the START_SYMBOL and END_SYMBOL . This will result in a tensor of shape (batch_size, target_length) . target_token_ids : an ArrayField of size (batch_size, target_length) . This is calculated in the same way as source_token_ids . See the \"Notes\" section below for a description of how these fields are used.","title":"CopyNetDatasetReader"},{"location":"models/generation/dataset_readers/copynet_seq2seq/#text_to_instance","text":"class CopyNetDatasetReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | source_string : str , | target_string : str = None | ) -> Instance Turn raw source string and target string into an Instance .","title":"text_to_instance"},{"location":"models/generation/dataset_readers/copynet_seq2seq/#apply_token_indexers","text":"class CopyNetDatasetReader ( DatasetReader ): | ... | @overrides | def apply_token_indexers ( self , instance : Instance ) -> None","title":"apply_token_indexers"},{"location":"models/generation/dataset_readers/seq2seq/","text":"allennlp_models .generation .dataset_readers .seq2seq [SOURCE] Seq2SeqDatasetReader # @DatasetReader . register ( \"seq2seq\" ) class Seq2SeqDatasetReader ( DatasetReader ): | def __init__ ( | self , | source_tokenizer : Tokenizer = None , | target_tokenizer : Tokenizer = None , | source_token_indexers : Dict [ str , TokenIndexer ] = None , | target_token_indexers : Dict [ str , TokenIndexer ] = None , | source_add_start_token : bool = True , | source_add_end_token : bool = True , | target_add_start_token : bool = True , | target_add_end_token : bool = True , | start_symbol : str = START_SYMBOL , | end_symbol : str = END_SYMBOL , | delimiter : str = \" \\t \" , | source_max_tokens : Optional [ int ] = None , | target_max_tokens : Optional [ int ] = None , | quoting : int = csv . QUOTE_MINIMAL , | ** kwargs | ) -> None Read a tsv file containing paired sequences, and create a dataset suitable for a ComposedSeq2Seq model, or any model with a matching API. Expected format for each input line: \\t The output of read is a list of Instance s with the fields: source_tokens : TextField and target_tokens : TextField START_SYMBOL and END_SYMBOL tokens are added to the source and target sequences. Parameters \u00b6 source_tokenizer : Tokenizer , optional Tokenizer to use to split the input sequences into words or other kinds of tokens. Defaults to SpacyTokenizer() . target_tokenizer : Tokenizer , optional Tokenizer to use to split the output sequences (during training) into words or other kinds of tokens. Defaults to source_tokenizer . source_token_indexers : Dict[str, TokenIndexer] , optional Indexers used to define input (source side) token representations. Defaults to {\"tokens\": SingleIdTokenIndexer()} . target_token_indexers : Dict[str, TokenIndexer] , optional Indexers used to define output (target side) token representations. Defaults to source_token_indexers . source_add_start_token : bool , optional (default = True ) Whether or not to add start_symbol to the beginning of the source sequence. source_add_end_token : bool , optional (default = True ) Whether or not to add end_symbol to the end of the source sequence. target_add_start_token : bool , optional (default = True ) Whether or not to add start_symbol to the beginning of the target sequence. target_add_end_token : bool , optional (default = True ) Whether or not to add end_symbol to the end of the target sequence. start_symbol : str , optional (default = START_SYMBOL ) The special token to add to the end of the source sequence or the target sequence if source_add_start_token or target_add_start_token respectively. end_symbol : str , optional (default = END_SYMBOL ) The special token to add to the end of the source sequence or the target sequence if source_add_end_token or target_add_end_token respectively. delimiter : str , optional (default = \"\\t\" ) Set delimiter for tsv/csv file. quoting : int , optional (default = csv.QUOTE_MINIMAL ) Quoting to use for csv reader. text_to_instance # class Seq2SeqDatasetReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | source_string : str , | target_string : str = None | ) -> Instance apply_token_indexers # class Seq2SeqDatasetReader ( DatasetReader ): | ... | @overrides | def apply_token_indexers ( self , instance : Instance ) -> None","title":"seq2seq"},{"location":"models/generation/dataset_readers/seq2seq/#seq2seqdatasetreader","text":"@DatasetReader . register ( \"seq2seq\" ) class Seq2SeqDatasetReader ( DatasetReader ): | def __init__ ( | self , | source_tokenizer : Tokenizer = None , | target_tokenizer : Tokenizer = None , | source_token_indexers : Dict [ str , TokenIndexer ] = None , | target_token_indexers : Dict [ str , TokenIndexer ] = None , | source_add_start_token : bool = True , | source_add_end_token : bool = True , | target_add_start_token : bool = True , | target_add_end_token : bool = True , | start_symbol : str = START_SYMBOL , | end_symbol : str = END_SYMBOL , | delimiter : str = \" \\t \" , | source_max_tokens : Optional [ int ] = None , | target_max_tokens : Optional [ int ] = None , | quoting : int = csv . QUOTE_MINIMAL , | ** kwargs | ) -> None Read a tsv file containing paired sequences, and create a dataset suitable for a ComposedSeq2Seq model, or any model with a matching API. Expected format for each input line: \\t The output of read is a list of Instance s with the fields: source_tokens : TextField and target_tokens : TextField START_SYMBOL and END_SYMBOL tokens are added to the source and target sequences.","title":"Seq2SeqDatasetReader"},{"location":"models/generation/dataset_readers/seq2seq/#text_to_instance","text":"class Seq2SeqDatasetReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | source_string : str , | target_string : str = None | ) -> Instance","title":"text_to_instance"},{"location":"models/generation/dataset_readers/seq2seq/#apply_token_indexers","text":"class Seq2SeqDatasetReader ( DatasetReader ): | ... | @overrides | def apply_token_indexers ( self , instance : Instance ) -> None","title":"apply_token_indexers"},{"location":"models/generation/models/bart/","text":"allennlp_models .generation .models .bart [SOURCE] DecoderCacheType # DecoderCacheType = Tuple [ Tuple [ torch . Tensor , torch . Tensor , torch . Tensor , torch . Tensor ], ... ] BartEncoder # @Seq2SeqEncoder . register ( \"bart_encoder\" ) class BartEncoder ( Seq2SeqEncoder ): | def __init__ ( self , model_name ) The BART encoder, implemented as a Seq2SeqEncoder , which assumes it operates on already embedded inputs. This means that we remove the token and position embeddings from BART in this module. For the typical use case of using BART to encode inputs to your model (where we include the token and position embeddings from BART), you should use PretrainedTransformerEmbedder(bart_model_name, sub_module=\"encoder\") instead of this. Parameters \u00b6 model_name : str Name of the pre-trained BART model to use. Available options can be found in transformers.models.bart.modeling_bart.BART_PRETRAINED_MODEL_ARCHIVE_MAP . get_input_dim # class BartEncoder ( Seq2SeqEncoder ): | ... | @overrides | def get_input_dim ( self ) -> int get_output_dim # class BartEncoder ( Seq2SeqEncoder ): | ... | @overrides | def get_output_dim ( self ) -> int is_bidirectional # class BartEncoder ( Seq2SeqEncoder ): | ... | @overrides | def is_bidirectional ( self ) -> bool forward # class BartEncoder ( Seq2SeqEncoder ): | ... | @overrides | def forward ( self , inputs : torch . Tensor , mask : torch . BoolTensor ) The first element is always the last encoder states for each input token. Depending on the config, the second output will contain a list of the encoder states after each transformer layer. Similarly, the third output can contain the attentions from each layer. We only care about the first element. Bart # @Model . register ( \"bart\" ) class Bart ( Model ): | def __init__ ( | self , | model_name : str , | vocab : Vocabulary , | indexer : PretrainedTransformerIndexer = None , | max_decoding_steps : int = 140 , | beam_size : int = 4 , | encoder : Seq2SeqEncoder = None | ) BART model from the paper \"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\" (https://arxiv.org/abs/1910.13461). The Bart model here uses a language modeling head and thus can be used for text generation. Parameters \u00b6 model_name : str Name of the pre-trained BART model to use. Available options can be found in transformers.models.bart.modeling_bart.BART_PRETRAINED_MODEL_ARCHIVE_MAP . vocab : Vocabulary Vocabulary containing source and target vocabularies. indexer : PretrainedTransformerIndexer , optional (default = None ) Indexer to be used for converting decoded sequences of ids to to sequences of tokens. max_decoding_steps : int , optional (default = 128 ) Number of decoding steps during beam search. beam_size : int , optional (default = 4 ) Number of beams to use in beam search. The default is from the BART paper. encoder : Seq2SeqEncoder , optional (default = None ) Encoder to used in BART. By default, the original BART encoder is used. forward # class Bart ( Model ): | ... | @overrides | def forward ( | self , | source_tokens : TextFieldTensors , | target_tokens : TextFieldTensors = None | ) -> Dict [ str , torch . Tensor ] Performs the forward step of Bart. Parameters \u00b6 source_tokens : TextFieldTensors The source tokens for the encoder. We assume they are stored under the tokens key. target_tokens : TextFieldTensors , optional (default = None ) The target tokens for the decoder. We assume they are stored under the tokens key. If no target tokens are given, the source tokens are shifted to the right by 1. Returns \u00b6 Dict[str, torch.Tensor] During training, this dictionary contains the decoder_logits of shape (batch_size, max_target_length, target_vocab_size) and the loss . During inference, it contains predictions of shape (batch_size, max_decoding_steps) and log_probabilities of shape (batch_size,) . take_step # class Bart ( Model ): | ... | def take_step ( | self , | last_predictions : torch . Tensor , | state : Dict [ str , torch . Tensor ], | step : int | ) -> Tuple [ torch . Tensor , Dict [ str , torch . Tensor ]] Take step during beam search. Parameters \u00b6 last_predictions : torch.Tensor The predicted token ids from the previous step. Shape: (group_size,) state : Dict[str, torch.Tensor] State required to generate next set of predictions step : int The time step in beam search decoding. Returns \u00b6 Tuple[torch.Tensor, Dict[str, torch.Tensor]] A tuple containing logits for the next tokens of shape (group_size, target_vocab_size) and an updated state dictionary. make_output_human_readable # class Bart ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , Any ] Parameters \u00b6 output_dict : Dict[str, torch.Tensor] A dictionary containing a batch of predictions with key predictions . The tensor should have shape (batch_size, max_sequence_length) Returns \u00b6 Dict[str, Any] Original output_dict with an additional predicted_tokens key that maps to a list of lists of tokens. get_metrics # class Bart ( Model ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"bart"},{"location":"models/generation/models/bart/#decodercachetype","text":"DecoderCacheType = Tuple [ Tuple [ torch . Tensor , torch . Tensor , torch . Tensor , torch . Tensor ], ... ]","title":"DecoderCacheType"},{"location":"models/generation/models/bart/#bartencoder","text":"@Seq2SeqEncoder . register ( \"bart_encoder\" ) class BartEncoder ( Seq2SeqEncoder ): | def __init__ ( self , model_name ) The BART encoder, implemented as a Seq2SeqEncoder , which assumes it operates on already embedded inputs. This means that we remove the token and position embeddings from BART in this module. For the typical use case of using BART to encode inputs to your model (where we include the token and position embeddings from BART), you should use PretrainedTransformerEmbedder(bart_model_name, sub_module=\"encoder\") instead of this.","title":"BartEncoder"},{"location":"models/generation/models/bart/#get_input_dim","text":"class BartEncoder ( Seq2SeqEncoder ): | ... | @overrides | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"models/generation/models/bart/#get_output_dim","text":"class BartEncoder ( Seq2SeqEncoder ): | ... | @overrides | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"models/generation/models/bart/#is_bidirectional","text":"class BartEncoder ( Seq2SeqEncoder ): | ... | @overrides | def is_bidirectional ( self ) -> bool","title":"is_bidirectional"},{"location":"models/generation/models/bart/#forward","text":"class BartEncoder ( Seq2SeqEncoder ): | ... | @overrides | def forward ( self , inputs : torch . Tensor , mask : torch . BoolTensor ) The first element is always the last encoder states for each input token. Depending on the config, the second output will contain a list of the encoder states after each transformer layer. Similarly, the third output can contain the attentions from each layer. We only care about the first element.","title":"forward"},{"location":"models/generation/models/bart/#bart","text":"@Model . register ( \"bart\" ) class Bart ( Model ): | def __init__ ( | self , | model_name : str , | vocab : Vocabulary , | indexer : PretrainedTransformerIndexer = None , | max_decoding_steps : int = 140 , | beam_size : int = 4 , | encoder : Seq2SeqEncoder = None | ) BART model from the paper \"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\" (https://arxiv.org/abs/1910.13461). The Bart model here uses a language modeling head and thus can be used for text generation.","title":"Bart"},{"location":"models/generation/models/bart/#forward_1","text":"class Bart ( Model ): | ... | @overrides | def forward ( | self , | source_tokens : TextFieldTensors , | target_tokens : TextFieldTensors = None | ) -> Dict [ str , torch . Tensor ] Performs the forward step of Bart.","title":"forward"},{"location":"models/generation/models/bart/#take_step","text":"class Bart ( Model ): | ... | def take_step ( | self , | last_predictions : torch . Tensor , | state : Dict [ str , torch . Tensor ], | step : int | ) -> Tuple [ torch . Tensor , Dict [ str , torch . Tensor ]] Take step during beam search.","title":"take_step"},{"location":"models/generation/models/bart/#make_output_human_readable","text":"class Bart ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , Any ]","title":"make_output_human_readable"},{"location":"models/generation/models/bart/#get_metrics","text":"class Bart ( Model ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"models/generation/models/composed_seq2seq/","text":"allennlp_models .generation .models .composed_seq2seq [SOURCE] ComposedSeq2Seq # @Model . register ( \"composed_seq2seq\" ) class ComposedSeq2Seq ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | source_text_embedder : TextFieldEmbedder , | encoder : Seq2SeqEncoder , | decoder : SeqDecoder , | tied_source_embedder_key : Optional [ str ] = None , | initializer : InitializerApplicator = InitializerApplicator (), | ** kwargs | ) -> None This ComposedSeq2Seq class is a Model which takes a sequence, encodes it, and then uses the encoded representations to decode another sequence. You can use this as the basis for a neural machine translation system, an abstractive summarization system, or any other common seq2seq problem. The model here is simple, but should be a decent starting place for implementing recent models for these tasks. The ComposedSeq2Seq class composes separate Seq2SeqEncoder and SeqDecoder classes. These parts are customizable and are independent from each other. Parameters \u00b6 vocab : Vocabulary Vocabulary containing source and target vocabularies. They may be under the same namespace ( tokens ) or the target tokens can have a different namespace, in which case it needs to be specified as target_namespace . source_text_embedders : TextFieldEmbedder Embedders for source side sequences encoder : Seq2SeqEncoder The encoder of the \"encoder/decoder\" model decoder : SeqDecoder The decoder of the \"encoder/decoder\" model tied_source_embedder_key : str , optional (default = None ) If specified, this key is used to obtain token_embedder in source_text_embedder and the weights are shared/tied with the decoder's target embedding weights. initializer : InitializerApplicator , optional (default = InitializerApplicator() ) Used to initialize the model parameters. forward # class ComposedSeq2Seq ( Model ): | ... | @overrides | def forward ( | self , | source_tokens : TextFieldTensors , | target_tokens : TextFieldTensors = None | ) -> Dict [ str , torch . Tensor ] Make forward pass on the encoder and decoder for producing the entire target sequence. Parameters \u00b6 source_tokens : TextFieldTensors The output of TextField.as_array() applied on the source TextField . This will be passed through a TextFieldEmbedder and then through an encoder. target_tokens : TextFieldTensors , optional (default = None ) Output of Textfield.as_array() applied on target TextField . We assume that the target tokens are also represented as a TextField . Returns \u00b6 Dict[str, torch.Tensor] The output tensors from the decoder. make_output_human_readable # class ComposedSeq2Seq ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Finalize predictions. get_metrics # class ComposedSeq2Seq ( Model ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] default_predictor # class ComposedSeq2Seq ( Model ): | ... | default_predictor = \"seq2seq\"","title":"composed_seq2seq"},{"location":"models/generation/models/composed_seq2seq/#composedseq2seq","text":"@Model . register ( \"composed_seq2seq\" ) class ComposedSeq2Seq ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | source_text_embedder : TextFieldEmbedder , | encoder : Seq2SeqEncoder , | decoder : SeqDecoder , | tied_source_embedder_key : Optional [ str ] = None , | initializer : InitializerApplicator = InitializerApplicator (), | ** kwargs | ) -> None This ComposedSeq2Seq class is a Model which takes a sequence, encodes it, and then uses the encoded representations to decode another sequence. You can use this as the basis for a neural machine translation system, an abstractive summarization system, or any other common seq2seq problem. The model here is simple, but should be a decent starting place for implementing recent models for these tasks. The ComposedSeq2Seq class composes separate Seq2SeqEncoder and SeqDecoder classes. These parts are customizable and are independent from each other.","title":"ComposedSeq2Seq"},{"location":"models/generation/models/composed_seq2seq/#forward","text":"class ComposedSeq2Seq ( Model ): | ... | @overrides | def forward ( | self , | source_tokens : TextFieldTensors , | target_tokens : TextFieldTensors = None | ) -> Dict [ str , torch . Tensor ] Make forward pass on the encoder and decoder for producing the entire target sequence.","title":"forward"},{"location":"models/generation/models/composed_seq2seq/#make_output_human_readable","text":"class ComposedSeq2Seq ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Finalize predictions.","title":"make_output_human_readable"},{"location":"models/generation/models/composed_seq2seq/#get_metrics","text":"class ComposedSeq2Seq ( Model ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"models/generation/models/composed_seq2seq/#default_predictor","text":"class ComposedSeq2Seq ( Model ): | ... | default_predictor = \"seq2seq\"","title":"default_predictor"},{"location":"models/generation/models/copynet_seq2seq/","text":"allennlp_models .generation .models .copynet_seq2seq [SOURCE] CopyNetSeq2Seq # @Model . register ( \"copynet_seq2seq\" ) class CopyNetSeq2Seq ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | source_embedder : TextFieldEmbedder , | encoder : Seq2SeqEncoder , | attention : Attention , | beam_size : int , | max_decoding_steps : int , | target_embedding_dim : int = 30 , | copy_token : str = \"@COPY@\" , | target_namespace : str = \"target_tokens\" , | tensor_based_metric : Metric = None , | token_based_metric : Metric = None , | initializer : InitializerApplicator = InitializerApplicator () | ) -> None This is an implementation of CopyNet . CopyNet is a sequence-to-sequence encoder-decoder model with a copying mechanism that can copy tokens from the source sentence into the target sentence instead of generating all target tokens only from the target vocabulary. It is very similar to a typical seq2seq model used in neural machine translation tasks, for example, except that in addition to providing a \"generation\" score at each timestep for the tokens in the target vocabulary, it also provides a \"copy\" score for each token that appears in the source sentence. In other words, you can think of CopyNet as a seq2seq model with a dynamic target vocabulary that changes based on the tokens in the source sentence, allowing it to predict tokens that are out-of-vocabulary (OOV) with respect to the actual target vocab. Parameters \u00b6 vocab : Vocabulary Vocabulary containing source and target vocabularies. source_embedder : TextFieldEmbedder Embedder for source side sequences encoder : Seq2SeqEncoder The encoder of the \"encoder/decoder\" model attention : Attention This is used to get a dynamic summary of encoder outputs at each timestep when producing the \"generation\" scores for the target vocab. beam_size : int Beam width to use for beam search prediction. max_decoding_steps : int Maximum sequence length of target predictions. target_embedding_dim : int , optional (default = 30 ) The size of the embeddings for the target vocabulary. copy_token : str , optional (default = '@COPY@' ) The token used to indicate that a target token was copied from the source. If this token is not already in your target vocabulary, it will be added. target_namespace : str , optional (default = 'target_tokens' ) The namespace for the target vocabulary. tensor_based_metric : Metric , optional (default = 'BLEU' ) A metric to track on validation data that takes raw tensors when its called. This metric must accept two arguments when called: a batched tensor of predicted token indices, and a batched tensor of gold token indices. token_based_metric : Metric , optional (default = None ) A metric to track on validation data that takes lists of lists of tokens as input. This metric must accept two arguments when called, both of type List[List[str]] . The first is a predicted sequence for each item in the batch and the second is a gold sequence for each item in the batch. initializer : InitializerApplicator , optional An initialization strategy for the model weights. forward # class CopyNetSeq2Seq ( Model ): | ... | @overrides | def forward ( | self , | source_tokens : TextFieldTensors , | source_token_ids : torch . Tensor , | source_to_target : torch . Tensor , | metadata : List [ Dict [ str , Any ]], | target_tokens : TextFieldTensors = None , | target_token_ids : torch . Tensor = None | ) -> Dict [ str , torch . Tensor ] Make foward pass with decoder logic for producing the entire target sequence. Parameters \u00b6 source_tokens : TextFieldTensors The output of TextField.as_array() applied on the source TextField . This will be passed through a TextFieldEmbedder and then through an encoder. source_token_ids : torch.Tensor Tensor containing IDs that indicate which source tokens match each other. Has shape: (batch_size, source_sequence_length) . source_to_target : torch.Tensor Tensor containing vocab index of each source token with respect to the target vocab namespace. Shape: (batch_size, source_sequence_length) . metadata : List[Dict[str, Any]] Metadata field that contains the original source tokens with key 'source_tokens' and any other meta fields. When 'target_tokens' is also passed, the metadata should also contain the original target tokens with key 'target_tokens'. target_tokens : TextFieldTensors , optional (default = None ) Output of Textfield.as_array() applied on target TextField . We assume that the target tokens are also represented as a TextField which must contain a \"tokens\" key that uses single ids. target_token_ids : torch.Tensor , optional (default = None ) A tensor of shape (batch_size, target_sequence_length) which indicates which tokens in the target sequence match tokens in the source sequence. Returns \u00b6 Dict[str, torch.Tensor] take_search_step # class CopyNetSeq2Seq ( Model ): | ... | def take_search_step ( | self , | last_predictions : torch . Tensor , | state : Dict [ str , torch . Tensor ], | step : int | ) -> Tuple [ torch . Tensor , Dict [ str , torch . Tensor ]] Take step during beam search. This function is what gets passed to the BeamSearch.search method. It takes predictions from the last timestep and the current state and outputs the log probabilities assigned to tokens for the next timestep, as well as the updated state. Since we are predicting tokens out of the extended vocab (target vocab + all unique tokens from the source sentence), this is a little more complicated that just making a forward pass through the model. The output log probs will have shape (group_size, target_vocab_size + source_sequence_length) so that each token in the target vocab and source sentence are assigned a probability. Note that copy scores are assigned to each source token based on their position, not unique value. So if a token appears more than once in the source sentence, it will have more than one score. Further, if a source token is also part of the target vocab, its final score will be the sum of the generation and copy scores. Therefore, in order to get the score for all tokens in the extended vocab at this step, we have to combine copy scores for re-occuring source tokens and potentially add them to the generation scores for the matching token in the target vocab, if there is one. So we can break down the final log probs output as the concatenation of two matrices, A: (group_size, target_vocab_size) , and B: (group_size, source_sequence_length) . Matrix A contains the sum of the generation score and copy scores (possibly 0) for each target token. Matrix B contains left-over copy scores for source tokens that do NOT appear in the target vocab, with zeros everywhere else. But since a source token may appear more than once in the source sentence, we also have to sum the scores for each appearance of each unique source token. So matrix B actually only has non-zero values at the first occurence of each source token that is not in the target vocab. Parameters \u00b6 last_predictions : torch.Tensor Shape: (group_size,) state : Dict[str, torch.Tensor] Contains all state tensors necessary to produce generation and copy scores for next step. step : int The time step in beam search decoding. Notes \u00b6 group_size != batch_size . In fact, group_size = batch_size * beam_size . make_output_human_readable # class CopyNetSeq2Seq ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , Any ] Finalize predictions. After a beam search, the predicted indices correspond to tokens in the target vocabulary OR tokens in source sentence. Here we gather the actual tokens corresponding to the indices. get_metrics # class CopyNetSeq2Seq ( Model ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] default_predictor # class CopyNetSeq2Seq ( Model ): | ... | default_predictor = \"seq2seq\"","title":"copynet_seq2seq"},{"location":"models/generation/models/copynet_seq2seq/#copynetseq2seq","text":"@Model . register ( \"copynet_seq2seq\" ) class CopyNetSeq2Seq ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | source_embedder : TextFieldEmbedder , | encoder : Seq2SeqEncoder , | attention : Attention , | beam_size : int , | max_decoding_steps : int , | target_embedding_dim : int = 30 , | copy_token : str = \"@COPY@\" , | target_namespace : str = \"target_tokens\" , | tensor_based_metric : Metric = None , | token_based_metric : Metric = None , | initializer : InitializerApplicator = InitializerApplicator () | ) -> None This is an implementation of CopyNet . CopyNet is a sequence-to-sequence encoder-decoder model with a copying mechanism that can copy tokens from the source sentence into the target sentence instead of generating all target tokens only from the target vocabulary. It is very similar to a typical seq2seq model used in neural machine translation tasks, for example, except that in addition to providing a \"generation\" score at each timestep for the tokens in the target vocabulary, it also provides a \"copy\" score for each token that appears in the source sentence. In other words, you can think of CopyNet as a seq2seq model with a dynamic target vocabulary that changes based on the tokens in the source sentence, allowing it to predict tokens that are out-of-vocabulary (OOV) with respect to the actual target vocab.","title":"CopyNetSeq2Seq"},{"location":"models/generation/models/copynet_seq2seq/#forward","text":"class CopyNetSeq2Seq ( Model ): | ... | @overrides | def forward ( | self , | source_tokens : TextFieldTensors , | source_token_ids : torch . Tensor , | source_to_target : torch . Tensor , | metadata : List [ Dict [ str , Any ]], | target_tokens : TextFieldTensors = None , | target_token_ids : torch . Tensor = None | ) -> Dict [ str , torch . Tensor ] Make foward pass with decoder logic for producing the entire target sequence.","title":"forward"},{"location":"models/generation/models/copynet_seq2seq/#take_search_step","text":"class CopyNetSeq2Seq ( Model ): | ... | def take_search_step ( | self , | last_predictions : torch . Tensor , | state : Dict [ str , torch . Tensor ], | step : int | ) -> Tuple [ torch . Tensor , Dict [ str , torch . Tensor ]] Take step during beam search. This function is what gets passed to the BeamSearch.search method. It takes predictions from the last timestep and the current state and outputs the log probabilities assigned to tokens for the next timestep, as well as the updated state. Since we are predicting tokens out of the extended vocab (target vocab + all unique tokens from the source sentence), this is a little more complicated that just making a forward pass through the model. The output log probs will have shape (group_size, target_vocab_size + source_sequence_length) so that each token in the target vocab and source sentence are assigned a probability. Note that copy scores are assigned to each source token based on their position, not unique value. So if a token appears more than once in the source sentence, it will have more than one score. Further, if a source token is also part of the target vocab, its final score will be the sum of the generation and copy scores. Therefore, in order to get the score for all tokens in the extended vocab at this step, we have to combine copy scores for re-occuring source tokens and potentially add them to the generation scores for the matching token in the target vocab, if there is one. So we can break down the final log probs output as the concatenation of two matrices, A: (group_size, target_vocab_size) , and B: (group_size, source_sequence_length) . Matrix A contains the sum of the generation score and copy scores (possibly 0) for each target token. Matrix B contains left-over copy scores for source tokens that do NOT appear in the target vocab, with zeros everywhere else. But since a source token may appear more than once in the source sentence, we also have to sum the scores for each appearance of each unique source token. So matrix B actually only has non-zero values at the first occurence of each source token that is not in the target vocab.","title":"take_search_step"},{"location":"models/generation/models/copynet_seq2seq/#make_output_human_readable","text":"class CopyNetSeq2Seq ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , Any ] Finalize predictions. After a beam search, the predicted indices correspond to tokens in the target vocabulary OR tokens in source sentence. Here we gather the actual tokens corresponding to the indices.","title":"make_output_human_readable"},{"location":"models/generation/models/copynet_seq2seq/#get_metrics","text":"class CopyNetSeq2Seq ( Model ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"models/generation/models/copynet_seq2seq/#default_predictor","text":"class CopyNetSeq2Seq ( Model ): | ... | default_predictor = \"seq2seq\"","title":"default_predictor"},{"location":"models/generation/models/simple_seq2seq/","text":"allennlp_models .generation .models .simple_seq2seq [SOURCE] SimpleSeq2Seq # @Model . register ( \"simple_seq2seq\" ) class SimpleSeq2Seq ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | source_embedder : TextFieldEmbedder , | encoder : Seq2SeqEncoder , | max_decoding_steps : int , | attention : Attention = None , | beam_size : int = None , | target_namespace : str = \"tokens\" , | target_embedding_dim : int = None , | scheduled_sampling_ratio : float = 0.0 , | use_bleu : bool = True , | bleu_ngram_weights : Iterable [ float ] = ( 0.25 , 0.25 , 0.25 , 0.25 ), | target_pretrain_file : str = None , | target_decoder_layers : int = 1 | ) -> None This SimpleSeq2Seq class is a Model which takes a sequence, encodes it, and then uses the encoded representations to decode another sequence. You can use this as the basis for a neural machine translation system, an abstractive summarization system, or any other common seq2seq problem. The model here is simple, but should be a decent starting place for implementing recent models for these tasks. Parameters \u00b6 vocab : Vocabulary Vocabulary containing source and target vocabularies. They may be under the same namespace ( tokens ) or the target tokens can have a different namespace, in which case it needs to be specified as target_namespace . source_embedder : TextFieldEmbedder Embedder for source side sequences encoder : Seq2SeqEncoder The encoder of the \"encoder/decoder\" model max_decoding_steps : int Maximum length of decoded sequences. target_namespace : str , optional (default = 'tokens' ) If the target side vocabulary is different from the source side's, you need to specify the target's namespace here. If not, we'll assume it is \"tokens\", which is also the default choice for the source side, and this might cause them to share vocabularies. target_embedding_dim : int , optional (default = 'source_embedding_dim' ) You can specify an embedding dimensionality for the target side. If not, we'll use the same value as the source embedder's. target_pretrain_file : str , optional (default = None ) Path to target pretrain embedding files target_decoder_layers : int , optional (default = 1 ) Nums of layer for decoder attention : Attention , optional (default = None ) If you want to use attention to get a dynamic summary of the encoder outputs at each step of decoding, this is the function used to compute similarity between the decoder hidden state and encoder outputs. beam_size : int , optional (default = None ) Width of the beam for beam search. If not specified, greedy decoding is used. scheduled_sampling_ratio : float , optional (default = 0. ) At each timestep during training, we sample a random number between 0 and 1, and if it is not less than this value, we use the ground truth labels for the whole batch. Else, we use the predictions from the previous time step for the whole batch. If this value is 0.0 (default), this corresponds to teacher forcing, and if it is 1.0, it corresponds to not using target side ground truth labels. See the following paper for more information: Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks. Bengio et al., 2015 . use_bleu : bool , optional (default = True ) If True, the BLEU metric will be calculated during validation. ngram_weights : Iterable[float] , optional (default = (0.25, 0.25, 0.25, 0.25) ) Weights to assign to scores for each ngram size. take_step # class SimpleSeq2Seq ( Model ): | ... | def take_step ( | self , | last_predictions : torch . Tensor , | state : Dict [ str , torch . Tensor ], | step : int | ) -> Tuple [ torch . Tensor , Dict [ str , torch . Tensor ]] Take a decoding step. This is called by the beam search class. Parameters \u00b6 last_predictions : torch.Tensor A tensor of shape (group_size,) , which gives the indices of the predictions during the last time step. state : Dict[str, torch.Tensor] A dictionary of tensors that contain the current state information needed to predict the next step, which includes the encoder outputs, the source mask, and the decoder hidden state and context. Each of these tensors has shape (group_size, *) , where * can be any other number of dimensions. step : int The time step in beam search decoding. Returns \u00b6 Tuple[torch.Tensor, Dict[str, torch.Tensor]] A tuple of (log_probabilities, updated_state) , where log_probabilities is a tensor of shape (group_size, num_classes) containing the predicted log probability of each class for the next step, for each item in the group, while updated_state is a dictionary of tensors containing the encoder outputs, source mask, and updated decoder hidden state and context. Notes We treat the inputs as a batch, even though `group_size` is not necessarily \u00b6 equal to `batch_size`, since the group may contain multiple states for each source sentence in the batch. forward # class SimpleSeq2Seq ( Model ): | ... | @overrides | def forward ( | self , | source_tokens : TextFieldTensors , | target_tokens : TextFieldTensors = None | ) -> Dict [ str , torch . Tensor ] Make foward pass with decoder logic for producing the entire target sequence. Parameters \u00b6 source_tokens : TextFieldTensors The output of TextField.as_array() applied on the source TextField . This will be passed through a TextFieldEmbedder and then through an encoder. target_tokens : TextFieldTensors , optional (default = None ) Output of Textfield.as_array() applied on target TextField . We assume that the target tokens are also represented as a TextField . Returns \u00b6 Dict[str, torch.Tensor] make_output_human_readable # class SimpleSeq2Seq ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , Any ] | ) -> Dict [ str , Any ] Finalize predictions. This method overrides Model.make_output_human_readable , which gets called after Model.forward , at test time, to finalize predictions. The logic for the decoder part of the encoder-decoder lives within the forward method. This method trims the output predictions to the first end symbol, replaces indices with corresponding tokens, and adds a field called predicted_tokens to the output_dict . get_metrics # class SimpleSeq2Seq ( Model ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] default_predictor # class SimpleSeq2Seq ( Model ): | ... | default_predictor = \"seq2seq\"","title":"simple_seq2seq"},{"location":"models/generation/models/simple_seq2seq/#simpleseq2seq","text":"@Model . register ( \"simple_seq2seq\" ) class SimpleSeq2Seq ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | source_embedder : TextFieldEmbedder , | encoder : Seq2SeqEncoder , | max_decoding_steps : int , | attention : Attention = None , | beam_size : int = None , | target_namespace : str = \"tokens\" , | target_embedding_dim : int = None , | scheduled_sampling_ratio : float = 0.0 , | use_bleu : bool = True , | bleu_ngram_weights : Iterable [ float ] = ( 0.25 , 0.25 , 0.25 , 0.25 ), | target_pretrain_file : str = None , | target_decoder_layers : int = 1 | ) -> None This SimpleSeq2Seq class is a Model which takes a sequence, encodes it, and then uses the encoded representations to decode another sequence. You can use this as the basis for a neural machine translation system, an abstractive summarization system, or any other common seq2seq problem. The model here is simple, but should be a decent starting place for implementing recent models for these tasks.","title":"SimpleSeq2Seq"},{"location":"models/generation/models/simple_seq2seq/#take_step","text":"class SimpleSeq2Seq ( Model ): | ... | def take_step ( | self , | last_predictions : torch . Tensor , | state : Dict [ str , torch . Tensor ], | step : int | ) -> Tuple [ torch . Tensor , Dict [ str , torch . Tensor ]] Take a decoding step. This is called by the beam search class.","title":"take_step"},{"location":"models/generation/models/simple_seq2seq/#forward","text":"class SimpleSeq2Seq ( Model ): | ... | @overrides | def forward ( | self , | source_tokens : TextFieldTensors , | target_tokens : TextFieldTensors = None | ) -> Dict [ str , torch . Tensor ] Make foward pass with decoder logic for producing the entire target sequence.","title":"forward"},{"location":"models/generation/models/simple_seq2seq/#make_output_human_readable","text":"class SimpleSeq2Seq ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , Any ] | ) -> Dict [ str , Any ] Finalize predictions. This method overrides Model.make_output_human_readable , which gets called after Model.forward , at test time, to finalize predictions. The logic for the decoder part of the encoder-decoder lives within the forward method. This method trims the output predictions to the first end symbol, replaces indices with corresponding tokens, and adds a field called predicted_tokens to the output_dict .","title":"make_output_human_readable"},{"location":"models/generation/models/simple_seq2seq/#get_metrics","text":"class SimpleSeq2Seq ( Model ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"models/generation/models/simple_seq2seq/#default_predictor","text":"class SimpleSeq2Seq ( Model ): | ... | default_predictor = \"seq2seq\"","title":"default_predictor"},{"location":"models/generation/models/t5/","text":"allennlp_models .generation .models .t5 [SOURCE] T5 # @Model . register ( \"t5\" ) class T5 ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | model_name : str , | ** kwargs | ) -> None tokenizer # class T5 ( Model ): | ... | @property | def tokenizer ( self ) -> PretrainedTransformerTokenizer forward # class T5 ( Model ): | ... | def forward ( | self , | source_tokens : TextFieldTensors , | target_tokens : Optional [ TextFieldTensors ] = None | ) -> Dict [ str , torch . Tensor ] Performs the forward step of T5. Parameters \u00b6 source_tokens : TextFieldTensors The source tokens for the encoder. We assume they are stored under the tokens key/namespace. target_tokens : TextFieldTensors , optional (default = None ) The target tokens for the decoder. We assume they are also stored under the tokens key/namespace. If no target tokens are given during training / validation, the source tokens are shifted to the right by 1. Returns \u00b6 Dict[str, torch.Tensor] Contains the loss when target_tokens is provided. And during prediction, includes predictions and predicted_log_probs from beam search. make_output_human_readable # class T5 ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , Any ] get_metrics # class T5 ( Model ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"t5"},{"location":"models/generation/models/t5/#t5","text":"@Model . register ( \"t5\" ) class T5 ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | model_name : str , | ** kwargs | ) -> None","title":"T5"},{"location":"models/generation/models/t5/#tokenizer","text":"class T5 ( Model ): | ... | @property | def tokenizer ( self ) -> PretrainedTransformerTokenizer","title":"tokenizer"},{"location":"models/generation/models/t5/#forward","text":"class T5 ( Model ): | ... | def forward ( | self , | source_tokens : TextFieldTensors , | target_tokens : Optional [ TextFieldTensors ] = None | ) -> Dict [ str , torch . Tensor ] Performs the forward step of T5.","title":"forward"},{"location":"models/generation/models/t5/#make_output_human_readable","text":"class T5 ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , Any ]","title":"make_output_human_readable"},{"location":"models/generation/models/t5/#get_metrics","text":"class T5 ( Model ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"models/generation/modules/decoder_nets/decoder_net/","text":"allennlp_models .generation .modules .decoder_nets .decoder_net [SOURCE] DecoderNet # class DecoderNet ( torch . nn . Module , Registrable ): | def __init__ ( | self , | decoding_dim : int , | target_embedding_dim : int , | decodes_parallel : bool | ) -> None This class abstracts the neural architectures for decoding the encoded states and embedded previous step prediction vectors into a new sequence of output vectors. The implementations of DecoderNet is used by implementations of allennlp.modules.seq2seq_decoders.seq_decoder.SeqDecoder such as allennlp.modules.seq2seq_decoders.seq_decoder.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder . The outputs of this module would be likely used by allennlp.modules.seq2seq_decoders.seq_decoder.SeqDecoder to apply the final output feedforward layer and softmax. Parameters \u00b6 decoding_dim : int Defines dimensionality of output vectors. target_embedding_dim : int Defines dimensionality of target embeddings. Since this model takes it's output on a previous step as input of following step, this is also an input dimensionality. decodes_parallel : bool Defines whether the decoder generates multiple next step predictions at in a single forward . get_output_dim # class DecoderNet ( torch . nn . Module , Registrable ): | ... | def get_output_dim ( self ) -> int Returns the dimension of each vector in the sequence output by this DecoderNet . This is not the shape of the returned tensor, but the last element of that shape. init_decoder_state # class DecoderNet ( torch . nn . Module , Registrable ): | ... | def init_decoder_state ( | self , | encoder_out : Dict [ str , torch . LongTensor ] | ) -> Dict [ str , torch . Tensor ] Initialize the encoded state to be passed to the first decoding time step. Parameters \u00b6 batch_size : int Size of batch final_encoder_output : torch.Tensor Last state of the Encoder Returns \u00b6 Dict[str, torch.Tensor] Initial state forward # class DecoderNet ( torch . nn . Module , Registrable ): | ... | def forward ( | self , | previous_state : Dict [ str , torch . Tensor ], | encoder_outputs : torch . Tensor , | source_mask : torch . BoolTensor , | previous_steps_predictions : torch . Tensor , | previous_steps_mask : Optional [ torch . BoolTensor ] = None | ) -> Tuple [ Dict [ str , torch . Tensor ], torch . Tensor ] Performs a decoding step, and returns dictionary with decoder hidden state or cache and the decoder output. The decoder output is a 3d tensor (group_size, steps_count, decoder_output_dim) if self.decodes_parallel is True, else it is a 2d tensor with (group_size, decoder_output_dim). Parameters \u00b6 previous_steps_predictions : torch.Tensor Embeddings of predictions on previous step. Shape: (group_size, steps_count, decoder_output_dim) encoder_outputs : torch.Tensor Vectors of all encoder outputs. Shape: (group_size, max_input_sequence_length, encoder_output_dim) source_mask : torch.BoolTensor This tensor contains mask for each input sequence. Shape: (group_size, max_input_sequence_length) previous_state : Dict[str, torch.Tensor] previous state of decoder Returns \u00b6 Tuple[Dict[str, torch.Tensor], torch.Tensor] Tuple of new decoder state and decoder output. Output should be used to generate out sequence elements","title":"decoder_net"},{"location":"models/generation/modules/decoder_nets/decoder_net/#decodernet","text":"class DecoderNet ( torch . nn . Module , Registrable ): | def __init__ ( | self , | decoding_dim : int , | target_embedding_dim : int , | decodes_parallel : bool | ) -> None This class abstracts the neural architectures for decoding the encoded states and embedded previous step prediction vectors into a new sequence of output vectors. The implementations of DecoderNet is used by implementations of allennlp.modules.seq2seq_decoders.seq_decoder.SeqDecoder such as allennlp.modules.seq2seq_decoders.seq_decoder.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder . The outputs of this module would be likely used by allennlp.modules.seq2seq_decoders.seq_decoder.SeqDecoder to apply the final output feedforward layer and softmax.","title":"DecoderNet"},{"location":"models/generation/modules/decoder_nets/decoder_net/#get_output_dim","text":"class DecoderNet ( torch . nn . Module , Registrable ): | ... | def get_output_dim ( self ) -> int Returns the dimension of each vector in the sequence output by this DecoderNet . This is not the shape of the returned tensor, but the last element of that shape.","title":"get_output_dim"},{"location":"models/generation/modules/decoder_nets/decoder_net/#init_decoder_state","text":"class DecoderNet ( torch . nn . Module , Registrable ): | ... | def init_decoder_state ( | self , | encoder_out : Dict [ str , torch . LongTensor ] | ) -> Dict [ str , torch . Tensor ] Initialize the encoded state to be passed to the first decoding time step.","title":"init_decoder_state"},{"location":"models/generation/modules/decoder_nets/decoder_net/#forward","text":"class DecoderNet ( torch . nn . Module , Registrable ): | ... | def forward ( | self , | previous_state : Dict [ str , torch . Tensor ], | encoder_outputs : torch . Tensor , | source_mask : torch . BoolTensor , | previous_steps_predictions : torch . Tensor , | previous_steps_mask : Optional [ torch . BoolTensor ] = None | ) -> Tuple [ Dict [ str , torch . Tensor ], torch . Tensor ] Performs a decoding step, and returns dictionary with decoder hidden state or cache and the decoder output. The decoder output is a 3d tensor (group_size, steps_count, decoder_output_dim) if self.decodes_parallel is True, else it is a 2d tensor with (group_size, decoder_output_dim).","title":"forward"},{"location":"models/generation/modules/decoder_nets/lstm_cell/","text":"allennlp_models .generation .modules .decoder_nets .lstm_cell [SOURCE] LstmCellDecoderNet # @DecoderNet . register ( \"lstm_cell\" ) class LstmCellDecoderNet ( DecoderNet ): | def __init__ ( | self , | decoding_dim : int , | target_embedding_dim : int , | attention : Optional [ Attention ] = None , | bidirectional_input : bool = False | ) -> None This decoder net implements simple decoding network with LSTMCell and Attention. Parameters \u00b6 decoding_dim : int Defines dimensionality of output vectors. target_embedding_dim : int Defines dimensionality of input target embeddings. Since this model takes it's output on a previous step as input of following step, this is also an input dimensionality. attention : Attention , optional (default = None ) If you want to use attention to get a dynamic summary of the encoder outputs at each step of decoding, this is the function used to compute similarity between the decoder hidden state and encoder outputs. init_decoder_state # class LstmCellDecoderNet ( DecoderNet ): | ... | def init_decoder_state ( | self , | encoder_out : Dict [ str , torch . LongTensor ] | ) -> Dict [ str , torch . Tensor ] forward # class LstmCellDecoderNet ( DecoderNet ): | ... | @overrides | def forward ( | self , | previous_state : Dict [ str , torch . Tensor ], | encoder_outputs : torch . Tensor , | source_mask : torch . BoolTensor , | previous_steps_predictions : torch . Tensor , | previous_steps_mask : Optional [ torch . BoolTensor ] = None | ) -> Tuple [ Dict [ str , torch . Tensor ], torch . Tensor ]","title":"lstm_cell"},{"location":"models/generation/modules/decoder_nets/lstm_cell/#lstmcelldecodernet","text":"@DecoderNet . register ( \"lstm_cell\" ) class LstmCellDecoderNet ( DecoderNet ): | def __init__ ( | self , | decoding_dim : int , | target_embedding_dim : int , | attention : Optional [ Attention ] = None , | bidirectional_input : bool = False | ) -> None This decoder net implements simple decoding network with LSTMCell and Attention.","title":"LstmCellDecoderNet"},{"location":"models/generation/modules/decoder_nets/lstm_cell/#init_decoder_state","text":"class LstmCellDecoderNet ( DecoderNet ): | ... | def init_decoder_state ( | self , | encoder_out : Dict [ str , torch . LongTensor ] | ) -> Dict [ str , torch . Tensor ]","title":"init_decoder_state"},{"location":"models/generation/modules/decoder_nets/lstm_cell/#forward","text":"class LstmCellDecoderNet ( DecoderNet ): | ... | @overrides | def forward ( | self , | previous_state : Dict [ str , torch . Tensor ], | encoder_outputs : torch . Tensor , | source_mask : torch . BoolTensor , | previous_steps_predictions : torch . Tensor , | previous_steps_mask : Optional [ torch . BoolTensor ] = None | ) -> Tuple [ Dict [ str , torch . Tensor ], torch . Tensor ]","title":"forward"},{"location":"models/generation/modules/decoder_nets/stacked_self_attention/","text":"allennlp_models .generation .modules .decoder_nets .stacked_self_attention [SOURCE] StackedSelfAttentionDecoderNet # @DecoderNet . register ( \"stacked_self_attention\" ) class StackedSelfAttentionDecoderNet ( DecoderNet ): | def __init__ ( | self , | decoding_dim : int , | target_embedding_dim : int , | feedforward_hidden_dim : int , | num_layers : int , | num_attention_heads : int , | use_positional_encoding : bool = True , | positional_encoding_max_steps : int = 5000 , | dropout_prob : float = 0.1 , | residual_dropout_prob : float = 0.2 , | attention_dropout_prob : float = 0.1 | ) -> None A Stacked self-attention decoder implementation. Parameters \u00b6 decoding_dim : int Defines dimensionality of output vectors. target_embedding_dim : int Defines dimensionality of input target embeddings. Since this model takes it's output on a previous step as input of following step, this is also an input dimensionality. feedforward_hidden_dim : int The middle dimension of the FeedForward network. The input and output dimensions are fixed to ensure sizes match up for the self attention layers. num_layers : int The number of stacked self attention -> feedfoward -> layer normalisation blocks. num_attention_heads : int The number of attention heads to use per layer. use_positional_encoding : bool , optional (default = True ) Whether to add sinusoidal frequencies to the input tensor. This is strongly recommended, as without this feature, the self attention layers have no idea of absolute or relative position (as they are just computing pairwise similarity between vectors of elements), which can be important features for many tasks. dropout_prob : float , optional (default = 0.1 ) The dropout probability for the feedforward network. residual_dropout_prob : float , optional (default = 0.2 ) The dropout probability for the residual connections. attention_dropout_prob : float , optional (default = 0.1 ) The dropout probability for the attention distributions in each attention layer. init_decoder_state # class StackedSelfAttentionDecoderNet ( DecoderNet ): | ... | @overrides | def init_decoder_state ( | self , | encoder_out : Dict [ str , torch . LongTensor ] | ) -> Dict [ str , torch . Tensor ] forward # class StackedSelfAttentionDecoderNet ( DecoderNet ): | ... | @overrides | def forward ( | self , | previous_state : Dict [ str , torch . Tensor ], | encoder_outputs : torch . Tensor , | source_mask : torch . BoolTensor , | previous_steps_predictions : torch . Tensor , | previous_steps_mask : Optional [ torch . BoolTensor ] = None | ) -> Tuple [ Dict [ str , torch . Tensor ], torch . Tensor ] Decoder # class Decoder ( nn . Module ): | def __init__ ( self , layer : nn . Module , num_layers : int ) -> None Transformer N layer decoder with masking. Code taken from http://nlp.seas.harvard.edu/2018/04/03/attention.html forward # class Decoder ( nn . Module ): | ... | @overrides | def forward ( | self , | x : torch . Tensor , | memory : torch . Tensor , | src_mask : torch . BoolTensor , | tgt_mask : torch . BoolTensor | ) -> torch . Tensor DecoderLayer # class DecoderLayer ( nn . Module ): | def __init__ ( | self , | size : int , | self_attn : MultiHeadedAttention , | src_attn : MultiHeadedAttention , | feed_forward : F , | dropout : float | ) -> None A single layer of transformer decoder. Code taken from http://nlp.seas.harvard.edu/2018/04/03/attention.html forward # class DecoderLayer ( nn . Module ): | ... | def forward ( | self , | x : torch . Tensor , | memory : torch . Tensor , | src_mask : torch . BoolTensor , | tgt_mask : torch . BoolTensor | ) -> torch . Tensor Follow Figure 1 (right) for connections.","title":"stacked_self_attention"},{"location":"models/generation/modules/decoder_nets/stacked_self_attention/#stackedselfattentiondecodernet","text":"@DecoderNet . register ( \"stacked_self_attention\" ) class StackedSelfAttentionDecoderNet ( DecoderNet ): | def __init__ ( | self , | decoding_dim : int , | target_embedding_dim : int , | feedforward_hidden_dim : int , | num_layers : int , | num_attention_heads : int , | use_positional_encoding : bool = True , | positional_encoding_max_steps : int = 5000 , | dropout_prob : float = 0.1 , | residual_dropout_prob : float = 0.2 , | attention_dropout_prob : float = 0.1 | ) -> None A Stacked self-attention decoder implementation.","title":"StackedSelfAttentionDecoderNet"},{"location":"models/generation/modules/decoder_nets/stacked_self_attention/#init_decoder_state","text":"class StackedSelfAttentionDecoderNet ( DecoderNet ): | ... | @overrides | def init_decoder_state ( | self , | encoder_out : Dict [ str , torch . LongTensor ] | ) -> Dict [ str , torch . Tensor ]","title":"init_decoder_state"},{"location":"models/generation/modules/decoder_nets/stacked_self_attention/#forward","text":"class StackedSelfAttentionDecoderNet ( DecoderNet ): | ... | @overrides | def forward ( | self , | previous_state : Dict [ str , torch . Tensor ], | encoder_outputs : torch . Tensor , | source_mask : torch . BoolTensor , | previous_steps_predictions : torch . Tensor , | previous_steps_mask : Optional [ torch . BoolTensor ] = None | ) -> Tuple [ Dict [ str , torch . Tensor ], torch . Tensor ]","title":"forward"},{"location":"models/generation/modules/decoder_nets/stacked_self_attention/#decoder","text":"class Decoder ( nn . Module ): | def __init__ ( self , layer : nn . Module , num_layers : int ) -> None Transformer N layer decoder with masking. Code taken from http://nlp.seas.harvard.edu/2018/04/03/attention.html","title":"Decoder"},{"location":"models/generation/modules/decoder_nets/stacked_self_attention/#forward_1","text":"class Decoder ( nn . Module ): | ... | @overrides | def forward ( | self , | x : torch . Tensor , | memory : torch . Tensor , | src_mask : torch . BoolTensor , | tgt_mask : torch . BoolTensor | ) -> torch . Tensor","title":"forward"},{"location":"models/generation/modules/decoder_nets/stacked_self_attention/#decoderlayer","text":"class DecoderLayer ( nn . Module ): | def __init__ ( | self , | size : int , | self_attn : MultiHeadedAttention , | src_attn : MultiHeadedAttention , | feed_forward : F , | dropout : float | ) -> None A single layer of transformer decoder. Code taken from http://nlp.seas.harvard.edu/2018/04/03/attention.html","title":"DecoderLayer"},{"location":"models/generation/modules/decoder_nets/stacked_self_attention/#forward_2","text":"class DecoderLayer ( nn . Module ): | ... | def forward ( | self , | x : torch . Tensor , | memory : torch . Tensor , | src_mask : torch . BoolTensor , | tgt_mask : torch . BoolTensor | ) -> torch . Tensor Follow Figure 1 (right) for connections.","title":"forward"},{"location":"models/generation/modules/seq_decoders/auto_regressive/","text":"allennlp_models .generation .modules .seq_decoders .auto_regressive [SOURCE] AutoRegressiveSeqDecoder # @SeqDecoder . register ( \"auto_regressive_seq_decoder\" ) class AutoRegressiveSeqDecoder ( SeqDecoder ): | def __init__ ( | self , | vocab : Vocabulary , | decoder_net : DecoderNet , | max_decoding_steps : int , | target_embedder : Embedding , | target_namespace : str = \"tokens\" , | tie_output_embedding : bool = False , | scheduled_sampling_ratio : float = 0 , | label_smoothing_ratio : Optional [ float ] = None , | beam_size : int = 4 , | tensor_based_metric : Metric = None , | token_based_metric : Metric = None | ) -> None An autoregressive decoder that can be used for most seq2seq tasks. Parameters \u00b6 vocab : Vocabulary Vocabulary containing source and target vocabularies. They may be under the same namespace ( tokens ) or the target tokens can have a different namespace, in which case it needs to be specified as target_namespace . decoder_net : DecoderNet Module that contains implementation of neural network for decoding output elements max_decoding_steps : int Maximum length of decoded sequences. target_embedder : Embedding Embedder for target tokens. target_namespace : str , optional (default = 'tokens' ) If the target side vocabulary is different from the source side's, you need to specify the target's namespace here. If not, we'll assume it is \"tokens\", which is also the default choice for the source side, and this might cause them to share vocabularies. beam_size : int , optional (default = 4 ) Width of the beam for beam search. tensor_based_metric : Metric , optional (default = None ) A metric to track on validation data that takes raw tensors when its called. This metric must accept two arguments when called: a batched tensor of predicted token indices, and a batched tensor of gold token indices. token_based_metric : Metric , optional (default = None ) A metric to track on validation data that takes lists of lists of tokens as input. This metric must accept two arguments when called, both of type List[List[str]] . The first is a predicted sequence for each item in the batch and the second is a gold sequence for each item in the batch. scheduled_sampling_ratio : float , optional (default = 0.0 ) Defines ratio between teacher forced training and real output usage. If its zero (teacher forcing only) and decoder_net supports parallel decoding, we get the output predictions in a single forward pass of the decoder_net . get_output_dim # class AutoRegressiveSeqDecoder ( SeqDecoder ): | ... | def get_output_dim ( self ) take_step # class AutoRegressiveSeqDecoder ( SeqDecoder ): | ... | def take_step ( | self , | last_predictions : torch . Tensor , | state : Dict [ str , torch . Tensor ], | step : int | ) -> Tuple [ torch . Tensor , Dict [ str , torch . Tensor ]] Take a decoding step. This is called by the beam search class. Parameters \u00b6 last_predictions : torch.Tensor A tensor of shape (group_size,) , which gives the indices of the predictions during the last time step. state : Dict[str, torch.Tensor] A dictionary of tensors that contain the current state information needed to predict the next step, which includes the encoder outputs, the source mask, and the decoder hidden state and context. Each of these tensors has shape (group_size, *) , where * can be any other number of dimensions. step : int The time step in beam search decoding. Returns \u00b6 Tuple[torch.Tensor, Dict[str, torch.Tensor]] A tuple of (log_probabilities, updated_state) , where log_probabilities is a tensor of shape (group_size, num_classes) containing the predicted log probability of each class for the next step, for each item in the group, while updated_state is a dictionary of tensors containing the encoder outputs, source mask, and updated decoder hidden state and context. Notes We treat the inputs as a batch, even though `group_size` is not necessarily \u00b6 equal to `batch_size`, since the group may contain multiple states for each source sentence in the batch. get_metrics # class AutoRegressiveSeqDecoder ( SeqDecoder ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] forward # class AutoRegressiveSeqDecoder ( SeqDecoder ): | ... | @overrides | def forward ( | self , | encoder_out : Dict [ str , torch . LongTensor ], | target_tokens : TextFieldTensors = None | ) -> Dict [ str , torch . Tensor ] post_process # class AutoRegressiveSeqDecoder ( SeqDecoder ): | ... | @overrides | def post_process ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] This method trims the output predictions to the first end symbol, replaces indices with corresponding tokens, and adds a field called predicted_tokens to the output_dict . indices_to_tokens # class AutoRegressiveSeqDecoder ( SeqDecoder ): | ... | def indices_to_tokens ( | self , | batch_indeces : numpy . ndarray | ) -> List [ List [ str ]]","title":"auto_regressive"},{"location":"models/generation/modules/seq_decoders/auto_regressive/#autoregressiveseqdecoder","text":"@SeqDecoder . register ( \"auto_regressive_seq_decoder\" ) class AutoRegressiveSeqDecoder ( SeqDecoder ): | def __init__ ( | self , | vocab : Vocabulary , | decoder_net : DecoderNet , | max_decoding_steps : int , | target_embedder : Embedding , | target_namespace : str = \"tokens\" , | tie_output_embedding : bool = False , | scheduled_sampling_ratio : float = 0 , | label_smoothing_ratio : Optional [ float ] = None , | beam_size : int = 4 , | tensor_based_metric : Metric = None , | token_based_metric : Metric = None | ) -> None An autoregressive decoder that can be used for most seq2seq tasks.","title":"AutoRegressiveSeqDecoder"},{"location":"models/generation/modules/seq_decoders/auto_regressive/#get_output_dim","text":"class AutoRegressiveSeqDecoder ( SeqDecoder ): | ... | def get_output_dim ( self )","title":"get_output_dim"},{"location":"models/generation/modules/seq_decoders/auto_regressive/#take_step","text":"class AutoRegressiveSeqDecoder ( SeqDecoder ): | ... | def take_step ( | self , | last_predictions : torch . Tensor , | state : Dict [ str , torch . Tensor ], | step : int | ) -> Tuple [ torch . Tensor , Dict [ str , torch . Tensor ]] Take a decoding step. This is called by the beam search class.","title":"take_step"},{"location":"models/generation/modules/seq_decoders/auto_regressive/#get_metrics","text":"class AutoRegressiveSeqDecoder ( SeqDecoder ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"models/generation/modules/seq_decoders/auto_regressive/#forward","text":"class AutoRegressiveSeqDecoder ( SeqDecoder ): | ... | @overrides | def forward ( | self , | encoder_out : Dict [ str , torch . LongTensor ], | target_tokens : TextFieldTensors = None | ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"models/generation/modules/seq_decoders/auto_regressive/#post_process","text":"class AutoRegressiveSeqDecoder ( SeqDecoder ): | ... | @overrides | def post_process ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] This method trims the output predictions to the first end symbol, replaces indices with corresponding tokens, and adds a field called predicted_tokens to the output_dict .","title":"post_process"},{"location":"models/generation/modules/seq_decoders/auto_regressive/#indices_to_tokens","text":"class AutoRegressiveSeqDecoder ( SeqDecoder ): | ... | def indices_to_tokens ( | self , | batch_indeces : numpy . ndarray | ) -> List [ List [ str ]]","title":"indices_to_tokens"},{"location":"models/generation/modules/seq_decoders/seq_decoder/","text":"allennlp_models .generation .modules .seq_decoders .seq_decoder [SOURCE] SeqDecoder # class SeqDecoder ( Module , Registrable ): | def __init__ ( self , target_embedder : Embedding ) -> None A SeqDecoder abstract class representing the entire decoder (embedding and neural network) of a Seq2Seq architecture. This is meant to be used with allennlp.models.encoder_decoder.composed_seq2seq.ComposedSeq2Seq . The implementation of this abstract class ideally uses a decoder neural net allennlp.modules.seq2seq_decoders.decoder_net.DecoderNet for decoding. The default_implementation allennlp.modules.seq2seq_decoders.seq_decoder.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder covers most use cases. More likely that we will use the default implementation instead of creating a new implementation. Parameters \u00b6 target_embedder : Embedding Embedder for target tokens. Needed in the base class to enable weight tying. default_implementation # class SeqDecoder ( Module , Registrable ): | ... | default_implementation = \"auto_regressive_seq_decoder\" get_output_dim # class SeqDecoder ( Module , Registrable ): | ... | def get_output_dim ( self ) -> int The dimension of each timestep of the hidden state in the layer before final softmax. Needed to check whether the model is compatible for embedding-final layer weight tying. get_metrics # class SeqDecoder ( Module , Registrable ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] The decoder is responsible for computing metrics using the target tokens. forward # class SeqDecoder ( Module , Registrable ): | ... | def forward ( | self , | encoder_out : Dict [ str , torch . LongTensor ], | target_tokens : Optional [ Dict [ str , torch . LongTensor ]] = None | ) -> Dict [ str , torch . Tensor ] Decoding from encoded states to sequence of outputs also computes loss if target_tokens are given. Parameters \u00b6 encoder_out : Dict[str, torch.LongTensor] Dictionary with encoded state, ideally containing the encoded vectors and the source mask. target_tokens : Dict[str, torch.LongTensor] , optional The output of TextField.as_array() applied on the target TextField . post_process # class SeqDecoder ( Module , Registrable ): | ... | def post_process ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Post processing for converting raw outputs to prediction during inference. The composing models such allennlp.models.encoder_decoders.composed_seq2seq.ComposedSeq2Seq can call this method when decode is called.","title":"seq_decoder"},{"location":"models/generation/modules/seq_decoders/seq_decoder/#seqdecoder","text":"class SeqDecoder ( Module , Registrable ): | def __init__ ( self , target_embedder : Embedding ) -> None A SeqDecoder abstract class representing the entire decoder (embedding and neural network) of a Seq2Seq architecture. This is meant to be used with allennlp.models.encoder_decoder.composed_seq2seq.ComposedSeq2Seq . The implementation of this abstract class ideally uses a decoder neural net allennlp.modules.seq2seq_decoders.decoder_net.DecoderNet for decoding. The default_implementation allennlp.modules.seq2seq_decoders.seq_decoder.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder covers most use cases. More likely that we will use the default implementation instead of creating a new implementation.","title":"SeqDecoder"},{"location":"models/generation/modules/seq_decoders/seq_decoder/#default_implementation","text":"class SeqDecoder ( Module , Registrable ): | ... | default_implementation = \"auto_regressive_seq_decoder\"","title":"default_implementation"},{"location":"models/generation/modules/seq_decoders/seq_decoder/#get_output_dim","text":"class SeqDecoder ( Module , Registrable ): | ... | def get_output_dim ( self ) -> int The dimension of each timestep of the hidden state in the layer before final softmax. Needed to check whether the model is compatible for embedding-final layer weight tying.","title":"get_output_dim"},{"location":"models/generation/modules/seq_decoders/seq_decoder/#get_metrics","text":"class SeqDecoder ( Module , Registrable ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] The decoder is responsible for computing metrics using the target tokens.","title":"get_metrics"},{"location":"models/generation/modules/seq_decoders/seq_decoder/#forward","text":"class SeqDecoder ( Module , Registrable ): | ... | def forward ( | self , | encoder_out : Dict [ str , torch . LongTensor ], | target_tokens : Optional [ Dict [ str , torch . LongTensor ]] = None | ) -> Dict [ str , torch . Tensor ] Decoding from encoded states to sequence of outputs also computes loss if target_tokens are given.","title":"forward"},{"location":"models/generation/modules/seq_decoders/seq_decoder/#post_process","text":"class SeqDecoder ( Module , Registrable ): | ... | def post_process ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Post processing for converting raw outputs to prediction during inference. The composing models such allennlp.models.encoder_decoders.composed_seq2seq.ComposedSeq2Seq can call this method when decode is called.","title":"post_process"},{"location":"models/generation/predictors/seq2seq/","text":"allennlp_models .generation .predictors .seq2seq [SOURCE] Seq2SeqPredictor # @Predictor . register ( \"seq2seq\" ) class Seq2SeqPredictor ( Predictor ) Predictor for sequence to sequence models, including ComposedSeq2Seq , SimpleSeq2Seq , CopyNetSeq2Seq , Bart , and T5 . predict # class Seq2SeqPredictor ( Predictor ): | ... | def predict ( self , source : str ) -> JsonDict pretrained_t5_for_generation # class Seq2SeqPredictor ( Predictor ): | ... | @classmethod | def pretrained_t5_for_generation ( | cls , | model_name : str = \"t5-base\" | ) -> \"Seq2SeqPredictor\" A helper method for creating a predictor for a pretrained T5 generation model. Examples \u00b6 from allennlp_models.generation.predictors import Seq2SeqPredictor ARTICLE_TO_SUMMARIZE = ''' summarize: The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct. ''' . strip () . replace ( \" \\n \" , \" \" ) predictor = Seq2SeqPredictor . pretrained_t5_for_generation ( \"t5-small\" ) predictor . predict ( ARTICLE_TO_SUMMARIZE )","title":"seq2seq"},{"location":"models/generation/predictors/seq2seq/#seq2seqpredictor","text":"@Predictor . register ( \"seq2seq\" ) class Seq2SeqPredictor ( Predictor ) Predictor for sequence to sequence models, including ComposedSeq2Seq , SimpleSeq2Seq , CopyNetSeq2Seq , Bart , and T5 .","title":"Seq2SeqPredictor"},{"location":"models/generation/predictors/seq2seq/#predict","text":"class Seq2SeqPredictor ( Predictor ): | ... | def predict ( self , source : str ) -> JsonDict","title":"predict"},{"location":"models/generation/predictors/seq2seq/#pretrained_t5_for_generation","text":"class Seq2SeqPredictor ( Predictor ): | ... | @classmethod | def pretrained_t5_for_generation ( | cls , | model_name : str = \"t5-base\" | ) -> \"Seq2SeqPredictor\" A helper method for creating a predictor for a pretrained T5 generation model.","title":"pretrained_t5_for_generation"},{"location":"models/lm/dataset_readers/masked_language_model/","text":"allennlp_models .lm .dataset_readers .masked_language_model [SOURCE] MaskedLanguageModelingReader # @DatasetReader . register ( \"masked_language_modeling\" ) class MaskedLanguageModelingReader ( DatasetReader ): | def __init__ ( | self , | tokenizer : Tokenizer = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | ** kwargs | ) -> None Reads a text file and converts it into a Dataset suitable for training a masked language model. The Field s that we create are the following: an input TextField , a mask position ListField[IndexField] , and a target token TextField (the target tokens aren't a single string of text, but we use a TextField so we can index the target tokens the same way as our input, typically with a single PretrainedTransformerIndexer ). The mask position and target token lists are the same length. NOTE: This is not fully functional! It was written to put together a demo for interpreting and attacking masked language modeling, not for actually training anything. text_to_instance is functional, but _read is not. To make this fully functional, you would want some sampling strategies for picking the locations for [MASK] tokens, and probably a bunch of efficiency / multi-processing stuff. Parameters \u00b6 tokenizer : Tokenizer , optional (default = WhitespaceTokenizer() ) We use this Tokenizer for the text. See Tokenizer . token_indexers : Dict[str, TokenIndexer] , optional (default = {\"tokens\": SingleIdTokenIndexer()} ) We use this to define the input representation for the text, and to get ids for the mask targets. See TokenIndexer . text_to_instance # class MaskedLanguageModelingReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | sentence : str = None , | tokens : List [ Token ] = None , | targets : List [ str ] = None | ) -> Instance Parameters \u00b6 sentence : str , optional A sentence containing [MASK] tokens that should be filled in by the model. This input is superceded and ignored if tokens is given. tokens : List[Token] , optional An already-tokenized sentence containing some number of [MASK] tokens to be predicted. targets : List[str] , optional Contains the target tokens to be predicted. The length of this list should be the same as the number of [MASK] tokens in the input.","title":"masked_language_model"},{"location":"models/lm/dataset_readers/masked_language_model/#maskedlanguagemodelingreader","text":"@DatasetReader . register ( \"masked_language_modeling\" ) class MaskedLanguageModelingReader ( DatasetReader ): | def __init__ ( | self , | tokenizer : Tokenizer = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | ** kwargs | ) -> None Reads a text file and converts it into a Dataset suitable for training a masked language model. The Field s that we create are the following: an input TextField , a mask position ListField[IndexField] , and a target token TextField (the target tokens aren't a single string of text, but we use a TextField so we can index the target tokens the same way as our input, typically with a single PretrainedTransformerIndexer ). The mask position and target token lists are the same length. NOTE: This is not fully functional! It was written to put together a demo for interpreting and attacking masked language modeling, not for actually training anything. text_to_instance is functional, but _read is not. To make this fully functional, you would want some sampling strategies for picking the locations for [MASK] tokens, and probably a bunch of efficiency / multi-processing stuff.","title":"MaskedLanguageModelingReader"},{"location":"models/lm/dataset_readers/masked_language_model/#text_to_instance","text":"class MaskedLanguageModelingReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | sentence : str = None , | tokens : List [ Token ] = None , | targets : List [ str ] = None | ) -> Instance","title":"text_to_instance"},{"location":"models/lm/dataset_readers/next_token_lm/","text":"allennlp_models .lm .dataset_readers .next_token_lm [SOURCE] NextTokenLMReader # @DatasetReader . register ( \"next_token_lm\" ) class NextTokenLMReader ( DatasetReader ): | def __init__ ( | self , | tokenizer : Tokenizer = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | max_tokens : int = None , | ** kwargs | ) -> None Creates Instances suitable for use in predicting a single next token using a language model. The Field s that we create are the following: an input TextField and a target token TextField (we only ver have a single token, but we use a TextField so we can index it the same way as our input, typically with a single PretrainedTransformerIndexer ). NOTE: This is not fully functional! It was written to put together a demo for interpreting and attacking language models, not for actually training anything. It would be a really bad idea to use this setup for training language models, as it would be incredibly inefficient. The only purpose of this class is for a demo. Parameters \u00b6 tokenizer : Tokenizer , optional (default = WhitespaceTokenizer() ) We use this Tokenizer for the text. See Tokenizer . token_indexers : Dict[str, TokenIndexer] , optional (default = {\"tokens\": SingleIdTokenIndexer()} ) We use this to define the input representation for the text, and to get ids for the mask targets. See TokenIndexer . max_tokens : int , optional (default = None ) If you don't handle truncation at the tokenizer level, you can specify max_tokens here, and the only the last max_tokens will be used. text_to_instance # class NextTokenLMReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | sentence : str = None , | tokens : List [ Token ] = None , | target : str = None | ) -> Instance","title":"next_token_lm"},{"location":"models/lm/dataset_readers/next_token_lm/#nexttokenlmreader","text":"@DatasetReader . register ( \"next_token_lm\" ) class NextTokenLMReader ( DatasetReader ): | def __init__ ( | self , | tokenizer : Tokenizer = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | max_tokens : int = None , | ** kwargs | ) -> None Creates Instances suitable for use in predicting a single next token using a language model. The Field s that we create are the following: an input TextField and a target token TextField (we only ver have a single token, but we use a TextField so we can index it the same way as our input, typically with a single PretrainedTransformerIndexer ). NOTE: This is not fully functional! It was written to put together a demo for interpreting and attacking language models, not for actually training anything. It would be a really bad idea to use this setup for training language models, as it would be incredibly inefficient. The only purpose of this class is for a demo.","title":"NextTokenLMReader"},{"location":"models/lm/dataset_readers/next_token_lm/#text_to_instance","text":"class NextTokenLMReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | sentence : str = None , | tokens : List [ Token ] = None , | target : str = None | ) -> Instance","title":"text_to_instance"},{"location":"models/lm/dataset_readers/simple_language_modeling/","text":"allennlp_models .lm .dataset_readers .simple_language_modeling [SOURCE] SimpleLanguageModelingDatasetReader # @DatasetReader . register ( \"simple_language_modeling\" ) class SimpleLanguageModelingDatasetReader ( DatasetReader ): | def __init__ ( | self , | tokenizer : Tokenizer = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | max_sequence_length : int = None , | start_tokens : List [ str ] = None , | end_tokens : List [ str ] = None , | ** kwargs | ) -> None Reads sentences, one per line, for language modeling. This does not handle arbitrarily formatted text with sentences spanning multiple lines. Parameters \u00b6 tokenizer : Tokenizer , optional Tokenizer to use to split the input sentences into words or other kinds of tokens. Defaults to SpacyTokenizer() . token_indexers : Dict[str, TokenIndexer] , optional Indexers used to define input token representations. Defaults to {\"tokens\": SingleIdTokenIndexer()} . max_sequence_length : int , optional If specified, sentences with more than this number of tokens will be dropped. start_tokens : List[str] , optional (default = None ) These are prepended to the tokens provided to the TextField . end_tokens : List[str] , optional (default = None ) These are appended to the tokens provided to the TextField . text_to_instance # class SimpleLanguageModelingDatasetReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( self , sentence : str ) -> Instance","title":"simple_language_modeling"},{"location":"models/lm/dataset_readers/simple_language_modeling/#simplelanguagemodelingdatasetreader","text":"@DatasetReader . register ( \"simple_language_modeling\" ) class SimpleLanguageModelingDatasetReader ( DatasetReader ): | def __init__ ( | self , | tokenizer : Tokenizer = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | max_sequence_length : int = None , | start_tokens : List [ str ] = None , | end_tokens : List [ str ] = None , | ** kwargs | ) -> None Reads sentences, one per line, for language modeling. This does not handle arbitrarily formatted text with sentences spanning multiple lines.","title":"SimpleLanguageModelingDatasetReader"},{"location":"models/lm/dataset_readers/simple_language_modeling/#text_to_instance","text":"class SimpleLanguageModelingDatasetReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( self , sentence : str ) -> Instance","title":"text_to_instance"},{"location":"models/lm/models/bidirectional_lm/","text":"allennlp_models .lm .models .bidirectional_lm [SOURCE] BidirectionalLanguageModel # @Model . register ( \"bidirectional-language-model\" ) class BidirectionalLanguageModel ( LanguageModel ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | contextualizer : Seq2SeqEncoder , | dropout : float = None , | num_samples : int = None , | sparse_embeddings : bool = False , | initializer : InitializerApplicator = None , | ** kwargs | ) -> None The BidirectionalLanguageModel applies a bidirectional \"contextualizing\" Seq2SeqEncoder to uncontextualized embeddings, using a SoftmaxLoss module (defined above) to compute the language modeling loss. It is IMPORTANT that your bidirectional Seq2SeqEncoder does not do any \"peeking ahead\". That is, for its forward direction it should only consider embeddings at previous timesteps, and for its backward direction only embeddings at subsequent timesteps. If this condition is not met, your language model is cheating. Parameters \u00b6 vocab : Vocabulary text_field_embedder : TextFieldEmbedder Used to embed the indexed tokens we get in forward . contextualizer : Seq2SeqEncoder Used to \"contextualize\" the embeddings. As described above, this encoder must not cheat by peeking ahead. dropout : float , optional (default = None ) If specified, dropout is applied to the contextualized embeddings before computation of the softmax. The contextualized embeddings themselves are returned without dropout. num_samples : int , optional (default = None ) If provided, the model will use SampledSoftmaxLoss with the specified number of samples. Otherwise, it will use the full _SoftmaxLoss defined above. sparse_embeddings : bool , optional (default = False ) Passed on to SampledSoftmaxLoss if True.","title":"bidirectional_lm"},{"location":"models/lm/models/bidirectional_lm/#bidirectionallanguagemodel","text":"@Model . register ( \"bidirectional-language-model\" ) class BidirectionalLanguageModel ( LanguageModel ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | contextualizer : Seq2SeqEncoder , | dropout : float = None , | num_samples : int = None , | sparse_embeddings : bool = False , | initializer : InitializerApplicator = None , | ** kwargs | ) -> None The BidirectionalLanguageModel applies a bidirectional \"contextualizing\" Seq2SeqEncoder to uncontextualized embeddings, using a SoftmaxLoss module (defined above) to compute the language modeling loss. It is IMPORTANT that your bidirectional Seq2SeqEncoder does not do any \"peeking ahead\". That is, for its forward direction it should only consider embeddings at previous timesteps, and for its backward direction only embeddings at subsequent timesteps. If this condition is not met, your language model is cheating.","title":"BidirectionalLanguageModel"},{"location":"models/lm/models/language_model/","text":"allennlp_models .lm .models .language_model [SOURCE] LanguageModel # @Model . register ( \"language_model\" ) class LanguageModel ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | contextualizer : Seq2SeqEncoder , | dropout : float = None , | num_samples : int = None , | sparse_embeddings : bool = False , | bidirectional : bool = False , | initializer : InitializerApplicator = None , | ** kwargs | ) -> None The LanguageModel applies a \"contextualizing\" Seq2SeqEncoder to uncontextualized embeddings, using a SoftmaxLoss module (defined above) to compute the language modeling loss. If bidirectional is True, the language model is trained to predict the next and previous tokens for each token in the input. In this case, the contextualizer must be bidirectional. If bidirectional is False, the language model is trained to only predict the next token for each token in the input; the contextualizer should also be unidirectional. If your language model is bidirectional, it is IMPORTANT that your bidirectional Seq2SeqEncoder contextualizer does not do any \"peeking ahead\". That is, for its forward direction it should only consider embeddings at previous timesteps, and for its backward direction only embeddings at subsequent timesteps. Similarly, if your language model is unidirectional, the unidirectional contextualizer should only consider embeddings at previous timesteps. If this condition is not met, your language model is cheating. Parameters \u00b6 vocab : Vocabulary text_field_embedder : TextFieldEmbedder Used to embed the indexed tokens we get in forward . contextualizer : Seq2SeqEncoder Used to \"contextualize\" the embeddings. As described above, this encoder must not cheat by peeking ahead. dropout : float , optional (default = None ) If specified, dropout is applied to the contextualized embeddings before computation of the softmax. The contextualized embeddings themselves are returned without dropout. num_samples : int , optional (default = None ) If provided, the model will use SampledSoftmaxLoss with the specified number of samples. Otherwise, it will use the full _SoftmaxLoss defined above. sparse_embeddings : bool , optional (default = False ) Passed on to SampledSoftmaxLoss if True. bidirectional : bool , optional (default = False ) Train a bidirectional language model, where the contextualizer is used to predict the next and previous token for each input token. This must match the bidirectionality of the contextualizer. delete_softmax # class LanguageModel ( Model ): | ... | def delete_softmax ( self ) -> None Remove the softmax weights. Useful for saving memory when calculating the loss is not necessary, e.g. in an embedder. num_layers # class LanguageModel ( Model ): | ... | def num_layers ( self ) -> int Returns the depth of this LM. That is, how many layers the contextualizer has plus one for the non-contextual layer. forward # class LanguageModel ( Model ): | ... | def forward ( self , source : TextFieldTensors ) -> Dict [ str , torch . Tensor ] Computes the averaged forward (and backward, if language model is bidirectional) LM loss from the batch. Parameters \u00b6 source : TextFieldTensors The output of Batch.as_tensor_dict() for a batch of sentences. By convention, it's required to have at least a \"tokens\" entry that's the output of a SingleIdTokenIndexer , which is used to compute the language model targets. Returns \u00b6 Dict with keys: 'loss' : torch.Tensor forward negative log likelihood, or the average of forward/backward if language model is bidirectional 'forward\\_loss' : torch.Tensor forward direction negative log likelihood 'backward\\_loss' : torch.Tensor or None backward direction negative log likelihood. If language model is not bidirectional, this is None . 'lm\\_embeddings' : Union[torch.Tensor, List[torch.Tensor]] (batch_size, timesteps, embed_dim) tensor of top layer contextual representations or list of all layers. No dropout applied. 'noncontextual\\_token\\_embeddings' : torch.Tensor (batch_size, timesteps, token_embed_dim) tensor of bottom layer noncontextual representations 'mask' : torch.BoolTensor (batch_size, timesteps) mask for the embeddings get_metrics # class LanguageModel ( Model ): | ... | def get_metrics ( self , reset : bool = False )","title":"language_model"},{"location":"models/lm/models/language_model/#languagemodel","text":"@Model . register ( \"language_model\" ) class LanguageModel ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | contextualizer : Seq2SeqEncoder , | dropout : float = None , | num_samples : int = None , | sparse_embeddings : bool = False , | bidirectional : bool = False , | initializer : InitializerApplicator = None , | ** kwargs | ) -> None The LanguageModel applies a \"contextualizing\" Seq2SeqEncoder to uncontextualized embeddings, using a SoftmaxLoss module (defined above) to compute the language modeling loss. If bidirectional is True, the language model is trained to predict the next and previous tokens for each token in the input. In this case, the contextualizer must be bidirectional. If bidirectional is False, the language model is trained to only predict the next token for each token in the input; the contextualizer should also be unidirectional. If your language model is bidirectional, it is IMPORTANT that your bidirectional Seq2SeqEncoder contextualizer does not do any \"peeking ahead\". That is, for its forward direction it should only consider embeddings at previous timesteps, and for its backward direction only embeddings at subsequent timesteps. Similarly, if your language model is unidirectional, the unidirectional contextualizer should only consider embeddings at previous timesteps. If this condition is not met, your language model is cheating.","title":"LanguageModel"},{"location":"models/lm/models/language_model/#delete_softmax","text":"class LanguageModel ( Model ): | ... | def delete_softmax ( self ) -> None Remove the softmax weights. Useful for saving memory when calculating the loss is not necessary, e.g. in an embedder.","title":"delete_softmax"},{"location":"models/lm/models/language_model/#num_layers","text":"class LanguageModel ( Model ): | ... | def num_layers ( self ) -> int Returns the depth of this LM. That is, how many layers the contextualizer has plus one for the non-contextual layer.","title":"num_layers"},{"location":"models/lm/models/language_model/#forward","text":"class LanguageModel ( Model ): | ... | def forward ( self , source : TextFieldTensors ) -> Dict [ str , torch . Tensor ] Computes the averaged forward (and backward, if language model is bidirectional) LM loss from the batch.","title":"forward"},{"location":"models/lm/models/language_model/#get_metrics","text":"class LanguageModel ( Model ): | ... | def get_metrics ( self , reset : bool = False )","title":"get_metrics"},{"location":"models/lm/models/masked_language_model/","text":"allennlp_models .lm .models .masked_language_model [SOURCE] MaskedLanguageModel # @Model . register ( \"masked_language_model\" ) class MaskedLanguageModel ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | language_model_head : LanguageModelHead , | contextualizer : Seq2SeqEncoder = None , | target_namespace : str = \"bert\" , | dropout : float = 0.0 , | initializer : InitializerApplicator = None , | ** kwargs | ) -> None The MaskedLanguageModel embeds some input tokens (including some which are masked), contextualizes them, then predicts targets for the masked tokens, computing a loss against known targets. NOTE: This was developed for use in a demo, not for training. It's possible that it will still work for training a masked LM, but it is very likely that some other code would be much more efficient for that. This does compute correct gradients of the loss, because we use that in our demo, so in principle it should be able to train a model, we just don't necessarily endorse that use. Parameters \u00b6 vocab : Vocabulary text_field_embedder : TextFieldEmbedder Used to embed the indexed tokens we get in forward . language_model_head : LanguageModelHead The torch.nn.Module that goes from the hidden states output by the contextualizer to logits over some output vocabulary. contextualizer : Seq2SeqEncoder , optional (default = None ) Used to \"contextualize\" the embeddings. This is optional because the contextualization might actually be done in the text field embedder. target_namespace : str , optional (default = 'bert' ) Namespace to use to convert predicted token ids to strings in Model.make_output_human_readable . dropout : float , optional (default = 0.0 ) If specified, dropout is applied to the contextualized embeddings before computation of the softmax. The contextualized embeddings themselves are returned without dropout. forward # class MaskedLanguageModel ( Model ): | ... | def forward ( | self , | tokens : TextFieldTensors , | mask_positions : torch . BoolTensor , | target_ids : TextFieldTensors = None | ) -> Dict [ str , torch . Tensor ] Parameters \u00b6 tokens : TextFieldTensors The output of TextField.as_tensor() for a batch of sentences. mask_positions : torch.LongTensor The positions in tokens that correspond to [MASK] tokens that we should try to fill in. Shape should be (batch_size, num_masks). target_ids : TextFieldTensors This is a list of token ids that correspond to the mask positions we're trying to fill. It is the output of a TextField , purely for convenience, so we can handle wordpiece tokenizers and such without having to do crazy things in the dataset reader. We assume that there is exactly one entry in the dictionary, and that it has a shape identical to mask_positions - one target token per mask position. get_metrics # class MaskedLanguageModel ( Model ): | ... | def get_metrics ( self , reset : bool = False ) make_output_human_readable # class MaskedLanguageModel ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] default_predictor # class MaskedLanguageModel ( Model ): | ... | default_predictor = \"masked_language_model\"","title":"masked_language_model"},{"location":"models/lm/models/masked_language_model/#maskedlanguagemodel","text":"@Model . register ( \"masked_language_model\" ) class MaskedLanguageModel ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | language_model_head : LanguageModelHead , | contextualizer : Seq2SeqEncoder = None , | target_namespace : str = \"bert\" , | dropout : float = 0.0 , | initializer : InitializerApplicator = None , | ** kwargs | ) -> None The MaskedLanguageModel embeds some input tokens (including some which are masked), contextualizes them, then predicts targets for the masked tokens, computing a loss against known targets. NOTE: This was developed for use in a demo, not for training. It's possible that it will still work for training a masked LM, but it is very likely that some other code would be much more efficient for that. This does compute correct gradients of the loss, because we use that in our demo, so in principle it should be able to train a model, we just don't necessarily endorse that use.","title":"MaskedLanguageModel"},{"location":"models/lm/models/masked_language_model/#forward","text":"class MaskedLanguageModel ( Model ): | ... | def forward ( | self , | tokens : TextFieldTensors , | mask_positions : torch . BoolTensor , | target_ids : TextFieldTensors = None | ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"models/lm/models/masked_language_model/#get_metrics","text":"class MaskedLanguageModel ( Model ): | ... | def get_metrics ( self , reset : bool = False )","title":"get_metrics"},{"location":"models/lm/models/masked_language_model/#make_output_human_readable","text":"class MaskedLanguageModel ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ]","title":"make_output_human_readable"},{"location":"models/lm/models/masked_language_model/#default_predictor","text":"class MaskedLanguageModel ( Model ): | ... | default_predictor = \"masked_language_model\"","title":"default_predictor"},{"location":"models/lm/models/next_token_lm/","text":"allennlp_models .lm .models .next_token_lm [SOURCE] NextTokenLM # @Model . register ( \"next_token_lm\" ) class NextTokenLM ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | language_model_head : LanguageModelHead , | contextualizer : Seq2SeqEncoder = None , | target_namespace : str = \"bert\" , | dropout : float = 0.0 , | initializer : InitializerApplicator = None , | n_best : int = 5 , | beam_search_generator : BeamSearchGenerator = None , | ** kwargs | ) -> None The NextTokenLM embeds some input tokens, contextualizes them, then predicts the next word, computing a loss against known target. If BeamSearch is given, this model will predict a sequence of next tokens. Note This was developed for use in a demo, not for training. You definitely don't want to train a language model using this code; it would be incredibly inefficient. But it does compute correct gradients of the loss, however, so you can use it for interesting visualization of the gradients of a pretrained model, and it appears to be fast enough to sample from, at least for one word at a time. Parameters \u00b6 vocab : Vocabulary text_field_embedder : TextFieldEmbedder Used to embed the indexed tokens we get in forward . language_model_head : LanguageModelHead The torch.nn.Module that goes from the hidden states output by the contextualizer to logits over some output vocabulary. contextualizer : Seq2SeqEncoder , optional (default = None ) Used to \"contextualize\" the embeddings. This is optional because the contextualization might actually be done in the text field embedder. target_namespace : str , optional (default = 'bert' ) Namespace to use to convert predicted token ids to strings in Model.make_output_human_readable . dropout : float , optional (default = 0.0 ) If specified, dropout is applied to the contextualized embeddings before computation of the softmax. The contextualized embeddings themselves are returned without dropout. n_best : int , optional (default = 5 ) The number of best tokens to predict. If beam_search is given, this option is ignored. beam_search_generator : BeamSearchGenerator , optional (default = None ) An optional BeamSearchGenerator . If given, the model will predict sequences of next tokens instead of just a single next token. forward # class NextTokenLM ( Model ): | ... | def forward ( | self , | tokens : TextFieldTensors , | target_ids : TextFieldTensors = None | ) -> Dict [ str , torch . Tensor ] Run a forward pass of the model, returning an output tensor dictionary with the following fields: \"probabilities\" : a tensor of shape (batch_size, n_best) representing the probabilities of the predicted tokens, where n_best is either self._n_best or beam_size if using beam search. \"top_indices\" : a tensor of shape (batch_size, n_best, num_predicted_tokens) containing the IDs of the predicted tokens, where num_predicted_tokens is just 1 unless using beam search, in which case it depends on the parameters of the beam search. \"token_ids\" : a tensor of shape (batch_size, num_input_tokens) containing the IDs of the input tokens. \"loss\" (optional): the loss of the batch, only given if target_ids is not None . get_metrics # class NextTokenLM ( Model ): | ... | def get_metrics ( self , reset : bool = False ) make_output_human_readable # class NextTokenLM ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Collects token strings from indices, adding two fields to the output_dict : \"top_tokens\" : a list (for each instance in the batch) of lists (for each of the n best predictions) of lists of strings (for each token in each prediction). \"tokens\" : a list of list (for each instance in the batch) of strings (for each input token in the instance). default_predictor # class NextTokenLM ( Model ): | ... | default_predictor = \"next_token_lm\"","title":"next_token_lm"},{"location":"models/lm/models/next_token_lm/#nexttokenlm","text":"@Model . register ( \"next_token_lm\" ) class NextTokenLM ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | language_model_head : LanguageModelHead , | contextualizer : Seq2SeqEncoder = None , | target_namespace : str = \"bert\" , | dropout : float = 0.0 , | initializer : InitializerApplicator = None , | n_best : int = 5 , | beam_search_generator : BeamSearchGenerator = None , | ** kwargs | ) -> None The NextTokenLM embeds some input tokens, contextualizes them, then predicts the next word, computing a loss against known target. If BeamSearch is given, this model will predict a sequence of next tokens. Note This was developed for use in a demo, not for training. You definitely don't want to train a language model using this code; it would be incredibly inefficient. But it does compute correct gradients of the loss, however, so you can use it for interesting visualization of the gradients of a pretrained model, and it appears to be fast enough to sample from, at least for one word at a time.","title":"NextTokenLM"},{"location":"models/lm/models/next_token_lm/#forward","text":"class NextTokenLM ( Model ): | ... | def forward ( | self , | tokens : TextFieldTensors , | target_ids : TextFieldTensors = None | ) -> Dict [ str , torch . Tensor ] Run a forward pass of the model, returning an output tensor dictionary with the following fields: \"probabilities\" : a tensor of shape (batch_size, n_best) representing the probabilities of the predicted tokens, where n_best is either self._n_best or beam_size if using beam search. \"top_indices\" : a tensor of shape (batch_size, n_best, num_predicted_tokens) containing the IDs of the predicted tokens, where num_predicted_tokens is just 1 unless using beam search, in which case it depends on the parameters of the beam search. \"token_ids\" : a tensor of shape (batch_size, num_input_tokens) containing the IDs of the input tokens. \"loss\" (optional): the loss of the batch, only given if target_ids is not None .","title":"forward"},{"location":"models/lm/models/next_token_lm/#get_metrics","text":"class NextTokenLM ( Model ): | ... | def get_metrics ( self , reset : bool = False )","title":"get_metrics"},{"location":"models/lm/models/next_token_lm/#make_output_human_readable","text":"class NextTokenLM ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Collects token strings from indices, adding two fields to the output_dict : \"top_tokens\" : a list (for each instance in the batch) of lists (for each of the n best predictions) of lists of strings (for each token in each prediction). \"tokens\" : a list of list (for each instance in the batch) of strings (for each input token in the instance).","title":"make_output_human_readable"},{"location":"models/lm/models/next_token_lm/#default_predictor","text":"class NextTokenLM ( Model ): | ... | default_predictor = \"next_token_lm\"","title":"default_predictor"},{"location":"models/lm/modules/language_model_heads/bert/","text":"allennlp_models .lm .modules .language_model_heads .bert [SOURCE] BertLanguageModelHead # @LanguageModelHead . register ( \"bert\" ) class BertLanguageModelHead ( LanguageModelHead ): | def __init__ ( self , model_name : str ) -> None Loads just the LM head from transformers.BertForMaskedLM . It was easiest to load the entire model before only pulling out the head, so this is a bit slower than it could be, but for practical use in a model, the few seconds of extra loading time is probably not a big deal. get_input_dim # class BertLanguageModelHead ( LanguageModelHead ): | ... | @overrides | def get_input_dim ( self ) -> int get_output_dim # class BertLanguageModelHead ( LanguageModelHead ): | ... | @overrides | def get_output_dim ( self ) -> int forward # class BertLanguageModelHead ( LanguageModelHead ): | ... | def forward ( self , hidden_states : torch . Tensor ) -> torch . Tensor","title":"bert"},{"location":"models/lm/modules/language_model_heads/bert/#bertlanguagemodelhead","text":"@LanguageModelHead . register ( \"bert\" ) class BertLanguageModelHead ( LanguageModelHead ): | def __init__ ( self , model_name : str ) -> None Loads just the LM head from transformers.BertForMaskedLM . It was easiest to load the entire model before only pulling out the head, so this is a bit slower than it could be, but for practical use in a model, the few seconds of extra loading time is probably not a big deal.","title":"BertLanguageModelHead"},{"location":"models/lm/modules/language_model_heads/bert/#get_input_dim","text":"class BertLanguageModelHead ( LanguageModelHead ): | ... | @overrides | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"models/lm/modules/language_model_heads/bert/#get_output_dim","text":"class BertLanguageModelHead ( LanguageModelHead ): | ... | @overrides | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"models/lm/modules/language_model_heads/bert/#forward","text":"class BertLanguageModelHead ( LanguageModelHead ): | ... | def forward ( self , hidden_states : torch . Tensor ) -> torch . Tensor","title":"forward"},{"location":"models/lm/modules/language_model_heads/gpt2/","text":"allennlp_models .lm .modules .language_model_heads .gpt2 [SOURCE] Gpt2LanguageModelHead # @LanguageModelHead . register ( \"gpt2\" ) class Gpt2LanguageModelHead ( LanguageModelHead ): | def __init__ ( self , model_name : str ) -> None Loads just the LM head from transformers.GPT2LMHeadModel . It was easiest to load the entire model before only pulling out the head, so this is a bit slower than it could be, but for practical use in a model, the few seconds of extra loading time is probably not a big deal. get_input_dim # class Gpt2LanguageModelHead ( LanguageModelHead ): | ... | @overrides | def get_input_dim ( self ) -> int get_output_dim # class Gpt2LanguageModelHead ( LanguageModelHead ): | ... | @overrides | def get_output_dim ( self ) -> int forward # class Gpt2LanguageModelHead ( LanguageModelHead ): | ... | def forward ( self , hidden_states : torch . Tensor ) -> torch . Tensor","title":"gpt2"},{"location":"models/lm/modules/language_model_heads/gpt2/#gpt2languagemodelhead","text":"@LanguageModelHead . register ( \"gpt2\" ) class Gpt2LanguageModelHead ( LanguageModelHead ): | def __init__ ( self , model_name : str ) -> None Loads just the LM head from transformers.GPT2LMHeadModel . It was easiest to load the entire model before only pulling out the head, so this is a bit slower than it could be, but for practical use in a model, the few seconds of extra loading time is probably not a big deal.","title":"Gpt2LanguageModelHead"},{"location":"models/lm/modules/language_model_heads/gpt2/#get_input_dim","text":"class Gpt2LanguageModelHead ( LanguageModelHead ): | ... | @overrides | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"models/lm/modules/language_model_heads/gpt2/#get_output_dim","text":"class Gpt2LanguageModelHead ( LanguageModelHead ): | ... | @overrides | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"models/lm/modules/language_model_heads/gpt2/#forward","text":"class Gpt2LanguageModelHead ( LanguageModelHead ): | ... | def forward ( self , hidden_states : torch . Tensor ) -> torch . Tensor","title":"forward"},{"location":"models/lm/modules/language_model_heads/language_model_head/","text":"allennlp_models .lm .modules .language_model_heads .language_model_head [SOURCE] LanguageModelHead # class LanguageModelHead ( torch . nn . Module , Registrable ) A LanguageModelHead encapsulates a function that goes from some hidden state to logits over a vocabulary. get_input_dim # class LanguageModelHead ( torch . nn . Module , Registrable ): | ... | def get_input_dim ( self ) -> int get_output_dim # class LanguageModelHead ( torch . nn . Module , Registrable ): | ... | def get_output_dim ( self ) -> int forward # class LanguageModelHead ( torch . nn . Module , Registrable ): | ... | def forward ( self , hidden_states : torch . Tensor ) -> torch . Tensor","title":"language_model_head"},{"location":"models/lm/modules/language_model_heads/language_model_head/#languagemodelhead","text":"class LanguageModelHead ( torch . nn . Module , Registrable ) A LanguageModelHead encapsulates a function that goes from some hidden state to logits over a vocabulary.","title":"LanguageModelHead"},{"location":"models/lm/modules/language_model_heads/language_model_head/#get_input_dim","text":"class LanguageModelHead ( torch . nn . Module , Registrable ): | ... | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"models/lm/modules/language_model_heads/language_model_head/#get_output_dim","text":"class LanguageModelHead ( torch . nn . Module , Registrable ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"models/lm/modules/language_model_heads/language_model_head/#forward","text":"class LanguageModelHead ( torch . nn . Module , Registrable ): | ... | def forward ( self , hidden_states : torch . Tensor ) -> torch . Tensor","title":"forward"},{"location":"models/lm/modules/language_model_heads/linear/","text":"allennlp_models .lm .modules .language_model_heads .linear [SOURCE] LinearLanguageModelHead # @LanguageModelHead . register ( \"linear\" ) class LinearLanguageModelHead ( LanguageModelHead ): | def __init__ ( | self , | vocab : Vocabulary , | input_dim : int , | vocab_namespace : str | ) -> None Uses torch.nn.Linear as a language model head. Does nothing else fancy. This was intended largely for testing code with small models and simple components. It's likely that you would want something nicer for actually training a language model, such as tying weights with an input embedding, or an adaptive softmax, or something. get_input_dim # class LinearLanguageModelHead ( LanguageModelHead ): | ... | @overrides | def get_input_dim ( self ) -> int get_output_dim # class LinearLanguageModelHead ( LanguageModelHead ): | ... | @overrides | def get_output_dim ( self ) -> int forward # class LinearLanguageModelHead ( LanguageModelHead ): | ... | def forward ( self , hidden_states : torch . Tensor ) -> torch . Tensor","title":"linear"},{"location":"models/lm/modules/language_model_heads/linear/#linearlanguagemodelhead","text":"@LanguageModelHead . register ( \"linear\" ) class LinearLanguageModelHead ( LanguageModelHead ): | def __init__ ( | self , | vocab : Vocabulary , | input_dim : int , | vocab_namespace : str | ) -> None Uses torch.nn.Linear as a language model head. Does nothing else fancy. This was intended largely for testing code with small models and simple components. It's likely that you would want something nicer for actually training a language model, such as tying weights with an input embedding, or an adaptive softmax, or something.","title":"LinearLanguageModelHead"},{"location":"models/lm/modules/language_model_heads/linear/#get_input_dim","text":"class LinearLanguageModelHead ( LanguageModelHead ): | ... | @overrides | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"models/lm/modules/language_model_heads/linear/#get_output_dim","text":"class LinearLanguageModelHead ( LanguageModelHead ): | ... | @overrides | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"models/lm/modules/language_model_heads/linear/#forward","text":"class LinearLanguageModelHead ( LanguageModelHead ): | ... | def forward ( self , hidden_states : torch . Tensor ) -> torch . Tensor","title":"forward"},{"location":"models/lm/modules/seq2seq_encoders/bidirectional_lm_transformer/","text":"allennlp_models .lm .modules .seq2seq_encoders .bidirectional_lm_transformer [SOURCE] The BidirectionalTransformerEncoder from Calypso. This is basically the transformer from https://nlp.seas.harvard.edu/2018/04/03/attention.html so credit to them. This code should be considered \"private\" in that we have several transformer implementations and may end up deleting this one. If you use it, consider yourself warned. attention # def attention ( query : torch . Tensor , key : torch . Tensor , value : torch . Tensor , mask : torch . BoolTensor = None , dropout : Callable = None ) -> Tuple [ torch . Tensor , torch . Tensor ] Compute 'Scaled Dot Product Attention' subsequent_mask # def subsequent_mask ( size : int , device : str = \"cpu\" ) -> torch . BoolTensor Mask out subsequent positions. PositionalEncoding # class PositionalEncoding ( torch . nn . Module , Registrable ): | def __init__ ( self , input_dim : int , max_len : int = 5000 ) -> None Implement the Positional Encoding function. forward # class PositionalEncoding ( torch . nn . Module , Registrable ): | ... | def forward ( self , x : torch . Tensor ) -> torch . Tensor PositionwiseFeedForward # class PositionwiseFeedForward ( torch . nn . Module ): | def __init__ ( | self , | input_dim : int , | ff_dim : int , | dropout : float = 0.1 | ) -> None Implements FFN equation. forward # class PositionwiseFeedForward ( torch . nn . Module ): | ... | def forward ( self , x : torch . Tensor ) -> torch . Tensor TransformerEncoder # class TransformerEncoder ( torch . nn . Module ): | def __init__ ( | self , | layer : torch . nn . Module , | num_layers : int , | return_all_layers : bool = False | ) -> None Core encoder is a stack of N layers forward # class TransformerEncoder ( torch . nn . Module ): | ... | def forward ( self , x , mask ) Pass the input (and mask) through each layer in turn. SublayerConnection # class SublayerConnection ( torch . nn . Module ): | def __init__ ( self , size : int , dropout : float ) -> None A residual connection followed by a layer norm. Note for code simplicity the norm is first as opposed to last. forward # class SublayerConnection ( torch . nn . Module ): | ... | def forward ( | self , | x : torch . Tensor , | sublayer : Callable [[ torch . Tensor ], torch . Tensor ] | ) -> torch . Tensor Apply residual connection to any sublayer with the same size. EncoderLayer # class EncoderLayer ( torch . nn . Module ): | def __init__ ( | self , | size : int , | self_attn : torch . nn . Module , | feed_forward : torch . nn . Module , | dropout : float | ) -> None Encoder is made up of self-attn and feed forward (defined below) forward # class EncoderLayer ( torch . nn . Module ): | ... | def forward ( | self , | x : torch . Tensor , | mask : torch . BoolTensor | ) -> torch . Tensor Follow Figure 1 (left) for connections. MultiHeadedAttention # class MultiHeadedAttention ( torch . nn . Module ): | def __init__ ( | self , | num_heads : int , | input_dim : int , | dropout : float = 0.1 | ) -> None forward # class MultiHeadedAttention ( torch . nn . Module ): | ... | def forward ( | self , | query : torch . Tensor , | key : torch . Tensor , | value : torch . Tensor , | mask : torch . BoolTensor = None | ) -> torch . Tensor make_model # def make_model ( num_layers : int = 6 , input_size : int = 512 , hidden_size : int = 2048 , heads : int = 8 , dropout : float = 0.1 , return_all_layers : bool = False ) -> TransformerEncoder Helper: Construct a model from hyperparameters. BidirectionalLanguageModelTransformer # @Seq2SeqEncoder . register ( \"bidirectional_language_model_transformer\" ) class BidirectionalLanguageModelTransformer ( Seq2SeqEncoder ): | def __init__ ( | self , | input_dim : int , | hidden_dim : int , | num_layers : int , | dropout : float = 0.1 , | input_dropout : float = None , | return_all_layers : bool = False | ) -> None get_attention_masks # class BidirectionalLanguageModelTransformer ( Seq2SeqEncoder ): | ... | def get_attention_masks ( | self , | mask : torch . BoolTensor | ) -> Tuple [ torch . Tensor , torch . Tensor ] Returns 2 masks of shape (batch_size, timesteps, timesteps) representing 1) non-padded elements, and 2) elements of the sequence which are permitted to be involved in attention at a given timestep. forward # class BidirectionalLanguageModelTransformer ( Seq2SeqEncoder ): | ... | def forward ( | self , | token_embeddings : torch . Tensor , | mask : torch . BoolTensor | ) -> torch . Tensor get_regularization_penalty # class BidirectionalLanguageModelTransformer ( Seq2SeqEncoder ): | ... | def get_regularization_penalty ( self ) get_input_dim # class BidirectionalLanguageModelTransformer ( Seq2SeqEncoder ): | ... | def get_input_dim ( self ) -> int get_output_dim # class BidirectionalLanguageModelTransformer ( Seq2SeqEncoder ): | ... | def get_output_dim ( self ) -> int is_bidirectional # class BidirectionalLanguageModelTransformer ( Seq2SeqEncoder ): | ... | def is_bidirectional ( self ) -> bool","title":"bidirectional_lm_transformer"},{"location":"models/lm/modules/seq2seq_encoders/bidirectional_lm_transformer/#attention","text":"def attention ( query : torch . Tensor , key : torch . Tensor , value : torch . Tensor , mask : torch . BoolTensor = None , dropout : Callable = None ) -> Tuple [ torch . Tensor , torch . Tensor ] Compute 'Scaled Dot Product Attention'","title":"attention"},{"location":"models/lm/modules/seq2seq_encoders/bidirectional_lm_transformer/#subsequent_mask","text":"def subsequent_mask ( size : int , device : str = \"cpu\" ) -> torch . BoolTensor Mask out subsequent positions.","title":"subsequent_mask"},{"location":"models/lm/modules/seq2seq_encoders/bidirectional_lm_transformer/#positionalencoding","text":"class PositionalEncoding ( torch . nn . Module , Registrable ): | def __init__ ( self , input_dim : int , max_len : int = 5000 ) -> None Implement the Positional Encoding function.","title":"PositionalEncoding"},{"location":"models/lm/modules/seq2seq_encoders/bidirectional_lm_transformer/#forward","text":"class PositionalEncoding ( torch . nn . Module , Registrable ): | ... | def forward ( self , x : torch . Tensor ) -> torch . Tensor","title":"forward"},{"location":"models/lm/modules/seq2seq_encoders/bidirectional_lm_transformer/#positionwisefeedforward","text":"class PositionwiseFeedForward ( torch . nn . Module ): | def __init__ ( | self , | input_dim : int , | ff_dim : int , | dropout : float = 0.1 | ) -> None Implements FFN equation.","title":"PositionwiseFeedForward"},{"location":"models/lm/modules/seq2seq_encoders/bidirectional_lm_transformer/#forward_1","text":"class PositionwiseFeedForward ( torch . nn . Module ): | ... | def forward ( self , x : torch . Tensor ) -> torch . Tensor","title":"forward"},{"location":"models/lm/modules/seq2seq_encoders/bidirectional_lm_transformer/#transformerencoder","text":"class TransformerEncoder ( torch . nn . Module ): | def __init__ ( | self , | layer : torch . nn . Module , | num_layers : int , | return_all_layers : bool = False | ) -> None Core encoder is a stack of N layers","title":"TransformerEncoder"},{"location":"models/lm/modules/seq2seq_encoders/bidirectional_lm_transformer/#forward_2","text":"class TransformerEncoder ( torch . nn . Module ): | ... | def forward ( self , x , mask ) Pass the input (and mask) through each layer in turn.","title":"forward"},{"location":"models/lm/modules/seq2seq_encoders/bidirectional_lm_transformer/#sublayerconnection","text":"class SublayerConnection ( torch . nn . Module ): | def __init__ ( self , size : int , dropout : float ) -> None A residual connection followed by a layer norm. Note for code simplicity the norm is first as opposed to last.","title":"SublayerConnection"},{"location":"models/lm/modules/seq2seq_encoders/bidirectional_lm_transformer/#forward_3","text":"class SublayerConnection ( torch . nn . Module ): | ... | def forward ( | self , | x : torch . Tensor , | sublayer : Callable [[ torch . Tensor ], torch . Tensor ] | ) -> torch . Tensor Apply residual connection to any sublayer with the same size.","title":"forward"},{"location":"models/lm/modules/seq2seq_encoders/bidirectional_lm_transformer/#encoderlayer","text":"class EncoderLayer ( torch . nn . Module ): | def __init__ ( | self , | size : int , | self_attn : torch . nn . Module , | feed_forward : torch . nn . Module , | dropout : float | ) -> None Encoder is made up of self-attn and feed forward (defined below)","title":"EncoderLayer"},{"location":"models/lm/modules/seq2seq_encoders/bidirectional_lm_transformer/#forward_4","text":"class EncoderLayer ( torch . nn . Module ): | ... | def forward ( | self , | x : torch . Tensor , | mask : torch . BoolTensor | ) -> torch . Tensor Follow Figure 1 (left) for connections.","title":"forward"},{"location":"models/lm/modules/seq2seq_encoders/bidirectional_lm_transformer/#multiheadedattention","text":"class MultiHeadedAttention ( torch . nn . Module ): | def __init__ ( | self , | num_heads : int , | input_dim : int , | dropout : float = 0.1 | ) -> None","title":"MultiHeadedAttention"},{"location":"models/lm/modules/seq2seq_encoders/bidirectional_lm_transformer/#forward_5","text":"class MultiHeadedAttention ( torch . nn . Module ): | ... | def forward ( | self , | query : torch . Tensor , | key : torch . Tensor , | value : torch . Tensor , | mask : torch . BoolTensor = None | ) -> torch . Tensor","title":"forward"},{"location":"models/lm/modules/seq2seq_encoders/bidirectional_lm_transformer/#make_model","text":"def make_model ( num_layers : int = 6 , input_size : int = 512 , hidden_size : int = 2048 , heads : int = 8 , dropout : float = 0.1 , return_all_layers : bool = False ) -> TransformerEncoder Helper: Construct a model from hyperparameters.","title":"make_model"},{"location":"models/lm/modules/seq2seq_encoders/bidirectional_lm_transformer/#bidirectionallanguagemodeltransformer","text":"@Seq2SeqEncoder . register ( \"bidirectional_language_model_transformer\" ) class BidirectionalLanguageModelTransformer ( Seq2SeqEncoder ): | def __init__ ( | self , | input_dim : int , | hidden_dim : int , | num_layers : int , | dropout : float = 0.1 , | input_dropout : float = None , | return_all_layers : bool = False | ) -> None","title":"BidirectionalLanguageModelTransformer"},{"location":"models/lm/modules/seq2seq_encoders/bidirectional_lm_transformer/#get_attention_masks","text":"class BidirectionalLanguageModelTransformer ( Seq2SeqEncoder ): | ... | def get_attention_masks ( | self , | mask : torch . BoolTensor | ) -> Tuple [ torch . Tensor , torch . Tensor ] Returns 2 masks of shape (batch_size, timesteps, timesteps) representing 1) non-padded elements, and 2) elements of the sequence which are permitted to be involved in attention at a given timestep.","title":"get_attention_masks"},{"location":"models/lm/modules/seq2seq_encoders/bidirectional_lm_transformer/#forward_6","text":"class BidirectionalLanguageModelTransformer ( Seq2SeqEncoder ): | ... | def forward ( | self , | token_embeddings : torch . Tensor , | mask : torch . BoolTensor | ) -> torch . Tensor","title":"forward"},{"location":"models/lm/modules/seq2seq_encoders/bidirectional_lm_transformer/#get_regularization_penalty","text":"class BidirectionalLanguageModelTransformer ( Seq2SeqEncoder ): | ... | def get_regularization_penalty ( self )","title":"get_regularization_penalty"},{"location":"models/lm/modules/seq2seq_encoders/bidirectional_lm_transformer/#get_input_dim","text":"class BidirectionalLanguageModelTransformer ( Seq2SeqEncoder ): | ... | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"models/lm/modules/seq2seq_encoders/bidirectional_lm_transformer/#get_output_dim","text":"class BidirectionalLanguageModelTransformer ( Seq2SeqEncoder ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"models/lm/modules/seq2seq_encoders/bidirectional_lm_transformer/#is_bidirectional","text":"class BidirectionalLanguageModelTransformer ( Seq2SeqEncoder ): | ... | def is_bidirectional ( self ) -> bool","title":"is_bidirectional"},{"location":"models/lm/modules/token_embedders/bidirectional_lm/","text":"allennlp_models .lm .modules .token_embedders .bidirectional_lm [SOURCE] BidirectionalLanguageModelTokenEmbedder # @TokenEmbedder . register ( \"bidirectional_lm_token_embedder\" ) class BidirectionalLanguageModelTokenEmbedder ( LanguageModelTokenEmbedder ): | def __init__ ( | self , | archive_file : str , | dropout : float = None , | bos_eos_tokens : Tuple [ str , str ] = ( \"<S>\" , \"</S>\" ), | remove_bos_eos : bool = True , | requires_grad : bool = False | ) -> None Compute a single layer of representations from a bidirectional language model. This is done by computing a learned scalar average of the layers from the LM. Typically the LM's weights will be fixed, but they can be fine tuned by setting requires_grad . Parameters \u00b6 archive_file : str An archive file, typically model.tar.gz, from a BidirectionalLanguageModel. The contextualizer used by the LM must satisfy two requirements: It must have a num_layers field. It must take a boolean return_all_layers parameter in its constructor. See BidirectionalLanguageModelTransformer for their definitions. dropout : float , optional The dropout value to be applied to the representations. bos_eos_tokens : Tuple[str, str] , optional (default = (\"<S>\", \"</S>\") ) These will be indexed and placed around the indexed tokens. Necessary if the language model was trained with them, but they were injected external to an indexer. remove_bos_eos : bool , optional (default = True ) Typically the provided token indexes will be augmented with begin-sentence and end-sentence tokens. (Alternatively, you can pass bos_eos_tokens.) If this flag is True the corresponding embeddings will be removed from the return values. Warning: This only removes a single start and single end token! - requires_grad : bool , optional (default = False ) If True, compute gradient of bidirectional language model parameters for fine tuning.","title":"bidirectional_lm"},{"location":"models/lm/modules/token_embedders/bidirectional_lm/#bidirectionallanguagemodeltokenembedder","text":"@TokenEmbedder . register ( \"bidirectional_lm_token_embedder\" ) class BidirectionalLanguageModelTokenEmbedder ( LanguageModelTokenEmbedder ): | def __init__ ( | self , | archive_file : str , | dropout : float = None , | bos_eos_tokens : Tuple [ str , str ] = ( \"<S>\" , \"</S>\" ), | remove_bos_eos : bool = True , | requires_grad : bool = False | ) -> None Compute a single layer of representations from a bidirectional language model. This is done by computing a learned scalar average of the layers from the LM. Typically the LM's weights will be fixed, but they can be fine tuned by setting requires_grad .","title":"BidirectionalLanguageModelTokenEmbedder"},{"location":"models/lm/modules/token_embedders/language_model/","text":"allennlp_models .lm .modules .token_embedders .language_model [SOURCE] LanguageModelTokenEmbedder # @TokenEmbedder . register ( \"language_model_token_embedder\" ) class LanguageModelTokenEmbedder ( TokenEmbedder ): | def __init__ ( | self , | archive_file : str , | dropout : float = None , | bos_eos_tokens : Tuple [ str , str ] = ( \"<S>\" , \"</S>\" ), | remove_bos_eos : bool = True , | requires_grad : bool = False | ) -> None Compute a single layer of representations from a (optionally bidirectional) language model. This is done by computing a learned scalar average of the layers from the LM. Typically the LM's weights will be fixed, but they can be fine tuned by setting requires_grad . Parameters \u00b6 archive_file : str An archive file, typically model.tar.gz, from a LanguageModel. The contextualizer used by the LM must satisfy two requirements: It must have a num_layers field. It must take a boolean return_all_layers parameter in its constructor. See BidirectionalLanguageModelTransformer for their definitions. dropout : float , optional The dropout value to be applied to the representations. bos_eos_tokens : Tuple[str, str] , optional (default = (\"<S>\", \"</S>\") ) These will be indexed and placed around the indexed tokens. Necessary if the language model was trained with them, but they were injected external to an indexer. remove_bos_eos : bool , optional (default = True ) Typically the provided token indexes will be augmented with begin-sentence and end-sentence tokens. (Alternatively, you can pass bos_eos_tokens.) If this flag is True the corresponding embeddings will be removed from the return values. Warning: This only removes a single start and single end token! - requires_grad : bool , optional (default = False ) If True, compute gradient of bidirectional language model parameters for fine tuning. get_output_dim # class LanguageModelTokenEmbedder ( TokenEmbedder ): | ... | def get_output_dim ( self ) -> int forward # class LanguageModelTokenEmbedder ( TokenEmbedder ): | ... | def forward ( self , tokens : torch . Tensor ) -> Dict [ str , torch . Tensor ] Parameters \u00b6 tokens : torch.Tensor Shape (batch_size, timesteps, ...) of token ids representing the current batch. These must have been produced using the same indexer the LM was trained on. Returns \u00b6 The bidirectional language model representations for the input sequence, shape (batch_size, timesteps, embedding_dim)","title":"language_model"},{"location":"models/lm/modules/token_embedders/language_model/#languagemodeltokenembedder","text":"@TokenEmbedder . register ( \"language_model_token_embedder\" ) class LanguageModelTokenEmbedder ( TokenEmbedder ): | def __init__ ( | self , | archive_file : str , | dropout : float = None , | bos_eos_tokens : Tuple [ str , str ] = ( \"<S>\" , \"</S>\" ), | remove_bos_eos : bool = True , | requires_grad : bool = False | ) -> None Compute a single layer of representations from a (optionally bidirectional) language model. This is done by computing a learned scalar average of the layers from the LM. Typically the LM's weights will be fixed, but they can be fine tuned by setting requires_grad .","title":"LanguageModelTokenEmbedder"},{"location":"models/lm/modules/token_embedders/language_model/#get_output_dim","text":"class LanguageModelTokenEmbedder ( TokenEmbedder ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"models/lm/modules/token_embedders/language_model/#forward","text":"class LanguageModelTokenEmbedder ( TokenEmbedder ): | ... | def forward ( self , tokens : torch . Tensor ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"models/lm/predictors/masked_language_model/","text":"allennlp_models .lm .predictors .masked_language_model [SOURCE] MaskedLanguageModelPredictor # @Predictor . register ( \"masked_language_model\" ) class MaskedLanguageModelPredictor ( Predictor ) predict # class MaskedLanguageModelPredictor ( Predictor ): | ... | def predict ( self , sentence_with_masks : str ) -> JsonDict predictions_to_labeled_instances # class MaskedLanguageModelPredictor ( Predictor ): | ... | @overrides | def predictions_to_labeled_instances ( | self , | instance : Instance , | outputs : Dict [ str , numpy . ndarray ] | )","title":"masked_language_model"},{"location":"models/lm/predictors/masked_language_model/#maskedlanguagemodelpredictor","text":"@Predictor . register ( \"masked_language_model\" ) class MaskedLanguageModelPredictor ( Predictor )","title":"MaskedLanguageModelPredictor"},{"location":"models/lm/predictors/masked_language_model/#predict","text":"class MaskedLanguageModelPredictor ( Predictor ): | ... | def predict ( self , sentence_with_masks : str ) -> JsonDict","title":"predict"},{"location":"models/lm/predictors/masked_language_model/#predictions_to_labeled_instances","text":"class MaskedLanguageModelPredictor ( Predictor ): | ... | @overrides | def predictions_to_labeled_instances ( | self , | instance : Instance , | outputs : Dict [ str , numpy . ndarray ] | )","title":"predictions_to_labeled_instances"},{"location":"models/lm/predictors/next_token_lm/","text":"allennlp_models .lm .predictors .next_token_lm [SOURCE] NextTokenLMPredictor # @Predictor . register ( \"next_token_lm\" ) class NextTokenLMPredictor ( Predictor ) predict # class NextTokenLMPredictor ( Predictor ): | ... | def predict ( self , sentence : str ) -> JsonDict predictions_to_labeled_instances # class NextTokenLMPredictor ( Predictor ): | ... | @overrides | def predictions_to_labeled_instances ( | self , | instance : Instance , | outputs : Dict [ str , numpy . ndarray ] | )","title":"next_token_lm"},{"location":"models/lm/predictors/next_token_lm/#nexttokenlmpredictor","text":"@Predictor . register ( \"next_token_lm\" ) class NextTokenLMPredictor ( Predictor )","title":"NextTokenLMPredictor"},{"location":"models/lm/predictors/next_token_lm/#predict","text":"class NextTokenLMPredictor ( Predictor ): | ... | def predict ( self , sentence : str ) -> JsonDict","title":"predict"},{"location":"models/lm/predictors/next_token_lm/#predictions_to_labeled_instances","text":"class NextTokenLMPredictor ( Predictor ): | ... | @overrides | def predictions_to_labeled_instances ( | self , | instance : Instance , | outputs : Dict [ str , numpy . ndarray ] | )","title":"predictions_to_labeled_instances"},{"location":"models/lm/util/beam_search_generators/beam_search_generator/","text":"allennlp_models .lm .util .beam_search_generators .beam_search_generator [SOURCE] BeamSearchGenerator # class BeamSearchGenerator ( Registrable ): | def __init__ ( self , beam_search : BeamSearch ) A beam search generator for next token language models. This is just a wrapper around allennlp.nn.beam_search.BeamSearch with custom logic for handling the state dict. The reason we need this is because the step function that BeamSearch uses needs to know how to handle different TextFieldTensors , the form of which depends on the exact embedder class that the NextTokenLm uses. So essentially we need a different BeamSearchGenerator implementation for each different text_field_embedder . validate_text_field_embedder # class BeamSearchGenerator ( Registrable ): | ... | def validate_text_field_embedder ( | self , | text_field_embedder : TextFieldEmbedder | ) This should be called after initialization to verify that the model's text_field_embedder is compatable. get_step_state # class BeamSearchGenerator ( Registrable ): | ... | def get_step_state ( | self , | inputs : TextFieldTensors | ) -> Dict [ str , torch . Tensor ] Create a state dictionary for BeamSearch from the TextFieldTensors inputs to the NextTokenLm model. By default this assumes the TextFieldTensors has a single TokenEmbedder , and just \"flattens\" the TextFieldTensors by returning the TokenEmbedder sub-dictionary. If you have TextFieldTensors with more than one TokenEmbedder sub-dictionary, you'll need to override this class. prepare_step_input # class BeamSearchGenerator ( Registrable ): | ... | def prepare_step_input ( | self , | predictions : torch . Tensor , | state : Dict [ str , torch . Tensor ] | ) -> TextFieldTensors This is like the reverse of get_step_state() . It takes predictions and state from the current step and returns a TextFieldTensors dictionary that can be fed through the embedder of the NextTokenLm model. This usually involves adding the predicted tokens to the proper field of the state dict, and expanding any mask tensors or other context tensors by 1 in the right dimension, and then unflattening the state so that it looks like a TextFieldTensors dict. search # class BeamSearchGenerator ( Registrable ): | ... | def search ( | self , | start_predictions : torch . Tensor , | state : Dict [ str , torch . Tensor ], | step_function : StepFunctionType | ) -> Tuple [ torch . Tensor , torch . Tensor ] Calls BeamSearch.search , return the top predicted indices and corresponding log probabilities.","title":"beam_search_generator"},{"location":"models/lm/util/beam_search_generators/beam_search_generator/#beamsearchgenerator","text":"class BeamSearchGenerator ( Registrable ): | def __init__ ( self , beam_search : BeamSearch ) A beam search generator for next token language models. This is just a wrapper around allennlp.nn.beam_search.BeamSearch with custom logic for handling the state dict. The reason we need this is because the step function that BeamSearch uses needs to know how to handle different TextFieldTensors , the form of which depends on the exact embedder class that the NextTokenLm uses. So essentially we need a different BeamSearchGenerator implementation for each different text_field_embedder .","title":"BeamSearchGenerator"},{"location":"models/lm/util/beam_search_generators/beam_search_generator/#validate_text_field_embedder","text":"class BeamSearchGenerator ( Registrable ): | ... | def validate_text_field_embedder ( | self , | text_field_embedder : TextFieldEmbedder | ) This should be called after initialization to verify that the model's text_field_embedder is compatable.","title":"validate_text_field_embedder"},{"location":"models/lm/util/beam_search_generators/beam_search_generator/#get_step_state","text":"class BeamSearchGenerator ( Registrable ): | ... | def get_step_state ( | self , | inputs : TextFieldTensors | ) -> Dict [ str , torch . Tensor ] Create a state dictionary for BeamSearch from the TextFieldTensors inputs to the NextTokenLm model. By default this assumes the TextFieldTensors has a single TokenEmbedder , and just \"flattens\" the TextFieldTensors by returning the TokenEmbedder sub-dictionary. If you have TextFieldTensors with more than one TokenEmbedder sub-dictionary, you'll need to override this class.","title":"get_step_state"},{"location":"models/lm/util/beam_search_generators/beam_search_generator/#prepare_step_input","text":"class BeamSearchGenerator ( Registrable ): | ... | def prepare_step_input ( | self , | predictions : torch . Tensor , | state : Dict [ str , torch . Tensor ] | ) -> TextFieldTensors This is like the reverse of get_step_state() . It takes predictions and state from the current step and returns a TextFieldTensors dictionary that can be fed through the embedder of the NextTokenLm model. This usually involves adding the predicted tokens to the proper field of the state dict, and expanding any mask tensors or other context tensors by 1 in the right dimension, and then unflattening the state so that it looks like a TextFieldTensors dict.","title":"prepare_step_input"},{"location":"models/lm/util/beam_search_generators/beam_search_generator/#search","text":"class BeamSearchGenerator ( Registrable ): | ... | def search ( | self , | start_predictions : torch . Tensor , | state : Dict [ str , torch . Tensor ], | step_function : StepFunctionType | ) -> Tuple [ torch . Tensor , torch . Tensor ] Calls BeamSearch.search , return the top predicted indices and corresponding log probabilities.","title":"search"},{"location":"models/lm/util/beam_search_generators/transformer_beam_search_generator/","text":"allennlp_models .lm .util .beam_search_generators .transformer_beam_search_generator [SOURCE] TransformerBeamSearchGenerator # @BeamSearchGenerator . register ( \"transformer\" ) class TransformerBeamSearchGenerator ( BeamSearchGenerator ): | def __init__ ( self , * args , namespace : str = None , ** kwargs ) -> None A BeamSearchGenerator for transformer-based NextTokenLM models. This can be used with any NextTokenLM that utilizes a single pretrained_transformer TokenEmbedder for it's text_field_embedder . validate_text_field_embedder # class TransformerBeamSearchGenerator ( BeamSearchGenerator ): | ... | @overrides | def validate_text_field_embedder ( | self , | text_field_embedder : TextFieldEmbedder | ) prepare_step_input # class TransformerBeamSearchGenerator ( BeamSearchGenerator ): | ... | @overrides | def prepare_step_input ( | self , | predictions : torch . Tensor , | state : Dict [ str , torch . Tensor ] | ) -> TextFieldTensors Add predicted_tokens to state[\"token_ids\"] and expand state[\"mask\"] .","title":"transformer_beam_search_generator"},{"location":"models/lm/util/beam_search_generators/transformer_beam_search_generator/#transformerbeamsearchgenerator","text":"@BeamSearchGenerator . register ( \"transformer\" ) class TransformerBeamSearchGenerator ( BeamSearchGenerator ): | def __init__ ( self , * args , namespace : str = None , ** kwargs ) -> None A BeamSearchGenerator for transformer-based NextTokenLM models. This can be used with any NextTokenLM that utilizes a single pretrained_transformer TokenEmbedder for it's text_field_embedder .","title":"TransformerBeamSearchGenerator"},{"location":"models/lm/util/beam_search_generators/transformer_beam_search_generator/#validate_text_field_embedder","text":"class TransformerBeamSearchGenerator ( BeamSearchGenerator ): | ... | @overrides | def validate_text_field_embedder ( | self , | text_field_embedder : TextFieldEmbedder | )","title":"validate_text_field_embedder"},{"location":"models/lm/util/beam_search_generators/transformer_beam_search_generator/#prepare_step_input","text":"class TransformerBeamSearchGenerator ( BeamSearchGenerator ): | ... | @overrides | def prepare_step_input ( | self , | predictions : torch . Tensor , | state : Dict [ str , torch . Tensor ] | ) -> TextFieldTensors Add predicted_tokens to state[\"token_ids\"] and expand state[\"mask\"] .","title":"prepare_step_input"},{"location":"models/mc/dataset_readers/commonsenseqa/","text":"allennlp_models .mc .dataset_readers .commonsenseqa [SOURCE] CommonsenseQaReader # @DatasetReader . register ( \"commonsenseqa\" ) class CommonsenseQaReader ( TransformerMCReader ) Reads the input data for the CommonsenseQA dataset (https://arxiv.org/abs/1811.00937).","title":"commonsenseqa"},{"location":"models/mc/dataset_readers/commonsenseqa/#commonsenseqareader","text":"@DatasetReader . register ( \"commonsenseqa\" ) class CommonsenseQaReader ( TransformerMCReader ) Reads the input data for the CommonsenseQA dataset (https://arxiv.org/abs/1811.00937).","title":"CommonsenseQaReader"},{"location":"models/mc/dataset_readers/fake/","text":"allennlp_models .mc .dataset_readers .fake [SOURCE] FakeReader # @DatasetReader . register ( \"fake\" ) class FakeReader ( DatasetReader ): | def __init__ ( | self , | transformer_model_name : str = \"roberta-large\" , | length_limit : int = 512 , | ** kwargs | ) -> None Creates fake multiple-choice input. If your model doesn't get 99% on this data, it is broken. Instances have two fields: * alternatives , a ListField of TextField * correct_alternative , IndexField with the correct answer among alternatives Parameterstransformer_model_name : `str`, optional (default=`roberta-large`) \u00b6 This reader chooses tokenizer and token indexer according to this setting. length_limit : int , optional (default=512) We will make sure that the length of the alternatives never exceeds this many word pieces. text_to_instance # class FakeReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | alternatives : List [ str ], | correct_alternative : int | ) -> Instance tokenize","title":"fake"},{"location":"models/mc/dataset_readers/fake/#fakereader","text":"@DatasetReader . register ( \"fake\" ) class FakeReader ( DatasetReader ): | def __init__ ( | self , | transformer_model_name : str = \"roberta-large\" , | length_limit : int = 512 , | ** kwargs | ) -> None Creates fake multiple-choice input. If your model doesn't get 99% on this data, it is broken. Instances have two fields: * alternatives , a ListField of TextField * correct_alternative , IndexField with the correct answer among alternatives","title":"FakeReader"},{"location":"models/mc/dataset_readers/fake/#text_to_instance","text":"class FakeReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | alternatives : List [ str ], | correct_alternative : int | ) -> Instance tokenize","title":"text_to_instance"},{"location":"models/mc/dataset_readers/piqa/","text":"allennlp_models .mc .dataset_readers .piqa [SOURCE] PiqaReader # @DatasetReader . register ( \"piqa\" ) class PiqaReader ( TransformerMCReader ) Reads the input data for the PIQA dataset (https://arxiv.org/abs/1911.11641).","title":"piqa"},{"location":"models/mc/dataset_readers/piqa/#piqareader","text":"@DatasetReader . register ( \"piqa\" ) class PiqaReader ( TransformerMCReader ) Reads the input data for the PIQA dataset (https://arxiv.org/abs/1911.11641).","title":"PiqaReader"},{"location":"models/mc/dataset_readers/swag/","text":"allennlp_models .mc .dataset_readers .swag [SOURCE] SwagReader # @DatasetReader . register ( \"swag\" ) class SwagReader ( TransformerMCReader ) Reads the input data for the SWAG dataset (https://arxiv.org/abs/1808.05326).","title":"swag"},{"location":"models/mc/dataset_readers/swag/#swagreader","text":"@DatasetReader . register ( \"swag\" ) class SwagReader ( TransformerMCReader ) Reads the input data for the SWAG dataset (https://arxiv.org/abs/1808.05326).","title":"SwagReader"},{"location":"models/mc/dataset_readers/transformer_mc/","text":"allennlp_models .mc .dataset_readers .transformer_mc [SOURCE] TransformerMCReader # class TransformerMCReader ( DatasetReader ): | def __init__ ( | self , | transformer_model_name : str = \"roberta-large\" , | length_limit : int = 512 , | ** kwargs | ) -> None Read input data for the TransformerMC model. This is the base class for all readers that produce data for TransformerMC. Instances have two fields: * alternatives , a ListField of TextField * correct_alternative , IndexField with the correct answer among alternatives Parameterstransformer_model_name : `str`, optional (default=`roberta-large`) \u00b6 This reader chooses tokenizer and token indexer according to this setting. length_limit : int , optional (default= 512 ) We will make sure that the length of an alternative never exceeds this many word pieces. text_to_instance # class TransformerMCReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | qid : str , | start : str , | alternatives : List [ str ], | label : Optional [ int ] = None | ) -> Instance tokenize","title":"transformer_mc"},{"location":"models/mc/dataset_readers/transformer_mc/#transformermcreader","text":"class TransformerMCReader ( DatasetReader ): | def __init__ ( | self , | transformer_model_name : str = \"roberta-large\" , | length_limit : int = 512 , | ** kwargs | ) -> None Read input data for the TransformerMC model. This is the base class for all readers that produce data for TransformerMC. Instances have two fields: * alternatives , a ListField of TextField * correct_alternative , IndexField with the correct answer among alternatives","title":"TransformerMCReader"},{"location":"models/mc/dataset_readers/transformer_mc/#text_to_instance","text":"class TransformerMCReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | qid : str , | start : str , | alternatives : List [ str ], | label : Optional [ int ] = None | ) -> Instance tokenize","title":"text_to_instance"},{"location":"models/mc/models/transformer_mc/","text":"allennlp_models .mc .models .transformer_mc [SOURCE] TransformerMC # @Model . register ( \"transformer_mc\" ) class TransformerMC ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | transformer_model : str = \"roberta-large\" , | override_weights_file : Optional [ str ] = None , | override_weights_strip_prefix : Optional [ str ] = None , | ** kwargs | ) -> None This class implements a multiple choice model patterned after the proposed model in RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al) . It calculates a score for each sequence on top of the CLS token, and then chooses the alternative with the highest score. Parametersvocab : ``Vocabulary`` \u00b6 transformer_model : str , optional (default= \"roberta-large\" ) This model chooses the embedder according to this setting. You probably want to make sure this matches the setting in the reader. forward # class TransformerMC ( Model ): | ... | def forward ( | self , | alternatives : TextFieldTensors , | correct_alternative : Optional [ torch . IntTensor ] = None , | qid : Optional [ List [ str ]] = None | ) -> Dict [ str , torch . Tensor ] Parametersalternatives : ``Dict[str, torch.LongTensor]`` \u00b6 From a ``ListField[TextField]``. Contains a list of alternatives to evaluate for every instance. correct_alternative : Optional[torch.IntTensor] From an IndexField . Contains the index of the correct answer for every instance. qid : Optional[List[str]] A list of question IDs for the questions being processed now. ReturnsAn output dictionary consisting of: \u00b6 loss : torch.FloatTensor , optional A scalar loss to be optimised. This is only returned when correct_alternative is not None . logits : torch.FloatTensor The logits for every possible answer choice best_alternative : List[int] The index of the highest scoring alternative for every instance in the batch get_metrics # class TransformerMC ( Model ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] default_predictor # class TransformerMC ( Model ): | ... | default_predictor = \"transformer_mc\"","title":"transformer_mc"},{"location":"models/mc/models/transformer_mc/#transformermc","text":"@Model . register ( \"transformer_mc\" ) class TransformerMC ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | transformer_model : str = \"roberta-large\" , | override_weights_file : Optional [ str ] = None , | override_weights_strip_prefix : Optional [ str ] = None , | ** kwargs | ) -> None This class implements a multiple choice model patterned after the proposed model in RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al) . It calculates a score for each sequence on top of the CLS token, and then chooses the alternative with the highest score.","title":"TransformerMC"},{"location":"models/mc/models/transformer_mc/#forward","text":"class TransformerMC ( Model ): | ... | def forward ( | self , | alternatives : TextFieldTensors , | correct_alternative : Optional [ torch . IntTensor ] = None , | qid : Optional [ List [ str ]] = None | ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"models/mc/models/transformer_mc/#get_metrics","text":"class TransformerMC ( Model ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"models/mc/models/transformer_mc/#default_predictor","text":"class TransformerMC ( Model ): | ... | default_predictor = \"transformer_mc\"","title":"default_predictor"},{"location":"models/mc/predictors/transformer_mc/","text":"allennlp_models .mc .predictors .transformer_mc [SOURCE] TransformerMCPredictor # @Predictor . register ( \"transformer_mc\" ) class TransformerMCPredictor ( Predictor ) Predictor for the TransformerMC model. predict # class TransformerMCPredictor ( Predictor ): | ... | def predict ( self , prefix : str , alternatives : List [ str ]) -> JsonDict","title":"transformer_mc"},{"location":"models/mc/predictors/transformer_mc/#transformermcpredictor","text":"@Predictor . register ( \"transformer_mc\" ) class TransformerMCPredictor ( Predictor ) Predictor for the TransformerMC model.","title":"TransformerMCPredictor"},{"location":"models/mc/predictors/transformer_mc/#predict","text":"class TransformerMCPredictor ( Predictor ): | ... | def predict ( self , prefix : str , alternatives : List [ str ]) -> JsonDict","title":"predict"},{"location":"models/pair_classification/dataset_readers/quora_paraphrase/","text":"allennlp_models .pair_classification .dataset_readers .quora_paraphrase [SOURCE] QuoraParaphraseDatasetReader # @DatasetReader . register ( \"quora_paraphrase\" ) class QuoraParaphraseDatasetReader ( DatasetReader ): | def __init__ ( | self , | tokenizer : Tokenizer = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | combine_input_fields : Optional [ bool ] = None , | ** kwargs | ) -> None Reads a file from the Quora Paraphrase dataset. The train/validation/test split of the data comes from the paper Bilateral Multi-Perspective Matching for Natural Language Sentences by Zhiguo Wang et al., 2017. Each file of the data is a tsv file without header. The columns are is_duplicate, question1, question2, and id. All questions are pre-tokenized and tokens are space separated. We convert these keys into fields named \"label\", \"premise\" and \"hypothesis\", so that it is compatible t some existing natural language inference algorithms. Registered as a DatasetReader with name \"quora_paraphrase\". Parameters \u00b6 tokenizer : Tokenizer , optional Tokenizer to use to split the premise and hypothesis into words or other kinds of tokens. Defaults to WhitespaceTokenizer . token_indexers : Dict[str, TokenIndexer] , optional Indexers used to define input token representations. Defaults to {\"tokens\": SingleIdTokenIndexer()} . text_to_instance # class QuoraParaphraseDatasetReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | premise : str , | hypothesis : str , | label : str = None | ) -> Instance","title":"quora_paraphrase"},{"location":"models/pair_classification/dataset_readers/quora_paraphrase/#quoraparaphrasedatasetreader","text":"@DatasetReader . register ( \"quora_paraphrase\" ) class QuoraParaphraseDatasetReader ( DatasetReader ): | def __init__ ( | self , | tokenizer : Tokenizer = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | combine_input_fields : Optional [ bool ] = None , | ** kwargs | ) -> None Reads a file from the Quora Paraphrase dataset. The train/validation/test split of the data comes from the paper Bilateral Multi-Perspective Matching for Natural Language Sentences by Zhiguo Wang et al., 2017. Each file of the data is a tsv file without header. The columns are is_duplicate, question1, question2, and id. All questions are pre-tokenized and tokens are space separated. We convert these keys into fields named \"label\", \"premise\" and \"hypothesis\", so that it is compatible t some existing natural language inference algorithms. Registered as a DatasetReader with name \"quora_paraphrase\".","title":"QuoraParaphraseDatasetReader"},{"location":"models/pair_classification/dataset_readers/quora_paraphrase/#text_to_instance","text":"class QuoraParaphraseDatasetReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | premise : str , | hypothesis : str , | label : str = None | ) -> Instance","title":"text_to_instance"},{"location":"models/pair_classification/dataset_readers/snli/","text":"allennlp_models .pair_classification .dataset_readers .snli [SOURCE] maybe_collapse_label # def maybe_collapse_label ( label : str , collapse : bool ) Helper function that optionally collapses the \"contradiction\" and \"neutral\" labels into \"non-entailment\". SnliReader # @DatasetReader . register ( \"snli\" ) class SnliReader ( DatasetReader ): | def __init__ ( | self , | tokenizer : Optional [ Tokenizer ] = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | combine_input_fields : Optional [ bool ] = None , | collapse_labels : Optional [ bool ] = False , | ** kwargs | ) -> None Reads a file from the Stanford Natural Language Inference (SNLI) dataset. This data is formatted as jsonl, one json-formatted instance per line. The keys in the data are \"gold_label\", \"sentence1\", and \"sentence2\". We convert these keys into fields named \"label\", \"premise\" and \"hypothesis\", along with a metadata field containing the tokenized strings of the premise and hypothesis. Registered as a DatasetReader with name \"snli\". Parameters \u00b6 tokenizer : Tokenizer , optional (default = SpacyTokenizer() ) We use this Tokenizer for both the premise and the hypothesis. See Tokenizer . token_indexers : Dict[str, TokenIndexer] , optional (default = {\"tokens\": SingleIdTokenIndexer()} ) We similarly use this for both the premise and the hypothesis. See TokenIndexer . combine_input_fields : bool , optional (default= isinstance(tokenizer, PretrainedTransformerTokenizer) ) If False, represent the premise and the hypothesis as separate fields in the instance. If True, tokenize them together using tokenizer.tokenize_sentence_pair() and provide a single tokens field in the instance. collapse_labels : bool , optional (default = False ) If True , the \"neutral\" and \"contradiction\" labels will be collapsed into \"non-entailment\"; \"entailment\" will be left unchanged. text_to_instance # class SnliReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | premise : str , | hypothesis : str , | label : str = None | ) -> Instance apply_token_indexers # class SnliReader ( DatasetReader ): | ... | @overrides | def apply_token_indexers ( self , instance : Instance ) -> Instance","title":"snli"},{"location":"models/pair_classification/dataset_readers/snli/#maybe_collapse_label","text":"def maybe_collapse_label ( label : str , collapse : bool ) Helper function that optionally collapses the \"contradiction\" and \"neutral\" labels into \"non-entailment\".","title":"maybe_collapse_label"},{"location":"models/pair_classification/dataset_readers/snli/#snlireader","text":"@DatasetReader . register ( \"snli\" ) class SnliReader ( DatasetReader ): | def __init__ ( | self , | tokenizer : Optional [ Tokenizer ] = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | combine_input_fields : Optional [ bool ] = None , | collapse_labels : Optional [ bool ] = False , | ** kwargs | ) -> None Reads a file from the Stanford Natural Language Inference (SNLI) dataset. This data is formatted as jsonl, one json-formatted instance per line. The keys in the data are \"gold_label\", \"sentence1\", and \"sentence2\". We convert these keys into fields named \"label\", \"premise\" and \"hypothesis\", along with a metadata field containing the tokenized strings of the premise and hypothesis. Registered as a DatasetReader with name \"snli\".","title":"SnliReader"},{"location":"models/pair_classification/dataset_readers/snli/#text_to_instance","text":"class SnliReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | premise : str , | hypothesis : str , | label : str = None | ) -> Instance","title":"text_to_instance"},{"location":"models/pair_classification/dataset_readers/snli/#apply_token_indexers","text":"class SnliReader ( DatasetReader ): | ... | @overrides | def apply_token_indexers ( self , instance : Instance ) -> Instance","title":"apply_token_indexers"},{"location":"models/pair_classification/dataset_readers/transformer_superglue_rte/","text":"allennlp_models .pair_classification .dataset_readers .transformer_superglue_rte [SOURCE] TransformerSuperGlueRteReader # @DatasetReader . register ( \"transformer_superglue_rte\" ) class TransformerSuperGlueRteReader ( DatasetReader ): | def __init__ ( | self , | transformer_model_name : str = \"roberta-base\" , | tokenizer_kwargs : Dict [ str , Any ] = None , | ** kwargs | ) -> None Dataset reader for the SuperGLUE Recognizing Textual Entailment task, to be used with a transformer model such as RoBERTa. The dataset is in the JSON Lines format. It will generate Instances with the following fields: tokens , a TextField that contains the concatenation of premise and hypothesis, label , a LabelField containing the label, if one exists. metadata , a MetadataField that stores the instance's index in the file, the original premise, the original hypothesis, both of these in tokenized form, and the gold label, accessible as metadata['index'] , metadata['premise'] , metadata['hypothesis'] , metadata['tokens'] , and metadata['label'] . Parameters \u00b6 type : str , optional (default = 'roberta-base' ) This reader chooses tokenizer according to this setting. text_to_instance # class TransformerSuperGlueRteReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | index : int , | label : str , | premise : str , | hypothesis : str | ) -> Instance apply_token_indexers # class TransformerSuperGlueRteReader ( DatasetReader ): | ... | @overrides | def apply_token_indexers ( self , instance : Instance ) -> None","title":"transformer_superglue_rte"},{"location":"models/pair_classification/dataset_readers/transformer_superglue_rte/#transformersupergluertereader","text":"@DatasetReader . register ( \"transformer_superglue_rte\" ) class TransformerSuperGlueRteReader ( DatasetReader ): | def __init__ ( | self , | transformer_model_name : str = \"roberta-base\" , | tokenizer_kwargs : Dict [ str , Any ] = None , | ** kwargs | ) -> None Dataset reader for the SuperGLUE Recognizing Textual Entailment task, to be used with a transformer model such as RoBERTa. The dataset is in the JSON Lines format. It will generate Instances with the following fields: tokens , a TextField that contains the concatenation of premise and hypothesis, label , a LabelField containing the label, if one exists. metadata , a MetadataField that stores the instance's index in the file, the original premise, the original hypothesis, both of these in tokenized form, and the gold label, accessible as metadata['index'] , metadata['premise'] , metadata['hypothesis'] , metadata['tokens'] , and metadata['label'] .","title":"TransformerSuperGlueRteReader"},{"location":"models/pair_classification/dataset_readers/transformer_superglue_rte/#text_to_instance","text":"class TransformerSuperGlueRteReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | index : int , | label : str , | premise : str , | hypothesis : str | ) -> Instance","title":"text_to_instance"},{"location":"models/pair_classification/dataset_readers/transformer_superglue_rte/#apply_token_indexers","text":"class TransformerSuperGlueRteReader ( DatasetReader ): | ... | @overrides | def apply_token_indexers ( self , instance : Instance ) -> None","title":"apply_token_indexers"},{"location":"models/pair_classification/models/bimpm/","text":"allennlp_models .pair_classification .models .bimpm [SOURCE] BiMPM (Bilateral Multi-Perspective Matching) model implementation. BiMpm # @Model . register ( \"bimpm\" ) class BiMpm ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | matcher_word : BiMpmMatching , | encoder1 : Seq2SeqEncoder , | matcher_forward1 : BiMpmMatching , | matcher_backward1 : BiMpmMatching , | encoder2 : Seq2SeqEncoder , | matcher_forward2 : BiMpmMatching , | matcher_backward2 : BiMpmMatching , | aggregator : Seq2VecEncoder , | classifier_feedforward : FeedForward , | dropout : float = 0.1 , | initializer : InitializerApplicator = InitializerApplicator (), | ** kwargs | ) -> None This Model implements BiMPM model described in Bilateral Multi-Perspective Matching for Natural Language Sentences by Zhiguo Wang et al., 2017. Also please refer to the TensorFlow implementation and PyTorch implementation . Registered as a Model with name \"bimpm\". Parameters \u00b6 vocab : Vocabulary text_field_embedder : TextFieldEmbedder Used to embed the premise and hypothesis TextFields we get as input to the model. matcher_word : BiMpmMatching BiMPM matching on the output of word embeddings of premise and hypothesis. encoder1 : Seq2SeqEncoder First encoder layer for the premise and hypothesis matcher_forward1 : BiMPMMatching BiMPM matching for the forward output of first encoder layer matcher_backward1 : BiMPMMatching BiMPM matching for the backward output of first encoder layer encoder2 : Seq2SeqEncoder Second encoder layer for the premise and hypothesis matcher_forward2 : BiMPMMatching BiMPM matching for the forward output of second encoder layer matcher_backward2 : BiMPMMatching BiMPM matching for the backward output of second encoder layer aggregator : Seq2VecEncoder Aggregator of all BiMPM matching vectors classifier_feedforward : FeedForward Fully connected layers for classification. dropout : float , optional (default = 0.1 ) Dropout percentage to use. initializer : InitializerApplicator , optional (default = InitializerApplicator() ) If provided, will be used to initialize the model parameters. forward # class BiMpm ( Model ): | ... | @overrides | def forward ( | self , | premise : TextFieldTensors , | hypothesis : TextFieldTensors , | label : torch . LongTensor = None , | metadata : List [ Dict [ str , Any ]] = None | ) -> Dict [ str , torch . Tensor ] Parameters \u00b6 premise : TextFieldTensors The premise from a TextField hypothesis : TextFieldTensors The hypothesis from a TextField label : torch.LongTensor , optional (default = None ) The label for the pair of the premise and the hypothesis metadata : List[Dict[str, Any]] , optional (default = None ) Additional information about the pair Returns \u00b6 An output dictionary consisting of: logits : torch.FloatTensor A tensor of shape (batch_size, num_labels) representing unnormalised log probabilities of the entailment label. loss : torch.FloatTensor , optional A scalar loss to be optimised. get_metrics # class BiMpm ( Model ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] make_output_human_readable # class BiMpm ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Does a simple argmax over the probabilities, converts index to string label, and add \"label\" key to the dictionary with the result. default_predictor # class BiMpm ( Model ): | ... | default_predictor = \"textual_entailment\"","title":"bimpm"},{"location":"models/pair_classification/models/bimpm/#bimpm","text":"@Model . register ( \"bimpm\" ) class BiMpm ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | matcher_word : BiMpmMatching , | encoder1 : Seq2SeqEncoder , | matcher_forward1 : BiMpmMatching , | matcher_backward1 : BiMpmMatching , | encoder2 : Seq2SeqEncoder , | matcher_forward2 : BiMpmMatching , | matcher_backward2 : BiMpmMatching , | aggregator : Seq2VecEncoder , | classifier_feedforward : FeedForward , | dropout : float = 0.1 , | initializer : InitializerApplicator = InitializerApplicator (), | ** kwargs | ) -> None This Model implements BiMPM model described in Bilateral Multi-Perspective Matching for Natural Language Sentences by Zhiguo Wang et al., 2017. Also please refer to the TensorFlow implementation and PyTorch implementation . Registered as a Model with name \"bimpm\".","title":"BiMpm"},{"location":"models/pair_classification/models/bimpm/#forward","text":"class BiMpm ( Model ): | ... | @overrides | def forward ( | self , | premise : TextFieldTensors , | hypothesis : TextFieldTensors , | label : torch . LongTensor = None , | metadata : List [ Dict [ str , Any ]] = None | ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"models/pair_classification/models/bimpm/#get_metrics","text":"class BiMpm ( Model ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"models/pair_classification/models/bimpm/#make_output_human_readable","text":"class BiMpm ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Does a simple argmax over the probabilities, converts index to string label, and add \"label\" key to the dictionary with the result.","title":"make_output_human_readable"},{"location":"models/pair_classification/models/bimpm/#default_predictor","text":"class BiMpm ( Model ): | ... | default_predictor = \"textual_entailment\"","title":"default_predictor"},{"location":"models/pair_classification/models/decomposable_attention/","text":"allennlp_models .pair_classification .models .decomposable_attention [SOURCE] DecomposableAttention # @Model . register ( \"decomposable_attention\" ) class DecomposableAttention ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | attend_feedforward : FeedForward , | matrix_attention : MatrixAttention , | compare_feedforward : FeedForward , | aggregate_feedforward : FeedForward , | premise_encoder : Optional [ Seq2SeqEncoder ] = None , | hypothesis_encoder : Optional [ Seq2SeqEncoder ] = None , | initializer : InitializerApplicator = InitializerApplicator (), | ** kwargs | ) -> None This Model implements the Decomposable Attention model described in A Decomposable Attention Model for Natural Language Inference by Parikh et al., 2016, with some optional enhancements before the decomposable attention actually happens. Parikh's original model allowed for computing an \"intra-sentence\" attention before doing the decomposable entailment step. We generalize this to any Seq2SeqEncoder that can be applied to the premise and/or the hypothesis before computing entailment. The basic outline of this model is to get an embedded representation of each word in the premise and hypothesis, align words between the two, compare the aligned phrases, and make a final entailment decision based on this aggregated comparison. Each step in this process uses a feedforward network to modify the representation. Registered as a Model with name \"decomposable_attention\". Parameters \u00b6 vocab : Vocabulary text_field_embedder : TextFieldEmbedder Used to embed the premise and hypothesis TextFields we get as input to the model. attend_feedforward : FeedForward This feedforward network is applied to the encoded sentence representations before the similarity matrix is computed between words in the premise and words in the hypothesis. matrix_attention : MatrixAttention This is the attention function used when computing the similarity matrix between words in the premise and words in the hypothesis. compare_feedforward : FeedForward This feedforward network is applied to the aligned premise and hypothesis representations, individually. aggregate_feedforward : FeedForward This final feedforward network is applied to the concatenated, summed result of the compare_feedforward network, and its output is used as the entailment class logits. premise_encoder : Seq2SeqEncoder , optional (default = None ) After embedding the premise, we can optionally apply an encoder. If this is None , we will do nothing. hypothesis_encoder : Seq2SeqEncoder , optional (default = None ) After embedding the hypothesis, we can optionally apply an encoder. If this is None , we will use the premise_encoder for the encoding (doing nothing if premise_encoder is also None ). initializer : InitializerApplicator , optional (default = InitializerApplicator() ) Used to initialize the model parameters. forward # class DecomposableAttention ( Model ): | ... | def forward ( | self , | premise : TextFieldTensors , | hypothesis : TextFieldTensors , | label : torch . IntTensor = None , | metadata : List [ Dict [ str , Any ]] = None | ) -> Dict [ str , torch . Tensor ] Parameters \u00b6 premise : TextFieldTensors From a TextField hypothesis : TextFieldTensors From a TextField label : torch.IntTensor , optional (default = None ) From a LabelField metadata : List[Dict[str, Any]] , optional (default = None ) Metadata containing the original tokenization of the premise and hypothesis with 'premise_tokens' and 'hypothesis_tokens' keys respectively. Returns \u00b6 An output dictionary consisting of: label_logits : torch.FloatTensor A tensor of shape (batch_size, num_labels) representing unnormalised log probabilities of the entailment label. label_probs : torch.FloatTensor A tensor of shape (batch_size, num_labels) representing probabilities of the entailment label. loss : torch.FloatTensor , optional A scalar loss to be optimised. get_metrics # class DecomposableAttention ( Model ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] make_output_human_readable # class DecomposableAttention ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Does a simple argmax over the probabilities, converts index to string label, and add \"label\" key to the dictionary with the result. default_predictor # class DecomposableAttention ( Model ): | ... | default_predictor = \"textual_entailment\"","title":"decomposable_attention"},{"location":"models/pair_classification/models/decomposable_attention/#decomposableattention","text":"@Model . register ( \"decomposable_attention\" ) class DecomposableAttention ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | attend_feedforward : FeedForward , | matrix_attention : MatrixAttention , | compare_feedforward : FeedForward , | aggregate_feedforward : FeedForward , | premise_encoder : Optional [ Seq2SeqEncoder ] = None , | hypothesis_encoder : Optional [ Seq2SeqEncoder ] = None , | initializer : InitializerApplicator = InitializerApplicator (), | ** kwargs | ) -> None This Model implements the Decomposable Attention model described in A Decomposable Attention Model for Natural Language Inference by Parikh et al., 2016, with some optional enhancements before the decomposable attention actually happens. Parikh's original model allowed for computing an \"intra-sentence\" attention before doing the decomposable entailment step. We generalize this to any Seq2SeqEncoder that can be applied to the premise and/or the hypothesis before computing entailment. The basic outline of this model is to get an embedded representation of each word in the premise and hypothesis, align words between the two, compare the aligned phrases, and make a final entailment decision based on this aggregated comparison. Each step in this process uses a feedforward network to modify the representation. Registered as a Model with name \"decomposable_attention\".","title":"DecomposableAttention"},{"location":"models/pair_classification/models/decomposable_attention/#forward","text":"class DecomposableAttention ( Model ): | ... | def forward ( | self , | premise : TextFieldTensors , | hypothesis : TextFieldTensors , | label : torch . IntTensor = None , | metadata : List [ Dict [ str , Any ]] = None | ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"models/pair_classification/models/decomposable_attention/#get_metrics","text":"class DecomposableAttention ( Model ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"models/pair_classification/models/decomposable_attention/#make_output_human_readable","text":"class DecomposableAttention ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Does a simple argmax over the probabilities, converts index to string label, and add \"label\" key to the dictionary with the result.","title":"make_output_human_readable"},{"location":"models/pair_classification/models/decomposable_attention/#default_predictor","text":"class DecomposableAttention ( Model ): | ... | default_predictor = \"textual_entailment\"","title":"default_predictor"},{"location":"models/pair_classification/models/esim/","text":"allennlp_models .pair_classification .models .esim [SOURCE] ESIM # @Model . register ( \"esim\" ) class ESIM ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | encoder : Seq2SeqEncoder , | matrix_attention : MatrixAttention , | projection_feedforward : FeedForward , | inference_encoder : Seq2SeqEncoder , | output_feedforward : FeedForward , | output_logit : FeedForward , | dropout : float = 0.5 , | initializer : InitializerApplicator = InitializerApplicator (), | ** kwargs | ) -> None This Model implements the ESIM sequence model described in Enhanced LSTM for Natural Language Inference by Chen et al., 2017. Registered as a Model with name \"esim\". Parameters \u00b6 vocab : Vocabulary text_field_embedder : TextFieldEmbedder Used to embed the premise and hypothesis TextFields we get as input to the model. encoder : Seq2SeqEncoder Used to encode the premise and hypothesis. matrix_attention : MatrixAttention This is the attention function used when computing the similarity matrix between encoded words in the premise and words in the hypothesis. projection_feedforward : FeedForward The feedforward network used to project down the encoded and enhanced premise and hypothesis. inference_encoder : Seq2SeqEncoder Used to encode the projected premise and hypothesis for prediction. output_feedforward : FeedForward Used to prepare the concatenated premise and hypothesis for prediction. output_logit : FeedForward This feedforward network computes the output logits. dropout : float , optional (default = 0.5 ) Dropout percentage to use. initializer : InitializerApplicator , optional (default = InitializerApplicator() ) Used to initialize the model parameters. forward # class ESIM ( Model ): | ... | def forward ( | self , | premise : TextFieldTensors , | hypothesis : TextFieldTensors , | label : torch . IntTensor = None , | metadata : List [ Dict [ str , Any ]] = None | ) -> Dict [ str , torch . Tensor ] Parameters \u00b6 premise : TextFieldTensors From a TextField hypothesis : TextFieldTensors From a TextField label : torch.IntTensor , optional (default = None ) From a LabelField metadata : List[Dict[str, Any]] , optional (default = None ) Metadata containing the original tokenization of the premise and hypothesis with 'premise_tokens' and 'hypothesis_tokens' keys respectively. Returns \u00b6 An output dictionary consisting of: label_logits : torch.FloatTensor A tensor of shape (batch_size, num_labels) representing unnormalised log probabilities of the entailment label. label_probs : torch.FloatTensor A tensor of shape (batch_size, num_labels) representing probabilities of the entailment label. loss : torch.FloatTensor , optional A scalar loss to be optimised. get_metrics # class ESIM ( Model ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] make_output_human_readable # class ESIM ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Does a simple argmax over the probabilities, converts index to string label, and add \"label\" key to the dictionary with the result. default_predictor # class ESIM ( Model ): | ... | default_predictor = \"textual_entailment\"","title":"esim"},{"location":"models/pair_classification/models/esim/#esim","text":"@Model . register ( \"esim\" ) class ESIM ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | encoder : Seq2SeqEncoder , | matrix_attention : MatrixAttention , | projection_feedforward : FeedForward , | inference_encoder : Seq2SeqEncoder , | output_feedforward : FeedForward , | output_logit : FeedForward , | dropout : float = 0.5 , | initializer : InitializerApplicator = InitializerApplicator (), | ** kwargs | ) -> None This Model implements the ESIM sequence model described in Enhanced LSTM for Natural Language Inference by Chen et al., 2017. Registered as a Model with name \"esim\".","title":"ESIM"},{"location":"models/pair_classification/models/esim/#forward","text":"class ESIM ( Model ): | ... | def forward ( | self , | premise : TextFieldTensors , | hypothesis : TextFieldTensors , | label : torch . IntTensor = None , | metadata : List [ Dict [ str , Any ]] = None | ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"models/pair_classification/models/esim/#get_metrics","text":"class ESIM ( Model ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"models/pair_classification/models/esim/#make_output_human_readable","text":"class ESIM ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Does a simple argmax over the probabilities, converts index to string label, and add \"label\" key to the dictionary with the result.","title":"make_output_human_readable"},{"location":"models/pair_classification/models/esim/#default_predictor","text":"class ESIM ( Model ): | ... | default_predictor = \"textual_entailment\"","title":"default_predictor"},{"location":"models/pair_classification/predictors/textual_entailment/","text":"allennlp_models .pair_classification .predictors .textual_entailment [SOURCE] TextualEntailmentPredictor # @Predictor . register ( \"textual_entailment\" ) class TextualEntailmentPredictor ( Predictor ) Predictor for the DecomposableAttention model. Registered as a Predictor with name \"textual_entailment\". predict # class TextualEntailmentPredictor ( Predictor ): | ... | def predict ( self , premise : str , hypothesis : str ) -> JsonDict Predicts whether the hypothesis is entailed by the premise text. Parameters \u00b6 premise : str A passage representing what is assumed to be true. hypothesis : str A sentence that may be entailed by the premise. Returns \u00b6 JsonDict A dictionary where the key \"label_probs\" determines the probabilities of each of [entailment, contradiction, neutral]. predictions_to_labeled_instances # class TextualEntailmentPredictor ( Predictor ): | ... | @overrides | def predictions_to_labeled_instances ( | self , | instance : Instance , | outputs : Dict [ str , numpy . ndarray ] | ) -> List [ Instance ]","title":"textual_entailment"},{"location":"models/pair_classification/predictors/textual_entailment/#textualentailmentpredictor","text":"@Predictor . register ( \"textual_entailment\" ) class TextualEntailmentPredictor ( Predictor ) Predictor for the DecomposableAttention model. Registered as a Predictor with name \"textual_entailment\".","title":"TextualEntailmentPredictor"},{"location":"models/pair_classification/predictors/textual_entailment/#predict","text":"class TextualEntailmentPredictor ( Predictor ): | ... | def predict ( self , premise : str , hypothesis : str ) -> JsonDict Predicts whether the hypothesis is entailed by the premise text.","title":"predict"},{"location":"models/pair_classification/predictors/textual_entailment/#predictions_to_labeled_instances","text":"class TextualEntailmentPredictor ( Predictor ): | ... | @overrides | def predictions_to_labeled_instances ( | self , | instance : Instance , | outputs : Dict [ str , numpy . ndarray ] | ) -> List [ Instance ]","title":"predictions_to_labeled_instances"},{"location":"models/rc/dataset_readers/drop/","text":"allennlp_models .rc .dataset_readers .drop [SOURCE] WORD_NUMBER_MAP # WORD_NUMBER_MAP = { \"zero\" : 0 , \"one\" : 1 , \"two\" : 2 , \"three\" : 3 , \"four\" : 4 , \"five\" : 5 , \"six\" ... DropReader # @DatasetReader . register ( \"drop\" ) class DropReader ( DatasetReader ): | def __init__ ( | self , | tokenizer : Tokenizer = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | passage_length_limit : int = None , | question_length_limit : int = None , | skip_when_all_empty : List [ str ] = None , | instance_format : str = \"drop\" , | relaxed_span_match_for_finding_labels : bool = True , | ** kwargs | ) -> None Reads a JSON-formatted DROP dataset file and returns instances in a few different possible formats. The input format is complicated; see the test fixture for an example of what it looks like. The output formats all contain a question TextField , a passage TextField , and some kind of answer representation. Because DROP has instances with several different kinds of answers, this dataset reader allows you to filter out questions that do not have answers of a particular type (e.g., remove questions that have numbers as answers, if you model can only give passage spans as answers). We typically return all possible ways of arriving at a given answer string, and expect models to marginalize over these possibilities. Parameters \u00b6 tokenizer : Tokenizer , optional (default = SpacyTokenizer() ) We use this Tokenizer for both the question and the passage. See Tokenizer . Default is SpacyTokenizer() . token_indexers : Dict[str, TokenIndexer] , optional We similarly use this for both the question and the passage. See TokenIndexer . Default is {\"tokens\": SingleIdTokenIndexer()} . passage_length_limit : int , optional (default = None ) If specified, we will cut the passage if the length of passage exceeds this limit. question_length_limit : int , optional (default = None ) If specified, we will cut the question if the length of passage exceeds this limit. skip_when_all_empty : List[str] , optional (default = None ) In some cases such as preparing for training examples, you may want to skip some examples when there are no gold labels. You can specify on what condition should the examples be skipped. Currently, you can put \"passage_span\", \"question_span\", \"addition_subtraction\", or \"counting\" in this list, to tell the reader skip when there are no such label found. If not specified, we will keep all the examples. instance_format : str , optional (default = \"drop\" ) We try to be generous in providing a few different formats for the instances in DROP, in terms of the Fields that we return for each Instance , to allow for several different kinds of models. \"drop\" format will do processing to detect numbers and various ways those numbers can be arrived at from the passage, and return Fields related to that. \"bert\" format only allows passage spans as answers, and provides a \"question_and_passage\" field with the two pieces of text joined as BERT expects. \"squad\" format provides the same fields that our BiDAF and other SQuAD models expect. relaxed_span_match_for_finding_labels : bool , optional (default = True ) DROP dataset contains multi-span answers, and the date-type answers are usually hard to find exact span matches for, also. In order to use as many examples as possible to train the model, we may not want a strict match for such cases when finding the gold span labels. If this argument is true, we will treat every span in the multi-span answers as correct, and every token in the date answer as correct, too. Because models trained on DROP typically marginalize over all possible answer positions, this is just being a little more generous in what is being marginalized. Note that this will not affect evaluation. text_to_instance # class DropReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | question_text : str , | passage_text : str , | question_id : str = None , | passage_id : str = None , | answer_annotations : List [ Dict ] = None , | passage_tokens : List [ Token ] = None | ) -> Union [ Instance , None ] make_marginal_drop_instance # class DropReader ( DatasetReader ): | ... | @staticmethod | def make_marginal_drop_instance ( | question_tokens : List [ Token ], | passage_tokens : List [ Token ], | number_tokens : List [ Token ], | number_indices : List [ int ], | token_indexers : Dict [ str , TokenIndexer ], | passage_text : str , | answer_info : Dict [ str , Any ] = None , | additional_metadata : Dict [ str , Any ] = None | ) -> Instance make_bert_drop_instance # class DropReader ( DatasetReader ): | ... | @staticmethod | def make_bert_drop_instance ( | question_tokens : List [ Token ], | passage_tokens : List [ Token ], | question_concat_passage_tokens : List [ Token ], | token_indexers : Dict [ str , TokenIndexer ], | passage_text : str , | answer_info : Dict [ str , Any ] = None , | additional_metadata : Dict [ str , Any ] = None | ) -> Instance extract_answer_info_from_annotation # class DropReader ( DatasetReader ): | ... | @staticmethod | def extract_answer_info_from_annotation ( | answer_annotation : Dict [ str , Any ] | ) -> Tuple [ str , List [ str ]] convert_word_to_number # class DropReader ( DatasetReader ): | ... | @staticmethod | def convert_word_to_number ( | word : str , | try_to_include_more_numbers = False | ) Currently we only support limited types of conversion. find_valid_spans # class DropReader ( DatasetReader ): | ... | @staticmethod | def find_valid_spans ( | passage_tokens : List [ Token ], | answer_texts : List [ str ] | ) -> List [ Tuple [ int , int ]] find_valid_add_sub_expressions # class DropReader ( DatasetReader ): | ... | @staticmethod | def find_valid_add_sub_expressions ( | numbers : List [ int ], | targets : List [ int ], | max_number_of_numbers_to_consider : int = 2 | ) -> List [ List [ int ]] find_valid_counts # class DropReader ( DatasetReader ): | ... | @staticmethod | def find_valid_counts ( | count_numbers : List [ int ], | targets : List [ int ] | ) -> List [ int ]","title":"drop"},{"location":"models/rc/dataset_readers/drop/#word_number_map","text":"WORD_NUMBER_MAP = { \"zero\" : 0 , \"one\" : 1 , \"two\" : 2 , \"three\" : 3 , \"four\" : 4 , \"five\" : 5 , \"six\" ...","title":"WORD_NUMBER_MAP"},{"location":"models/rc/dataset_readers/drop/#dropreader","text":"@DatasetReader . register ( \"drop\" ) class DropReader ( DatasetReader ): | def __init__ ( | self , | tokenizer : Tokenizer = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | passage_length_limit : int = None , | question_length_limit : int = None , | skip_when_all_empty : List [ str ] = None , | instance_format : str = \"drop\" , | relaxed_span_match_for_finding_labels : bool = True , | ** kwargs | ) -> None Reads a JSON-formatted DROP dataset file and returns instances in a few different possible formats. The input format is complicated; see the test fixture for an example of what it looks like. The output formats all contain a question TextField , a passage TextField , and some kind of answer representation. Because DROP has instances with several different kinds of answers, this dataset reader allows you to filter out questions that do not have answers of a particular type (e.g., remove questions that have numbers as answers, if you model can only give passage spans as answers). We typically return all possible ways of arriving at a given answer string, and expect models to marginalize over these possibilities.","title":"DropReader"},{"location":"models/rc/dataset_readers/drop/#text_to_instance","text":"class DropReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | question_text : str , | passage_text : str , | question_id : str = None , | passage_id : str = None , | answer_annotations : List [ Dict ] = None , | passage_tokens : List [ Token ] = None | ) -> Union [ Instance , None ]","title":"text_to_instance"},{"location":"models/rc/dataset_readers/drop/#make_marginal_drop_instance","text":"class DropReader ( DatasetReader ): | ... | @staticmethod | def make_marginal_drop_instance ( | question_tokens : List [ Token ], | passage_tokens : List [ Token ], | number_tokens : List [ Token ], | number_indices : List [ int ], | token_indexers : Dict [ str , TokenIndexer ], | passage_text : str , | answer_info : Dict [ str , Any ] = None , | additional_metadata : Dict [ str , Any ] = None | ) -> Instance","title":"make_marginal_drop_instance"},{"location":"models/rc/dataset_readers/drop/#make_bert_drop_instance","text":"class DropReader ( DatasetReader ): | ... | @staticmethod | def make_bert_drop_instance ( | question_tokens : List [ Token ], | passage_tokens : List [ Token ], | question_concat_passage_tokens : List [ Token ], | token_indexers : Dict [ str , TokenIndexer ], | passage_text : str , | answer_info : Dict [ str , Any ] = None , | additional_metadata : Dict [ str , Any ] = None | ) -> Instance","title":"make_bert_drop_instance"},{"location":"models/rc/dataset_readers/drop/#extract_answer_info_from_annotation","text":"class DropReader ( DatasetReader ): | ... | @staticmethod | def extract_answer_info_from_annotation ( | answer_annotation : Dict [ str , Any ] | ) -> Tuple [ str , List [ str ]]","title":"extract_answer_info_from_annotation"},{"location":"models/rc/dataset_readers/drop/#convert_word_to_number","text":"class DropReader ( DatasetReader ): | ... | @staticmethod | def convert_word_to_number ( | word : str , | try_to_include_more_numbers = False | ) Currently we only support limited types of conversion.","title":"convert_word_to_number"},{"location":"models/rc/dataset_readers/drop/#find_valid_spans","text":"class DropReader ( DatasetReader ): | ... | @staticmethod | def find_valid_spans ( | passage_tokens : List [ Token ], | answer_texts : List [ str ] | ) -> List [ Tuple [ int , int ]]","title":"find_valid_spans"},{"location":"models/rc/dataset_readers/drop/#find_valid_add_sub_expressions","text":"class DropReader ( DatasetReader ): | ... | @staticmethod | def find_valid_add_sub_expressions ( | numbers : List [ int ], | targets : List [ int ], | max_number_of_numbers_to_consider : int = 2 | ) -> List [ List [ int ]]","title":"find_valid_add_sub_expressions"},{"location":"models/rc/dataset_readers/drop/#find_valid_counts","text":"class DropReader ( DatasetReader ): | ... | @staticmethod | def find_valid_counts ( | count_numbers : List [ int ], | targets : List [ int ] | ) -> List [ int ]","title":"find_valid_counts"},{"location":"models/rc/dataset_readers/qangaroo/","text":"allennlp_models .rc .dataset_readers .qangaroo [SOURCE] QangarooReader # @DatasetReader . register ( \"qangaroo\" ) class QangarooReader ( DatasetReader ): | def __init__ ( | self , | tokenizer : Tokenizer = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | ** kwargs | ) -> None Reads a JSON-formatted Qangaroo file and returns a Dataset where the Instances have six fields: candidates , a ListField[TextField] , query , a TextField , supports , a ListField[TextField] , answer , a TextField , and answer_index , a IndexField . We also add a MetadataField that stores the instance's ID and annotations if they are present. Parameters \u00b6 tokenizer : Tokenizer , optional (default = SpacyTokenizer() ) We use this Tokenizer for both the question and the passage. See Tokenizer . Default is `SpacyTokenizer() . token_indexers : Dict[str, TokenIndexer] , optional We similarly use this for both the question and the passage. See TokenIndexer . Default is {\"tokens\": SingleIdTokenIndexer()} . text_to_instance # class QangarooReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | candidates : List [ str ], | query : str , | supports : List [ str ], | _id : str = None , | answer : str = None , | annotations : List [ List [ str ]] = None | ) -> Instance","title":"qangaroo"},{"location":"models/rc/dataset_readers/qangaroo/#qangarooreader","text":"@DatasetReader . register ( \"qangaroo\" ) class QangarooReader ( DatasetReader ): | def __init__ ( | self , | tokenizer : Tokenizer = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | ** kwargs | ) -> None Reads a JSON-formatted Qangaroo file and returns a Dataset where the Instances have six fields: candidates , a ListField[TextField] , query , a TextField , supports , a ListField[TextField] , answer , a TextField , and answer_index , a IndexField . We also add a MetadataField that stores the instance's ID and annotations if they are present.","title":"QangarooReader"},{"location":"models/rc/dataset_readers/qangaroo/#text_to_instance","text":"class QangarooReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | candidates : List [ str ], | query : str , | supports : List [ str ], | _id : str = None , | answer : str = None , | annotations : List [ List [ str ]] = None | ) -> Instance","title":"text_to_instance"},{"location":"models/rc/dataset_readers/quac/","text":"allennlp_models .rc .dataset_readers .quac [SOURCE] QuACReader # @DatasetReader . register ( \"quac\" ) class QuACReader ( DatasetReader ): | def __init__ ( | self , | tokenizer : Tokenizer = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | num_context_answers : int = 0 , | ** kwargs | ) -> None Reads a JSON-formatted Question Answering in Context (QuAC) data file and returns a Dataset where the Instances have four fields: question , a ListField , passage , another TextField , and span_start and span_end , both ListField composed of IndexFields into the passage TextField . Two ListField , composed of LabelField , yesno_list and followup_list is added. We also add a MetadataField that stores the instance's ID, the original passage text, gold answer strings, and token offsets into the original passage, accessible as metadata['id'] , metadata['original_passage'] , metadata['answer_text_lists'] and metadata['token_offsets'] . Parameters \u00b6 tokenizer : Tokenizer , optional (default = SpacyTokenizer() ) We use this Tokenizer for both the question and the passage. See Tokenizer . Default is SpacyTokenizer() . token_indexers : Dict[str, TokenIndexer] , optional We similarly use this for both the question and the passage. See TokenIndexer . Default is {\"tokens\": SingleIdTokenIndexer()} . num_context_answers : int , optional How many previous question answers to consider in a context. text_to_instance # class QuACReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | question_text_list : List [ str ], | passage_text : str , | start_span_list : List [ List [ int ]] = None , | end_span_list : List [ List [ int ]] = None , | passage_tokens : List [ Token ] = None , | yesno_list : List [ int ] = None , | followup_list : List [ int ] = None , | additional_metadata : Dict [ str , Any ] = None | ) -> Instance We need to convert character indices in passage_text to token indices in passage_tokens , as the latter is what we'll actually use for supervision.","title":"quac"},{"location":"models/rc/dataset_readers/quac/#quacreader","text":"@DatasetReader . register ( \"quac\" ) class QuACReader ( DatasetReader ): | def __init__ ( | self , | tokenizer : Tokenizer = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | num_context_answers : int = 0 , | ** kwargs | ) -> None Reads a JSON-formatted Question Answering in Context (QuAC) data file and returns a Dataset where the Instances have four fields: question , a ListField , passage , another TextField , and span_start and span_end , both ListField composed of IndexFields into the passage TextField . Two ListField , composed of LabelField , yesno_list and followup_list is added. We also add a MetadataField that stores the instance's ID, the original passage text, gold answer strings, and token offsets into the original passage, accessible as metadata['id'] , metadata['original_passage'] , metadata['answer_text_lists'] and metadata['token_offsets'] .","title":"QuACReader"},{"location":"models/rc/dataset_readers/quac/#text_to_instance","text":"class QuACReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | question_text_list : List [ str ], | passage_text : str , | start_span_list : List [ List [ int ]] = None , | end_span_list : List [ List [ int ]] = None , | passage_tokens : List [ Token ] = None , | yesno_list : List [ int ] = None , | followup_list : List [ int ] = None , | additional_metadata : Dict [ str , Any ] = None | ) -> Instance We need to convert character indices in passage_text to token indices in passage_tokens , as the latter is what we'll actually use for supervision.","title":"text_to_instance"},{"location":"models/rc/dataset_readers/squad/","text":"allennlp_models .rc .dataset_readers .squad [SOURCE] SQUAD2_NO_ANSWER_TOKEN # SQUAD2_NO_ANSWER_TOKEN = \"@@<NO_ANSWER>@@\" The default no_answer_token for the squad2 reader. SquadReader # @DatasetReader . register ( \"squad\" ) class SquadReader ( DatasetReader ): | def __init__ ( | self , | tokenizer : Tokenizer = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | passage_length_limit : int = None , | question_length_limit : int = None , | skip_impossible_questions : bool = False , | no_answer_token : Optional [ str ] = None , | ** kwargs | ) -> None Note If you're training on SQuAD v1.1 you should use the squad1() classmethod to instantiate this reader, and for SQuAD v2.0 you should use the squad2() classmethod. Also, for transformer-based models you should be using the TransformerSquadReader . Dataset reader suitable for JSON-formatted SQuAD-like datasets. It will generate Instances with the following fields: question , a TextField , passage , another TextField , span_start and span_end , both IndexFields into the passage TextField , and metadata , a MetadataField that stores the instance's ID, the original passage text, gold answer strings, and token offsets into the original passage, accessible as metadata['id'] , metadata['original_passage'] , metadata['answer_texts'] and metadata['token_offsets'] , respectively. This is so that we can more easily use the official SQuAD evaluation scripts to get metrics. We also support limiting the maximum length for both passage and question. However, some gold answer spans may exceed the maximum passage length, which will cause error in making instances. We simply skip these spans to avoid errors. If all of the gold answer spans of an example are skipped, during training, we will skip this example. During validating or testing, since we cannot skip examples, we use the last token as the pseudo gold answer span instead. The computed loss will not be accurate as a result. But this will not affect the answer evaluation, because we keep all the original gold answer texts. Parameters \u00b6 tokenizer : Tokenizer , optional (default = SpacyTokenizer() ) We use this Tokenizer for both the question and the passage. See Tokenizer . Default is SpacyTokenizer() . token_indexers : Dict[str, TokenIndexer] , optional We similarly use this for both the question and the passage. See TokenIndexer . Default is {\"tokens\": SingleIdTokenIndexer()} . passage_length_limit : int , optional (default = None ) If specified, we will cut the passage if the length of passage exceeds this limit. question_length_limit : int , optional (default = None ) If specified, we will cut the question if the length of question exceeds this limit. skip_impossible_questions : bool , optional (default = False ) If this is true, we will skip examples with questions that don't contain the answer spans. no_answer_token : Optional[str] , optional (default = None ) A special token to append to each context. If using a SQuAD 2.0-style dataset, this should be set, otherwise an exception will be raised if an impossible question is encountered. text_to_instance # class SquadReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | question_text : str , | passage_text : str , | is_impossible : bool = None , | char_spans : List [ Tuple [ int , int ]] = None , | answer_texts : List [ str ] = None , | passage_tokens : List [ Token ] = None , | additional_metadata : Dict [ str , Any ] = None | ) -> Optional [ Instance ] squad1 # class SquadReader ( DatasetReader ): | ... | @classmethod | def squad1 ( | cls , | tokenizer : Tokenizer = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | passage_length_limit : int = None , | question_length_limit : int = None , | skip_impossible_questions : bool = False , | ** kwargs | ) -> \"SquadReader\" Gives a SquadReader suitable for SQuAD v1.1. squad2 # class SquadReader ( DatasetReader ): | ... | @classmethod | def squad2 ( | cls , | tokenizer : Tokenizer = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | passage_length_limit : int = None , | question_length_limit : int = None , | skip_impossible_questions : bool = False , | no_answer_token : str = SQUAD2_NO_ANSWER_TOKEN , | ** kwargs | ) -> \"SquadReader\" Gives a SquadReader suitable for SQuAD v2.0.","title":"squad"},{"location":"models/rc/dataset_readers/squad/#squad2_no_answer_token","text":"SQUAD2_NO_ANSWER_TOKEN = \"@@<NO_ANSWER>@@\" The default no_answer_token for the squad2 reader.","title":"SQUAD2_NO_ANSWER_TOKEN"},{"location":"models/rc/dataset_readers/squad/#squadreader","text":"@DatasetReader . register ( \"squad\" ) class SquadReader ( DatasetReader ): | def __init__ ( | self , | tokenizer : Tokenizer = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | passage_length_limit : int = None , | question_length_limit : int = None , | skip_impossible_questions : bool = False , | no_answer_token : Optional [ str ] = None , | ** kwargs | ) -> None Note If you're training on SQuAD v1.1 you should use the squad1() classmethod to instantiate this reader, and for SQuAD v2.0 you should use the squad2() classmethod. Also, for transformer-based models you should be using the TransformerSquadReader . Dataset reader suitable for JSON-formatted SQuAD-like datasets. It will generate Instances with the following fields: question , a TextField , passage , another TextField , span_start and span_end , both IndexFields into the passage TextField , and metadata , a MetadataField that stores the instance's ID, the original passage text, gold answer strings, and token offsets into the original passage, accessible as metadata['id'] , metadata['original_passage'] , metadata['answer_texts'] and metadata['token_offsets'] , respectively. This is so that we can more easily use the official SQuAD evaluation scripts to get metrics. We also support limiting the maximum length for both passage and question. However, some gold answer spans may exceed the maximum passage length, which will cause error in making instances. We simply skip these spans to avoid errors. If all of the gold answer spans of an example are skipped, during training, we will skip this example. During validating or testing, since we cannot skip examples, we use the last token as the pseudo gold answer span instead. The computed loss will not be accurate as a result. But this will not affect the answer evaluation, because we keep all the original gold answer texts.","title":"SquadReader"},{"location":"models/rc/dataset_readers/squad/#text_to_instance","text":"class SquadReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | question_text : str , | passage_text : str , | is_impossible : bool = None , | char_spans : List [ Tuple [ int , int ]] = None , | answer_texts : List [ str ] = None , | passage_tokens : List [ Token ] = None , | additional_metadata : Dict [ str , Any ] = None | ) -> Optional [ Instance ]","title":"text_to_instance"},{"location":"models/rc/dataset_readers/squad/#squad1","text":"class SquadReader ( DatasetReader ): | ... | @classmethod | def squad1 ( | cls , | tokenizer : Tokenizer = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | passage_length_limit : int = None , | question_length_limit : int = None , | skip_impossible_questions : bool = False , | ** kwargs | ) -> \"SquadReader\" Gives a SquadReader suitable for SQuAD v1.1.","title":"squad1"},{"location":"models/rc/dataset_readers/squad/#squad2","text":"class SquadReader ( DatasetReader ): | ... | @classmethod | def squad2 ( | cls , | tokenizer : Tokenizer = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | passage_length_limit : int = None , | question_length_limit : int = None , | skip_impossible_questions : bool = False , | no_answer_token : str = SQUAD2_NO_ANSWER_TOKEN , | ** kwargs | ) -> \"SquadReader\" Gives a SquadReader suitable for SQuAD v2.0.","title":"squad2"},{"location":"models/rc/dataset_readers/transformer_squad/","text":"allennlp_models .rc .dataset_readers .transformer_squad [SOURCE] TransformerSquadReader # @DatasetReader . register ( \"transformer_squad\" ) class TransformerSquadReader ( DatasetReader ): | def __init__ ( | self , | transformer_model_name : str = \"bert-base-cased\" , | length_limit : int = 384 , | stride : int = 128 , | skip_impossible_questions : bool = False , | max_query_length : int = 64 , | tokenizer_kwargs : Dict [ str , Any ] = None , | ** kwargs | ) -> None Dataset reader suitable for JSON-formatted SQuAD-like datasets to be used with a transformer-based QA model, such as TransformerQA . It will generate Instances with the following fields: question_with_context , a TextField that contains the concatenation of question and context, answer_span , a SpanField into the question TextField denoting the answer. context_span , a SpanField into the question TextField denoting the context, i.e., the part of the text that potential answers can come from. cls_index (optional), an IndexField that holds the index of the [CLS] token within the question_with_context field. This is needed because the [CLS] token is used to indicate an impossible question. Since most tokenizers/models have the [CLS] token as the first token, this will only be included in the instance if the [CLS] token is NOT the first token. metadata , a MetadataField that stores the instance's ID, the original question, the original passage text, both of these in tokenized form, and the gold answer strings, accessible as metadata['id'] , metadata['question'] , metadata['context'] , metadata['question_tokens'] , metadata['context_tokens'] , and metadata['answers'] . This is so that we can more easily use the official SQuAD evaluation script to get metrics. For SQuAD v2.0-style datasets that contain impossible questions, we set the gold answer span to the span of the [CLS] token when there are no answers. We also support limiting the maximum length for the question. When the context+question is too long, we run a sliding window over the context and emit multiple instances for a single question. If skip_impossible_questions is True , then we only emit instances that contain a gold answer. As a result, the per-instance metrics you get during training and evaluation might not correspond 100% to the SQuAD task. To get a final number for SQuAD v1.1, you have to run python -m allennlp_models.rc.tools.transformer_qa_eval Parameters \u00b6 transformer_model_name : str , optional (default = 'bert-base-cased' ) This reader chooses tokenizer and token indexer according to this setting. length_limit : int , optional (default = 384 ) We will make sure that the length of context+question never exceeds this many word pieces. stride : int , optional (default = 128 ) When context+question are too long for the length limit, we emit multiple instances for one question, where the context is shifted. This parameter specifies the overlap between the shifted context window. It is called \"stride\" instead of \"overlap\" because that's what it's called in the original huggingface implementation. skip_impossible_questions : bool , optional (default = False ) If this is true, we will skip examples that don't have an answer. This could happen if the question is marked impossible in the dataset, or if the question+context is truncated according to length_limit such that the context no longer contains a gold answer. For SQuAD v1.1-style datasets, you should set this to True during training, and False any other time. For SQuAD v2.0-style datasets you should leave this as False . max_query_length : int , optional (default = 64 ) The maximum number of wordpieces dedicated to the question. If the question is longer than this, it will be truncated. make_instances # class TransformerSquadReader ( DatasetReader ): | ... | def make_instances ( | self , | qid : str , | question : str , | answers : List [ str ], | context : str , | first_answer_offset : Optional [ int ], | always_add_answer_span : bool = False , | is_training : bool = False | ) -> Iterable [ Instance ] Create training instances from a SQuAD example. text_to_instance # class TransformerSquadReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | question : str , | tokenized_question : List [ Token ], | context : str , | tokenized_context : List [ Token ], | answers : List [ str ] = None , | token_answer_span : Optional [ Tuple [ int , int ]] = None , | additional_metadata : Dict [ str , Any ] = None , | always_add_answer_span : bool = False | ) -> Instance apply_token_indexers # class TransformerSquadReader ( DatasetReader ): | ... | @overrides | def apply_token_indexers ( self , instance : Instance ) -> None","title":"transformer_squad"},{"location":"models/rc/dataset_readers/transformer_squad/#transformersquadreader","text":"@DatasetReader . register ( \"transformer_squad\" ) class TransformerSquadReader ( DatasetReader ): | def __init__ ( | self , | transformer_model_name : str = \"bert-base-cased\" , | length_limit : int = 384 , | stride : int = 128 , | skip_impossible_questions : bool = False , | max_query_length : int = 64 , | tokenizer_kwargs : Dict [ str , Any ] = None , | ** kwargs | ) -> None Dataset reader suitable for JSON-formatted SQuAD-like datasets to be used with a transformer-based QA model, such as TransformerQA . It will generate Instances with the following fields: question_with_context , a TextField that contains the concatenation of question and context, answer_span , a SpanField into the question TextField denoting the answer. context_span , a SpanField into the question TextField denoting the context, i.e., the part of the text that potential answers can come from. cls_index (optional), an IndexField that holds the index of the [CLS] token within the question_with_context field. This is needed because the [CLS] token is used to indicate an impossible question. Since most tokenizers/models have the [CLS] token as the first token, this will only be included in the instance if the [CLS] token is NOT the first token. metadata , a MetadataField that stores the instance's ID, the original question, the original passage text, both of these in tokenized form, and the gold answer strings, accessible as metadata['id'] , metadata['question'] , metadata['context'] , metadata['question_tokens'] , metadata['context_tokens'] , and metadata['answers'] . This is so that we can more easily use the official SQuAD evaluation script to get metrics. For SQuAD v2.0-style datasets that contain impossible questions, we set the gold answer span to the span of the [CLS] token when there are no answers. We also support limiting the maximum length for the question. When the context+question is too long, we run a sliding window over the context and emit multiple instances for a single question. If skip_impossible_questions is True , then we only emit instances that contain a gold answer. As a result, the per-instance metrics you get during training and evaluation might not correspond 100% to the SQuAD task. To get a final number for SQuAD v1.1, you have to run python -m allennlp_models.rc.tools.transformer_qa_eval","title":"TransformerSquadReader"},{"location":"models/rc/dataset_readers/transformer_squad/#make_instances","text":"class TransformerSquadReader ( DatasetReader ): | ... | def make_instances ( | self , | qid : str , | question : str , | answers : List [ str ], | context : str , | first_answer_offset : Optional [ int ], | always_add_answer_span : bool = False , | is_training : bool = False | ) -> Iterable [ Instance ] Create training instances from a SQuAD example.","title":"make_instances"},{"location":"models/rc/dataset_readers/transformer_squad/#text_to_instance","text":"class TransformerSquadReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | question : str , | tokenized_question : List [ Token ], | context : str , | tokenized_context : List [ Token ], | answers : List [ str ] = None , | token_answer_span : Optional [ Tuple [ int , int ]] = None , | additional_metadata : Dict [ str , Any ] = None , | always_add_answer_span : bool = False | ) -> Instance","title":"text_to_instance"},{"location":"models/rc/dataset_readers/transformer_squad/#apply_token_indexers","text":"class TransformerSquadReader ( DatasetReader ): | ... | @overrides | def apply_token_indexers ( self , instance : Instance ) -> None","title":"apply_token_indexers"},{"location":"models/rc/dataset_readers/triviaqa/","text":"allennlp_models .rc .dataset_readers .triviaqa [SOURCE] TriviaQaReader # @DatasetReader . register ( \"triviaqa\" ) class TriviaQaReader ( DatasetReader ): | def __init__ ( | self , | base_tarball_path : str , | unfiltered_tarball_path : str = None , | tokenizer : Tokenizer = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | ** kwargs | ) -> None Reads the TriviaQA dataset into a Dataset containing Instances with four fields: question (a TextField ), passage (another TextField ), span_start , and span_end (both IndexFields ). TriviaQA is split up into several JSON files defining the questions, and a lot of text files containing crawled web documents. We read these from a gzipped tarball, to avoid having to have millions of individual files on a filesystem. Because we need to read both train and validation files from the same tarball, we take the tarball itself as a constructor parameter, and take the question file as the argument to read . This means that you should give the path to the tarball in the dataset_reader parameters in your experiment configuration file, and something like \"wikipedia-train.json\" for the train_data_path and validation_data_path . Parameters \u00b6 base_tarball_path : str This is the path to the main tar.gz file you can download from the TriviaQA website, with directories evidence and qa . unfiltered_tarball_path : str , optional This is the path to the \"unfiltered\" TriviaQA data that you can download from the TriviaQA website, containing just question JSON files that point to evidence files in the base tarball. tokenizer : Tokenizer , optional We'll use this tokenizer on questions and evidence passages, defaulting to SpacyTokenizer if none is provided. token_indexers : Dict[str, TokenIndexer] , optional Determines how both the question and the evidence passages are represented as arrays. See TokenIndexer . Default is to have a single word ID for every token. pick_paragraphs # class TriviaQaReader ( DatasetReader ): | ... | def pick_paragraphs ( | self , | evidence_files : List [ List [ str ]], | question : str = None , | answer_texts : List [ str ] = None | ) -> List [ str ] Given a list of evidence documents, return a list of paragraphs to use as training examples. Each paragraph returned will be made into one training example. To aid in picking the best paragraph, you can also optionally pass the question text or the answer strings. Note, though, that if you actually use the answer strings for picking the paragraph on the dev or test sets, that's likely cheating, depending on how you've defined the task. text_to_instance # class TriviaQaReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | question_text : str , | passage_text : str , | token_spans : List [ Tuple [ int , int ]] = None , | answer_texts : List [ str ] = None , | question_tokens : List [ Token ] = None , | passage_tokens : List [ Token ] = None | ) -> Instance","title":"triviaqa"},{"location":"models/rc/dataset_readers/triviaqa/#triviaqareader","text":"@DatasetReader . register ( \"triviaqa\" ) class TriviaQaReader ( DatasetReader ): | def __init__ ( | self , | base_tarball_path : str , | unfiltered_tarball_path : str = None , | tokenizer : Tokenizer = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | ** kwargs | ) -> None Reads the TriviaQA dataset into a Dataset containing Instances with four fields: question (a TextField ), passage (another TextField ), span_start , and span_end (both IndexFields ). TriviaQA is split up into several JSON files defining the questions, and a lot of text files containing crawled web documents. We read these from a gzipped tarball, to avoid having to have millions of individual files on a filesystem. Because we need to read both train and validation files from the same tarball, we take the tarball itself as a constructor parameter, and take the question file as the argument to read . This means that you should give the path to the tarball in the dataset_reader parameters in your experiment configuration file, and something like \"wikipedia-train.json\" for the train_data_path and validation_data_path .","title":"TriviaQaReader"},{"location":"models/rc/dataset_readers/triviaqa/#pick_paragraphs","text":"class TriviaQaReader ( DatasetReader ): | ... | def pick_paragraphs ( | self , | evidence_files : List [ List [ str ]], | question : str = None , | answer_texts : List [ str ] = None | ) -> List [ str ] Given a list of evidence documents, return a list of paragraphs to use as training examples. Each paragraph returned will be made into one training example. To aid in picking the best paragraph, you can also optionally pass the question text or the answer strings. Note, though, that if you actually use the answer strings for picking the paragraph on the dev or test sets, that's likely cheating, depending on how you've defined the task.","title":"pick_paragraphs"},{"location":"models/rc/dataset_readers/triviaqa/#text_to_instance","text":"class TriviaQaReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | question_text : str , | passage_text : str , | token_spans : List [ Tuple [ int , int ]] = None , | answer_texts : List [ str ] = None , | question_tokens : List [ Token ] = None , | passage_tokens : List [ Token ] = None | ) -> Instance","title":"text_to_instance"},{"location":"models/rc/dataset_readers/utils/","text":"allennlp_models .rc .dataset_readers .utils [SOURCE] Utilities for reading comprehension dataset readers. IGNORED_TOKENS # IGNORED_TOKENS = { \"a\" , \"an\" , \"the\" } STRIPPED_CHARACTERS # STRIPPED_CHARACTERS = string . punctuation + \"\" . join ([ \"\u2018\" , \"\u2019\" , \"\u00b4\" , \"`\" , \"_\" ]) normalize_text # def normalize_text ( text : str ) -> str Performs a normalization that is very similar to that done by the normalization functions in SQuAD and TriviaQA. This involves splitting and rejoining the text, and could be a somewhat expensive operation. char_span_to_token_span # def char_span_to_token_span ( token_offsets : List [ Optional [ Tuple [ int , int ]]], character_span : Tuple [ int , int ] ) -> Tuple [ Tuple [ int , int ], bool ] Converts a character span from a passage into the corresponding token span in the tokenized version of the passage. If you pass in a character span that does not correspond to complete tokens in the tokenized version, we'll do our best, but the behavior is officially undefined. We return an error flag in this case, and have some debug logging so you can figure out the cause of this issue (in SQuAD, these are mostly either tokenization problems or annotation problems; there's a fair amount of both). The basic outline of this method is to find the token span that has the same offsets as the input character span. If the tokenizer tokenized the passage correctly and has matching offsets, this is easy. We try to be a little smart about cases where they don't match exactly, but mostly just find the closest thing we can. The returned (begin, end) indices are inclusive for both begin and end . So, for example, (2, 2) is the one word span beginning at token index 2, (3, 4) is the two-word span beginning at token index 3, and so on. Returnstoken_span : ``Tuple[int, int]`` \u00b6 `Inclusive` span start and end token indices that match as closely as possible to the input character spans. error : bool Whether there was an error while matching the token spans exactly. If this is True , it means there was an error in either the tokenization or the annotated character span. If this is False , it means that we found tokens that match the character span exactly. find_valid_answer_spans # def find_valid_answer_spans ( passage_tokens : List [ Token ], answer_texts : List [ str ] ) -> List [ Tuple [ int , int ]] Finds a list of token spans in passage_tokens that match the given answer_texts . This tries to find all spans that would evaluate to correct given the SQuAD and TriviaQA official evaluation scripts, which do some normalization of the input text. Note that this could return duplicate spans! The caller is expected to be able to handle possible duplicates (as already happens in the SQuAD dev set, for instance). make_reading_comprehension_instance # def make_reading_comprehension_instance ( question_tokens : List [ Token ], passage_tokens : List [ Token ], token_indexers : Dict [ str , TokenIndexer ], passage_text : str , token_spans : List [ Tuple [ int , int ]] = None , answer_texts : List [ str ] = None , additional_metadata : Dict [ str , Any ] = None ) -> Instance Converts a question, a passage, and an optional answer (or answers) to an Instance for use in a reading comprehension model. Creates an Instance with at least these fields: question and passage , both TextFields ; and metadata , a MetadataField . Additionally, if both answer_texts and char_span_starts are given, the Instance has span_start and span_end fields, which are both IndexFields . Parameters \u00b6 question_tokens : List[Token] An already-tokenized question. passage_tokens : List[Token] An already-tokenized passage that contains the answer to the given question. token_indexers : Dict[str, TokenIndexer] Determines how the question and passage TextFields will be converted into tensors that get input to a model. See TokenIndexer . passage_text : str The original passage text. We need this so that we can recover the actual span from the original passage that the model predicts as the answer to the question. This is used in official evaluation scripts. token_spans : List[Tuple[int, int]] , optional Indices into passage_tokens to use as the answer to the question for training. This is a list because there might be several possible correct answer spans in the passage. Currently, we just select the most frequent span in this list (i.e., SQuAD has multiple annotations on the dev set; this will select the span that the most annotators gave as correct). answer_texts : List[str] , optional All valid answer strings for the given question. In SQuAD, e.g., the training set has exactly one answer per question, but the dev and test sets have several. TriviaQA has many possible answers, which are the aliases for the known correct entity. This is put into the metadata for use with official evaluation scripts, but not used anywhere else. additional_metadata : Dict[str, Any] , optional The constructed metadata field will by default contain original_passage , token_offsets , question_tokens , passage_tokens , and answer_texts keys. If you want any other metadata to be associated with each instance, you can pass that in here. This dictionary will get added to the metadata dictionary we already construct. make_reading_comprehension_instance_quac # def make_reading_comprehension_instance_quac ( question_list_tokens : List [ List [ Token ]], passage_tokens : List [ Token ], token_indexers : Dict [ str , TokenIndexer ], passage_text : str , token_span_lists : List [ List [ Tuple [ int , int ]]] = None , yesno_list : List [ int ] = None , followup_list : List [ int ] = None , additional_metadata : Dict [ str , Any ] = None , num_context_answers : int = 0 ) -> Instance Converts a question, a passage, and an optional answer (or answers) to an Instance for use in a reading comprehension model. Creates an Instance with at least these fields: question and passage , both TextFields ; and metadata , a MetadataField . Additionally, if both answer_texts and char_span_starts are given, the Instance has span_start and span_end fields, which are both IndexFields . Parameters \u00b6 question_list_tokens : List[List[Token]] An already-tokenized list of questions. Each dialog have multiple questions. passage_tokens : List[Token] An already-tokenized passage that contains the answer to the given question. token_indexers : Dict[str, TokenIndexer] Determines how the question and passage TextFields will be converted into tensors that get input to a model. See TokenIndexer . passage_text : str The original passage text. We need this so that we can recover the actual span from the original passage that the model predicts as the answer to the question. This is used in official evaluation scripts. token_span_lists : List[List[Tuple[int, int]]] , optional Indices into passage_tokens to use as the answer to the question for training. This is a list of list, first because there is multiple questions per dialog, and because there might be several possible correct answer spans in the passage. Currently, we just select the last span in this list (i.e., QuAC has multiple annotations on the dev set; this will select the last span, which was given by the original annotator). yesno_list : List[int] List of the affirmation bit for each question answer pairs. followup_list : List[int] List of the continuation bit for each question answer pairs. num_context_answers : int , optional How many answers to encode into the passage. additional_metadata : Dict[str, Any] , optional The constructed metadata field will by default contain original_passage , token_offsets , question_tokens , passage_tokens , and answer_texts keys. If you want any other metadata to be associated with each instance, you can pass that in here. This dictionary will get added to the metadata dictionary we already construct. handle_cannot # def handle_cannot ( reference_answers : List [ str ]) Process a list of reference answers. If equal or more than half of the reference answers are \"CANNOTANSWER\", take it as gold. Otherwise, return answers that are not \"CANNOTANSWER\". split_token_by_delimiter # def split_token_by_delimiter ( token : Token , delimiter : str ) -> List [ Token ] split_tokens_by_hyphen # def split_tokens_by_hyphen ( tokens : List [ Token ]) -> List [ Token ]","title":"utils"},{"location":"models/rc/dataset_readers/utils/#ignored_tokens","text":"IGNORED_TOKENS = { \"a\" , \"an\" , \"the\" }","title":"IGNORED_TOKENS"},{"location":"models/rc/dataset_readers/utils/#stripped_characters","text":"STRIPPED_CHARACTERS = string . punctuation + \"\" . join ([ \"\u2018\" , \"\u2019\" , \"\u00b4\" , \"`\" , \"_\" ])","title":"STRIPPED_CHARACTERS"},{"location":"models/rc/dataset_readers/utils/#normalize_text","text":"def normalize_text ( text : str ) -> str Performs a normalization that is very similar to that done by the normalization functions in SQuAD and TriviaQA. This involves splitting and rejoining the text, and could be a somewhat expensive operation.","title":"normalize_text"},{"location":"models/rc/dataset_readers/utils/#char_span_to_token_span","text":"def char_span_to_token_span ( token_offsets : List [ Optional [ Tuple [ int , int ]]], character_span : Tuple [ int , int ] ) -> Tuple [ Tuple [ int , int ], bool ] Converts a character span from a passage into the corresponding token span in the tokenized version of the passage. If you pass in a character span that does not correspond to complete tokens in the tokenized version, we'll do our best, but the behavior is officially undefined. We return an error flag in this case, and have some debug logging so you can figure out the cause of this issue (in SQuAD, these are mostly either tokenization problems or annotation problems; there's a fair amount of both). The basic outline of this method is to find the token span that has the same offsets as the input character span. If the tokenizer tokenized the passage correctly and has matching offsets, this is easy. We try to be a little smart about cases where they don't match exactly, but mostly just find the closest thing we can. The returned (begin, end) indices are inclusive for both begin and end . So, for example, (2, 2) is the one word span beginning at token index 2, (3, 4) is the two-word span beginning at token index 3, and so on.","title":"char_span_to_token_span"},{"location":"models/rc/dataset_readers/utils/#find_valid_answer_spans","text":"def find_valid_answer_spans ( passage_tokens : List [ Token ], answer_texts : List [ str ] ) -> List [ Tuple [ int , int ]] Finds a list of token spans in passage_tokens that match the given answer_texts . This tries to find all spans that would evaluate to correct given the SQuAD and TriviaQA official evaluation scripts, which do some normalization of the input text. Note that this could return duplicate spans! The caller is expected to be able to handle possible duplicates (as already happens in the SQuAD dev set, for instance).","title":"find_valid_answer_spans"},{"location":"models/rc/dataset_readers/utils/#make_reading_comprehension_instance","text":"def make_reading_comprehension_instance ( question_tokens : List [ Token ], passage_tokens : List [ Token ], token_indexers : Dict [ str , TokenIndexer ], passage_text : str , token_spans : List [ Tuple [ int , int ]] = None , answer_texts : List [ str ] = None , additional_metadata : Dict [ str , Any ] = None ) -> Instance Converts a question, a passage, and an optional answer (or answers) to an Instance for use in a reading comprehension model. Creates an Instance with at least these fields: question and passage , both TextFields ; and metadata , a MetadataField . Additionally, if both answer_texts and char_span_starts are given, the Instance has span_start and span_end fields, which are both IndexFields .","title":"make_reading_comprehension_instance"},{"location":"models/rc/dataset_readers/utils/#make_reading_comprehension_instance_quac","text":"def make_reading_comprehension_instance_quac ( question_list_tokens : List [ List [ Token ]], passage_tokens : List [ Token ], token_indexers : Dict [ str , TokenIndexer ], passage_text : str , token_span_lists : List [ List [ Tuple [ int , int ]]] = None , yesno_list : List [ int ] = None , followup_list : List [ int ] = None , additional_metadata : Dict [ str , Any ] = None , num_context_answers : int = 0 ) -> Instance Converts a question, a passage, and an optional answer (or answers) to an Instance for use in a reading comprehension model. Creates an Instance with at least these fields: question and passage , both TextFields ; and metadata , a MetadataField . Additionally, if both answer_texts and char_span_starts are given, the Instance has span_start and span_end fields, which are both IndexFields .","title":"make_reading_comprehension_instance_quac"},{"location":"models/rc/dataset_readers/utils/#handle_cannot","text":"def handle_cannot ( reference_answers : List [ str ]) Process a list of reference answers. If equal or more than half of the reference answers are \"CANNOTANSWER\", take it as gold. Otherwise, return answers that are not \"CANNOTANSWER\".","title":"handle_cannot"},{"location":"models/rc/dataset_readers/utils/#split_token_by_delimiter","text":"def split_token_by_delimiter ( token : Token , delimiter : str ) -> List [ Token ]","title":"split_token_by_delimiter"},{"location":"models/rc/dataset_readers/utils/#split_tokens_by_hyphen","text":"def split_tokens_by_hyphen ( tokens : List [ Token ]) -> List [ Token ]","title":"split_tokens_by_hyphen"},{"location":"models/rc/metrics/drop_em_and_f1/","text":"allennlp_models .rc .metrics .drop_em_and_f1 [SOURCE] DropEmAndF1 # @Metric . register ( \"drop\" ) class DropEmAndF1 ( Metric ): | def __init__ ( self ) -> None This Metric takes the best span string computed by a model, along with the answer strings labeled in the data, and computes exact match and F1 score using the official DROP evaluator (which has special handling for numbers and for questions with multiple answer spans, among other things). __call__ # class DropEmAndF1 ( Metric ): | ... | @overrides | def __call__ ( | self , | prediction : Union [ str , List ], | ground_truths : List | ) Parametersprediction: ``Union[str, List]`` \u00b6 The predicted answer from the model evaluated. This could be a string, or a list of string when multiple spans are predicted as answer. ground_truths: List All the ground truth answer annotations. get_metric # class DropEmAndF1 ( Metric ): | ... | @overrides | def get_metric ( self , reset : bool = False ) -> Tuple [ float , float ] ReturnsAverage exact match and F1 score (in that order) as computed by the official DROP script \u00b6 over all inputs. reset # class DropEmAndF1 ( Metric ): | ... | @overrides | def reset ( self )","title":"drop_em_and_f1"},{"location":"models/rc/metrics/drop_em_and_f1/#dropemandf1","text":"@Metric . register ( \"drop\" ) class DropEmAndF1 ( Metric ): | def __init__ ( self ) -> None This Metric takes the best span string computed by a model, along with the answer strings labeled in the data, and computes exact match and F1 score using the official DROP evaluator (which has special handling for numbers and for questions with multiple answer spans, among other things).","title":"DropEmAndF1"},{"location":"models/rc/metrics/drop_em_and_f1/#__call__","text":"class DropEmAndF1 ( Metric ): | ... | @overrides | def __call__ ( | self , | prediction : Union [ str , List ], | ground_truths : List | )","title":"__call__"},{"location":"models/rc/metrics/drop_em_and_f1/#get_metric","text":"class DropEmAndF1 ( Metric ): | ... | @overrides | def get_metric ( self , reset : bool = False ) -> Tuple [ float , float ]","title":"get_metric"},{"location":"models/rc/metrics/drop_em_and_f1/#reset","text":"class DropEmAndF1 ( Metric ): | ... | @overrides | def reset ( self )","title":"reset"},{"location":"models/rc/metrics/squad_em_and_f1/","text":"allennlp_models .rc .metrics .squad_em_and_f1 [SOURCE] SquadEmAndF1 # @Metric . register ( \"squad\" ) class SquadEmAndF1 ( Metric ): | def __init__ ( self ) -> None This Metric takes the best span string computed by a model, along with the answer strings labeled in the data, and computed exact match and F1 score using functions from the official SQuAD2 and SQuAD1.1 evaluation scripts. __call__ # class SquadEmAndF1 ( Metric ): | ... | @overrides | def __call__ ( | self , | best_span_strings : Union [ str , List [ str ]], | answer_strings : Union [ List [ str ], List [ List [ str ]]] | ) get_metric # class SquadEmAndF1 ( Metric ): | ... | @overrides | def get_metric ( self , reset : bool = False ) -> Tuple [ float , float ] ReturnsAverage exact match and F1 score (in that order) as computed by the official SQuAD script \u00b6 over all inputs. reset # class SquadEmAndF1 ( Metric ): | ... | @overrides | def reset ( self )","title":"squad_em_and_f1"},{"location":"models/rc/metrics/squad_em_and_f1/#squademandf1","text":"@Metric . register ( \"squad\" ) class SquadEmAndF1 ( Metric ): | def __init__ ( self ) -> None This Metric takes the best span string computed by a model, along with the answer strings labeled in the data, and computed exact match and F1 score using functions from the official SQuAD2 and SQuAD1.1 evaluation scripts.","title":"SquadEmAndF1"},{"location":"models/rc/metrics/squad_em_and_f1/#__call__","text":"class SquadEmAndF1 ( Metric ): | ... | @overrides | def __call__ ( | self , | best_span_strings : Union [ str , List [ str ]], | answer_strings : Union [ List [ str ], List [ List [ str ]]] | )","title":"__call__"},{"location":"models/rc/metrics/squad_em_and_f1/#get_metric","text":"class SquadEmAndF1 ( Metric ): | ... | @overrides | def get_metric ( self , reset : bool = False ) -> Tuple [ float , float ]","title":"get_metric"},{"location":"models/rc/metrics/squad_em_and_f1/#reset","text":"class SquadEmAndF1 ( Metric ): | ... | @overrides | def reset ( self )","title":"reset"},{"location":"models/rc/models/bidaf/","text":"allennlp_models .rc .models .bidaf [SOURCE] BidirectionalAttentionFlow # @Model . register ( \"bidaf\" ) class BidirectionalAttentionFlow ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | num_highway_layers : int , | phrase_layer : Seq2SeqEncoder , | matrix_attention : MatrixAttention , | modeling_layer : Seq2SeqEncoder , | span_end_encoder : Seq2SeqEncoder , | dropout : float = 0.2 , | mask_lstms : bool = True , | initializer : InitializerApplicator = InitializerApplicator (), | regularizer : Optional [ RegularizerApplicator ] = None | ) -> None This class implements Minjoon Seo's Bidirectional Attention Flow model for answering reading comprehension questions (ICLR 2017). The basic layout is pretty simple: encode words as a combination of word embeddings and a character-level encoder, pass the word representations through a bi-LSTM/GRU, use a matrix of attentions to put question information into the passage word representations (this is the only part that is at all non-standard), pass this through another few layers of bi-LSTMs/GRUs, and do a softmax over span start and span end. Parameters \u00b6 vocab : Vocabulary text_field_embedder : TextFieldEmbedder Used to embed the question and passage TextFields we get as input to the model. num_highway_layers : int The number of highway layers to use in between embedding the input and passing it through the phrase layer. phrase_layer : Seq2SeqEncoder The encoder (with its own internal stacking) that we will use in between embedding tokens and doing the bidirectional attention. matrix_attention : MatrixAttention The attention function that we will use when comparing encoded passage and question representations. modeling_layer : Seq2SeqEncoder The encoder (with its own internal stacking) that we will use in between the bidirectional attention and predicting span start and end. span_end_encoder : Seq2SeqEncoder The encoder that we will use to incorporate span start predictions into the passage state before predicting span end. dropout : float , optional (default = 0.2 ) If greater than 0, we will apply dropout with this probability after all encoders (pytorch LSTMs do not apply dropout to their last layer). mask_lstms : bool , optional (default = True ) If False , we will skip passing the mask to the LSTM layers. This gives a ~2x speedup, with only a slight performance decrease, if any. We haven't experimented much with this yet, but have confirmed that we still get very similar performance with much faster training times. We still use the mask for all softmaxes, but avoid the shuffling that's required when using masking with pytorch LSTMs. initializer : InitializerApplicator , optional (default = InitializerApplicator() ) Used to initialize the model parameters. regularizer : RegularizerApplicator , optional (default = None ) If provided, will be used to calculate the regularization penalty during training. forward # class BidirectionalAttentionFlow ( Model ): | ... | def forward ( | self , | question : Dict [ str , torch . LongTensor ], | passage : Dict [ str , torch . LongTensor ], | span_start : torch . IntTensor = None , | span_end : torch . IntTensor = None , | metadata : List [ Dict [ str , Any ]] = None | ) -> Dict [ str , torch . Tensor ] Parameters \u00b6 question : Dict[str, torch.LongTensor] From a TextField . passage : Dict[str, torch.LongTensor] From a TextField . The model assumes that this passage contains the answer to the question, and predicts the beginning and ending positions of the answer within the passage. span_start : torch.IntTensor , optional From an IndexField . This is one of the things we are trying to predict - the beginning position of the answer with the passage. This is an inclusive token index. If this is given, we will compute a loss that gets included in the output dictionary. span_end : torch.IntTensor , optional From an IndexField . This is one of the things we are trying to predict - the ending position of the answer with the passage. This is an inclusive token index. If this is given, we will compute a loss that gets included in the output dictionary. metadata : List[Dict[str, Any]] , optional metadata : List[Dict[str, Any]] , optional If present, this should contain the question tokens, passage tokens, original passage text, and token offsets into the passage for each instance in the batch. The length of this list should be the batch size, and each dictionary should have the keys question_tokens , passage_tokens , original_passage , and token_offsets . ReturnsAn output dictionary consisting of: \u00b6 span_start_logits : torch.FloatTensor A tensor of shape (batch_size, passage_length) representing unnormalized log probabilities of the span start position. span_start_probs : torch.FloatTensor The result of softmax(span_start_logits) . span_end_logits : torch.FloatTensor A tensor of shape (batch_size, passage_length) representing unnormalized log probabilities of the span end position (inclusive). span_end_probs : torch.FloatTensor The result of softmax(span_end_logits) . best_span : torch.IntTensor The result of a constrained inference over span_start_logits and span_end_logits to find the most probable span. Shape is (batch_size, 2) and each offset is a token index. loss : torch.FloatTensor , optional A scalar loss to be optimised. best_span_str : List[str] If sufficient metadata was provided for the instances in the batch, we also return the string from the original passage that the model thinks is the best answer to the question. get_metrics # class BidirectionalAttentionFlow ( Model ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] get_best_span # class BidirectionalAttentionFlow ( Model ): | ... | @staticmethod | def get_best_span ( | span_start_logits : torch . Tensor , | span_end_logits : torch . Tensor | ) -> torch . Tensor We call the inputs \"logits\" - they could either be unnormalized logits or normalized log probabilities. A log_softmax operation is a constant shifting of the entire logit vector, so taking an argmax over either one gives the same result. default_predictor # class BidirectionalAttentionFlow ( Model ): | ... | default_predictor = \"reading_comprehension\"","title":"bidaf"},{"location":"models/rc/models/bidaf/#bidirectionalattentionflow","text":"@Model . register ( \"bidaf\" ) class BidirectionalAttentionFlow ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | num_highway_layers : int , | phrase_layer : Seq2SeqEncoder , | matrix_attention : MatrixAttention , | modeling_layer : Seq2SeqEncoder , | span_end_encoder : Seq2SeqEncoder , | dropout : float = 0.2 , | mask_lstms : bool = True , | initializer : InitializerApplicator = InitializerApplicator (), | regularizer : Optional [ RegularizerApplicator ] = None | ) -> None This class implements Minjoon Seo's Bidirectional Attention Flow model for answering reading comprehension questions (ICLR 2017). The basic layout is pretty simple: encode words as a combination of word embeddings and a character-level encoder, pass the word representations through a bi-LSTM/GRU, use a matrix of attentions to put question information into the passage word representations (this is the only part that is at all non-standard), pass this through another few layers of bi-LSTMs/GRUs, and do a softmax over span start and span end.","title":"BidirectionalAttentionFlow"},{"location":"models/rc/models/bidaf/#forward","text":"class BidirectionalAttentionFlow ( Model ): | ... | def forward ( | self , | question : Dict [ str , torch . LongTensor ], | passage : Dict [ str , torch . LongTensor ], | span_start : torch . IntTensor = None , | span_end : torch . IntTensor = None , | metadata : List [ Dict [ str , Any ]] = None | ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"models/rc/models/bidaf/#get_metrics","text":"class BidirectionalAttentionFlow ( Model ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"models/rc/models/bidaf/#get_best_span","text":"class BidirectionalAttentionFlow ( Model ): | ... | @staticmethod | def get_best_span ( | span_start_logits : torch . Tensor , | span_end_logits : torch . Tensor | ) -> torch . Tensor We call the inputs \"logits\" - they could either be unnormalized logits or normalized log probabilities. A log_softmax operation is a constant shifting of the entire logit vector, so taking an argmax over either one gives the same result.","title":"get_best_span"},{"location":"models/rc/models/bidaf/#default_predictor","text":"class BidirectionalAttentionFlow ( Model ): | ... | default_predictor = \"reading_comprehension\"","title":"default_predictor"},{"location":"models/rc/models/bidaf_ensemble/","text":"allennlp_models .rc .models .bidaf_ensemble [SOURCE] BidafEnsemble # @Model . register ( \"bidaf-ensemble\" ) class BidafEnsemble ( Model ): | def __init__ ( | self , | submodels : List [ BidirectionalAttentionFlow ] | ) -> None This class ensembles the output from multiple BiDAF models. It combines results from the submodels by averaging the start and end span probabilities. forward # class BidafEnsemble ( Model ): | ... | @overrides | def forward ( | self , | question : Dict [ str , torch . LongTensor ], | passage : Dict [ str , torch . LongTensor ], | span_start : torch . IntTensor = None , | span_end : torch . IntTensor = None , | metadata : List [ Dict [ str , Any ]] = None | ) -> Dict [ str , torch . Tensor ] The forward method runs each of the submodels, then selects the best span from the subresults. The best span is determined by averaging the probabilities for the start and end of the spans. Parametersquestion : Dict[str, torch.LongTensor] \u00b6 From a ``TextField``. passage : Dict[str, torch.LongTensor] From a TextField . The model assumes that this passage contains the answer to the question, and predicts the beginning and ending positions of the answer within the passage. span_start : torch.IntTensor , optional From an IndexField . This is one of the things we are trying to predict - the beginning position of the answer with the passage. This is an inclusive token index. If this is given, we will compute a loss that gets included in the output dictionary. span_end : torch.IntTensor , optional From an IndexField . This is one of the things we are trying to predict - the ending position of the answer with the passage. This is an inclusive token index. If this is given, we will compute a loss that gets included in the output dictionary. metadata : List[Dict[str, Any]] , optional If present, this should contain the question ID, original passage text, and token offsets into the passage for each instance in the batch. We use this for computing official metrics using the official SQuAD evaluation script. The length of this list should be the batch size, and each dictionary should have the keys id , original_passage , and token_offsets . If you only want the best span string and don't care about official metrics, you can omit the id key. ReturnsAn output dictionary consisting of: \u00b6 best_span : torch.IntTensor The result of a constrained inference over span_start_logits and span_end_logits to find the most probable span. Shape is (batch_size, 2) and each offset is a token index. best_span_str : List[str] If sufficient metadata was provided for the instances in the batch, we also return the string from the original passage that the model thinks is the best answer to the question. get_metrics # class BidafEnsemble ( Model ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] from_params # class BidafEnsemble ( Model ): | ... | @classmethod | def from_params ( | cls , | vocab : Vocabulary , | params : Params | ) -> \"BidafEnsemble\" default_predictor # class BidafEnsemble ( Model ): | ... | default_predictor = \"reading_comprehension\" ensemble # def ensemble ( subresults : List [ Dict [ str , torch . Tensor ]] ) -> torch . Tensor Identifies the best prediction given the results from the submodels. Parameterssubresults : List[Dict[str, torch.Tensor]] \u00b6 Results of each submodel. ReturnsThe index of the best submodel. \u00b6","title":"bidaf_ensemble"},{"location":"models/rc/models/bidaf_ensemble/#bidafensemble","text":"@Model . register ( \"bidaf-ensemble\" ) class BidafEnsemble ( Model ): | def __init__ ( | self , | submodels : List [ BidirectionalAttentionFlow ] | ) -> None This class ensembles the output from multiple BiDAF models. It combines results from the submodels by averaging the start and end span probabilities.","title":"BidafEnsemble"},{"location":"models/rc/models/bidaf_ensemble/#forward","text":"class BidafEnsemble ( Model ): | ... | @overrides | def forward ( | self , | question : Dict [ str , torch . LongTensor ], | passage : Dict [ str , torch . LongTensor ], | span_start : torch . IntTensor = None , | span_end : torch . IntTensor = None , | metadata : List [ Dict [ str , Any ]] = None | ) -> Dict [ str , torch . Tensor ] The forward method runs each of the submodels, then selects the best span from the subresults. The best span is determined by averaging the probabilities for the start and end of the spans.","title":"forward"},{"location":"models/rc/models/bidaf_ensemble/#get_metrics","text":"class BidafEnsemble ( Model ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"models/rc/models/bidaf_ensemble/#from_params","text":"class BidafEnsemble ( Model ): | ... | @classmethod | def from_params ( | cls , | vocab : Vocabulary , | params : Params | ) -> \"BidafEnsemble\"","title":"from_params"},{"location":"models/rc/models/bidaf_ensemble/#default_predictor","text":"class BidafEnsemble ( Model ): | ... | default_predictor = \"reading_comprehension\"","title":"default_predictor"},{"location":"models/rc/models/bidaf_ensemble/#ensemble","text":"def ensemble ( subresults : List [ Dict [ str , torch . Tensor ]] ) -> torch . Tensor Identifies the best prediction given the results from the submodels.","title":"ensemble"},{"location":"models/rc/models/dialog_qa/","text":"allennlp_models .rc .models .dialog_qa [SOURCE] DialogQA # @Model . register ( \"dialog_qa\" ) class DialogQA ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | phrase_layer : Seq2SeqEncoder , | residual_encoder : Seq2SeqEncoder , | span_start_encoder : Seq2SeqEncoder , | span_end_encoder : Seq2SeqEncoder , | initializer : Optional [ InitializerApplicator ] = None , | dropout : float = 0.2 , | num_context_answers : int = 0 , | marker_embedding_dim : int = 10 , | max_span_length : int = 30 , | max_turn_length : int = 12 | ) -> None This class implements modified version of BiDAF (with self attention and residual layer, from Clark and Gardner ACL 17 paper) model as used in Question Answering in Context (EMNLP 2018) paper [https://arxiv.org/pdf/1808.07036.pdf]. In this set-up, a single instance is a dialog, list of question answer pairs. Parametersvocab : ``Vocabulary`` \u00b6 text_field_embedder : TextFieldEmbedder Used to embed the question and passage TextFields we get as input to the model. phrase_layer : Seq2SeqEncoder The encoder (with its own internal stacking) that we will use in between embedding tokens and doing the bidirectional attention. span_start_encoder : Seq2SeqEncoder The encoder that we will use to incorporate span start predictions into the passage state before predicting span end. span_end_encoder : Seq2SeqEncoder The encoder that we will use to incorporate span end predictions into the passage state. dropout : float , optional (default=0.2) If greater than 0, we will apply dropout with this probability after all encoders (pytorch LSTMs do not apply dropout to their last layer). num_context_answers : int , optional (default=0) If greater than 0, the model will consider previous question answering context. max_span_length: int , optional (default=0) Maximum token length of the output span. max_turn_length: int , optional (default=12) Maximum length of an interaction. forward # class DialogQA ( Model ): | ... | def forward ( | self , | question : Dict [ str , torch . LongTensor ], | passage : Dict [ str , torch . LongTensor ], | span_start : torch . IntTensor = None , | span_end : torch . IntTensor = None , | p1_answer_marker : torch . IntTensor = None , | p2_answer_marker : torch . IntTensor = None , | p3_answer_marker : torch . IntTensor = None , | yesno_list : torch . IntTensor = None , | followup_list : torch . IntTensor = None , | metadata : List [ Dict [ str , Any ]] = None | ) -> Dict [ str , torch . Tensor ] Parametersquestion : Dict[str, torch.LongTensor] \u00b6 From a ``TextField``. passage : Dict[str, torch.LongTensor] From a TextField . The model assumes that this passage contains the answer to the question, and predicts the beginning and ending positions of the answer within the passage. span_start : torch.IntTensor , optional From an IndexField . This is one of the things we are trying to predict - the beginning position of the answer with the passage. This is an inclusive token index. If this is given, we will compute a loss that gets included in the output dictionary. span_end : torch.IntTensor , optional From an IndexField . This is one of the things we are trying to predict - the ending position of the answer with the passage. This is an inclusive token index. If this is given, we will compute a loss that gets included in the output dictionary. p1_answer_marker : torch.IntTensor , optional This is one of the inputs, but only when num_context_answers > 0. This is a tensor that has a shape [batch_size, max_qa_count, max_passage_length]. Most passage token will have assigned 'O', except the passage tokens belongs to the previous answer in the dialog, which will be assigned labels such as <1_start>, <1_in>, <1_end>. For more details, look into dataset_readers/util/make_reading_comprehension_instance_quac p2_answer_marker : torch.IntTensor , optional This is one of the inputs, but only when num_context_answers > 1. It is similar to p1_answer_marker, but marking previous previous answer in passage. p3_answer_marker : torch.IntTensor , optional This is one of the inputs, but only when num_context_answers > 2. It is similar to p1_answer_marker, but marking previous previous previous answer in passage. yesno_list : torch.IntTensor , optional This is one of the outputs that we are trying to predict. Three way classification (the yes/no/not a yes no question). followup_list : torch.IntTensor , optional This is one of the outputs that we are trying to predict. Three way classification (followup / maybe followup / don't followup). metadata : List[Dict[str, Any]] , optional If present, this should contain the question ID, original passage text, and token offsets into the passage for each instance in the batch. We use this for computing official metrics using the official SQuAD evaluation script. The length of this list should be the batch size, and each dictionary should have the keys id , original_passage , and token_offsets . If you only want the best span string and don't care about official metrics, you can omit the id key. ReturnsAn output dictionary consisting of the followings. \u00b6 Each of the followings is a nested list because first iterates over dialog, then questions in dialog. qid : List[List[str]] A list of list, consisting of question ids. followup : List[List[int]] A list of list, consisting of continuation marker prediction index. (y :yes, m: maybe follow up, n: don't follow up) yesno : List[List[int]] A list of list, consisting of affirmation marker prediction index. (y :yes, x: not a yes/no question, n: np) best_span_str : List[List[str]] If sufficient metadata was provided for the instances in the batch, we also return the string from the original passage that the model thinks is the best answer to the question. loss : torch.FloatTensor, optional A scalar loss to be optimised. make_output_human_readable # class DialogQA ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] get_metrics # class DialogQA ( Model ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] default_predictor # class DialogQA ( Model ): | ... | default_predictor = \"dialog_qa\"","title":"dialog_qa"},{"location":"models/rc/models/dialog_qa/#dialogqa","text":"@Model . register ( \"dialog_qa\" ) class DialogQA ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | phrase_layer : Seq2SeqEncoder , | residual_encoder : Seq2SeqEncoder , | span_start_encoder : Seq2SeqEncoder , | span_end_encoder : Seq2SeqEncoder , | initializer : Optional [ InitializerApplicator ] = None , | dropout : float = 0.2 , | num_context_answers : int = 0 , | marker_embedding_dim : int = 10 , | max_span_length : int = 30 , | max_turn_length : int = 12 | ) -> None This class implements modified version of BiDAF (with self attention and residual layer, from Clark and Gardner ACL 17 paper) model as used in Question Answering in Context (EMNLP 2018) paper [https://arxiv.org/pdf/1808.07036.pdf]. In this set-up, a single instance is a dialog, list of question answer pairs.","title":"DialogQA"},{"location":"models/rc/models/dialog_qa/#forward","text":"class DialogQA ( Model ): | ... | def forward ( | self , | question : Dict [ str , torch . LongTensor ], | passage : Dict [ str , torch . LongTensor ], | span_start : torch . IntTensor = None , | span_end : torch . IntTensor = None , | p1_answer_marker : torch . IntTensor = None , | p2_answer_marker : torch . IntTensor = None , | p3_answer_marker : torch . IntTensor = None , | yesno_list : torch . IntTensor = None , | followup_list : torch . IntTensor = None , | metadata : List [ Dict [ str , Any ]] = None | ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"models/rc/models/dialog_qa/#make_output_human_readable","text":"class DialogQA ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ]","title":"make_output_human_readable"},{"location":"models/rc/models/dialog_qa/#get_metrics","text":"class DialogQA ( Model ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"models/rc/models/dialog_qa/#default_predictor","text":"class DialogQA ( Model ): | ... | default_predictor = \"dialog_qa\"","title":"default_predictor"},{"location":"models/rc/models/naqanet/","text":"allennlp_models .rc .models .naqanet [SOURCE] NumericallyAugmentedQaNet # @Model . register ( \"naqanet\" ) class NumericallyAugmentedQaNet ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | num_highway_layers : int , | phrase_layer : Seq2SeqEncoder , | matrix_attention_layer : MatrixAttention , | modeling_layer : Seq2SeqEncoder , | dropout_prob : float = 0.1 , | initializer : InitializerApplicator = InitializerApplicator (), | regularizer : Optional [ RegularizerApplicator ] = None , | answering_abilities : List [ str ] = None | ) -> None This class augments the QANet model with some rudimentary numerical reasoning abilities, as published in the original DROP paper. The main idea here is that instead of just predicting a passage span after doing all of the QANet modeling stuff, we add several different \"answer abilities\": predicting a span from the question, predicting a count, or predicting an arithmetic expression. Near the end of the QANet model, we have a variable that predicts what kind of answer type we need, and each branch has separate modeling logic to predict that answer type. We then marginalize over all possible ways of getting to the right answer through each of these answer types. forward # class NumericallyAugmentedQaNet ( Model ): | ... | def forward ( | self , | question : Dict [ str , torch . LongTensor ], | passage : Dict [ str , torch . LongTensor ], | number_indices : torch . LongTensor , | answer_as_passage_spans : torch . LongTensor = None , | answer_as_question_spans : torch . LongTensor = None , | answer_as_add_sub_expressions : torch . LongTensor = None , | answer_as_counts : torch . LongTensor = None , | metadata : List [ Dict [ str , Any ]] = None | ) -> Dict [ str , torch . Tensor ] get_metrics # class NumericallyAugmentedQaNet ( Model ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] default_predictor # class NumericallyAugmentedQaNet ( Model ): | ... | default_predictor = \"reading_comprehension\"","title":"naqanet"},{"location":"models/rc/models/naqanet/#numericallyaugmentedqanet","text":"@Model . register ( \"naqanet\" ) class NumericallyAugmentedQaNet ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | num_highway_layers : int , | phrase_layer : Seq2SeqEncoder , | matrix_attention_layer : MatrixAttention , | modeling_layer : Seq2SeqEncoder , | dropout_prob : float = 0.1 , | initializer : InitializerApplicator = InitializerApplicator (), | regularizer : Optional [ RegularizerApplicator ] = None , | answering_abilities : List [ str ] = None | ) -> None This class augments the QANet model with some rudimentary numerical reasoning abilities, as published in the original DROP paper. The main idea here is that instead of just predicting a passage span after doing all of the QANet modeling stuff, we add several different \"answer abilities\": predicting a span from the question, predicting a count, or predicting an arithmetic expression. Near the end of the QANet model, we have a variable that predicts what kind of answer type we need, and each branch has separate modeling logic to predict that answer type. We then marginalize over all possible ways of getting to the right answer through each of these answer types.","title":"NumericallyAugmentedQaNet"},{"location":"models/rc/models/naqanet/#forward","text":"class NumericallyAugmentedQaNet ( Model ): | ... | def forward ( | self , | question : Dict [ str , torch . LongTensor ], | passage : Dict [ str , torch . LongTensor ], | number_indices : torch . LongTensor , | answer_as_passage_spans : torch . LongTensor = None , | answer_as_question_spans : torch . LongTensor = None , | answer_as_add_sub_expressions : torch . LongTensor = None , | answer_as_counts : torch . LongTensor = None , | metadata : List [ Dict [ str , Any ]] = None | ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"models/rc/models/naqanet/#get_metrics","text":"class NumericallyAugmentedQaNet ( Model ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"models/rc/models/naqanet/#default_predictor","text":"class NumericallyAugmentedQaNet ( Model ): | ... | default_predictor = \"reading_comprehension\"","title":"default_predictor"},{"location":"models/rc/models/qanet/","text":"allennlp_models .rc .models .qanet [SOURCE] QaNet # @Model . register ( \"qanet\" ) @Model . register ( \"rc-qanet\" ) class QaNet ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | num_highway_layers : int , | phrase_layer : Seq2SeqEncoder , | matrix_attention_layer : MatrixAttention , | modeling_layer : Seq2SeqEncoder , | dropout_prob : float = 0.1 , | initializer : InitializerApplicator = InitializerApplicator (), | regularizer : Optional [ RegularizerApplicator ] = None | ) -> None This class implements Adams Wei Yu's QANet Model <https://openreview.net/forum?id=B14TlG-RW> _ for machine reading comprehension published at ICLR 2018. The overall architecture of QANet is very similar to BiDAF. The main difference is that QANet replaces the RNN encoder with CNN + self-attention. There are also some minor differences in the modeling layer and output layer. Parametersvocab : ``Vocabulary`` \u00b6 text_field_embedder : TextFieldEmbedder Used to embed the question and passage TextFields we get as input to the model. num_highway_layers : int The number of highway layers to use in between embedding the input and passing it through the phrase layer. phrase_layer : Seq2SeqEncoder The encoder (with its own internal stacking) that we will use in between embedding tokens and doing the passage-question attention. matrix_attention_layer : MatrixAttention The matrix attention function that we will use when comparing encoded passage and question representations. modeling_layer : Seq2SeqEncoder The encoder (with its own internal stacking) that we will use in between the bidirectional attention and predicting span start and end. dropout_prob : float , optional (default=0.1) If greater than 0, we will apply dropout with this probability between layers. initializer : InitializerApplicator , optional (default= InitializerApplicator() ) Used to initialize the model parameters. regularizer : RegularizerApplicator , optional (default= None ) If provided, will be used to calculate the regularization penalty during training. forward # class QaNet ( Model ): | ... | def forward ( | self , | question : Dict [ str , torch . LongTensor ], | passage : Dict [ str , torch . LongTensor ], | span_start : torch . IntTensor = None , | span_end : torch . IntTensor = None , | metadata : List [ Dict [ str , Any ]] = None | ) -> Dict [ str , torch . Tensor ] Parametersquestion : Dict[str, torch.LongTensor] \u00b6 From a ``TextField``. passage : Dict[str, torch.LongTensor] From a TextField . The model assumes that this passage contains the answer to the question, and predicts the beginning and ending positions of the answer within the passage. span_start : torch.IntTensor , optional From an IndexField . This is one of the things we are trying to predict - the beginning position of the answer with the passage. This is an inclusive token index. If this is given, we will compute a loss that gets included in the output dictionary. span_end : torch.IntTensor , optional From an IndexField . This is one of the things we are trying to predict - the ending position of the answer with the passage. This is an inclusive token index. If this is given, we will compute a loss that gets included in the output dictionary. metadata : List[Dict[str, Any]] , optional If present, this should contain the question tokens, passage tokens, original passage text, and token offsets into the passage for each instance in the batch. The length of this list should be the batch size, and each dictionary should have the keys question_tokens , passage_tokens , original_passage , and token_offsets . ReturnsAn output dictionary consisting of: \u00b6 span_start_logits : torch.FloatTensor A tensor of shape (batch_size, passage_length) representing unnormalized log probabilities of the span start position. span_start_probs : torch.FloatTensor The result of softmax(span_start_logits) . span_end_logits : torch.FloatTensor A tensor of shape (batch_size, passage_length) representing unnormalized log probabilities of the span end position (inclusive). span_end_probs : torch.FloatTensor The result of softmax(span_end_logits) . best_span : torch.IntTensor The result of a constrained inference over span_start_logits and span_end_logits to find the most probable span. Shape is (batch_size, 2) and each offset is a token index. loss : torch.FloatTensor, optional A scalar loss to be optimised. best_span_str : List[str] If sufficient metadata was provided for the instances in the batch, we also return the string from the original passage that the model thinks is the best answer to the question. get_metrics # class QaNet ( Model ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] default_predictor # class QaNet ( Model ): | ... | default_predictor = \"reading_comprehension\"","title":"qanet"},{"location":"models/rc/models/qanet/#qanet","text":"@Model . register ( \"qanet\" ) @Model . register ( \"rc-qanet\" ) class QaNet ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | num_highway_layers : int , | phrase_layer : Seq2SeqEncoder , | matrix_attention_layer : MatrixAttention , | modeling_layer : Seq2SeqEncoder , | dropout_prob : float = 0.1 , | initializer : InitializerApplicator = InitializerApplicator (), | regularizer : Optional [ RegularizerApplicator ] = None | ) -> None This class implements Adams Wei Yu's QANet Model <https://openreview.net/forum?id=B14TlG-RW> _ for machine reading comprehension published at ICLR 2018. The overall architecture of QANet is very similar to BiDAF. The main difference is that QANet replaces the RNN encoder with CNN + self-attention. There are also some minor differences in the modeling layer and output layer.","title":"QaNet"},{"location":"models/rc/models/qanet/#forward","text":"class QaNet ( Model ): | ... | def forward ( | self , | question : Dict [ str , torch . LongTensor ], | passage : Dict [ str , torch . LongTensor ], | span_start : torch . IntTensor = None , | span_end : torch . IntTensor = None , | metadata : List [ Dict [ str , Any ]] = None | ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"models/rc/models/qanet/#get_metrics","text":"class QaNet ( Model ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"models/rc/models/qanet/#default_predictor","text":"class QaNet ( Model ): | ... | default_predictor = \"reading_comprehension\"","title":"default_predictor"},{"location":"models/rc/models/transformer_qa/","text":"allennlp_models .rc .models .transformer_qa [SOURCE] TransformerQA # @Model . register ( \"transformer_qa\" ) class TransformerQA ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | transformer_model_name : str = \"bert-base-cased\" , | ** kwargs | ) -> None Registered as \"transformer_qa\" , this class implements a reading comprehension model patterned after the proposed model in Devlin et al , with improvements borrowed from the SQuAD model in the transformers project. It predicts start tokens and end tokens with a linear layer on top of word piece embeddings. If you want to use this model on SQuAD datasets, you can use it with the TransformerSquadReader dataset reader, registered as \"transformer_squad\" . Note that the metrics that the model produces are calculated on a per-instance basis only. Since there could be more than one instance per question, these metrics are not the official numbers on either SQuAD task. To get official numbers for SQuAD v1.1, for example, you can run python -m allennlp_models.rc.tools.transformer_qa_eval Parameters \u00b6 vocab : Vocabulary transformer_model_name : str , optional (default = 'bert-base-cased' ) This model chooses the embedder according to this setting. You probably want to make sure this is set to the same thing as the reader. forward # class TransformerQA ( Model ): | ... | def forward ( | self , | question_with_context : Dict [ str , Dict [ str , torch . LongTensor ]], | context_span : torch . IntTensor , | cls_index : torch . LongTensor = None , | answer_span : torch . IntTensor = None , | metadata : List [ Dict [ str , Any ]] = None | ) -> Dict [ str , torch . Tensor ] Parameters \u00b6 question_with_context : Dict[str, torch.LongTensor] From a TextField . The model assumes that this text field contains the context followed by the question. It further assumes that the tokens have type ids set such that any token that can be part of the answer (i.e., tokens from the context) has type id 0, and any other token (including [CLS] and [SEP] ) has type id 1. context_span : torch.IntTensor From a SpanField . This marks the span of word pieces in question from which answers can come. cls_index : torch.LongTensor , optional A tensor of shape (batch_size,) that provides the index of the [CLS] token in the question_with_context for each instance. This is needed because the [CLS] token is used to indicate that the question is impossible. If this is None , it's assumed that the [CLS] token is at index 0 for each instance in the batch. answer_span : torch.IntTensor , optional From a SpanField . This is the thing we are trying to predict - the span of text that marks the answer. If given, we compute a loss that gets included in the output directory. metadata : List[Dict[str, Any]] , optional If present, this should contain the question id, and the original texts of context, question, tokenized version of both, and a list of possible answers. The length of the metadata list should be the batch size, and each dictionary should have the keys id , question , context , question_tokens , context_tokens , and answers . Returns \u00b6 Dict[str, torch.Tensor] : An output dictionary with the following fields: span_start_logits ( torch.FloatTensor ) : A tensor of shape (batch_size, passage_length) representing unnormalized log probabilities of the span start position. span_end_logits ( torch.FloatTensor ) : A tensor of shape (batch_size, passage_length) representing unnormalized log probabilities of the span end position (inclusive). best_span_scores ( torch.FloatTensor ) : The score for each of the best spans. loss ( torch.FloatTensor , optional) : A scalar loss to be optimised, evaluated against answer_span . best_span ( torch.IntTensor , optional) : Provided when not in train mode and sufficient metadata given for the instance. The result of a constrained inference over span_start_logits and span_end_logits to find the most probable span. Shape is (batch_size, 2) and each offset is a token index, unless the best span for an instance was predicted to be the [CLS] token, in which case the span will be (-1, -1). best_span_str ( List[str] , optional) : Provided when not in train mode and sufficient metadata given for the instance. This is the string from the original passage that the model thinks is the best answer to the question. get_metrics # class TransformerQA ( Model ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] default_predictor # class TransformerQA ( Model ): | ... | default_predictor = \"transformer_qa\"","title":"transformer_qa"},{"location":"models/rc/models/transformer_qa/#transformerqa","text":"@Model . register ( \"transformer_qa\" ) class TransformerQA ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | transformer_model_name : str = \"bert-base-cased\" , | ** kwargs | ) -> None Registered as \"transformer_qa\" , this class implements a reading comprehension model patterned after the proposed model in Devlin et al , with improvements borrowed from the SQuAD model in the transformers project. It predicts start tokens and end tokens with a linear layer on top of word piece embeddings. If you want to use this model on SQuAD datasets, you can use it with the TransformerSquadReader dataset reader, registered as \"transformer_squad\" . Note that the metrics that the model produces are calculated on a per-instance basis only. Since there could be more than one instance per question, these metrics are not the official numbers on either SQuAD task. To get official numbers for SQuAD v1.1, for example, you can run python -m allennlp_models.rc.tools.transformer_qa_eval","title":"TransformerQA"},{"location":"models/rc/models/transformer_qa/#forward","text":"class TransformerQA ( Model ): | ... | def forward ( | self , | question_with_context : Dict [ str , Dict [ str , torch . LongTensor ]], | context_span : torch . IntTensor , | cls_index : torch . LongTensor = None , | answer_span : torch . IntTensor = None , | metadata : List [ Dict [ str , Any ]] = None | ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"models/rc/models/transformer_qa/#get_metrics","text":"class TransformerQA ( Model ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"models/rc/models/transformer_qa/#default_predictor","text":"class TransformerQA ( Model ): | ... | default_predictor = \"transformer_qa\"","title":"default_predictor"},{"location":"models/rc/models/utils/","text":"allennlp_models .rc .models .utils [SOURCE] get_best_span # def get_best_span ( span_start_logits : torch . Tensor , span_end_logits : torch . Tensor ) -> torch . Tensor This acts the same as the static method BidirectionalAttentionFlow.get_best_span() in allennlp/models/reading_comprehension/bidaf.py . We keep it here so that users can directly import this function without the class. We call the inputs \"logits\" - they could either be unnormalized logits or normalized log probabilities. A log_softmax operation is a constant shifting of the entire logit vector, so taking an argmax over either one gives the same result. replace_masked_values_with_big_negative_number # def replace_masked_values_with_big_negative_number ( x : torch . Tensor , mask : torch . Tensor ) Replace the masked values in a tensor something really negative so that they won't affect a max operation.","title":"utils"},{"location":"models/rc/models/utils/#get_best_span","text":"def get_best_span ( span_start_logits : torch . Tensor , span_end_logits : torch . Tensor ) -> torch . Tensor This acts the same as the static method BidirectionalAttentionFlow.get_best_span() in allennlp/models/reading_comprehension/bidaf.py . We keep it here so that users can directly import this function without the class. We call the inputs \"logits\" - they could either be unnormalized logits or normalized log probabilities. A log_softmax operation is a constant shifting of the entire logit vector, so taking an argmax over either one gives the same result.","title":"get_best_span"},{"location":"models/rc/models/utils/#replace_masked_values_with_big_negative_number","text":"def replace_masked_values_with_big_negative_number ( x : torch . Tensor , mask : torch . Tensor ) Replace the masked values in a tensor something really negative so that they won't affect a max operation.","title":"replace_masked_values_with_big_negative_number"},{"location":"models/rc/modules/seq2seq_encoders/multi_head_self_attention/","text":"allennlp_models .rc .modules .seq2seq_encoders .multi_head_self_attention [SOURCE] MultiHeadSelfAttention # @Seq2SeqEncoder . register ( \"multi_head_self_attention\" , exist_ok = True ) class MultiHeadSelfAttention ( Seq2SeqEncoder ): | def __init__ ( | self , | num_heads : int , | input_dim : int , | attention_dim : int , | values_dim : int , | output_projection_dim : int = None , | attention_dropout_prob : float = 0.1 | ) -> None This class implements the key-value scaled dot product attention mechanism detailed in the paper Attention is all you Need . The attention mechanism is a weighted sum of a projection V of the inputs, with respect to the scaled, normalised dot product of Q and K, which are also both linear projections of the input. This procedure is repeated for each attention head, using different parameters. Parameters \u00b6 num_heads : int The number of attention heads to use. input_dim : int The size of the last dimension of the input tensor. attention_dim int , required. The total dimension of the query and key projections which comprise the dot product attention function. Must be divisible by num_heads . values_dim : int The total dimension which the input is projected to for representing the values, which are combined using the attention. Must be divisible by num_heads . output_projection_dim : int , optional (default = None ) The dimensionality of the final output projection. If this is not passed explicitly, the projection has size input_size . attention_dropout_prob : float , optional (default = 0.1 ) The dropout probability applied to the normalised attention distributions. get_input_dim # class MultiHeadSelfAttention ( Seq2SeqEncoder ): | ... | def get_input_dim ( self ) get_output_dim # class MultiHeadSelfAttention ( Seq2SeqEncoder ): | ... | def get_output_dim ( self ) is_bidirectional # class MultiHeadSelfAttention ( Seq2SeqEncoder ): | ... | @overrides | def is_bidirectional ( self ) forward # class MultiHeadSelfAttention ( Seq2SeqEncoder ): | ... | @overrides | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor = None | ) -> torch . FloatTensor Parameters \u00b6 inputs : torch.FloatTensor A tensor of shape (batch_size, timesteps, input_dim) mask : torch.BoolTensor , optional (default = None ) A tensor of shape (batch_size, timesteps). Returns \u00b6 A tensor of shape (batch_size, timesteps, output_projection_dim) , where output_projection_dim = input_dim by default.","title":"multi_head_self_attention"},{"location":"models/rc/modules/seq2seq_encoders/multi_head_self_attention/#multiheadselfattention","text":"@Seq2SeqEncoder . register ( \"multi_head_self_attention\" , exist_ok = True ) class MultiHeadSelfAttention ( Seq2SeqEncoder ): | def __init__ ( | self , | num_heads : int , | input_dim : int , | attention_dim : int , | values_dim : int , | output_projection_dim : int = None , | attention_dropout_prob : float = 0.1 | ) -> None This class implements the key-value scaled dot product attention mechanism detailed in the paper Attention is all you Need . The attention mechanism is a weighted sum of a projection V of the inputs, with respect to the scaled, normalised dot product of Q and K, which are also both linear projections of the input. This procedure is repeated for each attention head, using different parameters.","title":"MultiHeadSelfAttention"},{"location":"models/rc/modules/seq2seq_encoders/multi_head_self_attention/#get_input_dim","text":"class MultiHeadSelfAttention ( Seq2SeqEncoder ): | ... | def get_input_dim ( self )","title":"get_input_dim"},{"location":"models/rc/modules/seq2seq_encoders/multi_head_self_attention/#get_output_dim","text":"class MultiHeadSelfAttention ( Seq2SeqEncoder ): | ... | def get_output_dim ( self )","title":"get_output_dim"},{"location":"models/rc/modules/seq2seq_encoders/multi_head_self_attention/#is_bidirectional","text":"class MultiHeadSelfAttention ( Seq2SeqEncoder ): | ... | @overrides | def is_bidirectional ( self )","title":"is_bidirectional"},{"location":"models/rc/modules/seq2seq_encoders/multi_head_self_attention/#forward","text":"class MultiHeadSelfAttention ( Seq2SeqEncoder ): | ... | @overrides | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor = None | ) -> torch . FloatTensor","title":"forward"},{"location":"models/rc/modules/seq2seq_encoders/qanet_encoder/","text":"allennlp_models .rc .modules .seq2seq_encoders .qanet_encoder [SOURCE] QaNetEncoder # @Seq2SeqEncoder . register ( \"qanet_encoder\" , exist_ok = True ) class QaNetEncoder ( Seq2SeqEncoder ): | def __init__ ( | self , | input_dim : int , | hidden_dim : int , | attention_projection_dim : int , | feedforward_hidden_dim : int , | num_blocks : int , | num_convs_per_block : int , | conv_kernel_size : int , | num_attention_heads : int , | use_positional_encoding : bool = True , | dropout_prob : float = 0.1 , | layer_dropout_undecayed_prob : float = 0.1 , | attention_dropout_prob : float = 0 | ) -> None Stack multiple QANetEncoderBlock into one sequence encoder. Parameters \u00b6 input_dim : int The input dimension of the encoder. hidden_dim : int The hidden dimension used for convolution output channels, multi-head attention output and the final output of feedforward layer. attention_projection_dim : int The dimension of the linear projections for the self-attention layers. feedforward_hidden_dim : int The middle dimension of the FeedForward network. The input and output dimensions are fixed to ensure sizes match up for the self attention layers. num_blocks : int The number of stacked encoder blocks. num_convs_per_block : int The number of convolutions in each block. conv_kernel_size : int The kernel size for convolution. num_attention_heads : int The number of attention heads to use per layer. use_positional_encoding : bool , optional (default = True ) Whether to add sinusoidal frequencies to the input tensor. This is strongly recommended, as without this feature, the self attention layers have no idea of absolute or relative position (as they are just computing pairwise similarity between vectors of elements), which can be important features for many tasks. dropout_prob : float , optional (default = 0.1 ) The dropout probability for the feedforward network. layer_dropout_undecayed_prob : float , optional (default = 0.1 ) The initial dropout probability for layer dropout, and this might decay w.r.t the depth of the layer. For each mini-batch, the convolution/attention/ffn sublayer is stochastically dropped according to its layer dropout probability. attention_dropout_prob : float , optional (default = 0.0 ) The dropout probability for the attention distributions in the attention layer. get_input_dim # class QaNetEncoder ( Seq2SeqEncoder ): | ... | @overrides | def get_input_dim ( self ) -> int get_output_dim # class QaNetEncoder ( Seq2SeqEncoder ): | ... | @overrides | def get_output_dim ( self ) -> int is_bidirectional # class QaNetEncoder ( Seq2SeqEncoder ): | ... | @overrides | def is_bidirectional ( self ) -> bool forward # class QaNetEncoder ( Seq2SeqEncoder ): | ... | @overrides | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor = None | ) -> torch . Tensor QaNetEncoderBlock # @Seq2SeqEncoder . register ( \"qanet_encoder_block\" , exist_ok = True ) class QaNetEncoderBlock ( Seq2SeqEncoder ): | def __init__ ( | self , | input_dim : int , | hidden_dim : int , | attention_projection_dim : int , | feedforward_hidden_dim : int , | num_convs : int , | conv_kernel_size : int , | num_attention_heads : int , | use_positional_encoding : bool = True , | dropout_prob : float = 0.1 , | layer_dropout_undecayed_prob : float = 0.1 , | attention_dropout_prob : float = 0 | ) -> None Implements the encoder block described in QANet: Combining Local Convolution with Global Self-attention for Reading Comprehension . One encoder block mainly contains 4 parts: 1. Add position embedding. 2. Several depthwise seperable convolutions. 3. Multi-headed self attention, which uses 2 learnt linear projections to perform a dot-product similarity between every pair of elements scaled by the square root of the sequence length. 4. A two-layer FeedForward network. Parameters \u00b6 input_dim : int The input dimension of the encoder. hidden_dim : int The hidden dimension used for convolution output channels, multi-head attention output and the final output of feedforward layer. attention_projection_dim : int The dimension of the linear projections for the self-attention layers. feedforward_hidden_dim : int The middle dimension of the FeedForward network. The input and output dimensions are fixed to ensure sizes match up for the self attention layers. num_convs : int The number of convolutions in each block. conv_kernel_size : int The kernel size for convolution. num_attention_heads : int The number of attention heads to use per layer. use_positional_encoding : bool , optional (default = True ) Whether to add sinusoidal frequencies to the input tensor. This is strongly recommended, as without this feature, the self attention layers have no idea of absolute or relative position (as they are just computing pairwise similarity between vectors of elements), which can be important features for many tasks. dropout_prob : float , optional (default = 0.1 ) The dropout probability for the feedforward network. layer_dropout_undecayed_prob : float , optional (default = 0.1 ) The initial dropout probability for layer dropout, and this might decay w.r.t the depth of the layer. For each mini-batch, the convolution/attention/ffn sublayer is randomly dropped according to its layer dropout probability. attention_dropout_prob : float , optional (default = 0.0 ) The dropout probability for the attention distributions in the attention layer. get_input_dim # class QaNetEncoderBlock ( Seq2SeqEncoder ): | ... | @overrides | def get_input_dim ( self ) -> int get_output_dim # class QaNetEncoderBlock ( Seq2SeqEncoder ): | ... | @overrides | def get_output_dim ( self ) -> int is_bidirectional # class QaNetEncoderBlock ( Seq2SeqEncoder ): | ... | @overrides | def is_bidirectional ( self ) forward # class QaNetEncoderBlock ( Seq2SeqEncoder ): | ... | @overrides | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor = None | ) -> torch . Tensor","title":"qanet_encoder"},{"location":"models/rc/modules/seq2seq_encoders/qanet_encoder/#qanetencoder","text":"@Seq2SeqEncoder . register ( \"qanet_encoder\" , exist_ok = True ) class QaNetEncoder ( Seq2SeqEncoder ): | def __init__ ( | self , | input_dim : int , | hidden_dim : int , | attention_projection_dim : int , | feedforward_hidden_dim : int , | num_blocks : int , | num_convs_per_block : int , | conv_kernel_size : int , | num_attention_heads : int , | use_positional_encoding : bool = True , | dropout_prob : float = 0.1 , | layer_dropout_undecayed_prob : float = 0.1 , | attention_dropout_prob : float = 0 | ) -> None Stack multiple QANetEncoderBlock into one sequence encoder.","title":"QaNetEncoder"},{"location":"models/rc/modules/seq2seq_encoders/qanet_encoder/#get_input_dim","text":"class QaNetEncoder ( Seq2SeqEncoder ): | ... | @overrides | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"models/rc/modules/seq2seq_encoders/qanet_encoder/#get_output_dim","text":"class QaNetEncoder ( Seq2SeqEncoder ): | ... | @overrides | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"models/rc/modules/seq2seq_encoders/qanet_encoder/#is_bidirectional","text":"class QaNetEncoder ( Seq2SeqEncoder ): | ... | @overrides | def is_bidirectional ( self ) -> bool","title":"is_bidirectional"},{"location":"models/rc/modules/seq2seq_encoders/qanet_encoder/#forward","text":"class QaNetEncoder ( Seq2SeqEncoder ): | ... | @overrides | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor = None | ) -> torch . Tensor","title":"forward"},{"location":"models/rc/modules/seq2seq_encoders/qanet_encoder/#qanetencoderblock","text":"@Seq2SeqEncoder . register ( \"qanet_encoder_block\" , exist_ok = True ) class QaNetEncoderBlock ( Seq2SeqEncoder ): | def __init__ ( | self , | input_dim : int , | hidden_dim : int , | attention_projection_dim : int , | feedforward_hidden_dim : int , | num_convs : int , | conv_kernel_size : int , | num_attention_heads : int , | use_positional_encoding : bool = True , | dropout_prob : float = 0.1 , | layer_dropout_undecayed_prob : float = 0.1 , | attention_dropout_prob : float = 0 | ) -> None Implements the encoder block described in QANet: Combining Local Convolution with Global Self-attention for Reading Comprehension . One encoder block mainly contains 4 parts: 1. Add position embedding. 2. Several depthwise seperable convolutions. 3. Multi-headed self attention, which uses 2 learnt linear projections to perform a dot-product similarity between every pair of elements scaled by the square root of the sequence length. 4. A two-layer FeedForward network.","title":"QaNetEncoderBlock"},{"location":"models/rc/modules/seq2seq_encoders/qanet_encoder/#get_input_dim_1","text":"class QaNetEncoderBlock ( Seq2SeqEncoder ): | ... | @overrides | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"models/rc/modules/seq2seq_encoders/qanet_encoder/#get_output_dim_1","text":"class QaNetEncoderBlock ( Seq2SeqEncoder ): | ... | @overrides | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"models/rc/modules/seq2seq_encoders/qanet_encoder/#is_bidirectional_1","text":"class QaNetEncoderBlock ( Seq2SeqEncoder ): | ... | @overrides | def is_bidirectional ( self )","title":"is_bidirectional"},{"location":"models/rc/modules/seq2seq_encoders/qanet_encoder/#forward_1","text":"class QaNetEncoderBlock ( Seq2SeqEncoder ): | ... | @overrides | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor = None | ) -> torch . Tensor","title":"forward"},{"location":"models/rc/modules/seq2seq_encoders/stacked_self_attention/","text":"allennlp_models .rc .modules .seq2seq_encoders .stacked_self_attention [SOURCE] StackedSelfAttentionEncoder # @Seq2SeqEncoder . register ( \"stacked_self_attention\" ) class StackedSelfAttentionEncoder ( Seq2SeqEncoder ): | def __init__ ( | self , | input_dim : int , | hidden_dim : int , | projection_dim : int , | feedforward_hidden_dim : int , | num_layers : int , | num_attention_heads : int , | use_positional_encoding : bool = True , | dropout_prob : float = 0.1 , | residual_dropout_prob : float = 0.2 , | attention_dropout_prob : float = 0.1 | ) -> None Implements a stacked self-attention encoder similar to, but different from, the Transformer architecture in Attention is all you Need . This encoder combines 3 layers in a 'block': A 2 layer FeedForward network. Multi-headed self attention, which uses 2 learnt linear projections to perform a dot-product similarity between every pair of elements scaled by the square root of the sequence length. Layer Normalisation. These are then stacked into num_layers layers. Parameters \u00b6 input_dim : int The input dimension of the encoder. hidden_dim : int The hidden dimension used for the input to self attention layers and the output from the feedforward layers. projection_dim : int The dimension of the linear projections for the self-attention layers. feedforward_hidden_dim : int The middle dimension of the FeedForward network. The input and output dimensions are fixed to ensure sizes match up for the self attention layers. num_layers : int The number of stacked self attention -> feedforward -> layer normalisation blocks. num_attention_heads : int The number of attention heads to use per layer. use_positional_encoding : bool , optional (default = True ) Whether to add sinusoidal frequencies to the input tensor. This is strongly recommended, as without this feature, the self attention layers have no idea of absolute or relative position (as they are just computing pairwise similarity between vectors of elements), which can be important features for many tasks. dropout_prob : float , optional (default = 0.1 ) The dropout probability for the feedforward network. residual_dropout_prob : float , optional (default = 0.2 ) The dropout probability for the residual connections. attention_dropout_prob : float , optional (default = 0.1 ) The dropout probability for the attention distributions in each attention layer. get_input_dim # class StackedSelfAttentionEncoder ( Seq2SeqEncoder ): | ... | @overrides | def get_input_dim ( self ) -> int get_output_dim # class StackedSelfAttentionEncoder ( Seq2SeqEncoder ): | ... | @overrides | def get_output_dim ( self ) -> int is_bidirectional # class StackedSelfAttentionEncoder ( Seq2SeqEncoder ): | ... | @overrides | def is_bidirectional ( self ) forward # class StackedSelfAttentionEncoder ( Seq2SeqEncoder ): | ... | @overrides | def forward ( self , inputs : torch . Tensor , mask : torch . BoolTensor )","title":"stacked_self_attention"},{"location":"models/rc/modules/seq2seq_encoders/stacked_self_attention/#stackedselfattentionencoder","text":"@Seq2SeqEncoder . register ( \"stacked_self_attention\" ) class StackedSelfAttentionEncoder ( Seq2SeqEncoder ): | def __init__ ( | self , | input_dim : int , | hidden_dim : int , | projection_dim : int , | feedforward_hidden_dim : int , | num_layers : int , | num_attention_heads : int , | use_positional_encoding : bool = True , | dropout_prob : float = 0.1 , | residual_dropout_prob : float = 0.2 , | attention_dropout_prob : float = 0.1 | ) -> None Implements a stacked self-attention encoder similar to, but different from, the Transformer architecture in Attention is all you Need . This encoder combines 3 layers in a 'block': A 2 layer FeedForward network. Multi-headed self attention, which uses 2 learnt linear projections to perform a dot-product similarity between every pair of elements scaled by the square root of the sequence length. Layer Normalisation. These are then stacked into num_layers layers.","title":"StackedSelfAttentionEncoder"},{"location":"models/rc/modules/seq2seq_encoders/stacked_self_attention/#get_input_dim","text":"class StackedSelfAttentionEncoder ( Seq2SeqEncoder ): | ... | @overrides | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"models/rc/modules/seq2seq_encoders/stacked_self_attention/#get_output_dim","text":"class StackedSelfAttentionEncoder ( Seq2SeqEncoder ): | ... | @overrides | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"models/rc/modules/seq2seq_encoders/stacked_self_attention/#is_bidirectional","text":"class StackedSelfAttentionEncoder ( Seq2SeqEncoder ): | ... | @overrides | def is_bidirectional ( self )","title":"is_bidirectional"},{"location":"models/rc/modules/seq2seq_encoders/stacked_self_attention/#forward","text":"class StackedSelfAttentionEncoder ( Seq2SeqEncoder ): | ... | @overrides | def forward ( self , inputs : torch . Tensor , mask : torch . BoolTensor )","title":"forward"},{"location":"models/rc/predictors/bidaf/","text":"allennlp_models .rc .predictors .bidaf [SOURCE] ReadingComprehensionPredictor # @Predictor . register ( \"reading_comprehension\" ) class ReadingComprehensionPredictor ( Predictor ) Predictor for the allennlp_rc.models.bidaf.BidirectionalAttentionFlow model, and any other model that takes a question and passage as input. predict # class ReadingComprehensionPredictor ( Predictor ): | ... | def predict ( self , question : str , passage : str ) -> JsonDict Make a machine comprehension prediction on the supplied input. See https://rajpurkar.github.io/SQuAD-explorer/ for more information about the machine comprehension task. Parametersquestion : ``str`` \u00b6 A question about the content in the supplied paragraph. The question must be answerable by a span in the paragraph. passage : str A paragraph of information relevant to the question. ReturnsA dictionary that represents the prediction made by the system. The answer string will be under the \u00b6 \"best_span_str\" key. predictions_to_labeled_instances # class ReadingComprehensionPredictor ( Predictor ): | ... | @overrides | def predictions_to_labeled_instances ( | self , | instance : Instance , | outputs : Dict [ str , numpy . ndarray ] | ) -> List [ Instance ]","title":"bidaf"},{"location":"models/rc/predictors/bidaf/#readingcomprehensionpredictor","text":"@Predictor . register ( \"reading_comprehension\" ) class ReadingComprehensionPredictor ( Predictor ) Predictor for the allennlp_rc.models.bidaf.BidirectionalAttentionFlow model, and any other model that takes a question and passage as input.","title":"ReadingComprehensionPredictor"},{"location":"models/rc/predictors/bidaf/#predict","text":"class ReadingComprehensionPredictor ( Predictor ): | ... | def predict ( self , question : str , passage : str ) -> JsonDict Make a machine comprehension prediction on the supplied input. See https://rajpurkar.github.io/SQuAD-explorer/ for more information about the machine comprehension task.","title":"predict"},{"location":"models/rc/predictors/bidaf/#predictions_to_labeled_instances","text":"class ReadingComprehensionPredictor ( Predictor ): | ... | @overrides | def predictions_to_labeled_instances ( | self , | instance : Instance , | outputs : Dict [ str , numpy . ndarray ] | ) -> List [ Instance ]","title":"predictions_to_labeled_instances"},{"location":"models/rc/predictors/dialog_qa/","text":"allennlp_models .rc .predictors .dialog_qa [SOURCE] DialogQAPredictor # @Predictor . register ( \"dialog_qa\" ) class DialogQAPredictor ( Predictor ): | def __init__ ( | self , | model : Model , | dataset_reader : DatasetReader , | language : str = \"en_core_web_sm\" | ) -> None predict # class DialogQAPredictor ( Predictor ): | ... | def predict ( self , jsonline : str ) -> JsonDict Make a dialog-style question answering prediction on the supplied input. The supplied input json must contain a list of question answer pairs, containing question, answer, yesno, followup, id as well as the context (passage). Parametersjsonline : ``str`` \u00b6 A json line that has the same format as the quac data file. ReturnsA dictionary that represents the prediction made by the system. The answer string will be under the \u00b6 \"best_span_str\" key.","title":"dialog_qa"},{"location":"models/rc/predictors/dialog_qa/#dialogqapredictor","text":"@Predictor . register ( \"dialog_qa\" ) class DialogQAPredictor ( Predictor ): | def __init__ ( | self , | model : Model , | dataset_reader : DatasetReader , | language : str = \"en_core_web_sm\" | ) -> None","title":"DialogQAPredictor"},{"location":"models/rc/predictors/dialog_qa/#predict","text":"class DialogQAPredictor ( Predictor ): | ... | def predict ( self , jsonline : str ) -> JsonDict Make a dialog-style question answering prediction on the supplied input. The supplied input json must contain a list of question answer pairs, containing question, answer, yesno, followup, id as well as the context (passage).","title":"predict"},{"location":"models/rc/predictors/transformer_qa/","text":"allennlp_models .rc .predictors .transformer_qa [SOURCE] TransformerQAPredictor # @Predictor . register ( \"transformer_qa\" ) class TransformerQAPredictor ( Predictor ): | def __init__ ( | self , | model : Model , | dataset_reader : DatasetReader | ) -> None Predictor for the TransformerQA model, and any other model that takes a question and passage as input. predict # class TransformerQAPredictor ( Predictor ): | ... | def predict ( self , question : str , passage : str ) -> JsonDict Make a machine comprehension prediction on the supplied input. See https://rajpurkar.github.io/SQuAD-explorer/ for more information about the machine comprehension task. Parameters \u00b6 question : str A question about the content in the supplied paragraph. passage : str A paragraph of information relevant to the question. Returns \u00b6 JsonDict A dictionary that represents the prediction made by the system. The answer string will be under the \"best_span_str\" key. predict_json # class TransformerQAPredictor ( Predictor ): | ... | def predict_json ( self , inputs : JsonDict ) -> JsonDict predictions_to_labeled_instances # class TransformerQAPredictor ( Predictor ): | ... | @overrides | def predictions_to_labeled_instances ( | self , | instance : Instance , | outputs : Dict [ str , numpy . ndarray ] | ) -> List [ Instance ] predict_batch_json # class TransformerQAPredictor ( Predictor ): | ... | @overrides | def predict_batch_json ( self , inputs : List [ JsonDict ]) -> List [ JsonDict ] predict_batch_instance # class TransformerQAPredictor ( Predictor ): | ... | @overrides | def predict_batch_instance ( | self , | instances : List [ Instance ] | ) -> List [ JsonDict ]","title":"transformer_qa"},{"location":"models/rc/predictors/transformer_qa/#transformerqapredictor","text":"@Predictor . register ( \"transformer_qa\" ) class TransformerQAPredictor ( Predictor ): | def __init__ ( | self , | model : Model , | dataset_reader : DatasetReader | ) -> None Predictor for the TransformerQA model, and any other model that takes a question and passage as input.","title":"TransformerQAPredictor"},{"location":"models/rc/predictors/transformer_qa/#predict","text":"class TransformerQAPredictor ( Predictor ): | ... | def predict ( self , question : str , passage : str ) -> JsonDict Make a machine comprehension prediction on the supplied input. See https://rajpurkar.github.io/SQuAD-explorer/ for more information about the machine comprehension task.","title":"predict"},{"location":"models/rc/predictors/transformer_qa/#predict_json","text":"class TransformerQAPredictor ( Predictor ): | ... | def predict_json ( self , inputs : JsonDict ) -> JsonDict","title":"predict_json"},{"location":"models/rc/predictors/transformer_qa/#predictions_to_labeled_instances","text":"class TransformerQAPredictor ( Predictor ): | ... | @overrides | def predictions_to_labeled_instances ( | self , | instance : Instance , | outputs : Dict [ str , numpy . ndarray ] | ) -> List [ Instance ]","title":"predictions_to_labeled_instances"},{"location":"models/rc/predictors/transformer_qa/#predict_batch_json","text":"class TransformerQAPredictor ( Predictor ): | ... | @overrides | def predict_batch_json ( self , inputs : List [ JsonDict ]) -> List [ JsonDict ]","title":"predict_batch_json"},{"location":"models/rc/predictors/transformer_qa/#predict_batch_instance","text":"class TransformerQAPredictor ( Predictor ): | ... | @overrides | def predict_batch_instance ( | self , | instances : List [ Instance ] | ) -> List [ JsonDict ]","title":"predict_batch_instance"},{"location":"models/rc/tools/drop/","text":"allennlp_models .rc .tools .drop [SOURCE] !/usr/bin/python EXCLUDE # EXCLUDE = set ( string . punctuation ) get_metrics # def get_metrics ( predicted : Union [ str , List [ str ], Tuple [ str , ... ]], gold : Union [ str , List [ str ], Tuple [ str , ... ]] ) -> Tuple [ float , float ] Takes a predicted answer and a gold answer (that are both either a string or a list of strings), and returns exact match and the DROP F1 metric for the prediction. If you are writing a script for evaluating objects in memory (say, the output of predictions during validation, or while training), this is the function you want to call, after using answer_json_to_strings when reading the gold answer from the released data file. answer_json_to_strings # def answer_json_to_strings ( answer : Dict [ str , Any ] ) -> Tuple [ Tuple [ str , ... ], str ] Takes an answer JSON blob from the DROP data release and converts it into strings used for evaluation. evaluate_json # def evaluate_json ( annotations : Dict [ str , Any ], predicted_answers : Dict [ str , Any ] ) -> Tuple [ float , float ] Takes gold annotations and predicted answers and evaluates the predictions for each question in the gold annotations. Both JSON dictionaries must have query_id keys, which are used to match predictions to gold annotations (note that these are somewhat deep in the JSON for the gold annotations, but must be top-level keys in the predicted answers). The annotations are assumed to have the format of the dev set in the DROP data release. The predicted_answers JSON must be a dictionary keyed by query id, where the value is a string (or list of strings) that is the answer. evaluate_prediction_file # def evaluate_prediction_file ( prediction_path : str , gold_path : str , output_path : Optional [ str ] = None ) -> Tuple [ float , float ] Takes a prediction file and a gold file and evaluates the predictions for each question in the gold file. Both files must be json formatted and must have query_id keys, which are used to match predictions to gold annotations. The gold file is assumed to have the format of the dev set in the DROP data release. The prediction file must be a JSON dictionary keyed by query id, where the value is either a JSON dictionary with an \"answer\" key, or just a string (or list of strings) that is the answer. Writes a json with global_em and global_f1 metrics to file at the specified output path, unless None is passed as output path.","title":"drop"},{"location":"models/rc/tools/drop/#exclude","text":"EXCLUDE = set ( string . punctuation )","title":"EXCLUDE"},{"location":"models/rc/tools/drop/#get_metrics","text":"def get_metrics ( predicted : Union [ str , List [ str ], Tuple [ str , ... ]], gold : Union [ str , List [ str ], Tuple [ str , ... ]] ) -> Tuple [ float , float ] Takes a predicted answer and a gold answer (that are both either a string or a list of strings), and returns exact match and the DROP F1 metric for the prediction. If you are writing a script for evaluating objects in memory (say, the output of predictions during validation, or while training), this is the function you want to call, after using answer_json_to_strings when reading the gold answer from the released data file.","title":"get_metrics"},{"location":"models/rc/tools/drop/#answer_json_to_strings","text":"def answer_json_to_strings ( answer : Dict [ str , Any ] ) -> Tuple [ Tuple [ str , ... ], str ] Takes an answer JSON blob from the DROP data release and converts it into strings used for evaluation.","title":"answer_json_to_strings"},{"location":"models/rc/tools/drop/#evaluate_json","text":"def evaluate_json ( annotations : Dict [ str , Any ], predicted_answers : Dict [ str , Any ] ) -> Tuple [ float , float ] Takes gold annotations and predicted answers and evaluates the predictions for each question in the gold annotations. Both JSON dictionaries must have query_id keys, which are used to match predictions to gold annotations (note that these are somewhat deep in the JSON for the gold annotations, but must be top-level keys in the predicted answers). The annotations are assumed to have the format of the dev set in the DROP data release. The predicted_answers JSON must be a dictionary keyed by query id, where the value is a string (or list of strings) that is the answer.","title":"evaluate_json"},{"location":"models/rc/tools/drop/#evaluate_prediction_file","text":"def evaluate_prediction_file ( prediction_path : str , gold_path : str , output_path : Optional [ str ] = None ) -> Tuple [ float , float ] Takes a prediction file and a gold file and evaluates the predictions for each question in the gold file. Both files must be json formatted and must have query_id keys, which are used to match predictions to gold annotations. The gold file is assumed to have the format of the dev set in the DROP data release. The prediction file must be a JSON dictionary keyed by query id, where the value is either a JSON dictionary with an \"answer\" key, or just a string (or list of strings) that is the answer. Writes a json with global_em and global_f1 metrics to file at the specified output path, unless None is passed as output path.","title":"evaluate_prediction_file"},{"location":"models/rc/tools/narrativeqa/","text":"allennlp_models .rc .tools .narrativeqa [SOURCE] Evaluation script for NarrativeQA dataset. rouge_l_evaluator # rouge_l_evaluator = rouge . Rouge ( metrics = [ \"rouge-l\" ], max_n = 4 , limit_length = True , length_limit = 100 , ... bleu_1 # def bleu_1 ( p , g ) bleu_4 # def bleu_4 ( p , g ) meteor # def meteor ( p , g ) rouge_l # def rouge_l ( p , g ) metric_max_over_ground_truths # def metric_max_over_ground_truths ( metric_fn , prediction , ground_truths , tokenize = False ) get_metric_score # def get_metric_score ( prediction , ground_truths )","title":"narrativeqa"},{"location":"models/rc/tools/narrativeqa/#rouge_l_evaluator","text":"rouge_l_evaluator = rouge . Rouge ( metrics = [ \"rouge-l\" ], max_n = 4 , limit_length = True , length_limit = 100 , ...","title":"rouge_l_evaluator"},{"location":"models/rc/tools/narrativeqa/#bleu_1","text":"def bleu_1 ( p , g )","title":"bleu_1"},{"location":"models/rc/tools/narrativeqa/#bleu_4","text":"def bleu_4 ( p , g )","title":"bleu_4"},{"location":"models/rc/tools/narrativeqa/#meteor","text":"def meteor ( p , g )","title":"meteor"},{"location":"models/rc/tools/narrativeqa/#rouge_l","text":"def rouge_l ( p , g )","title":"rouge_l"},{"location":"models/rc/tools/narrativeqa/#metric_max_over_ground_truths","text":"def metric_max_over_ground_truths ( metric_fn , prediction , ground_truths , tokenize = False )","title":"metric_max_over_ground_truths"},{"location":"models/rc/tools/narrativeqa/#get_metric_score","text":"def get_metric_score ( prediction , ground_truths )","title":"get_metric_score"},{"location":"models/rc/tools/orb/","text":"allennlp_models .rc .tools .orb [SOURCE] Official evaluation script for ORB. Usage: python evaluation_script.py --dataset_file --prediction_file --metrics_output_file read_predictions # def read_predictions ( json_file ) read_labels # def read_labels ( jsonl_file ) compute_averages # def compute_averages ( all_metrics ) evaluate # def evaluate ( answers , predictions ) process_for_output # def process_for_output ( metrics )","title":"orb"},{"location":"models/rc/tools/orb/#read_predictions","text":"def read_predictions ( json_file )","title":"read_predictions"},{"location":"models/rc/tools/orb/#read_labels","text":"def read_labels ( jsonl_file )","title":"read_labels"},{"location":"models/rc/tools/orb/#compute_averages","text":"def compute_averages ( all_metrics )","title":"compute_averages"},{"location":"models/rc/tools/orb/#evaluate","text":"def evaluate ( answers , predictions )","title":"evaluate"},{"location":"models/rc/tools/orb/#process_for_output","text":"def process_for_output ( metrics )","title":"process_for_output"},{"location":"models/rc/tools/orb_utils/","text":"allennlp_models .rc .tools .orb_utils [SOURCE] get_metric_drop # def get_metric_drop ( predicted : str , ground_truths : List [ str ] ) -> Tuple [ float , float ] update_extractive_metrics # def update_extractive_metrics ( metrics , dataset_name , exact_match , f1 ) update_abstractive_metrics # def update_abstractive_metrics ( metrics , bleu_1_score , bleu_4_score , meteor_score , rouge_f , rouge_p , rouge_r ) evaluate_dataset # def evaluate_dataset ( dataset_name , prediction , ground_truths , metrics )","title":"orb_utils"},{"location":"models/rc/tools/orb_utils/#get_metric_drop","text":"def get_metric_drop ( predicted : str , ground_truths : List [ str ] ) -> Tuple [ float , float ]","title":"get_metric_drop"},{"location":"models/rc/tools/orb_utils/#update_extractive_metrics","text":"def update_extractive_metrics ( metrics , dataset_name , exact_match , f1 )","title":"update_extractive_metrics"},{"location":"models/rc/tools/orb_utils/#update_abstractive_metrics","text":"def update_abstractive_metrics ( metrics , bleu_1_score , bleu_4_score , meteor_score , rouge_f , rouge_p , rouge_r )","title":"update_abstractive_metrics"},{"location":"models/rc/tools/orb_utils/#evaluate_dataset","text":"def evaluate_dataset ( dataset_name , prediction , ground_truths , metrics )","title":"evaluate_dataset"},{"location":"models/rc/tools/quoref/","text":"allennlp_models .rc .tools .quoref [SOURCE] This evaluation script relies heavily on the one for DROP ( allennlp/tools/drop_eval.py ). We need a separate script for Quoref only because the data formats are slightly different. evaluate_json # def evaluate_json ( annotations : Dict [ str , Any ], predicted_answers : Dict [ str , Any ] ) -> Tuple [ float , float ] Takes gold annotations and predicted answers and evaluates the predictions for each question in the gold annotations. Both JSON dictionaries must have query_id keys, which are used to match predictions to gold annotations. The predicted_answers JSON must be a dictionary keyed by query id, where the value is a list of strings (or just one string) that is the answer. The annotations are assumed to have either the format of the dev set in the Quoref data release, or the same format as the predicted answers file. evaluate_prediction_file # def evaluate_prediction_file ( prediction_path : str , gold_path : str , output_path : Optional [ str ] = None ) -> Tuple [ float , float ] Takes a prediction file and a gold file and evaluates the predictions for each question in the gold file. Both files must be json formatted and must have query_id keys, which are used to match predictions to gold annotations. Writes a json with global_em and global_f1 metrics to file at the specified output path, unless None is passed as output path.","title":"quoref"},{"location":"models/rc/tools/quoref/#evaluate_json","text":"def evaluate_json ( annotations : Dict [ str , Any ], predicted_answers : Dict [ str , Any ] ) -> Tuple [ float , float ] Takes gold annotations and predicted answers and evaluates the predictions for each question in the gold annotations. Both JSON dictionaries must have query_id keys, which are used to match predictions to gold annotations. The predicted_answers JSON must be a dictionary keyed by query id, where the value is a list of strings (or just one string) that is the answer. The annotations are assumed to have either the format of the dev set in the Quoref data release, or the same format as the predicted answers file.","title":"evaluate_json"},{"location":"models/rc/tools/quoref/#evaluate_prediction_file","text":"def evaluate_prediction_file ( prediction_path : str , gold_path : str , output_path : Optional [ str ] = None ) -> Tuple [ float , float ] Takes a prediction file and a gold file and evaluates the predictions for each question in the gold file. Both files must be json formatted and must have query_id keys, which are used to match predictions to gold annotations. Writes a json with global_em and global_f1 metrics to file at the specified output path, unless None is passed as output path.","title":"evaluate_prediction_file"},{"location":"models/rc/tools/squad/","text":"allennlp_models .rc .tools .squad [SOURCE] Functions taken from the official evaluation script for SQuAD version 2.0. make_qid_to_has_ans # def make_qid_to_has_ans ( dataset ) normalize_answer # def normalize_answer ( s ) Lower text and remove punctuation, articles and extra whitespace. get_tokens # def get_tokens ( s ) compute_exact # def compute_exact ( a_pred : str , a_gold : str ) -> int compute_f1 # def compute_f1 ( a_pred : str , a_gold : str ) -> float metric_max_over_ground_truths # def metric_max_over_ground_truths ( metric_fn : Callable [[ _P , _G ], _T ], prediction : _P , ground_truths : Sequence [ _G ] ) -> _T get_metric_score # def get_metric_score ( prediction : str , gold_answers : Sequence [ str ] ) -> Tuple [ int , float ]","title":"squad"},{"location":"models/rc/tools/squad/#make_qid_to_has_ans","text":"def make_qid_to_has_ans ( dataset )","title":"make_qid_to_has_ans"},{"location":"models/rc/tools/squad/#normalize_answer","text":"def normalize_answer ( s ) Lower text and remove punctuation, articles and extra whitespace.","title":"normalize_answer"},{"location":"models/rc/tools/squad/#get_tokens","text":"def get_tokens ( s )","title":"get_tokens"},{"location":"models/rc/tools/squad/#compute_exact","text":"def compute_exact ( a_pred : str , a_gold : str ) -> int","title":"compute_exact"},{"location":"models/rc/tools/squad/#compute_f1","text":"def compute_f1 ( a_pred : str , a_gold : str ) -> float","title":"compute_f1"},{"location":"models/rc/tools/squad/#metric_max_over_ground_truths","text":"def metric_max_over_ground_truths ( metric_fn : Callable [[ _P , _G ], _T ], prediction : _P , ground_truths : Sequence [ _G ] ) -> _T","title":"metric_max_over_ground_truths"},{"location":"models/rc/tools/squad/#get_metric_score","text":"def get_metric_score ( prediction : str , gold_answers : Sequence [ str ] ) -> Tuple [ int , float ]","title":"get_metric_score"},{"location":"models/rc/tools/transformer_qa_eval/","text":"allennlp_models .rc .tools .transformer_qa_eval [SOURCE]","title":"transformer_qa_eval"},{"location":"models/structured_prediction/dataset_readers/penn_tree_bank/","text":"allennlp_models .structured_prediction .dataset_readers .penn_tree_bank [SOURCE] PTB_PARENTHESES # PTB_PARENTHESES = { \"-LRB-\" : \"(\" , \"-RRB-\" : \")\" , \"-LCB-\" : \"{\" , \"-RCB-\" : \"}\" , \"-LSB-\" : \"[\" , \"-RS ... PennTreeBankConstituencySpanDatasetReader # @DatasetReader . register ( \"ptb_trees\" ) class PennTreeBankConstituencySpanDatasetReader ( DatasetReader ): | def __init__ ( | self , | token_indexers : Dict [ str , TokenIndexer ] = None , | use_pos_tags : bool = True , | convert_parentheses : bool = False , | label_namespace_prefix : str = \"\" , | pos_label_namespace : str = \"pos\" , | ** kwargs | ) -> None Reads constituency parses from the WSJ part of the Penn Tree Bank from the LDC. This DatasetReader is designed for use with a span labelling model, so it enumerates all possible spans in the sentence and returns them, along with gold labels for the relevant spans present in a gold tree, if provided. Parameters \u00b6 token_indexers : Dict[str, TokenIndexer] , optional (default = {\"tokens\": SingleIdTokenIndexer()} ) We use this to define the input representation for the text. See TokenIndexer . Note that the output tags will always correspond to single token IDs based on how they are pre-tokenised in the data file. use_pos_tags : bool , optional (default = True ) Whether or not the instance should contain gold POS tags as a field. convert_parentheses : bool , optional (default = False ) Whether or not to convert special PTB parentheses tokens (e.g., \"-LRB-\") to the corresponding parentheses tokens (i.e., \"(\"). label_namespace_prefix : str , optional (default = \"\" ) Prefix used for the label namespace. The span_labels will use namespace label_namespace_prefix + 'labels' , and if using POS tags their namespace is label_namespace_prefix + pos_label_namespace . pos_label_namespace : str , optional (default = \"pos\" ) The POS tag namespace is label_namespace_prefix + pos_label_namespace . text_to_instance # class PennTreeBankConstituencySpanDatasetReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | tokens : List [ str ], | pos_tags : List [ str ] = None , | gold_tree : Tree = None | ) -> Instance We take pre-tokenized input here, because we don't have a tokenizer in this class. Parameters \u00b6 tokens : List[str] The tokens in a given sentence. pos_tags : List[str] , optional (default = None ) The POS tags for the words in the sentence. gold_tree : Tree , optional (default = None ) The gold parse tree to create span labels from. Returns \u00b6 An Instance containing the following fields: tokens : TextField The tokens in the sentence. pos_tags : SequenceLabelField The POS tags of the words in the sentence. Only returned if use_pos_tags is True spans : ListField[SpanField] A ListField containing all possible subspans of the sentence. span_labels : SequenceLabelField , optional. The constituency tags for each of the possible spans, with respect to a gold parse tree. If a span is not contained within the tree, a span will have a NO-LABEL label. gold_tree : MetadataField(Tree) The gold NLTK parse tree for use in evaluation.","title":"penn_tree_bank"},{"location":"models/structured_prediction/dataset_readers/penn_tree_bank/#ptb_parentheses","text":"PTB_PARENTHESES = { \"-LRB-\" : \"(\" , \"-RRB-\" : \")\" , \"-LCB-\" : \"{\" , \"-RCB-\" : \"}\" , \"-LSB-\" : \"[\" , \"-RS ...","title":"PTB_PARENTHESES"},{"location":"models/structured_prediction/dataset_readers/penn_tree_bank/#penntreebankconstituencyspandatasetreader","text":"@DatasetReader . register ( \"ptb_trees\" ) class PennTreeBankConstituencySpanDatasetReader ( DatasetReader ): | def __init__ ( | self , | token_indexers : Dict [ str , TokenIndexer ] = None , | use_pos_tags : bool = True , | convert_parentheses : bool = False , | label_namespace_prefix : str = \"\" , | pos_label_namespace : str = \"pos\" , | ** kwargs | ) -> None Reads constituency parses from the WSJ part of the Penn Tree Bank from the LDC. This DatasetReader is designed for use with a span labelling model, so it enumerates all possible spans in the sentence and returns them, along with gold labels for the relevant spans present in a gold tree, if provided.","title":"PennTreeBankConstituencySpanDatasetReader"},{"location":"models/structured_prediction/dataset_readers/penn_tree_bank/#text_to_instance","text":"class PennTreeBankConstituencySpanDatasetReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | tokens : List [ str ], | pos_tags : List [ str ] = None , | gold_tree : Tree = None | ) -> Instance We take pre-tokenized input here, because we don't have a tokenizer in this class.","title":"text_to_instance"},{"location":"models/structured_prediction/dataset_readers/semantic_dependencies/","text":"allennlp_models .structured_prediction .dataset_readers .semantic_dependencies [SOURCE] FIELDS # FIELDS = [ \"id\" , \"form\" , \"lemma\" , \"pos\" , \"head\" , \"deprel\" , \"top\" , \"pred\" , \"frame\" ] parse_sentence # def parse_sentence ( sentence_blob : str ) -> Tuple [ List [ Dict [ str , str ]], List [ Tuple [ int , int ]], List [ str ]] Parses a chunk of text in the SemEval SDP format. Each word in the sentence is returned as a dictionary with the following format: 'id': '1', 'form': 'Pierre', 'lemma': 'Pierre', 'pos': 'NNP', 'head': '2', # Note that this is the `syntactic` head. 'deprel': 'nn', 'top': '-', 'pred': '+', 'frame': 'named:x-c' Along with a list of arcs and their corresponding tags. Note that in semantic dependency parsing words can have more than one head (it is not a tree), meaning that the list of arcs and tags are not tied to the length of the sentence. lazy_parse # def lazy_parse ( text : str ) SemanticDependenciesDatasetReader # @DatasetReader . register ( \"semantic_dependencies\" ) class SemanticDependenciesDatasetReader ( DatasetReader ): | def __init__ ( | self , | token_indexers : Dict [ str , TokenIndexer ] = None , | skip_when_no_arcs : bool = True , | ** kwargs | ) -> None Reads a file in the SemEval 2015 Task 18 (Broad-coverage Semantic Dependency Parsing) format. Registered as a DatasetReader with name \"semantic_dependencies\". Parameters \u00b6 token_indexers : Dict[str, TokenIndexer] , optional (default = {\"tokens\": SingleIdTokenIndexer()} ) The token indexers to be applied to the words TextField. skip_when_no_arcs : bool , optional (default = True ) If this is true, skip examples containing no semantic arcs. text_to_instance # class SemanticDependenciesDatasetReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | tokens : List [ str ], | pos_tags : List [ str ] = None , | arc_indices : List [ Tuple [ int , int ]] = None , | arc_tags : List [ str ] = None | ) -> Instance","title":"semantic_dependencies"},{"location":"models/structured_prediction/dataset_readers/semantic_dependencies/#fields","text":"FIELDS = [ \"id\" , \"form\" , \"lemma\" , \"pos\" , \"head\" , \"deprel\" , \"top\" , \"pred\" , \"frame\" ]","title":"FIELDS"},{"location":"models/structured_prediction/dataset_readers/semantic_dependencies/#parse_sentence","text":"def parse_sentence ( sentence_blob : str ) -> Tuple [ List [ Dict [ str , str ]], List [ Tuple [ int , int ]], List [ str ]] Parses a chunk of text in the SemEval SDP format. Each word in the sentence is returned as a dictionary with the following format: 'id': '1', 'form': 'Pierre', 'lemma': 'Pierre', 'pos': 'NNP', 'head': '2', # Note that this is the `syntactic` head. 'deprel': 'nn', 'top': '-', 'pred': '+', 'frame': 'named:x-c' Along with a list of arcs and their corresponding tags. Note that in semantic dependency parsing words can have more than one head (it is not a tree), meaning that the list of arcs and tags are not tied to the length of the sentence.","title":"parse_sentence"},{"location":"models/structured_prediction/dataset_readers/semantic_dependencies/#lazy_parse","text":"def lazy_parse ( text : str )","title":"lazy_parse"},{"location":"models/structured_prediction/dataset_readers/semantic_dependencies/#semanticdependenciesdatasetreader","text":"@DatasetReader . register ( \"semantic_dependencies\" ) class SemanticDependenciesDatasetReader ( DatasetReader ): | def __init__ ( | self , | token_indexers : Dict [ str , TokenIndexer ] = None , | skip_when_no_arcs : bool = True , | ** kwargs | ) -> None Reads a file in the SemEval 2015 Task 18 (Broad-coverage Semantic Dependency Parsing) format. Registered as a DatasetReader with name \"semantic_dependencies\".","title":"SemanticDependenciesDatasetReader"},{"location":"models/structured_prediction/dataset_readers/semantic_dependencies/#text_to_instance","text":"class SemanticDependenciesDatasetReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | tokens : List [ str ], | pos_tags : List [ str ] = None , | arc_indices : List [ Tuple [ int , int ]] = None , | arc_tags : List [ str ] = None | ) -> Instance","title":"text_to_instance"},{"location":"models/structured_prediction/dataset_readers/srl/","text":"allennlp_models .structured_prediction .dataset_readers .srl [SOURCE] SrlReader # @DatasetReader . register ( \"srl\" ) class SrlReader ( DatasetReader ): | def __init__ ( | self , | token_indexers : Dict [ str , TokenIndexer ] = None , | domain_identifier : str = None , | bert_model_name : str = None , | ** kwargs | ) -> None This DatasetReader is designed to read in the English OntoNotes v5.0 data for semantic role labelling. It returns a dataset of instances with the following fields: tokens : TextField The tokens in the sentence. verb_indicator : SequenceLabelField A sequence of binary indicators for whether the word is the verb for this frame. tags : SequenceLabelField A sequence of Propbank tags for the given verb in a BIO format. Parameters \u00b6 token_indexers : Dict[str, TokenIndexer] , optional We similarly use this for both the premise and the hypothesis. See TokenIndexer . Default is {\"tokens\": SingleIdTokenIndexer()} . domain_identifier : str , optional (default = None ) A string denoting a sub-domain of the Ontonotes 5.0 dataset to use. If present, only conll files under paths containing this domain identifier will be processed. bert_model_name : Optional[str] , optional (default = None ) The BERT model to be wrapped. If you specify a bert_model here, then we will assume you want to use BERT throughout; we will use the bert tokenizer, and will expand your tags and verb indicators accordingly. If not, the tokens will be indexed as normal with the token_indexers. Returns \u00b6 A Dataset of Instances for Semantic Role Labelling. text_to_instance # class SrlReader ( DatasetReader ): | ... | def text_to_instance ( | self , | tokens : List [ Token ], | verb_label : List [ int ], | tags : List [ str ] = None | ) -> Instance We take pre-tokenized input here, along with a verb label. The verb label should be a one-hot binary vector, the same length as the tokens, indicating the position of the verb to find arguments for.","title":"srl"},{"location":"models/structured_prediction/dataset_readers/srl/#srlreader","text":"@DatasetReader . register ( \"srl\" ) class SrlReader ( DatasetReader ): | def __init__ ( | self , | token_indexers : Dict [ str , TokenIndexer ] = None , | domain_identifier : str = None , | bert_model_name : str = None , | ** kwargs | ) -> None This DatasetReader is designed to read in the English OntoNotes v5.0 data for semantic role labelling. It returns a dataset of instances with the following fields: tokens : TextField The tokens in the sentence. verb_indicator : SequenceLabelField A sequence of binary indicators for whether the word is the verb for this frame. tags : SequenceLabelField A sequence of Propbank tags for the given verb in a BIO format.","title":"SrlReader"},{"location":"models/structured_prediction/dataset_readers/srl/#text_to_instance","text":"class SrlReader ( DatasetReader ): | ... | def text_to_instance ( | self , | tokens : List [ Token ], | verb_label : List [ int ], | tags : List [ str ] = None | ) -> Instance We take pre-tokenized input here, along with a verb label. The verb label should be a one-hot binary vector, the same length as the tokens, indicating the position of the verb to find arguments for.","title":"text_to_instance"},{"location":"models/structured_prediction/dataset_readers/universal_dependencies/","text":"allennlp_models .structured_prediction .dataset_readers .universal_dependencies [SOURCE] UniversalDependenciesDatasetReader # @DatasetReader . register ( \"universal_dependencies\" , exist_ok = True ) class UniversalDependenciesDatasetReader ( DatasetReader ): | def __init__ ( | self , | token_indexers : Dict [ str , TokenIndexer ] = None , | use_language_specific_pos : bool = False , | tokenizer : Tokenizer = None , | ** kwargs | ) -> None Reads a file in the conllu Universal Dependencies format. Parameters \u00b6 token_indexers : Dict[str, TokenIndexer] , optional (default = {\"tokens\": SingleIdTokenIndexer()} ) The token indexers to be applied to the words TextField. use_language_specific_pos : bool , optional (default = False ) Whether to use UD POS tags, or to use the language specific POS tags provided in the conllu format. tokenizer : Tokenizer , optional (default = None ) A tokenizer to use to split the text. This is useful when the tokens that you pass into the model need to have some particular attribute. Typically it is not necessary. text_to_instance # class UniversalDependenciesDatasetReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | words : List [ str ], | upos_tags : List [ str ], | dependencies : List [ Tuple [ str , int ]] = None | ) -> Instance Parameters \u00b6 words : List[str] The words in the sentence to be encoded. upos_tags : List[str] The universal dependencies POS tags for each word. dependencies : List[Tuple[str, int]] , optional (default = None ) A list of (head tag, head index) tuples. Indices are 1 indexed, meaning an index of 0 corresponds to that word being the root of the dependency tree. Returns \u00b6 An instance containing words, upos tags, dependency head tags and head indices as fields.","title":"universal_dependencies"},{"location":"models/structured_prediction/dataset_readers/universal_dependencies/#universaldependenciesdatasetreader","text":"@DatasetReader . register ( \"universal_dependencies\" , exist_ok = True ) class UniversalDependenciesDatasetReader ( DatasetReader ): | def __init__ ( | self , | token_indexers : Dict [ str , TokenIndexer ] = None , | use_language_specific_pos : bool = False , | tokenizer : Tokenizer = None , | ** kwargs | ) -> None Reads a file in the conllu Universal Dependencies format.","title":"UniversalDependenciesDatasetReader"},{"location":"models/structured_prediction/dataset_readers/universal_dependencies/#text_to_instance","text":"class UniversalDependenciesDatasetReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | words : List [ str ], | upos_tags : List [ str ], | dependencies : List [ Tuple [ str , int ]] = None | ) -> Instance","title":"text_to_instance"},{"location":"models/structured_prediction/metrics/srl_eval_scorer/","text":"allennlp_models .structured_prediction .metrics .srl_eval_scorer [SOURCE] DEFAULT_SRL_EVAL_PATH # DEFAULT_SRL_EVAL_PATH = os . path . abspath ( os . path . join ( os . path . dirname ( os . path . realpath ( __file__ )), \"..\" , \"tools\" , \"srl-e ... SrlEvalScorer # @Metric . register ( \"srl_eval\" ) class SrlEvalScorer ( Metric ): | def __init__ ( | self , | srl_eval_path : str = DEFAULT_SRL_EVAL_PATH , | ignore_classes : List [ str ] = None | ) -> None This class uses the external srl-eval.pl script for computing the CoNLL SRL metrics. AllenNLP contains the srl-eval.pl script, but you will need perl 5.x. Note that this metric reads and writes from disk quite a bit. In particular, it writes and subsequently reads two files per call , which is typically invoked once per batch. You probably don't want to include it in your training loop; instead, you should calculate this on a validation set only. Parameters \u00b6 srl_eval_path : str , optional The path to the srl-eval.pl script. ignore_classes : List[str] , optional (default = None ) A list of classes to ignore. __call__ # class SrlEvalScorer ( Metric ): | ... | @overrides | def __call__ ( | self , | batch_verb_indices : List [ Optional [ int ]], | batch_sentences : List [ List [ str ]], | batch_conll_formatted_predicted_tags : List [ List [ str ]], | batch_conll_formatted_gold_tags : List [ List [ str ]] | ) -> None Parameters \u00b6 batch_verb_indices : List[Optional[int]] The indices of the verbal predicate in the sentences which the gold labels are the arguments for, or None if the sentence contains no verbal predicate. batch_sentences : List[List[str]] The word tokens for each instance in the batch. batch_conll_formatted_predicted_tags : List[List[str]] A list of predicted CoNLL-formatted SRL tags (itself a list) to compute score for. Use allennlp.models.semantic_role_labeler.convert_bio_tags_to_conll_format to convert from BIO to CoNLL format before passing the tags into the metric, if applicable. batch_conll_formatted_gold_tags : List[List[str]] A list of gold CoNLL-formatted SRL tags (itself a list) to use as a reference. Use allennlp.models.semantic_role_labeler.convert_bio_tags_to_conll_format to convert from BIO to CoNLL format before passing the tags into the metric, if applicable. get_metric # class SrlEvalScorer ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns \u00b6 A Dict per label containing following the span based metrics: - precision : float - recall : float - f1-measure : float Additionally, an overall key is included, which provides the precision, recall and f1-measure for all spans. reset # class SrlEvalScorer ( Metric ): | ... | def reset ( self )","title":"srl_eval_scorer"},{"location":"models/structured_prediction/metrics/srl_eval_scorer/#default_srl_eval_path","text":"DEFAULT_SRL_EVAL_PATH = os . path . abspath ( os . path . join ( os . path . dirname ( os . path . realpath ( __file__ )), \"..\" , \"tools\" , \"srl-e ...","title":"DEFAULT_SRL_EVAL_PATH"},{"location":"models/structured_prediction/metrics/srl_eval_scorer/#srlevalscorer","text":"@Metric . register ( \"srl_eval\" ) class SrlEvalScorer ( Metric ): | def __init__ ( | self , | srl_eval_path : str = DEFAULT_SRL_EVAL_PATH , | ignore_classes : List [ str ] = None | ) -> None This class uses the external srl-eval.pl script for computing the CoNLL SRL metrics. AllenNLP contains the srl-eval.pl script, but you will need perl 5.x. Note that this metric reads and writes from disk quite a bit. In particular, it writes and subsequently reads two files per call , which is typically invoked once per batch. You probably don't want to include it in your training loop; instead, you should calculate this on a validation set only.","title":"SrlEvalScorer"},{"location":"models/structured_prediction/metrics/srl_eval_scorer/#__call__","text":"class SrlEvalScorer ( Metric ): | ... | @overrides | def __call__ ( | self , | batch_verb_indices : List [ Optional [ int ]], | batch_sentences : List [ List [ str ]], | batch_conll_formatted_predicted_tags : List [ List [ str ]], | batch_conll_formatted_gold_tags : List [ List [ str ]] | ) -> None","title":"__call__"},{"location":"models/structured_prediction/metrics/srl_eval_scorer/#get_metric","text":"class SrlEvalScorer ( Metric ): | ... | def get_metric ( self , reset : bool = False )","title":"get_metric"},{"location":"models/structured_prediction/metrics/srl_eval_scorer/#reset","text":"class SrlEvalScorer ( Metric ): | ... | def reset ( self )","title":"reset"},{"location":"models/structured_prediction/models/biaffine_dependency_parser/","text":"allennlp_models .structured_prediction .models .biaffine_dependency_parser [SOURCE] POS_TO_IGNORE # POS_TO_IGNORE = { \"`\" , \"''\" , \":\" , \",\" , \".\" , \"PU\" , \"PUNCT\" , \"SYM\" } BiaffineDependencyParser # @Model . register ( \"biaffine_parser\" ) class BiaffineDependencyParser ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | encoder : Seq2SeqEncoder , | tag_representation_dim : int , | arc_representation_dim : int , | tag_feedforward : FeedForward = None , | arc_feedforward : FeedForward = None , | pos_tag_embedding : Embedding = None , | use_mst_decoding_for_validation : bool = True , | dropout : float = 0.0 , | input_dropout : float = 0.0 , | initializer : InitializerApplicator = InitializerApplicator (), | ** kwargs | ) -> None This dependency parser follows the model of Deep Biaffine Attention for Neural Dependency Parsing (Dozat and Manning, 2016) . Word representations are generated using a bidirectional LSTM, followed by separate biaffine classifiers for pairs of words, predicting whether a directed arc exists between the two words and the dependency label the arc should have. Decoding can either be done greedily, or the optimal Minimum Spanning Tree can be decoded using Edmond's algorithm by viewing the dependency tree as a MST on a fully connected graph, where nodes are words and edges are scored dependency arcs. Parameters \u00b6 vocab : Vocabulary A Vocabulary, required in order to compute sizes for input/output projections. text_field_embedder : TextFieldEmbedder Used to embed the tokens TextField we get as input to the model. encoder : Seq2SeqEncoder The encoder (with its own internal stacking) that we will use to generate representations of tokens. tag_representation_dim : int The dimension of the MLPs used for dependency tag prediction. arc_representation_dim : int The dimension of the MLPs used for head arc prediction. tag_feedforward : FeedForward , optional (default = None ) The feedforward network used to produce tag representations. By default, a 1 layer feedforward network with an elu activation is used. arc_feedforward : FeedForward , optional (default = None ) The feedforward network used to produce arc representations. By default, a 1 layer feedforward network with an elu activation is used. pos_tag_embedding : Embedding , optional Used to embed the pos_tags SequenceLabelField we get as input to the model. use_mst_decoding_for_validation : bool , optional (default = True ) Whether to use Edmond's algorithm to find the optimal minimum spanning tree during validation. If false, decoding is greedy. dropout : float , optional (default = 0.0 ) The variational dropout applied to the output of the encoder and MLP layers. input_dropout : float , optional (default = 0.0 ) The dropout applied to the embedded text input. initializer : InitializerApplicator , optional (default = InitializerApplicator() ) Used to initialize the model parameters. forward # class BiaffineDependencyParser ( Model ): | ... | @overrides | def forward ( | self , | words : TextFieldTensors , | pos_tags : torch . LongTensor , | metadata : List [ Dict [ str , Any ]], | head_tags : torch . LongTensor = None , | head_indices : torch . LongTensor = None | ) -> Dict [ str , torch . Tensor ] Parameters \u00b6 words : TextFieldTensors The output of TextField.as_array() , which should typically be passed directly to a TextFieldEmbedder . This output is a dictionary mapping keys to TokenIndexer tensors. At its most basic, using a SingleIdTokenIndexer this is : {\"tokens\": Tensor(batch_size, sequence_length)} . This dictionary will have the same keys as were used for the TokenIndexers when you created the TextField representing your sequence. The dictionary is designed to be passed directly to a TextFieldEmbedder , which knows how to combine different word representations into a single vector per token in your input. pos_tags : torch.LongTensor The output of a SequenceLabelField containing POS tags. POS tags are required regardless of whether they are used in the model, because they are used to filter the evaluation metric to only consider heads of words which are not punctuation. metadata : List[Dict[str, Any]] , optional (default = None ) A dictionary of metadata for each batch element which has keys: words : List[str] , required. The tokens in the original sentence. pos : List[str] , required. The dependencies POS tags for each word. head_tags : torch.LongTensor , optional (default = None ) A torch tensor representing the sequence of integer gold class labels for the arcs in the dependency parse. Has shape (batch_size, sequence_length) . head_indices : torch.LongTensor , optional (default = None ) A torch tensor representing the sequence of integer indices denoting the parent of every word in the dependency parse. Has shape (batch_size, sequence_length) . Returns \u00b6 An output dictionary consisting of: loss : torch.FloatTensor , optional A scalar loss to be optimised. arc_loss : torch.FloatTensor The loss contribution from the unlabeled arcs. loss : torch.FloatTensor , optional The loss contribution from predicting the dependency tags for the gold arcs. heads : torch.FloatTensor The predicted head indices for each word. A tensor of shape (batch_size, sequence_length). head_types : torch.FloatTensor The predicted head types for each arc. A tensor of shape (batch_size, sequence_length). mask : torch.BoolTensor A mask denoting the padded elements in the batch. make_output_human_readable # class BiaffineDependencyParser ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] get_metrics # class BiaffineDependencyParser ( Model ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] default_predictor # class BiaffineDependencyParser ( Model ): | ... | default_predictor = \"biaffine_dependency_parser\"","title":"biaffine_dependency_parser"},{"location":"models/structured_prediction/models/biaffine_dependency_parser/#pos_to_ignore","text":"POS_TO_IGNORE = { \"`\" , \"''\" , \":\" , \",\" , \".\" , \"PU\" , \"PUNCT\" , \"SYM\" }","title":"POS_TO_IGNORE"},{"location":"models/structured_prediction/models/biaffine_dependency_parser/#biaffinedependencyparser","text":"@Model . register ( \"biaffine_parser\" ) class BiaffineDependencyParser ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | encoder : Seq2SeqEncoder , | tag_representation_dim : int , | arc_representation_dim : int , | tag_feedforward : FeedForward = None , | arc_feedforward : FeedForward = None , | pos_tag_embedding : Embedding = None , | use_mst_decoding_for_validation : bool = True , | dropout : float = 0.0 , | input_dropout : float = 0.0 , | initializer : InitializerApplicator = InitializerApplicator (), | ** kwargs | ) -> None This dependency parser follows the model of Deep Biaffine Attention for Neural Dependency Parsing (Dozat and Manning, 2016) . Word representations are generated using a bidirectional LSTM, followed by separate biaffine classifiers for pairs of words, predicting whether a directed arc exists between the two words and the dependency label the arc should have. Decoding can either be done greedily, or the optimal Minimum Spanning Tree can be decoded using Edmond's algorithm by viewing the dependency tree as a MST on a fully connected graph, where nodes are words and edges are scored dependency arcs.","title":"BiaffineDependencyParser"},{"location":"models/structured_prediction/models/biaffine_dependency_parser/#forward","text":"class BiaffineDependencyParser ( Model ): | ... | @overrides | def forward ( | self , | words : TextFieldTensors , | pos_tags : torch . LongTensor , | metadata : List [ Dict [ str , Any ]], | head_tags : torch . LongTensor = None , | head_indices : torch . LongTensor = None | ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"models/structured_prediction/models/biaffine_dependency_parser/#make_output_human_readable","text":"class BiaffineDependencyParser ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ]","title":"make_output_human_readable"},{"location":"models/structured_prediction/models/biaffine_dependency_parser/#get_metrics","text":"class BiaffineDependencyParser ( Model ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"models/structured_prediction/models/biaffine_dependency_parser/#default_predictor","text":"class BiaffineDependencyParser ( Model ): | ... | default_predictor = \"biaffine_dependency_parser\"","title":"default_predictor"},{"location":"models/structured_prediction/models/constituency_parser/","text":"allennlp_models .structured_prediction .models .constituency_parser [SOURCE] SpanInformation # class SpanInformation ( NamedTuple ) A helper namedtuple for handling decoding information. Parameters \u00b6 start : int The start index of the span. end : int The exclusive end index of the span. no_label_prob : float The probability of this span being assigned the NO-LABEL label. label_prob : float The probability of the most likely label. start # class SpanInformation ( NamedTuple ): | ... | start : int = None end # class SpanInformation ( NamedTuple ): | ... | end : int = None label_prob # class SpanInformation ( NamedTuple ): | ... | label_prob : float = None no_label_prob # class SpanInformation ( NamedTuple ): | ... | no_label_prob : float = None label_index # class SpanInformation ( NamedTuple ): | ... | label_index : int = None SpanConstituencyParser # @Model . register ( \"constituency_parser\" ) class SpanConstituencyParser ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | span_extractor : SpanExtractor , | encoder : Seq2SeqEncoder , | feedforward : FeedForward = None , | pos_tag_embedding : Embedding = None , | initializer : InitializerApplicator = InitializerApplicator (), | evalb_directory_path : str = DEFAULT_EVALB_DIR , | ** kwargs | ) -> None This SpanConstituencyParser simply encodes a sequence of text with a stacked Seq2SeqEncoder , extracts span representations using a SpanExtractor , and then predicts a label for each span in the sequence. These labels are non-terminal nodes in a constituency parse tree, which we then greedily reconstruct. Parameters \u00b6 vocab : Vocabulary A Vocabulary, required in order to compute sizes for input/output projections. text_field_embedder : TextFieldEmbedder Used to embed the tokens TextField we get as input to the model. span_extractor : SpanExtractor The method used to extract the spans from the encoded sequence. encoder : Seq2SeqEncoder The encoder that we will use in between embedding tokens and generating span representations. feedforward : FeedForward The FeedForward layer that we will use in between the encoder and the linear projection to a distribution over span labels. pos_tag_embedding : Embedding , optional Used to embed the pos_tags SequenceLabelField we get as input to the model. initializer : InitializerApplicator , optional (default = InitializerApplicator() ) Used to initialize the model parameters. evalb_directory_path : str , optional (default = DEFAULT_EVALB_DIR ) The path to the directory containing the EVALB executable used to score bracketed parses. By default, will use the EVALB included with allennlp, which is located at allennlp/tools/EVALB . If None , EVALB scoring is not used. forward # class SpanConstituencyParser ( Model ): | ... | @overrides | def forward ( | self , | tokens : TextFieldTensors , | spans : torch . LongTensor , | metadata : List [ Dict [ str , Any ]], | pos_tags : TextFieldTensors = None , | span_labels : torch . LongTensor = None | ) -> Dict [ str , torch . Tensor ] Parameters \u00b6 tokens : TextFieldTensors The output of TextField.as_array() , which should typically be passed directly to a TextFieldEmbedder . This output is a dictionary mapping keys to TokenIndexer tensors. At its most basic, using a SingleIdTokenIndexer this is : {\"tokens\": Tensor(batch_size, num_tokens)} . This dictionary will have the same keys as were used for the TokenIndexers when you created the TextField representing your sequence. The dictionary is designed to be passed directly to a TextFieldEmbedder , which knows how to combine different word representations into a single vector per token in your input. spans : torch.LongTensor A tensor of shape (batch_size, num_spans, 2) representing the inclusive start and end indices of all possible spans in the sentence. metadata : List[Dict[str, Any]] A dictionary of metadata for each batch element which has keys: tokens : List[str] , required. The original string tokens in the sentence. gold_tree : nltk.Tree , optional (default = None ) Gold NLTK trees for use in evaluation. pos_tags : List[str] , optional. The POS tags for the sentence. These can be used in the model as embedded features, but they are passed here in addition for use in constructing the tree. pos_tags : torch.LongTensor , optional (default = None ) The output of a SequenceLabelField containing POS tags. span_labels : torch.LongTensor , optional (default = None ) A torch tensor representing the integer gold class labels for all possible spans, of shape (batch_size, num_spans) . Returns \u00b6 An output dictionary consisting of: class_probabilities : torch.FloatTensor A tensor of shape (batch_size, num_spans, span_label_vocab_size) representing a distribution over the label classes per span. spans : torch.LongTensor The original spans tensor. tokens : List[List[str]] , required. A list of tokens in the sentence for each element in the batch. pos_tags : List[List[str]] , required. A list of POS tags in the sentence for each element in the batch. num_spans : torch.LongTensor , required. A tensor of shape (batch_size), representing the lengths of non-padded spans in enumerated_spans . loss : torch.FloatTensor , optional A scalar loss to be optimised. make_output_human_readable # class SpanConstituencyParser ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Constructs an NLTK Tree given the scored spans. We also switch to exclusive span ends when constructing the tree representation, because it makes indexing into lists cleaner for ranges of text, rather than individual indices. Finally, for batch prediction, we will have padded spans and class probabilities. In order to make this less confusing, we remove all the padded spans and distributions from spans and class_probabilities respectively. construct_trees # class SpanConstituencyParser ( Model ): | ... | def construct_trees ( | self , | predictions : torch . FloatTensor , | all_spans : torch . LongTensor , | num_spans : torch . LongTensor , | sentences : List [ List [ str ]], | pos_tags : List [ List [ str ]] = None | ) -> List [ Tree ] Construct nltk.Tree 's for each batch element by greedily nesting spans. The trees use exclusive end indices, which contrasts with how spans are represented in the rest of the model. Parameters \u00b6 predictions : torch.FloatTensor A tensor of shape (batch_size, num_spans, span_label_vocab_size) representing a distribution over the label classes per span. all_spans : torch.LongTensor A tensor of shape (batch_size, num_spans, 2), representing the span indices we scored. num_spans : torch.LongTensor A tensor of shape (batch_size), representing the lengths of non-padded spans in enumerated_spans . sentences : List[List[str]] A list of tokens in the sentence for each element in the batch. pos_tags : List[List[str]] , optional (default = None ) A list of POS tags for each word in the sentence for each element in the batch. Returns \u00b6 A List[Tree] containing the decoded trees for each element in the batch. resolve_overlap_conflicts_greedily # class SpanConstituencyParser ( Model ): | ... | @staticmethod | def resolve_overlap_conflicts_greedily ( | spans : List [ SpanInformation ] | ) -> List [ SpanInformation ] Given a set of spans, removes spans which overlap by evaluating the difference in probability between one being labeled and the other explicitly having no label and vice-versa. The worst case time complexity of this method is O(k * n^4) where n is the length of the sentence that the spans were enumerated from (and therefore k * m^2 complexity with respect to the number of spans m ) and k is the number of conflicts. However, in practice, there are very few conflicts. Hopefully. This function modifies spans to remove overlapping spans. Parameters \u00b6 spans : List[SpanInformation] A list of spans, where each span is a namedtuple containing the following attributes: start : int The start index of the span. end : int The exclusive end index of the span. no_label_prob : float The probability of this span being assigned the NO-LABEL label. label_prob : float The probability of the most likely label. Returns \u00b6 A modified list of spans , with the conflicts resolved by considering local differences between pairs of spans and removing one of the two spans. construct_tree_from_spans # class SpanConstituencyParser ( Model ): | ... | @staticmethod | def construct_tree_from_spans ( | spans_to_labels : Dict [ Tuple [ int , int ], str ], | sentence : List [ str ], | pos_tags : List [ str ] = None | ) -> Tree Parameters \u00b6 spans_to_labels : Dict[Tuple[int, int], str] A mapping from spans to constituency labels. sentence : List[str] A list of tokens forming the sentence to be parsed. pos_tags : List[str] , optional (default = None ) A list of the pos tags for the words in the sentence, if they were either predicted or taken as input to the model. Returns \u00b6 An nltk.Tree constructed from the labelled spans. get_metrics # class SpanConstituencyParser ( Model ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] default_predictor # class SpanConstituencyParser ( Model ): | ... | default_predictor = \"constituency_parser\"","title":"constituency_parser"},{"location":"models/structured_prediction/models/constituency_parser/#spaninformation","text":"class SpanInformation ( NamedTuple ) A helper namedtuple for handling decoding information.","title":"SpanInformation"},{"location":"models/structured_prediction/models/constituency_parser/#start","text":"class SpanInformation ( NamedTuple ): | ... | start : int = None","title":"start"},{"location":"models/structured_prediction/models/constituency_parser/#end","text":"class SpanInformation ( NamedTuple ): | ... | end : int = None","title":"end"},{"location":"models/structured_prediction/models/constituency_parser/#label_prob","text":"class SpanInformation ( NamedTuple ): | ... | label_prob : float = None","title":"label_prob"},{"location":"models/structured_prediction/models/constituency_parser/#no_label_prob","text":"class SpanInformation ( NamedTuple ): | ... | no_label_prob : float = None","title":"no_label_prob"},{"location":"models/structured_prediction/models/constituency_parser/#label_index","text":"class SpanInformation ( NamedTuple ): | ... | label_index : int = None","title":"label_index"},{"location":"models/structured_prediction/models/constituency_parser/#spanconstituencyparser","text":"@Model . register ( \"constituency_parser\" ) class SpanConstituencyParser ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | span_extractor : SpanExtractor , | encoder : Seq2SeqEncoder , | feedforward : FeedForward = None , | pos_tag_embedding : Embedding = None , | initializer : InitializerApplicator = InitializerApplicator (), | evalb_directory_path : str = DEFAULT_EVALB_DIR , | ** kwargs | ) -> None This SpanConstituencyParser simply encodes a sequence of text with a stacked Seq2SeqEncoder , extracts span representations using a SpanExtractor , and then predicts a label for each span in the sequence. These labels are non-terminal nodes in a constituency parse tree, which we then greedily reconstruct.","title":"SpanConstituencyParser"},{"location":"models/structured_prediction/models/constituency_parser/#forward","text":"class SpanConstituencyParser ( Model ): | ... | @overrides | def forward ( | self , | tokens : TextFieldTensors , | spans : torch . LongTensor , | metadata : List [ Dict [ str , Any ]], | pos_tags : TextFieldTensors = None , | span_labels : torch . LongTensor = None | ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"models/structured_prediction/models/constituency_parser/#make_output_human_readable","text":"class SpanConstituencyParser ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Constructs an NLTK Tree given the scored spans. We also switch to exclusive span ends when constructing the tree representation, because it makes indexing into lists cleaner for ranges of text, rather than individual indices. Finally, for batch prediction, we will have padded spans and class probabilities. In order to make this less confusing, we remove all the padded spans and distributions from spans and class_probabilities respectively.","title":"make_output_human_readable"},{"location":"models/structured_prediction/models/constituency_parser/#construct_trees","text":"class SpanConstituencyParser ( Model ): | ... | def construct_trees ( | self , | predictions : torch . FloatTensor , | all_spans : torch . LongTensor , | num_spans : torch . LongTensor , | sentences : List [ List [ str ]], | pos_tags : List [ List [ str ]] = None | ) -> List [ Tree ] Construct nltk.Tree 's for each batch element by greedily nesting spans. The trees use exclusive end indices, which contrasts with how spans are represented in the rest of the model.","title":"construct_trees"},{"location":"models/structured_prediction/models/constituency_parser/#resolve_overlap_conflicts_greedily","text":"class SpanConstituencyParser ( Model ): | ... | @staticmethod | def resolve_overlap_conflicts_greedily ( | spans : List [ SpanInformation ] | ) -> List [ SpanInformation ] Given a set of spans, removes spans which overlap by evaluating the difference in probability between one being labeled and the other explicitly having no label and vice-versa. The worst case time complexity of this method is O(k * n^4) where n is the length of the sentence that the spans were enumerated from (and therefore k * m^2 complexity with respect to the number of spans m ) and k is the number of conflicts. However, in practice, there are very few conflicts. Hopefully. This function modifies spans to remove overlapping spans.","title":"resolve_overlap_conflicts_greedily"},{"location":"models/structured_prediction/models/constituency_parser/#construct_tree_from_spans","text":"class SpanConstituencyParser ( Model ): | ... | @staticmethod | def construct_tree_from_spans ( | spans_to_labels : Dict [ Tuple [ int , int ], str ], | sentence : List [ str ], | pos_tags : List [ str ] = None | ) -> Tree","title":"construct_tree_from_spans"},{"location":"models/structured_prediction/models/constituency_parser/#get_metrics","text":"class SpanConstituencyParser ( Model ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"models/structured_prediction/models/constituency_parser/#default_predictor","text":"class SpanConstituencyParser ( Model ): | ... | default_predictor = \"constituency_parser\"","title":"default_predictor"},{"location":"models/structured_prediction/models/graph_parser/","text":"allennlp_models .structured_prediction .models .graph_parser [SOURCE] GraphParser # @Model . register ( \"graph_parser\" ) @Model . register ( \"sp-graph-parser\" ) class GraphParser ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | encoder : Seq2SeqEncoder , | tag_representation_dim : int , | arc_representation_dim : int , | tag_feedforward : FeedForward = None , | arc_feedforward : FeedForward = None , | pos_tag_embedding : Embedding = None , | dropout : float = 0.0 , | input_dropout : float = 0.0 , | edge_prediction_threshold : float = 0.5 , | initializer : InitializerApplicator = InitializerApplicator (), | ** kwargs | ) -> None A Parser for arbitrary graph structures. Registered as a Model with name \"graph_parser\". Parameters \u00b6 vocab : Vocabulary A Vocabulary, required in order to compute sizes for input/output projections. text_field_embedder : TextFieldEmbedder Used to embed the tokens TextField we get as input to the model. encoder : Seq2SeqEncoder The encoder (with its own internal stacking) that we will use to generate representations of tokens. tag_representation_dim : int The dimension of the MLPs used for arc tag prediction. arc_representation_dim : int The dimension of the MLPs used for arc prediction. tag_feedforward : FeedForward , optional (default = None ) The feedforward network used to produce tag representations. By default, a 1 layer feedforward network with an elu activation is used. arc_feedforward : FeedForward , optional (default = None ) The feedforward network used to produce arc representations. By default, a 1 layer feedforward network with an elu activation is used. pos_tag_embedding : Embedding , optional Used to embed the pos_tags SequenceLabelField we get as input to the model. dropout : float , optional (default = 0.0 ) The variational dropout applied to the output of the encoder and MLP layers. input_dropout : float , optional (default = 0.0 ) The dropout applied to the embedded text input. edge_prediction_threshold : int , optional (default = 0.5 ) The probability at which to consider a scored edge to be 'present' in the decoded graph. Must be between 0 and 1. initializer : InitializerApplicator , optional (default = InitializerApplicator() ) Used to initialize the model parameters. forward # class GraphParser ( Model ): | ... | @overrides | def forward ( | self , | tokens : TextFieldTensors , | pos_tags : torch . LongTensor = None , | metadata : List [ Dict [ str , Any ]] = None , | arc_tags : torch . LongTensor = None | ) -> Dict [ str , torch . Tensor ] Parameters \u00b6 tokens : TextFieldTensors The output of TextField.as_array() . pos_tags : torch.LongTensor , optional (default = None ) The output of a SequenceLabelField containing POS tags. metadata : List[Dict[str, Any]] , optional (default = None ) A dictionary of metadata for each batch element which has keys: tokens : List[str] , required. The original string tokens in the sentence. arc_tags : torch.LongTensor , optional (default = None ) A torch tensor representing the sequence of integer indices denoting the parent of every word in the dependency parse. Has shape (batch_size, sequence_length, sequence_length) . Returns \u00b6 An output dictionary. make_output_human_readable # class GraphParser ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] get_metrics # class GraphParser ( Model ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"graph_parser"},{"location":"models/structured_prediction/models/graph_parser/#graphparser","text":"@Model . register ( \"graph_parser\" ) @Model . register ( \"sp-graph-parser\" ) class GraphParser ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | encoder : Seq2SeqEncoder , | tag_representation_dim : int , | arc_representation_dim : int , | tag_feedforward : FeedForward = None , | arc_feedforward : FeedForward = None , | pos_tag_embedding : Embedding = None , | dropout : float = 0.0 , | input_dropout : float = 0.0 , | edge_prediction_threshold : float = 0.5 , | initializer : InitializerApplicator = InitializerApplicator (), | ** kwargs | ) -> None A Parser for arbitrary graph structures. Registered as a Model with name \"graph_parser\".","title":"GraphParser"},{"location":"models/structured_prediction/models/graph_parser/#forward","text":"class GraphParser ( Model ): | ... | @overrides | def forward ( | self , | tokens : TextFieldTensors , | pos_tags : torch . LongTensor = None , | metadata : List [ Dict [ str , Any ]] = None , | arc_tags : torch . LongTensor = None | ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"models/structured_prediction/models/graph_parser/#make_output_human_readable","text":"class GraphParser ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ]","title":"make_output_human_readable"},{"location":"models/structured_prediction/models/graph_parser/#get_metrics","text":"class GraphParser ( Model ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"models/structured_prediction/models/srl/","text":"allennlp_models .structured_prediction .models .srl [SOURCE] write_bio_formatted_tags_to_file # def write_bio_formatted_tags_to_file ( prediction_file : TextIO , gold_file : TextIO , verb_index : Optional [ int ], sentence : List [ str ], prediction : List [ str ], gold_labels : List [ str ] ) Prints predicate argument predictions and gold labels for a single verbal predicate in a sentence to two provided file references. The CoNLL SRL format is described in the shared task data README . This function expects IOB2-formatted tags, where the B- tag is used in the beginning of every chunk (i.e. all chunks start with the B- tag). Parameters \u00b6 prediction_file : TextIO A file reference to print predictions to. gold_file : TextIO A file reference to print gold labels to. verb_index : Optional[int] The index of the verbal predicate in the sentence which the gold labels are the arguments for, or None if the sentence contains no verbal predicate. sentence : List[str] The word tokens. prediction : List[str] The predicted BIO labels. gold_labels : List[str] The gold BIO labels. write_conll_formatted_tags_to_file # def write_conll_formatted_tags_to_file ( prediction_file : TextIO , gold_file : TextIO , verb_index : Optional [ int ], sentence : List [ str ], conll_formatted_predictions : List [ str ], conll_formatted_gold_labels : List [ str ] ) Prints predicate argument predictions and gold labels for a single verbal predicate in a sentence to two provided file references. The CoNLL SRL format is described in the shared task data README . This function expects IOB2-formatted tags, where the B- tag is used in the beginning of every chunk (i.e. all chunks start with the B- tag). Parameters \u00b6 prediction_file : TextIO A file reference to print predictions to. gold_file : TextIO A file reference to print gold labels to. verb_index : Optional[int] The index of the verbal predicate in the sentence which the gold labels are the arguments for, or None if the sentence contains no verbal predicate. sentence : List[str] The word tokens. conll_formatted_predictions : List[str] The predicted CoNLL-formatted labels. conll_formatted_gold_labels : List[str] The gold CoNLL-formatted labels. convert_bio_tags_to_conll_format # def convert_bio_tags_to_conll_format ( labels : List [ str ]) Converts BIO formatted SRL tags to the format required for evaluation with the official CONLL 2005 perl script. Spans are represented by bracketed labels, with the labels of words inside spans being the same as those outside spans. Beginning spans always have a opening bracket and a closing asterisk (e.g. \"(ARG-1 \" ) and closing spans always have a closing bracket (e.g. \" )\" ). This applies even for length 1 spans, (e.g \"(ARG-0*)\"). A full example of the conversion performed: [B-ARG-1, I-ARG-1, I-ARG-1, I-ARG-1, I-ARG-1, O] [ \"(ARG-1*\", \"*\", \"*\", \"*\", \"*)\", \"*\"] Parameters \u00b6 labels : List[str] A list of BIO tags to convert to the CONLL span based format. Returns \u00b6 A list of labels in the CONLL span based format. SemanticRoleLabeler # @Model . register ( \"srl\" ) class SemanticRoleLabeler ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | encoder : Seq2SeqEncoder , | binary_feature_dim : int , | embedding_dropout : float = 0.0 , | initializer : InitializerApplicator = InitializerApplicator (), | label_smoothing : float = None , | ignore_span_metric : bool = False , | srl_eval_path : str = DEFAULT_SRL_EVAL_PATH , | ** kwargs | ) -> None This model performs semantic role labeling using BIO tags using Propbank semantic roles. Specifically, it is an implementation of Deep Semantic Role Labeling - What works and what's next . This implementation is effectively a series of stacked interleaved LSTMs with highway connections, applied to embedded sequences of words concatenated with a binary indicator containing whether or not a word is the verbal predicate to generate predictions for in the sentence. Additionally, during inference, Viterbi decoding is applied to constrain the predictions to contain valid BIO sequences. Specifically, the model expects and outputs IOB2-formatted tags, where the B- tag is used in the beginning of every chunk (i.e. all chunks start with the B- tag). Parameters \u00b6 vocab : Vocabulary A Vocabulary, required in order to compute sizes for input/output projections. text_field_embedder : TextFieldEmbedder Used to embed the tokens TextField we get as input to the model. encoder : Seq2SeqEncoder The encoder (with its own internal stacking) that we will use in between embedding tokens and predicting output tags. binary_feature_dim : int The dimensionality of the embedding of the binary verb predicate features. initializer : InitializerApplicator , optional (default = InitializerApplicator() ) Used to initialize the model parameters. label_smoothing : float , optional (default = 0.0 ) Whether or not to use label smoothing on the labels when computing cross entropy loss. ignore_span_metric : bool , optional (default = False ) Whether to calculate span loss, which is irrelevant when predicting BIO for Open Information Extraction. srl_eval_path : str , optional (default = DEFAULT_SRL_EVAL_PATH ) The path to the srl-eval.pl script. By default, will use the srl-eval.pl included with allennlp, which is located at allennlp/tools/srl-eval.pl . If None , srl-eval.pl is not used. forward # class SemanticRoleLabeler ( Model ): | ... | def forward ( | self , | tokens : TextFieldTensors , | verb_indicator : torch . LongTensor , | tags : torch . LongTensor = None , | metadata : List [ Dict [ str , Any ]] = None | ) -> Dict [ str , torch . Tensor ] Parameters \u00b6 tokens : TextFieldTensors The output of TextField.as_array() , which should typically be passed directly to a TextFieldEmbedder . This output is a dictionary mapping keys to TokenIndexer tensors. At its most basic, using a SingleIdTokenIndexer this is : {\"tokens\": Tensor(batch_size, num_tokens)} . This dictionary will have the same keys as were used for the TokenIndexers when you created the TextField representing your sequence. The dictionary is designed to be passed directly to a TextFieldEmbedder , which knows how to combine different word representations into a single vector per token in your input. verb_indicator : torch.LongTensor An integer SequenceFeatureField representation of the position of the verb in the sentence. This should have shape (batch_size, num_tokens) and importantly, can be all zeros, in the case that the sentence has no verbal predicate. tags : torch.LongTensor , optional (default = None ) A torch tensor representing the sequence of integer gold class labels of shape (batch_size, num_tokens) metadata : List[Dict[str, Any]] , optional (default = None ) metadata containg the original words in the sentence and the verb to compute the frame for, under 'words' and 'verb' keys, respectively. Returns \u00b6 An output dictionary consisting of: logits : torch.FloatTensor A tensor of shape (batch_size, num_tokens, tag_vocab_size) representing unnormalised log probabilities of the tag classes. class_probabilities : torch.FloatTensor A tensor of shape (batch_size, num_tokens, tag_vocab_size) representing a distribution of the tag classes per word. loss : torch.FloatTensor , optional A scalar loss to be optimised. make_output_human_readable # class SemanticRoleLabeler ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Does constrained viterbi decoding on class probabilities output in forward . The constraint simply specifies that the output tags must be a valid BIO sequence. We add a \"tags\" key to the dictionary with the result. get_metrics # class SemanticRoleLabeler ( Model ): | ... | def get_metrics ( self , reset : bool = False ) get_viterbi_pairwise_potentials # class SemanticRoleLabeler ( Model ): | ... | def get_viterbi_pairwise_potentials ( self ) Generate a matrix of pairwise transition potentials for the BIO labels. The only constraint implemented here is that I-XXX labels must be preceded by either an identical I-XXX tag or a B-XXX tag. In order to achieve this constraint, pairs of labels which do not satisfy this constraint have a pairwise potential of -inf. Returns \u00b6 transition_matrix : torch.Tensor A (num_labels, num_labels) matrix of pairwise potentials. get_start_transitions # class SemanticRoleLabeler ( Model ): | ... | def get_start_transitions ( self ) In the BIO sequence, we cannot start the sequence with an I-XXX tag. This transition sequence is passed to viterbi_decode to specify this constraint. Returns \u00b6 start_transitions : torch.Tensor The pairwise potentials between a START token and the first token of the sequence. default_predictor # class SemanticRoleLabeler ( Model ): | ... | default_predictor = \"semantic_role_labeling\" write_to_conll_eval_file # def write_to_conll_eval_file ( prediction_file : TextIO , gold_file : TextIO , verb_index : Optional [ int ], sentence : List [ str ], prediction : List [ str ], gold_labels : List [ str ] ) .. deprecated:: 0.8.4 The write_to_conll_eval_file function was deprecated in favor of the identical write_bio_formatted_tags_to_file in version 0.8.4. Prints predicate argument predictions and gold labels for a single verbal predicate in a sentence to two provided file references. The CoNLL SRL format is described in the shared task data README . This function expects IOB2-formatted tags, where the B- tag is used in the beginning of every chunk (i.e. all chunks start with the B- tag). Parameters \u00b6 prediction_file : TextIO A file reference to print predictions to. gold_file : TextIO A file reference to print gold labels to. verb_index : Optional[int] The index of the verbal predicate in the sentence which the gold labels are the arguments for, or None if the sentence contains no verbal predicate. sentence : List[str] The word tokens. prediction : List[str] The predicted BIO labels. gold_labels : List[str] The gold BIO labels.","title":"srl"},{"location":"models/structured_prediction/models/srl/#write_bio_formatted_tags_to_file","text":"def write_bio_formatted_tags_to_file ( prediction_file : TextIO , gold_file : TextIO , verb_index : Optional [ int ], sentence : List [ str ], prediction : List [ str ], gold_labels : List [ str ] ) Prints predicate argument predictions and gold labels for a single verbal predicate in a sentence to two provided file references. The CoNLL SRL format is described in the shared task data README . This function expects IOB2-formatted tags, where the B- tag is used in the beginning of every chunk (i.e. all chunks start with the B- tag).","title":"write_bio_formatted_tags_to_file"},{"location":"models/structured_prediction/models/srl/#write_conll_formatted_tags_to_file","text":"def write_conll_formatted_tags_to_file ( prediction_file : TextIO , gold_file : TextIO , verb_index : Optional [ int ], sentence : List [ str ], conll_formatted_predictions : List [ str ], conll_formatted_gold_labels : List [ str ] ) Prints predicate argument predictions and gold labels for a single verbal predicate in a sentence to two provided file references. The CoNLL SRL format is described in the shared task data README . This function expects IOB2-formatted tags, where the B- tag is used in the beginning of every chunk (i.e. all chunks start with the B- tag).","title":"write_conll_formatted_tags_to_file"},{"location":"models/structured_prediction/models/srl/#convert_bio_tags_to_conll_format","text":"def convert_bio_tags_to_conll_format ( labels : List [ str ]) Converts BIO formatted SRL tags to the format required for evaluation with the official CONLL 2005 perl script. Spans are represented by bracketed labels, with the labels of words inside spans being the same as those outside spans. Beginning spans always have a opening bracket and a closing asterisk (e.g. \"(ARG-1 \" ) and closing spans always have a closing bracket (e.g. \" )\" ). This applies even for length 1 spans, (e.g \"(ARG-0*)\"). A full example of the conversion performed: [B-ARG-1, I-ARG-1, I-ARG-1, I-ARG-1, I-ARG-1, O] [ \"(ARG-1*\", \"*\", \"*\", \"*\", \"*)\", \"*\"]","title":"convert_bio_tags_to_conll_format"},{"location":"models/structured_prediction/models/srl/#semanticrolelabeler","text":"@Model . register ( \"srl\" ) class SemanticRoleLabeler ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | encoder : Seq2SeqEncoder , | binary_feature_dim : int , | embedding_dropout : float = 0.0 , | initializer : InitializerApplicator = InitializerApplicator (), | label_smoothing : float = None , | ignore_span_metric : bool = False , | srl_eval_path : str = DEFAULT_SRL_EVAL_PATH , | ** kwargs | ) -> None This model performs semantic role labeling using BIO tags using Propbank semantic roles. Specifically, it is an implementation of Deep Semantic Role Labeling - What works and what's next . This implementation is effectively a series of stacked interleaved LSTMs with highway connections, applied to embedded sequences of words concatenated with a binary indicator containing whether or not a word is the verbal predicate to generate predictions for in the sentence. Additionally, during inference, Viterbi decoding is applied to constrain the predictions to contain valid BIO sequences. Specifically, the model expects and outputs IOB2-formatted tags, where the B- tag is used in the beginning of every chunk (i.e. all chunks start with the B- tag).","title":"SemanticRoleLabeler"},{"location":"models/structured_prediction/models/srl/#forward","text":"class SemanticRoleLabeler ( Model ): | ... | def forward ( | self , | tokens : TextFieldTensors , | verb_indicator : torch . LongTensor , | tags : torch . LongTensor = None , | metadata : List [ Dict [ str , Any ]] = None | ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"models/structured_prediction/models/srl/#make_output_human_readable","text":"class SemanticRoleLabeler ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Does constrained viterbi decoding on class probabilities output in forward . The constraint simply specifies that the output tags must be a valid BIO sequence. We add a \"tags\" key to the dictionary with the result.","title":"make_output_human_readable"},{"location":"models/structured_prediction/models/srl/#get_metrics","text":"class SemanticRoleLabeler ( Model ): | ... | def get_metrics ( self , reset : bool = False )","title":"get_metrics"},{"location":"models/structured_prediction/models/srl/#get_viterbi_pairwise_potentials","text":"class SemanticRoleLabeler ( Model ): | ... | def get_viterbi_pairwise_potentials ( self ) Generate a matrix of pairwise transition potentials for the BIO labels. The only constraint implemented here is that I-XXX labels must be preceded by either an identical I-XXX tag or a B-XXX tag. In order to achieve this constraint, pairs of labels which do not satisfy this constraint have a pairwise potential of -inf.","title":"get_viterbi_pairwise_potentials"},{"location":"models/structured_prediction/models/srl/#get_start_transitions","text":"class SemanticRoleLabeler ( Model ): | ... | def get_start_transitions ( self ) In the BIO sequence, we cannot start the sequence with an I-XXX tag. This transition sequence is passed to viterbi_decode to specify this constraint.","title":"get_start_transitions"},{"location":"models/structured_prediction/models/srl/#default_predictor","text":"class SemanticRoleLabeler ( Model ): | ... | default_predictor = \"semantic_role_labeling\"","title":"default_predictor"},{"location":"models/structured_prediction/models/srl/#write_to_conll_eval_file","text":"def write_to_conll_eval_file ( prediction_file : TextIO , gold_file : TextIO , verb_index : Optional [ int ], sentence : List [ str ], prediction : List [ str ], gold_labels : List [ str ] ) .. deprecated:: 0.8.4 The write_to_conll_eval_file function was deprecated in favor of the identical write_bio_formatted_tags_to_file in version 0.8.4. Prints predicate argument predictions and gold labels for a single verbal predicate in a sentence to two provided file references. The CoNLL SRL format is described in the shared task data README . This function expects IOB2-formatted tags, where the B- tag is used in the beginning of every chunk (i.e. all chunks start with the B- tag).","title":"write_to_conll_eval_file"},{"location":"models/structured_prediction/models/srl_bert/","text":"allennlp_models .structured_prediction .models .srl_bert [SOURCE] SrlBert # @Model . register ( \"srl_bert\" ) class SrlBert ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | bert_model : Union [ str , BertModel ], | embedding_dropout : float = 0.0 , | initializer : InitializerApplicator = InitializerApplicator (), | label_smoothing : float = None , | ignore_span_metric : bool = False , | srl_eval_path : str = DEFAULT_SRL_EVAL_PATH , | ** kwargs | ) -> None A BERT based model Simple BERT Models for Relation Extraction and Semantic Role Labeling (Shi et al, 2019) with some modifications (no additional parameters apart from a linear classification layer), which is currently the state-of-the-art single model for English PropBank SRL (Newswire sentences). Parameters \u00b6 vocab : Vocabulary A Vocabulary, required in order to compute sizes for input/output projections. model : Union[str, BertModel] A string describing the BERT model to load or an already constructed BertModel. initializer : InitializerApplicator , optional (default = InitializerApplicator() ) Used to initialize the model parameters. label_smoothing : float , optional (default = 0.0 ) Whether or not to use label smoothing on the labels when computing cross entropy loss. ignore_span_metric : bool , optional (default = False ) Whether to calculate span loss, which is irrelevant when predicting BIO for Open Information Extraction. srl_eval_path : str , optional (default = DEFAULT_SRL_EVAL_PATH ) The path to the srl-eval.pl script. By default, will use the srl-eval.pl included with allennlp, which is located at allennlp/tools/srl-eval.pl . If None , srl-eval.pl is not used. forward # class SrlBert ( Model ): | ... | def forward ( | self , | tokens : TextFieldTensors , | verb_indicator : torch . Tensor , | metadata : List [ Any ], | tags : torch . LongTensor = None | ) Parameters \u00b6 tokens : TextFieldTensors The output of TextField.as_array() , which should typically be passed directly to a TextFieldEmbedder . For this model, this must be a SingleIdTokenIndexer which indexes wordpieces from the BERT vocabulary. verb_indicator : torch.LongTensor An integer SequenceFeatureField representation of the position of the verb in the sentence. This should have shape (batch_size, num_tokens) and importantly, can be all zeros, in the case that the sentence has no verbal predicate. tags : torch.LongTensor , optional (default = None ) A torch tensor representing the sequence of integer gold class labels of shape (batch_size, num_tokens) metadata : List[Dict[str, Any]] , optional (default = None ) metadata containing the original words in the sentence, the verb to compute the frame for, and start offsets for converting wordpieces back to a sequence of words, under 'words', 'verb' and 'offsets' keys, respectively. Returns \u00b6 An output dictionary consisting of: logits : torch.FloatTensor A tensor of shape (batch_size, num_tokens, tag_vocab_size) representing unnormalised log probabilities of the tag classes. class_probabilities : torch.FloatTensor A tensor of shape (batch_size, num_tokens, tag_vocab_size) representing a distribution of the tag classes per word. loss : torch.FloatTensor , optional A scalar loss to be optimised. make_output_human_readable # class SrlBert ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Does constrained viterbi decoding on class probabilities output in forward . The constraint simply specifies that the output tags must be a valid BIO sequence. We add a \"tags\" key to the dictionary with the result. NOTE: First, we decode a BIO sequence on top of the wordpieces. This is important; viterbi decoding produces low quality output if you decode on top of word representations directly, because the model gets confused by the 'missing' positions (which is sensible as it is trained to perform tagging on wordpieces, not words). Secondly, it's important that the indices we use to recover words from the wordpieces are the start_offsets (i.e offsets which correspond to using the first wordpiece of words which are tokenized into multiple wordpieces) as otherwise, we might get an ill-formed BIO sequence when we select out the word tags from the wordpiece tags. This happens in the case that a word is split into multiple word pieces, and then we take the last tag of the word, which might correspond to, e.g, I-V, which would not be allowed as it is not preceeded by a B tag. get_metrics # class SrlBert ( Model ): | ... | def get_metrics ( self , reset : bool = False ) get_viterbi_pairwise_potentials # class SrlBert ( Model ): | ... | def get_viterbi_pairwise_potentials ( self ) Generate a matrix of pairwise transition potentials for the BIO labels. The only constraint implemented here is that I-XXX labels must be preceded by either an identical I-XXX tag or a B-XXX tag. In order to achieve this constraint, pairs of labels which do not satisfy this constraint have a pairwise potential of -inf. Returns \u00b6 transition_matrix : torch.Tensor A (num_labels, num_labels) matrix of pairwise potentials. get_start_transitions # class SrlBert ( Model ): | ... | def get_start_transitions ( self ) In the BIO sequence, we cannot start the sequence with an I-XXX tag. This transition sequence is passed to viterbi_decode to specify this constraint. Returns \u00b6 start_transitions : torch.Tensor The pairwise potentials between a START token and the first token of the sequence. default_predictor # class SrlBert ( Model ): | ... | default_predictor = \"semantic_role_labeling\"","title":"srl_bert"},{"location":"models/structured_prediction/models/srl_bert/#srlbert","text":"@Model . register ( \"srl_bert\" ) class SrlBert ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | bert_model : Union [ str , BertModel ], | embedding_dropout : float = 0.0 , | initializer : InitializerApplicator = InitializerApplicator (), | label_smoothing : float = None , | ignore_span_metric : bool = False , | srl_eval_path : str = DEFAULT_SRL_EVAL_PATH , | ** kwargs | ) -> None A BERT based model Simple BERT Models for Relation Extraction and Semantic Role Labeling (Shi et al, 2019) with some modifications (no additional parameters apart from a linear classification layer), which is currently the state-of-the-art single model for English PropBank SRL (Newswire sentences).","title":"SrlBert"},{"location":"models/structured_prediction/models/srl_bert/#forward","text":"class SrlBert ( Model ): | ... | def forward ( | self , | tokens : TextFieldTensors , | verb_indicator : torch . Tensor , | metadata : List [ Any ], | tags : torch . LongTensor = None | )","title":"forward"},{"location":"models/structured_prediction/models/srl_bert/#make_output_human_readable","text":"class SrlBert ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Does constrained viterbi decoding on class probabilities output in forward . The constraint simply specifies that the output tags must be a valid BIO sequence. We add a \"tags\" key to the dictionary with the result. NOTE: First, we decode a BIO sequence on top of the wordpieces. This is important; viterbi decoding produces low quality output if you decode on top of word representations directly, because the model gets confused by the 'missing' positions (which is sensible as it is trained to perform tagging on wordpieces, not words). Secondly, it's important that the indices we use to recover words from the wordpieces are the start_offsets (i.e offsets which correspond to using the first wordpiece of words which are tokenized into multiple wordpieces) as otherwise, we might get an ill-formed BIO sequence when we select out the word tags from the wordpiece tags. This happens in the case that a word is split into multiple word pieces, and then we take the last tag of the word, which might correspond to, e.g, I-V, which would not be allowed as it is not preceeded by a B tag.","title":"make_output_human_readable"},{"location":"models/structured_prediction/models/srl_bert/#get_metrics","text":"class SrlBert ( Model ): | ... | def get_metrics ( self , reset : bool = False )","title":"get_metrics"},{"location":"models/structured_prediction/models/srl_bert/#get_viterbi_pairwise_potentials","text":"class SrlBert ( Model ): | ... | def get_viterbi_pairwise_potentials ( self ) Generate a matrix of pairwise transition potentials for the BIO labels. The only constraint implemented here is that I-XXX labels must be preceded by either an identical I-XXX tag or a B-XXX tag. In order to achieve this constraint, pairs of labels which do not satisfy this constraint have a pairwise potential of -inf.","title":"get_viterbi_pairwise_potentials"},{"location":"models/structured_prediction/models/srl_bert/#get_start_transitions","text":"class SrlBert ( Model ): | ... | def get_start_transitions ( self ) In the BIO sequence, we cannot start the sequence with an I-XXX tag. This transition sequence is passed to viterbi_decode to specify this constraint.","title":"get_start_transitions"},{"location":"models/structured_prediction/models/srl_bert/#default_predictor","text":"class SrlBert ( Model ): | ... | default_predictor = \"semantic_role_labeling\"","title":"default_predictor"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/","text":"allennlp_models .structured_prediction .predictors .biaffine_dependency_parser [SOURCE] NODE_TYPE_TO_STYLE # NODE_TYPE_TO_STYLE = {} NODE_TYPE_TO_STYLE[\"root\"] # NODE_TYPE_TO_STYLE [ \"root\" ] = [ \"color5\" , \"strong\" ] NODE_TYPE_TO_STYLE[\"dep\"] # NODE_TYPE_TO_STYLE [ \"dep\" ] = [ \"color5\" , \"strong\" ] NODE_TYPE_TO_STYLE[\"nsubj\"] # NODE_TYPE_TO_STYLE [ \"nsubj\" ] = [ \"color1\" ] NODE_TYPE_TO_STYLE[\"nsubjpass\"] # NODE_TYPE_TO_STYLE [ \"nsubjpass\" ] = [ \"color1\" ] NODE_TYPE_TO_STYLE[\"csubj\"] # NODE_TYPE_TO_STYLE [ \"csubj\" ] = [ \"color1\" ] NODE_TYPE_TO_STYLE[\"csubjpass\"] # NODE_TYPE_TO_STYLE [ \"csubjpass\" ] = [ \"color1\" ] NODE_TYPE_TO_STYLE[\"pobj\"] # NODE_TYPE_TO_STYLE [ \"pobj\" ] = [ \"color2\" ] NODE_TYPE_TO_STYLE[\"dobj\"] # NODE_TYPE_TO_STYLE [ \"dobj\" ] = [ \"color2\" ] NODE_TYPE_TO_STYLE[\"iobj\"] # NODE_TYPE_TO_STYLE [ \"iobj\" ] = [ \"color2\" ] NODE_TYPE_TO_STYLE[\"mark\"] # NODE_TYPE_TO_STYLE [ \"mark\" ] = [ \"color2\" ] NODE_TYPE_TO_STYLE[\"pcomp\"] # NODE_TYPE_TO_STYLE [ \"pcomp\" ] = [ \"color2\" ] NODE_TYPE_TO_STYLE[\"xcomp\"] # NODE_TYPE_TO_STYLE [ \"xcomp\" ] = [ \"color2\" ] NODE_TYPE_TO_STYLE[\"ccomp\"] # NODE_TYPE_TO_STYLE [ \"ccomp\" ] = [ \"color2\" ] NODE_TYPE_TO_STYLE[\"acomp\"] # NODE_TYPE_TO_STYLE [ \"acomp\" ] = [ \"color2\" ] NODE_TYPE_TO_STYLE[\"aux\"] # NODE_TYPE_TO_STYLE [ \"aux\" ] = [ \"color3\" ] NODE_TYPE_TO_STYLE[\"cop\"] # NODE_TYPE_TO_STYLE [ \"cop\" ] = [ \"color3\" ] NODE_TYPE_TO_STYLE[\"det\"] # NODE_TYPE_TO_STYLE [ \"det\" ] = [ \"color3\" ] NODE_TYPE_TO_STYLE[\"conj\"] # NODE_TYPE_TO_STYLE [ \"conj\" ] = [ \"color3\" ] NODE_TYPE_TO_STYLE[\"cc\"] # NODE_TYPE_TO_STYLE [ \"cc\" ] = [ \"color3\" ] NODE_TYPE_TO_STYLE[\"prep\"] # NODE_TYPE_TO_STYLE [ \"prep\" ] = [ \"color3\" ] NODE_TYPE_TO_STYLE[\"number\"] # NODE_TYPE_TO_STYLE [ \"number\" ] = [ \"color3\" ] NODE_TYPE_TO_STYLE[\"possesive\"] # NODE_TYPE_TO_STYLE [ \"possesive\" ] = [ \"color3\" ] NODE_TYPE_TO_STYLE[\"poss\"] # NODE_TYPE_TO_STYLE [ \"poss\" ] = [ \"color3\" ] NODE_TYPE_TO_STYLE[\"discourse\"] # NODE_TYPE_TO_STYLE [ \"discourse\" ] = [ \"color3\" ] NODE_TYPE_TO_STYLE[\"expletive\"] # NODE_TYPE_TO_STYLE [ \"expletive\" ] = [ \"color3\" ] NODE_TYPE_TO_STYLE[\"prt\"] # NODE_TYPE_TO_STYLE [ \"prt\" ] = [ \"color3\" ] NODE_TYPE_TO_STYLE[\"advcl\"] # NODE_TYPE_TO_STYLE [ \"advcl\" ] = [ \"color3\" ] NODE_TYPE_TO_STYLE[\"mod\"] # NODE_TYPE_TO_STYLE [ \"mod\" ] = [ \"color4\" ] NODE_TYPE_TO_STYLE[\"amod\"] # NODE_TYPE_TO_STYLE [ \"amod\" ] = [ \"color4\" ] NODE_TYPE_TO_STYLE[\"tmod\"] # NODE_TYPE_TO_STYLE [ \"tmod\" ] = [ \"color4\" ] NODE_TYPE_TO_STYLE[\"quantmod\"] # NODE_TYPE_TO_STYLE [ \"quantmod\" ] = [ \"color4\" ] NODE_TYPE_TO_STYLE[\"npadvmod\"] # NODE_TYPE_TO_STYLE [ \"npadvmod\" ] = [ \"color4\" ] NODE_TYPE_TO_STYLE[\"infmod\"] # NODE_TYPE_TO_STYLE [ \"infmod\" ] = [ \"color4\" ] NODE_TYPE_TO_STYLE[\"advmod\"] # NODE_TYPE_TO_STYLE [ \"advmod\" ] = [ \"color4\" ] NODE_TYPE_TO_STYLE[\"appos\"] # NODE_TYPE_TO_STYLE [ \"appos\" ] = [ \"color4\" ] NODE_TYPE_TO_STYLE[\"nn\"] # NODE_TYPE_TO_STYLE [ \"nn\" ] = [ \"color4\" ] NODE_TYPE_TO_STYLE[\"neg\"] # NODE_TYPE_TO_STYLE [ \"neg\" ] = [ \"color0\" ] NODE_TYPE_TO_STYLE[\"punct\"] # NODE_TYPE_TO_STYLE [ \"punct\" ] = [ \"color0\" ] LINK_TO_POSITION # LINK_TO_POSITION = {} LINK_TO_POSITION[\"nsubj\"] # LINK_TO_POSITION [ \"nsubj\" ] = \"left\" LINK_TO_POSITION[\"nsubjpass\"] # LINK_TO_POSITION [ \"nsubjpass\" ] = \"left\" LINK_TO_POSITION[\"csubj\"] # LINK_TO_POSITION [ \"csubj\" ] = \"left\" LINK_TO_POSITION[\"csubjpass\"] # LINK_TO_POSITION [ \"csubjpass\" ] = \"left\" LINK_TO_POSITION[\"pobj\"] # LINK_TO_POSITION [ \"pobj\" ] = \"right\" LINK_TO_POSITION[\"dobj\"] # LINK_TO_POSITION [ \"dobj\" ] = \"right\" LINK_TO_POSITION[\"iobj\"] # LINK_TO_POSITION [ \"iobj\" ] = \"right\" LINK_TO_POSITION[\"pcomp\"] # LINK_TO_POSITION [ \"pcomp\" ] = \"right\" LINK_TO_POSITION[\"xcomp\"] # LINK_TO_POSITION [ \"xcomp\" ] = \"right\" LINK_TO_POSITION[\"ccomp\"] # LINK_TO_POSITION [ \"ccomp\" ] = \"right\" LINK_TO_POSITION[\"acomp\"] # LINK_TO_POSITION [ \"acomp\" ] = \"right\" BiaffineDependencyParserPredictor # @Predictor . register ( \"biaffine_dependency_parser\" , exist_ok = True ) class BiaffineDependencyParserPredictor ( Predictor ): | def __init__ ( | self , | model : Model , | dataset_reader : DatasetReader , | language : str = \"en_core_web_sm\" | ) -> None Predictor for the BiaffineDependencyParser model. predict # class BiaffineDependencyParserPredictor ( Predictor ): | ... | def predict ( self , sentence : str ) -> JsonDict Predict a dependency parse for the given sentence. Parameters \u00b6 sentence The sentence to parse. Returns \u00b6 A dictionary representation of the dependency tree. predict_instance # class BiaffineDependencyParserPredictor ( Predictor ): | ... | @overrides | def predict_instance ( self , instance : Instance ) -> JsonDict predict_batch_instance # class BiaffineDependencyParserPredictor ( Predictor ): | ... | @overrides | def predict_batch_instance ( | self , | instances : List [ Instance ] | ) -> List [ JsonDict ]","title":"biaffine_dependency_parser"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_style","text":"NODE_TYPE_TO_STYLE = {}","title":"NODE_TYPE_TO_STYLE"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_styleroot","text":"NODE_TYPE_TO_STYLE [ \"root\" ] = [ \"color5\" , \"strong\" ]","title":"NODE_TYPE_TO_STYLE[\"root\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_styledep","text":"NODE_TYPE_TO_STYLE [ \"dep\" ] = [ \"color5\" , \"strong\" ]","title":"NODE_TYPE_TO_STYLE[\"dep\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_stylensubj","text":"NODE_TYPE_TO_STYLE [ \"nsubj\" ] = [ \"color1\" ]","title":"NODE_TYPE_TO_STYLE[\"nsubj\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_stylensubjpass","text":"NODE_TYPE_TO_STYLE [ \"nsubjpass\" ] = [ \"color1\" ]","title":"NODE_TYPE_TO_STYLE[\"nsubjpass\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_stylecsubj","text":"NODE_TYPE_TO_STYLE [ \"csubj\" ] = [ \"color1\" ]","title":"NODE_TYPE_TO_STYLE[\"csubj\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_stylecsubjpass","text":"NODE_TYPE_TO_STYLE [ \"csubjpass\" ] = [ \"color1\" ]","title":"NODE_TYPE_TO_STYLE[\"csubjpass\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_stylepobj","text":"NODE_TYPE_TO_STYLE [ \"pobj\" ] = [ \"color2\" ]","title":"NODE_TYPE_TO_STYLE[\"pobj\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_styledobj","text":"NODE_TYPE_TO_STYLE [ \"dobj\" ] = [ \"color2\" ]","title":"NODE_TYPE_TO_STYLE[\"dobj\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_styleiobj","text":"NODE_TYPE_TO_STYLE [ \"iobj\" ] = [ \"color2\" ]","title":"NODE_TYPE_TO_STYLE[\"iobj\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_stylemark","text":"NODE_TYPE_TO_STYLE [ \"mark\" ] = [ \"color2\" ]","title":"NODE_TYPE_TO_STYLE[\"mark\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_stylepcomp","text":"NODE_TYPE_TO_STYLE [ \"pcomp\" ] = [ \"color2\" ]","title":"NODE_TYPE_TO_STYLE[\"pcomp\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_stylexcomp","text":"NODE_TYPE_TO_STYLE [ \"xcomp\" ] = [ \"color2\" ]","title":"NODE_TYPE_TO_STYLE[\"xcomp\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_styleccomp","text":"NODE_TYPE_TO_STYLE [ \"ccomp\" ] = [ \"color2\" ]","title":"NODE_TYPE_TO_STYLE[\"ccomp\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_styleacomp","text":"NODE_TYPE_TO_STYLE [ \"acomp\" ] = [ \"color2\" ]","title":"NODE_TYPE_TO_STYLE[\"acomp\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_styleaux","text":"NODE_TYPE_TO_STYLE [ \"aux\" ] = [ \"color3\" ]","title":"NODE_TYPE_TO_STYLE[\"aux\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_stylecop","text":"NODE_TYPE_TO_STYLE [ \"cop\" ] = [ \"color3\" ]","title":"NODE_TYPE_TO_STYLE[\"cop\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_styledet","text":"NODE_TYPE_TO_STYLE [ \"det\" ] = [ \"color3\" ]","title":"NODE_TYPE_TO_STYLE[\"det\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_styleconj","text":"NODE_TYPE_TO_STYLE [ \"conj\" ] = [ \"color3\" ]","title":"NODE_TYPE_TO_STYLE[\"conj\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_stylecc","text":"NODE_TYPE_TO_STYLE [ \"cc\" ] = [ \"color3\" ]","title":"NODE_TYPE_TO_STYLE[\"cc\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_styleprep","text":"NODE_TYPE_TO_STYLE [ \"prep\" ] = [ \"color3\" ]","title":"NODE_TYPE_TO_STYLE[\"prep\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_stylenumber","text":"NODE_TYPE_TO_STYLE [ \"number\" ] = [ \"color3\" ]","title":"NODE_TYPE_TO_STYLE[\"number\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_stylepossesive","text":"NODE_TYPE_TO_STYLE [ \"possesive\" ] = [ \"color3\" ]","title":"NODE_TYPE_TO_STYLE[\"possesive\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_styleposs","text":"NODE_TYPE_TO_STYLE [ \"poss\" ] = [ \"color3\" ]","title":"NODE_TYPE_TO_STYLE[\"poss\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_stylediscourse","text":"NODE_TYPE_TO_STYLE [ \"discourse\" ] = [ \"color3\" ]","title":"NODE_TYPE_TO_STYLE[\"discourse\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_styleexpletive","text":"NODE_TYPE_TO_STYLE [ \"expletive\" ] = [ \"color3\" ]","title":"NODE_TYPE_TO_STYLE[\"expletive\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_styleprt","text":"NODE_TYPE_TO_STYLE [ \"prt\" ] = [ \"color3\" ]","title":"NODE_TYPE_TO_STYLE[\"prt\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_styleadvcl","text":"NODE_TYPE_TO_STYLE [ \"advcl\" ] = [ \"color3\" ]","title":"NODE_TYPE_TO_STYLE[\"advcl\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_stylemod","text":"NODE_TYPE_TO_STYLE [ \"mod\" ] = [ \"color4\" ]","title":"NODE_TYPE_TO_STYLE[\"mod\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_styleamod","text":"NODE_TYPE_TO_STYLE [ \"amod\" ] = [ \"color4\" ]","title":"NODE_TYPE_TO_STYLE[\"amod\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_styletmod","text":"NODE_TYPE_TO_STYLE [ \"tmod\" ] = [ \"color4\" ]","title":"NODE_TYPE_TO_STYLE[\"tmod\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_stylequantmod","text":"NODE_TYPE_TO_STYLE [ \"quantmod\" ] = [ \"color4\" ]","title":"NODE_TYPE_TO_STYLE[\"quantmod\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_stylenpadvmod","text":"NODE_TYPE_TO_STYLE [ \"npadvmod\" ] = [ \"color4\" ]","title":"NODE_TYPE_TO_STYLE[\"npadvmod\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_styleinfmod","text":"NODE_TYPE_TO_STYLE [ \"infmod\" ] = [ \"color4\" ]","title":"NODE_TYPE_TO_STYLE[\"infmod\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_styleadvmod","text":"NODE_TYPE_TO_STYLE [ \"advmod\" ] = [ \"color4\" ]","title":"NODE_TYPE_TO_STYLE[\"advmod\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_styleappos","text":"NODE_TYPE_TO_STYLE [ \"appos\" ] = [ \"color4\" ]","title":"NODE_TYPE_TO_STYLE[\"appos\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_stylenn","text":"NODE_TYPE_TO_STYLE [ \"nn\" ] = [ \"color4\" ]","title":"NODE_TYPE_TO_STYLE[\"nn\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_styleneg","text":"NODE_TYPE_TO_STYLE [ \"neg\" ] = [ \"color0\" ]","title":"NODE_TYPE_TO_STYLE[\"neg\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#node_type_to_stylepunct","text":"NODE_TYPE_TO_STYLE [ \"punct\" ] = [ \"color0\" ]","title":"NODE_TYPE_TO_STYLE[\"punct\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#link_to_position","text":"LINK_TO_POSITION = {}","title":"LINK_TO_POSITION"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#link_to_positionnsubj","text":"LINK_TO_POSITION [ \"nsubj\" ] = \"left\"","title":"LINK_TO_POSITION[\"nsubj\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#link_to_positionnsubjpass","text":"LINK_TO_POSITION [ \"nsubjpass\" ] = \"left\"","title":"LINK_TO_POSITION[\"nsubjpass\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#link_to_positioncsubj","text":"LINK_TO_POSITION [ \"csubj\" ] = \"left\"","title":"LINK_TO_POSITION[\"csubj\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#link_to_positioncsubjpass","text":"LINK_TO_POSITION [ \"csubjpass\" ] = \"left\"","title":"LINK_TO_POSITION[\"csubjpass\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#link_to_positionpobj","text":"LINK_TO_POSITION [ \"pobj\" ] = \"right\"","title":"LINK_TO_POSITION[\"pobj\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#link_to_positiondobj","text":"LINK_TO_POSITION [ \"dobj\" ] = \"right\"","title":"LINK_TO_POSITION[\"dobj\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#link_to_positioniobj","text":"LINK_TO_POSITION [ \"iobj\" ] = \"right\"","title":"LINK_TO_POSITION[\"iobj\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#link_to_positionpcomp","text":"LINK_TO_POSITION [ \"pcomp\" ] = \"right\"","title":"LINK_TO_POSITION[\"pcomp\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#link_to_positionxcomp","text":"LINK_TO_POSITION [ \"xcomp\" ] = \"right\"","title":"LINK_TO_POSITION[\"xcomp\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#link_to_positionccomp","text":"LINK_TO_POSITION [ \"ccomp\" ] = \"right\"","title":"LINK_TO_POSITION[\"ccomp\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#link_to_positionacomp","text":"LINK_TO_POSITION [ \"acomp\" ] = \"right\"","title":"LINK_TO_POSITION[\"acomp\"]"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#biaffinedependencyparserpredictor","text":"@Predictor . register ( \"biaffine_dependency_parser\" , exist_ok = True ) class BiaffineDependencyParserPredictor ( Predictor ): | def __init__ ( | self , | model : Model , | dataset_reader : DatasetReader , | language : str = \"en_core_web_sm\" | ) -> None Predictor for the BiaffineDependencyParser model.","title":"BiaffineDependencyParserPredictor"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#predict","text":"class BiaffineDependencyParserPredictor ( Predictor ): | ... | def predict ( self , sentence : str ) -> JsonDict Predict a dependency parse for the given sentence.","title":"predict"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#predict_instance","text":"class BiaffineDependencyParserPredictor ( Predictor ): | ... | @overrides | def predict_instance ( self , instance : Instance ) -> JsonDict","title":"predict_instance"},{"location":"models/structured_prediction/predictors/biaffine_dependency_parser/#predict_batch_instance","text":"class BiaffineDependencyParserPredictor ( Predictor ): | ... | @overrides | def predict_batch_instance ( | self , | instances : List [ Instance ] | ) -> List [ JsonDict ]","title":"predict_batch_instance"},{"location":"models/structured_prediction/predictors/constituency_parser/","text":"allennlp_models .structured_prediction .predictors .constituency_parser [SOURCE] LINK_TO_LABEL # LINK_TO_LABEL = { x : \"pos\" for x in TAG_MAP } NODE_TYPE_TO_STYLE # NODE_TYPE_TO_STYLE = { x : [ \"color0\" ] for x in TAG_MAP } NODE_TYPE_TO_STYLE[\"NP\"] # NODE_TYPE_TO_STYLE [ \"NP\" ] = [ \"color1\" ] NODE_TYPE_TO_STYLE[\"NX\"] # NODE_TYPE_TO_STYLE [ \"NX\" ] = [ \"color1\" ] NODE_TYPE_TO_STYLE[\"QP\"] # NODE_TYPE_TO_STYLE [ \"QP\" ] = [ \"color1\" ] NODE_TYPE_TO_STYLE[\"NAC\"] # NODE_TYPE_TO_STYLE [ \"NAC\" ] = [ \"color1\" ] NODE_TYPE_TO_STYLE[\"VP\"] # NODE_TYPE_TO_STYLE [ \"VP\" ] = [ \"color2\" ] NODE_TYPE_TO_STYLE[\"S\"] # NODE_TYPE_TO_STYLE [ \"S\" ] = [ \"color3\" ] NODE_TYPE_TO_STYLE[\"SQ\"] # NODE_TYPE_TO_STYLE [ \"SQ\" ] = [ \"color3\" ] NODE_TYPE_TO_STYLE[\"SBAR\"] # NODE_TYPE_TO_STYLE [ \"SBAR\" ] = [ \"color3\" ] NODE_TYPE_TO_STYLE[\"SBARQ\"] # NODE_TYPE_TO_STYLE [ \"SBARQ\" ] = [ \"color3\" ] NODE_TYPE_TO_STYLE[\"SINQ\"] # NODE_TYPE_TO_STYLE [ \"SINQ\" ] = [ \"color3\" ] NODE_TYPE_TO_STYLE[\"FRAG\"] # NODE_TYPE_TO_STYLE [ \"FRAG\" ] = [ \"color3\" ] NODE_TYPE_TO_STYLE[\"X\"] # NODE_TYPE_TO_STYLE [ \"X\" ] = [ \"color3\" ] NODE_TYPE_TO_STYLE[\"WHADVP\"] # NODE_TYPE_TO_STYLE [ \"WHADVP\" ] = [ \"color4\" ] NODE_TYPE_TO_STYLE[\"WHADJP\"] # NODE_TYPE_TO_STYLE [ \"WHADJP\" ] = [ \"color4\" ] NODE_TYPE_TO_STYLE[\"WHNP\"] # NODE_TYPE_TO_STYLE [ \"WHNP\" ] = [ \"color4\" ] NODE_TYPE_TO_STYLE[\"WHPP\"] # NODE_TYPE_TO_STYLE [ \"WHPP\" ] = [ \"color4\" ] NODE_TYPE_TO_STYLE[\"PP\"] # NODE_TYPE_TO_STYLE [ \"PP\" ] = [ \"color6\" ] NODE_TYPE_TO_STYLE[\"ADJP\"] # NODE_TYPE_TO_STYLE [ \"ADJP\" ] = [ \"color5\" ] NODE_TYPE_TO_STYLE[\"ADVP\"] # NODE_TYPE_TO_STYLE [ \"ADVP\" ] = [ \"color5\" ] NODE_TYPE_TO_STYLE[\"CONJP\"] # NODE_TYPE_TO_STYLE [ \"CONJP\" ] = [ \"color5\" ] NODE_TYPE_TO_STYLE[\"INTJ\"] # NODE_TYPE_TO_STYLE [ \"INTJ\" ] = [ \"color5\" ] NODE_TYPE_TO_STYLE[\"LST\"] # NODE_TYPE_TO_STYLE [ \"LST\" ] = [ \"color5\" , \"seq\" ] NODE_TYPE_TO_STYLE[\"PRN\"] # NODE_TYPE_TO_STYLE [ \"PRN\" ] = [ \"color5\" ] NODE_TYPE_TO_STYLE[\"PRT\"] # NODE_TYPE_TO_STYLE [ \"PRT\" ] = [ \"color5\" ] NODE_TYPE_TO_STYLE[\"RRC\"] # NODE_TYPE_TO_STYLE [ \"RRC\" ] = [ \"color5\" ] NODE_TYPE_TO_STYLE[\"UCP\"] # NODE_TYPE_TO_STYLE [ \"UCP\" ] = [ \"color5\" ] ConstituencyParserPredictor # @Predictor . register ( \"constituency_parser\" ) class ConstituencyParserPredictor ( Predictor ): | def __init__ ( | self , | model : Model , | dataset_reader : DatasetReader , | language : str = \"en_core_web_sm\" | ) -> None Predictor for the SpanConstituencyParser model. predict # class ConstituencyParserPredictor ( Predictor ): | ... | def predict ( self , sentence : str ) -> JsonDict Predict a constituency parse for the given sentence. Parameters \u00b6 sentence : str The sentence to parse. Returns \u00b6 A dictionary representation of the constituency tree. predict_instance # class ConstituencyParserPredictor ( Predictor ): | ... | @overrides | def predict_instance ( self , instance : Instance ) -> JsonDict predict_batch_instance # class ConstituencyParserPredictor ( Predictor ): | ... | @overrides | def predict_batch_instance ( | self , | instances : List [ Instance ] | ) -> List [ JsonDict ]","title":"constituency_parser"},{"location":"models/structured_prediction/predictors/constituency_parser/#link_to_label","text":"LINK_TO_LABEL = { x : \"pos\" for x in TAG_MAP }","title":"LINK_TO_LABEL"},{"location":"models/structured_prediction/predictors/constituency_parser/#node_type_to_style","text":"NODE_TYPE_TO_STYLE = { x : [ \"color0\" ] for x in TAG_MAP }","title":"NODE_TYPE_TO_STYLE"},{"location":"models/structured_prediction/predictors/constituency_parser/#node_type_to_stylenp","text":"NODE_TYPE_TO_STYLE [ \"NP\" ] = [ \"color1\" ]","title":"NODE_TYPE_TO_STYLE[\"NP\"]"},{"location":"models/structured_prediction/predictors/constituency_parser/#node_type_to_stylenx","text":"NODE_TYPE_TO_STYLE [ \"NX\" ] = [ \"color1\" ]","title":"NODE_TYPE_TO_STYLE[\"NX\"]"},{"location":"models/structured_prediction/predictors/constituency_parser/#node_type_to_styleqp","text":"NODE_TYPE_TO_STYLE [ \"QP\" ] = [ \"color1\" ]","title":"NODE_TYPE_TO_STYLE[\"QP\"]"},{"location":"models/structured_prediction/predictors/constituency_parser/#node_type_to_stylenac","text":"NODE_TYPE_TO_STYLE [ \"NAC\" ] = [ \"color1\" ]","title":"NODE_TYPE_TO_STYLE[\"NAC\"]"},{"location":"models/structured_prediction/predictors/constituency_parser/#node_type_to_stylevp","text":"NODE_TYPE_TO_STYLE [ \"VP\" ] = [ \"color2\" ]","title":"NODE_TYPE_TO_STYLE[\"VP\"]"},{"location":"models/structured_prediction/predictors/constituency_parser/#node_type_to_styles","text":"NODE_TYPE_TO_STYLE [ \"S\" ] = [ \"color3\" ]","title":"NODE_TYPE_TO_STYLE[\"S\"]"},{"location":"models/structured_prediction/predictors/constituency_parser/#node_type_to_stylesq","text":"NODE_TYPE_TO_STYLE [ \"SQ\" ] = [ \"color3\" ]","title":"NODE_TYPE_TO_STYLE[\"SQ\"]"},{"location":"models/structured_prediction/predictors/constituency_parser/#node_type_to_stylesbar","text":"NODE_TYPE_TO_STYLE [ \"SBAR\" ] = [ \"color3\" ]","title":"NODE_TYPE_TO_STYLE[\"SBAR\"]"},{"location":"models/structured_prediction/predictors/constituency_parser/#node_type_to_stylesbarq","text":"NODE_TYPE_TO_STYLE [ \"SBARQ\" ] = [ \"color3\" ]","title":"NODE_TYPE_TO_STYLE[\"SBARQ\"]"},{"location":"models/structured_prediction/predictors/constituency_parser/#node_type_to_stylesinq","text":"NODE_TYPE_TO_STYLE [ \"SINQ\" ] = [ \"color3\" ]","title":"NODE_TYPE_TO_STYLE[\"SINQ\"]"},{"location":"models/structured_prediction/predictors/constituency_parser/#node_type_to_stylefrag","text":"NODE_TYPE_TO_STYLE [ \"FRAG\" ] = [ \"color3\" ]","title":"NODE_TYPE_TO_STYLE[\"FRAG\"]"},{"location":"models/structured_prediction/predictors/constituency_parser/#node_type_to_stylex","text":"NODE_TYPE_TO_STYLE [ \"X\" ] = [ \"color3\" ]","title":"NODE_TYPE_TO_STYLE[\"X\"]"},{"location":"models/structured_prediction/predictors/constituency_parser/#node_type_to_stylewhadvp","text":"NODE_TYPE_TO_STYLE [ \"WHADVP\" ] = [ \"color4\" ]","title":"NODE_TYPE_TO_STYLE[\"WHADVP\"]"},{"location":"models/structured_prediction/predictors/constituency_parser/#node_type_to_stylewhadjp","text":"NODE_TYPE_TO_STYLE [ \"WHADJP\" ] = [ \"color4\" ]","title":"NODE_TYPE_TO_STYLE[\"WHADJP\"]"},{"location":"models/structured_prediction/predictors/constituency_parser/#node_type_to_stylewhnp","text":"NODE_TYPE_TO_STYLE [ \"WHNP\" ] = [ \"color4\" ]","title":"NODE_TYPE_TO_STYLE[\"WHNP\"]"},{"location":"models/structured_prediction/predictors/constituency_parser/#node_type_to_stylewhpp","text":"NODE_TYPE_TO_STYLE [ \"WHPP\" ] = [ \"color4\" ]","title":"NODE_TYPE_TO_STYLE[\"WHPP\"]"},{"location":"models/structured_prediction/predictors/constituency_parser/#node_type_to_stylepp","text":"NODE_TYPE_TO_STYLE [ \"PP\" ] = [ \"color6\" ]","title":"NODE_TYPE_TO_STYLE[\"PP\"]"},{"location":"models/structured_prediction/predictors/constituency_parser/#node_type_to_styleadjp","text":"NODE_TYPE_TO_STYLE [ \"ADJP\" ] = [ \"color5\" ]","title":"NODE_TYPE_TO_STYLE[\"ADJP\"]"},{"location":"models/structured_prediction/predictors/constituency_parser/#node_type_to_styleadvp","text":"NODE_TYPE_TO_STYLE [ \"ADVP\" ] = [ \"color5\" ]","title":"NODE_TYPE_TO_STYLE[\"ADVP\"]"},{"location":"models/structured_prediction/predictors/constituency_parser/#node_type_to_styleconjp","text":"NODE_TYPE_TO_STYLE [ \"CONJP\" ] = [ \"color5\" ]","title":"NODE_TYPE_TO_STYLE[\"CONJP\"]"},{"location":"models/structured_prediction/predictors/constituency_parser/#node_type_to_styleintj","text":"NODE_TYPE_TO_STYLE [ \"INTJ\" ] = [ \"color5\" ]","title":"NODE_TYPE_TO_STYLE[\"INTJ\"]"},{"location":"models/structured_prediction/predictors/constituency_parser/#node_type_to_stylelst","text":"NODE_TYPE_TO_STYLE [ \"LST\" ] = [ \"color5\" , \"seq\" ]","title":"NODE_TYPE_TO_STYLE[\"LST\"]"},{"location":"models/structured_prediction/predictors/constituency_parser/#node_type_to_styleprn","text":"NODE_TYPE_TO_STYLE [ \"PRN\" ] = [ \"color5\" ]","title":"NODE_TYPE_TO_STYLE[\"PRN\"]"},{"location":"models/structured_prediction/predictors/constituency_parser/#node_type_to_styleprt","text":"NODE_TYPE_TO_STYLE [ \"PRT\" ] = [ \"color5\" ]","title":"NODE_TYPE_TO_STYLE[\"PRT\"]"},{"location":"models/structured_prediction/predictors/constituency_parser/#node_type_to_stylerrc","text":"NODE_TYPE_TO_STYLE [ \"RRC\" ] = [ \"color5\" ]","title":"NODE_TYPE_TO_STYLE[\"RRC\"]"},{"location":"models/structured_prediction/predictors/constituency_parser/#node_type_to_styleucp","text":"NODE_TYPE_TO_STYLE [ \"UCP\" ] = [ \"color5\" ]","title":"NODE_TYPE_TO_STYLE[\"UCP\"]"},{"location":"models/structured_prediction/predictors/constituency_parser/#constituencyparserpredictor","text":"@Predictor . register ( \"constituency_parser\" ) class ConstituencyParserPredictor ( Predictor ): | def __init__ ( | self , | model : Model , | dataset_reader : DatasetReader , | language : str = \"en_core_web_sm\" | ) -> None Predictor for the SpanConstituencyParser model.","title":"ConstituencyParserPredictor"},{"location":"models/structured_prediction/predictors/constituency_parser/#predict","text":"class ConstituencyParserPredictor ( Predictor ): | ... | def predict ( self , sentence : str ) -> JsonDict Predict a constituency parse for the given sentence.","title":"predict"},{"location":"models/structured_prediction/predictors/constituency_parser/#predict_instance","text":"class ConstituencyParserPredictor ( Predictor ): | ... | @overrides | def predict_instance ( self , instance : Instance ) -> JsonDict","title":"predict_instance"},{"location":"models/structured_prediction/predictors/constituency_parser/#predict_batch_instance","text":"class ConstituencyParserPredictor ( Predictor ): | ... | @overrides | def predict_batch_instance ( | self , | instances : List [ Instance ] | ) -> List [ JsonDict ]","title":"predict_batch_instance"},{"location":"models/structured_prediction/predictors/openie/","text":"allennlp_models .structured_prediction .predictors .openie [SOURCE] join_mwp # def join_mwp ( tags : List [ str ]) -> List [ str ] Join multi-word predicates to a single predicate ('V') token. make_oie_string # def make_oie_string ( tokens : List [ Token ], tags : List [ str ]) -> str Converts a list of model outputs (i.e., a list of lists of bio tags, each pertaining to a single word), returns an inline bracket representation of the prediction. get_predicate_indices # def get_predicate_indices ( tags : List [ str ]) -> List [ int ] Return the word indices of a predicate in BIO tags. get_predicate_text # def get_predicate_text ( sent_tokens : List [ Token ], tags : List [ str ] ) -> str Get the predicate in this prediction. predicates_overlap # def predicates_overlap ( tags1 : List [ str ], tags2 : List [ str ]) -> bool Tests whether the predicate in BIO tags1 overlap with those of tags2. get_coherent_next_tag # def get_coherent_next_tag ( prev_label : str , cur_label : str ) -> str Generate a coherent tag, given previous tag and current label. merge_overlapping_predictions # def merge_overlapping_predictions ( tags1 : List [ str ], tags2 : List [ str ] ) -> List [ str ] Merge two predictions into one. Assumes the predicate in tags1 overlap with the predicate of tags2. consolidate_predictions # def consolidate_predictions ( outputs : List [ List [ str ]], sent_tokens : List [ Token ] ) -> Dict [ str , List [ str ]] Identify that certain predicates are part of a multiword predicate (e.g., \"decided to run\") in which case, we don't need to return the embedded predicate (\"run\"). sanitize_label # def sanitize_label ( label : str ) -> str Sanitize a BIO label - this deals with OIE labels sometimes having some noise, as parentheses. OpenIePredictor # @Predictor . register ( \"open_information_extraction\" ) class OpenIePredictor ( Predictor ): | def __init__ ( | self , | model : Model , | dataset_reader : DatasetReader , | language : str = \"en_core_web_sm\" | ) -> None Predictor for the SemanticRolelabeler model (in its Open Information variant). Used by online demo and for prediction on an input file using command line. predict_json # class OpenIePredictor ( Predictor ): | ... | @overrides | def predict_json ( self , inputs : JsonDict ) -> JsonDict Create instance(s) after predicting the format. One sentence containing multiple verbs will lead to multiple instances. Expects JSON that looks like {\"sentence\": \"...\"} Returns a JSON that looks like: {\"tokens\": [...], \"tag_spans\": [{\"ARG0\": \"...\", \"V\": \"...\", \"ARG1\": \"...\", ...}]}","title":"openie"},{"location":"models/structured_prediction/predictors/openie/#join_mwp","text":"def join_mwp ( tags : List [ str ]) -> List [ str ] Join multi-word predicates to a single predicate ('V') token.","title":"join_mwp"},{"location":"models/structured_prediction/predictors/openie/#make_oie_string","text":"def make_oie_string ( tokens : List [ Token ], tags : List [ str ]) -> str Converts a list of model outputs (i.e., a list of lists of bio tags, each pertaining to a single word), returns an inline bracket representation of the prediction.","title":"make_oie_string"},{"location":"models/structured_prediction/predictors/openie/#get_predicate_indices","text":"def get_predicate_indices ( tags : List [ str ]) -> List [ int ] Return the word indices of a predicate in BIO tags.","title":"get_predicate_indices"},{"location":"models/structured_prediction/predictors/openie/#get_predicate_text","text":"def get_predicate_text ( sent_tokens : List [ Token ], tags : List [ str ] ) -> str Get the predicate in this prediction.","title":"get_predicate_text"},{"location":"models/structured_prediction/predictors/openie/#predicates_overlap","text":"def predicates_overlap ( tags1 : List [ str ], tags2 : List [ str ]) -> bool Tests whether the predicate in BIO tags1 overlap with those of tags2.","title":"predicates_overlap"},{"location":"models/structured_prediction/predictors/openie/#get_coherent_next_tag","text":"def get_coherent_next_tag ( prev_label : str , cur_label : str ) -> str Generate a coherent tag, given previous tag and current label.","title":"get_coherent_next_tag"},{"location":"models/structured_prediction/predictors/openie/#merge_overlapping_predictions","text":"def merge_overlapping_predictions ( tags1 : List [ str ], tags2 : List [ str ] ) -> List [ str ] Merge two predictions into one. Assumes the predicate in tags1 overlap with the predicate of tags2.","title":"merge_overlapping_predictions"},{"location":"models/structured_prediction/predictors/openie/#consolidate_predictions","text":"def consolidate_predictions ( outputs : List [ List [ str ]], sent_tokens : List [ Token ] ) -> Dict [ str , List [ str ]] Identify that certain predicates are part of a multiword predicate (e.g., \"decided to run\") in which case, we don't need to return the embedded predicate (\"run\").","title":"consolidate_predictions"},{"location":"models/structured_prediction/predictors/openie/#sanitize_label","text":"def sanitize_label ( label : str ) -> str Sanitize a BIO label - this deals with OIE labels sometimes having some noise, as parentheses.","title":"sanitize_label"},{"location":"models/structured_prediction/predictors/openie/#openiepredictor","text":"@Predictor . register ( \"open_information_extraction\" ) class OpenIePredictor ( Predictor ): | def __init__ ( | self , | model : Model , | dataset_reader : DatasetReader , | language : str = \"en_core_web_sm\" | ) -> None Predictor for the SemanticRolelabeler model (in its Open Information variant). Used by online demo and for prediction on an input file using command line.","title":"OpenIePredictor"},{"location":"models/structured_prediction/predictors/openie/#predict_json","text":"class OpenIePredictor ( Predictor ): | ... | @overrides | def predict_json ( self , inputs : JsonDict ) -> JsonDict Create instance(s) after predicting the format. One sentence containing multiple verbs will lead to multiple instances. Expects JSON that looks like {\"sentence\": \"...\"} Returns a JSON that looks like: {\"tokens\": [...], \"tag_spans\": [{\"ARG0\": \"...\", \"V\": \"...\", \"ARG1\": \"...\", ...}]}","title":"predict_json"},{"location":"models/structured_prediction/predictors/srl/","text":"allennlp_models .structured_prediction .predictors .srl [SOURCE] SemanticRoleLabelerPredictor # @Predictor . register ( \"semantic_role_labeling\" ) class SemanticRoleLabelerPredictor ( Predictor ): | def __init__ ( | self , | model : Model , | dataset_reader : DatasetReader , | language : str = \"en_core_web_sm\" | ) -> None Predictor for the SemanticRoleLabeler model. predict # class SemanticRoleLabelerPredictor ( Predictor ): | ... | def predict ( self , sentence : str ) -> JsonDict Predicts the semantic roles of the supplied sentence and returns a dictionary with the results. {\"words\": [...], \"verbs\": [ {\"verb\": \"...\", \"description\": \"...\", \"tags\": [...]}, ... {\"verb\": \"...\", \"description\": \"...\", \"tags\": [...]}, ]} Parameters \u00b6 sentence, str The sentence to parse via semantic role labeling. Returns \u00b6 A dictionary representation of the semantic roles in the sentence. predict_tokenized # class SemanticRoleLabelerPredictor ( Predictor ): | ... | def predict_tokenized ( self , tokenized_sentence : List [ str ]) -> JsonDict Predicts the semantic roles of the supplied sentence tokens and returns a dictionary with the results. Parameters \u00b6 tokenized_sentence, List[str] The sentence tokens to parse via semantic role labeling. Returns \u00b6 A dictionary representation of the semantic roles in the sentence. make_srl_string # class SemanticRoleLabelerPredictor ( Predictor ): | ... | @staticmethod | def make_srl_string ( words : List [ str ], tags : List [ str ]) -> str tokens_to_instances # class SemanticRoleLabelerPredictor ( Predictor ): | ... | def tokens_to_instances ( self , tokens ) predict_batch_json # class SemanticRoleLabelerPredictor ( Predictor ): | ... | @overrides | def predict_batch_json ( self , inputs : List [ JsonDict ]) -> List [ JsonDict ] Expects JSON that looks like [{\"sentence\": \"...\"}, {\"sentence\": \"...\"}, ...] and returns JSON that looks like [ {\"words\": [...], \"verbs\": [ {\"verb\": \"...\", \"description\": \"...\", \"tags\": [...]}, ... {\"verb\": \"...\", \"description\": \"...\", \"tags\": [...]}, ]}, {\"words\": [...], \"verbs\": [ {\"verb\": \"...\", \"description\": \"...\", \"tags\": [...]}, ... {\"verb\": \"...\", \"description\": \"...\", \"tags\": [...]}, ]} ] predict_instances # class SemanticRoleLabelerPredictor ( Predictor ): | ... | def predict_instances ( self , instances : List [ Instance ]) -> JsonDict predict_json # class SemanticRoleLabelerPredictor ( Predictor ): | ... | @overrides | def predict_json ( self , inputs : JsonDict ) -> JsonDict Expects JSON that looks like {\"sentence\": \"...\"} and returns JSON that looks like {\"words\": [...], \"verbs\": [ {\"verb\": \"...\", \"description\": \"...\", \"tags\": [...]}, ... {\"verb\": \"...\", \"description\": \"...\", \"tags\": [...]}, ]}","title":"srl"},{"location":"models/structured_prediction/predictors/srl/#semanticrolelabelerpredictor","text":"@Predictor . register ( \"semantic_role_labeling\" ) class SemanticRoleLabelerPredictor ( Predictor ): | def __init__ ( | self , | model : Model , | dataset_reader : DatasetReader , | language : str = \"en_core_web_sm\" | ) -> None Predictor for the SemanticRoleLabeler model.","title":"SemanticRoleLabelerPredictor"},{"location":"models/structured_prediction/predictors/srl/#predict","text":"class SemanticRoleLabelerPredictor ( Predictor ): | ... | def predict ( self , sentence : str ) -> JsonDict Predicts the semantic roles of the supplied sentence and returns a dictionary with the results. {\"words\": [...], \"verbs\": [ {\"verb\": \"...\", \"description\": \"...\", \"tags\": [...]}, ... {\"verb\": \"...\", \"description\": \"...\", \"tags\": [...]}, ]}","title":"predict"},{"location":"models/structured_prediction/predictors/srl/#predict_tokenized","text":"class SemanticRoleLabelerPredictor ( Predictor ): | ... | def predict_tokenized ( self , tokenized_sentence : List [ str ]) -> JsonDict Predicts the semantic roles of the supplied sentence tokens and returns a dictionary with the results.","title":"predict_tokenized"},{"location":"models/structured_prediction/predictors/srl/#make_srl_string","text":"class SemanticRoleLabelerPredictor ( Predictor ): | ... | @staticmethod | def make_srl_string ( words : List [ str ], tags : List [ str ]) -> str","title":"make_srl_string"},{"location":"models/structured_prediction/predictors/srl/#tokens_to_instances","text":"class SemanticRoleLabelerPredictor ( Predictor ): | ... | def tokens_to_instances ( self , tokens )","title":"tokens_to_instances"},{"location":"models/structured_prediction/predictors/srl/#predict_batch_json","text":"class SemanticRoleLabelerPredictor ( Predictor ): | ... | @overrides | def predict_batch_json ( self , inputs : List [ JsonDict ]) -> List [ JsonDict ] Expects JSON that looks like [{\"sentence\": \"...\"}, {\"sentence\": \"...\"}, ...] and returns JSON that looks like [ {\"words\": [...], \"verbs\": [ {\"verb\": \"...\", \"description\": \"...\", \"tags\": [...]}, ... {\"verb\": \"...\", \"description\": \"...\", \"tags\": [...]}, ]}, {\"words\": [...], \"verbs\": [ {\"verb\": \"...\", \"description\": \"...\", \"tags\": [...]}, ... {\"verb\": \"...\", \"description\": \"...\", \"tags\": [...]}, ]} ]","title":"predict_batch_json"},{"location":"models/structured_prediction/predictors/srl/#predict_instances","text":"class SemanticRoleLabelerPredictor ( Predictor ): | ... | def predict_instances ( self , instances : List [ Instance ]) -> JsonDict","title":"predict_instances"},{"location":"models/structured_prediction/predictors/srl/#predict_json","text":"class SemanticRoleLabelerPredictor ( Predictor ): | ... | @overrides | def predict_json ( self , inputs : JsonDict ) -> JsonDict Expects JSON that looks like {\"sentence\": \"...\"} and returns JSON that looks like {\"words\": [...], \"verbs\": [ {\"verb\": \"...\", \"description\": \"...\", \"tags\": [...]}, ... {\"verb\": \"...\", \"description\": \"...\", \"tags\": [...]}, ]}","title":"predict_json"},{"location":"models/structured_prediction/predictors/util/","text":"allennlp_models .structured_prediction .predictors .util [SOURCE] Copied from spaCy version 2.x TAG_MAP # TAG_MAP = { \".\" : { POS : PUNCT , \"PunctType\" : \"peri\" }, \",\" : { POS : PUNCT , \"PunctType\" : \"comm\" }, \"-LRB- ...","title":"util"},{"location":"models/structured_prediction/predictors/util/#tag_map","text":"TAG_MAP = { \".\" : { POS : PUNCT , \"PunctType\" : \"peri\" }, \",\" : { POS : PUNCT , \"PunctType\" : \"comm\" }, \"-LRB- ...","title":"TAG_MAP"},{"location":"models/structured_prediction/tools/convert_openie_to_conll/","text":"allennlp_models .structured_prediction .tools .convert_openie_to_conll [SOURCE] Extraction # Extraction = namedtuple ( \"Extraction\" , # Open IE extraction [ \"sent\" , # Sentence in which this ... Element # Element = namedtuple ( \"Element\" , # An element (predicate or argument) in an Open IE extraction [ ... main # def main ( inp_fn : str , domain : str , out_fn : str ) -> None inp_fn: str Path to file from which to read Open IE extractions in Open IE4's format. domain: str Domain to be used when writing CoNLL format. out_fn: str Path to file to which to write the CoNLL format Open IE extractions. safe_zip # def safe_zip ( * args ) Zip which ensures all lists are of same size. char_to_word_index # def char_to_word_index ( char_ind : int , sent : str ) -> int Convert a character index to word index in the given sentence. element_from_span # def element_from_span ( span : List [ Token ], span_type : str ) -> Element Return an Element from span (list of spaCy tokens). split_predicate # def split_predicate ( ex : Extraction ) -> Extraction Ensure single word predicate by adding \"before-predicate\" and \"after-predicate\" arguments. extraction_to_conll # def extraction_to_conll ( ex : Extraction ) -> List [ str ] Return a CoNLL representation of a given input Extraction. interpret_span # def interpret_span ( text_spans : str ) -> Tuple [ int , int ] Return an integer tuple from textual representation of closed/open spans. interpret_element # def interpret_element ( element_type : str , text : str , span : str ) -> Element Construct an Element instance from regexp groups. parse_element # def parse_element ( raw_element : str ) -> List [ Element ] Parse a raw element into text and indices (integers). read # def read ( fn : str ) -> Iterable [ List [ Extraction ]] convert_sent_to_conll # def convert_sent_to_conll ( sent_ls : List [ Extraction ] ) -> Iterable [ Tuple [ str , ... ]] Given a list of extractions for a single sentence, converts it to CoNLL representation. pad_line_to_ontonotes # def pad_line_to_ontonotes ( line : Tuple [ str , ... ], domain : str ) -> List [ str ] Pad line to conform to OntoNotes representation. convert_sent_to_conll_str # def convert_sent_to_conll_str ( sent_ls : List [ Extraction ], domain : str ) -> str Given a dictionary from sentence -> extractions, return a corresponding CoNLL representation.","title":"convert_openie_to_conll"},{"location":"models/structured_prediction/tools/convert_openie_to_conll/#extraction","text":"Extraction = namedtuple ( \"Extraction\" , # Open IE extraction [ \"sent\" , # Sentence in which this ...","title":"Extraction"},{"location":"models/structured_prediction/tools/convert_openie_to_conll/#element","text":"Element = namedtuple ( \"Element\" , # An element (predicate or argument) in an Open IE extraction [ ...","title":"Element"},{"location":"models/structured_prediction/tools/convert_openie_to_conll/#main","text":"def main ( inp_fn : str , domain : str , out_fn : str ) -> None inp_fn: str Path to file from which to read Open IE extractions in Open IE4's format. domain: str Domain to be used when writing CoNLL format. out_fn: str Path to file to which to write the CoNLL format Open IE extractions.","title":"main"},{"location":"models/structured_prediction/tools/convert_openie_to_conll/#safe_zip","text":"def safe_zip ( * args ) Zip which ensures all lists are of same size.","title":"safe_zip"},{"location":"models/structured_prediction/tools/convert_openie_to_conll/#char_to_word_index","text":"def char_to_word_index ( char_ind : int , sent : str ) -> int Convert a character index to word index in the given sentence.","title":"char_to_word_index"},{"location":"models/structured_prediction/tools/convert_openie_to_conll/#element_from_span","text":"def element_from_span ( span : List [ Token ], span_type : str ) -> Element Return an Element from span (list of spaCy tokens).","title":"element_from_span"},{"location":"models/structured_prediction/tools/convert_openie_to_conll/#split_predicate","text":"def split_predicate ( ex : Extraction ) -> Extraction Ensure single word predicate by adding \"before-predicate\" and \"after-predicate\" arguments.","title":"split_predicate"},{"location":"models/structured_prediction/tools/convert_openie_to_conll/#extraction_to_conll","text":"def extraction_to_conll ( ex : Extraction ) -> List [ str ] Return a CoNLL representation of a given input Extraction.","title":"extraction_to_conll"},{"location":"models/structured_prediction/tools/convert_openie_to_conll/#interpret_span","text":"def interpret_span ( text_spans : str ) -> Tuple [ int , int ] Return an integer tuple from textual representation of closed/open spans.","title":"interpret_span"},{"location":"models/structured_prediction/tools/convert_openie_to_conll/#interpret_element","text":"def interpret_element ( element_type : str , text : str , span : str ) -> Element Construct an Element instance from regexp groups.","title":"interpret_element"},{"location":"models/structured_prediction/tools/convert_openie_to_conll/#parse_element","text":"def parse_element ( raw_element : str ) -> List [ Element ] Parse a raw element into text and indices (integers).","title":"parse_element"},{"location":"models/structured_prediction/tools/convert_openie_to_conll/#read","text":"def read ( fn : str ) -> Iterable [ List [ Extraction ]]","title":"read"},{"location":"models/structured_prediction/tools/convert_openie_to_conll/#convert_sent_to_conll","text":"def convert_sent_to_conll ( sent_ls : List [ Extraction ] ) -> Iterable [ Tuple [ str , ... ]] Given a list of extractions for a single sentence, converts it to CoNLL representation.","title":"convert_sent_to_conll"},{"location":"models/structured_prediction/tools/convert_openie_to_conll/#pad_line_to_ontonotes","text":"def pad_line_to_ontonotes ( line : Tuple [ str , ... ], domain : str ) -> List [ str ] Pad line to conform to OntoNotes representation.","title":"pad_line_to_ontonotes"},{"location":"models/structured_prediction/tools/convert_openie_to_conll/#convert_sent_to_conll_str","text":"def convert_sent_to_conll_str ( sent_ls : List [ Extraction ], domain : str ) -> str Given a dictionary from sentence -> extractions, return a corresponding CoNLL representation.","title":"convert_sent_to_conll_str"},{"location":"models/structured_prediction/tools/write_srl_predictions_to_conll_format/","text":"allennlp_models .structured_prediction .tools .write_srl_predictions_to_conll_format [SOURCE] flake8: noqa main # def main ( serialization_directory : str , device : int , data : str , prefix : str , domain : str = None ) serialization_directory : str, required. The directory containing the serialized weights. device: int, default = -1 The device to run the evaluation on. data: str, default = None The data to evaluate on. By default, we use the validation data from the original experiment. prefix: str, default=\"\" The prefix to prepend to the generated gold and prediction files, to distinguish different models/data. domain: str, optional (default = None) If passed, filters the ontonotes evaluation/test dataset to only contain the specified domain. This overwrites the domain in the config file from the model, to allow evaluation on domains other than the one the model was trained on.","title":"write_srl_predictions_to_conll_format"},{"location":"models/structured_prediction/tools/write_srl_predictions_to_conll_format/#main","text":"def main ( serialization_directory : str , device : int , data : str , prefix : str , domain : str = None ) serialization_directory : str, required. The directory containing the serialized weights. device: int, default = -1 The device to run the evaluation on. data: str, default = None The data to evaluate on. By default, we use the validation data from the original experiment. prefix: str, default=\"\" The prefix to prepend to the generated gold and prediction files, to distinguish different models/data. domain: str, optional (default = None) If passed, filters the ontonotes evaluation/test dataset to only contain the specified domain. This overwrites the domain in the config file from the model, to allow evaluation on domains other than the one the model was trained on.","title":"main"},{"location":"models/tagging/dataset_readers/ccgbank/","text":"allennlp_models .tagging .dataset_readers .ccgbank [SOURCE] CcgBankDatasetReader # @DatasetReader . register ( \"ccgbank\" ) class CcgBankDatasetReader ( DatasetReader ): | def __init__ ( | self , | token_indexers : Dict [ str , TokenIndexer ] = None , | tag_label : str = \"ccg\" , | feature_labels : Sequence [ str ] = (), | label_namespace : str = \"labels\" , | ** kwargs | ) -> None Reads data in the \"machine-readable derivation\" format of the CCGbank dataset. (see https://catalog.ldc.upenn.edu/docs/LDC2005T13/CCGbankManual.pdf, section D.2) In particular, it pulls out the leaf nodes, which are represented as (<L ccg_category modified_pos original_pos token predicate_arg_category>) The tarballed version of the dataset contains many files worth of this data, in files /data/AUTO/xx/wsj_xxxx.auto This dataset reader expects a single text file. Accordingly, if you're using that dataset, you'll need to first concatenate some of those files into a training set, a validation set, and a test set. Registered as a DatasetReader with name \"ccgbank\". Parameters \u00b6 token_indexers : Dict[str, TokenIndexer] , optional (default = {\"tokens\": SingleIdTokenIndexer()} ) We use this to define the input representation for the text. See TokenIndexer . Note that the output tags will always correspond to single token IDs based on how they are pre-tokenised in the data file. tag_label : str , optional (default = ccg ) Specify ccg , modified_pos , original_pos , or predicate_arg to have that tag loaded into the instance field tag . feature_labels : Sequence[str] , optional (default = () ) These labels will be loaded as features into the corresponding instance fields: ccg -> ccg_tags , modified_pos -> modified_pos_tags , original_pos -> original_pos_tags , or predicate_arg -> predicate_arg_tags Each will have its own namespace : ccg_tags , modified_pos_tags , original_pos_tags , predicate_arg_tags . If you want to use one of the tags as a feature in your model, it should be specified here. label_namespace : str , optional (default = labels ) Specifies the namespace for the chosen tag_label . text_to_instance # class CcgBankDatasetReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | tokens : List [ str ], | ccg_categories : List [ str ] = None , | original_pos_tags : List [ str ] = None , | modified_pos_tags : List [ str ] = None , | predicate_arg_categories : List [ str ] = None | ) -> Instance We take pre-tokenized input here, because we don't have a tokenizer in this class. Parameters \u00b6 tokens : List[str] The tokens in a given sentence. ccg_categories : List[str] , optional (default = None ) The CCG categories for the words in the sentence. (e.g. N/N) original_pos_tags : List[str] , optional (default = None ) The tag assigned to the word in the Penn Treebank. modified_pos_tags : List[str] , optional (default = None ) The POS tag might have changed during the translation to CCG. predicate_arg_categories : List[str] , optional (default = None ) Encodes the word-word dependencies in the underlying predicate- argument structure. Returns \u00b6 An Instance containing the following fields: tokens : TextField The tokens in the sentence. tags : SequenceLabelField The tags corresponding to the tag_label constructor argument. feature_label_tags : SequenceLabelField Tags corresponding to each feature_label (if any) specified in the feature_labels constructor argument.","title":"ccgbank"},{"location":"models/tagging/dataset_readers/ccgbank/#ccgbankdatasetreader","text":"@DatasetReader . register ( \"ccgbank\" ) class CcgBankDatasetReader ( DatasetReader ): | def __init__ ( | self , | token_indexers : Dict [ str , TokenIndexer ] = None , | tag_label : str = \"ccg\" , | feature_labels : Sequence [ str ] = (), | label_namespace : str = \"labels\" , | ** kwargs | ) -> None Reads data in the \"machine-readable derivation\" format of the CCGbank dataset. (see https://catalog.ldc.upenn.edu/docs/LDC2005T13/CCGbankManual.pdf, section D.2) In particular, it pulls out the leaf nodes, which are represented as (<L ccg_category modified_pos original_pos token predicate_arg_category>) The tarballed version of the dataset contains many files worth of this data, in files /data/AUTO/xx/wsj_xxxx.auto This dataset reader expects a single text file. Accordingly, if you're using that dataset, you'll need to first concatenate some of those files into a training set, a validation set, and a test set. Registered as a DatasetReader with name \"ccgbank\".","title":"CcgBankDatasetReader"},{"location":"models/tagging/dataset_readers/ccgbank/#text_to_instance","text":"class CcgBankDatasetReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | tokens : List [ str ], | ccg_categories : List [ str ] = None , | original_pos_tags : List [ str ] = None , | modified_pos_tags : List [ str ] = None , | predicate_arg_categories : List [ str ] = None | ) -> Instance We take pre-tokenized input here, because we don't have a tokenizer in this class.","title":"text_to_instance"},{"location":"models/tagging/dataset_readers/conll2000/","text":"allennlp_models .tagging .dataset_readers .conll2000 [SOURCE] Conll2000DatasetReader # @DatasetReader . register ( \"conll2000\" ) class Conll2000DatasetReader ( DatasetReader ): | def __init__ ( | self , | token_indexers : Dict [ str , TokenIndexer ] = None , | tag_label : str = \"chunk\" , | feature_labels : Sequence [ str ] = (), | convert_to_coding_scheme : Optional [ str ] = None , | label_namespace : str = \"labels\" , | ** kwargs | ) -> None Reads instances from a pretokenised file where each line is in the following format: WORD POS-TAG CHUNK-TAG with a blank line indicating the end of each sentence and converts it into a Dataset suitable for sequence tagging. Each Instance contains the words in the \"tokens\" TextField . The values corresponding to the tag_label values will get loaded into the \"tags\" SequenceLabelField . And if you specify any feature_labels (you probably shouldn't), the corresponding values will get loaded into their own SequenceLabelField s. Registered as a DatasetReader with name \"conll2000\". Parameters \u00b6 token_indexers : Dict[str, TokenIndexer] , optional (default = {\"tokens\": SingleIdTokenIndexer()} ) We use this to define the input representation for the text. See TokenIndexer . tag_label : str , optional (default = chunk ) Specify pos , or chunk to have that tag loaded into the instance field tag . feature_labels : Sequence[str] , optional (default = () ) These labels will be loaded as features into the corresponding instance fields: pos -> pos_tags or chunk -> chunk_tags . Each will have its own namespace : pos_tags or chunk_tags . If you want to use one of the tags as a feature in your model, it should be specified here. convert_to_coding_scheme : str , optional (default = BIO ) Specifies the coding scheme for chunk_labels . Conll2000DatasetReader assumes a coding scheme of input data is BIO . Valid options are None and BIOUL . The None default maintains the original BIO scheme in the CoNLL 2000 chunking data. In the BIO scheme, B is a token starting a span, I is a token continuing a span, and O is a token outside of a span. coding_scheme : str , optional (default = BIO ) This parameter is deprecated. If you specify coding_scheme to BIO , consider simply removing it or specifying convert_to_coding_scheme to None . If you want to specify BIOUL for coding_scheme , replace it with convert_to_coding_scheme . label_namespace : str , optional (default = labels ) Specifies the namespace for the chosen tag_label . text_to_instance # class Conll2000DatasetReader ( DatasetReader ): | ... | def text_to_instance ( | self , | tokens : List [ Token ], | pos_tags : List [ str ] = None , | chunk_tags : List [ str ] = None | ) -> Instance We take pre-tokenized input here, because we don't have a tokenizer in this class.","title":"conll2000"},{"location":"models/tagging/dataset_readers/conll2000/#conll2000datasetreader","text":"@DatasetReader . register ( \"conll2000\" ) class Conll2000DatasetReader ( DatasetReader ): | def __init__ ( | self , | token_indexers : Dict [ str , TokenIndexer ] = None , | tag_label : str = \"chunk\" , | feature_labels : Sequence [ str ] = (), | convert_to_coding_scheme : Optional [ str ] = None , | label_namespace : str = \"labels\" , | ** kwargs | ) -> None Reads instances from a pretokenised file where each line is in the following format: WORD POS-TAG CHUNK-TAG with a blank line indicating the end of each sentence and converts it into a Dataset suitable for sequence tagging. Each Instance contains the words in the \"tokens\" TextField . The values corresponding to the tag_label values will get loaded into the \"tags\" SequenceLabelField . And if you specify any feature_labels (you probably shouldn't), the corresponding values will get loaded into their own SequenceLabelField s. Registered as a DatasetReader with name \"conll2000\".","title":"Conll2000DatasetReader"},{"location":"models/tagging/dataset_readers/conll2000/#text_to_instance","text":"class Conll2000DatasetReader ( DatasetReader ): | ... | def text_to_instance ( | self , | tokens : List [ Token ], | pos_tags : List [ str ] = None , | chunk_tags : List [ str ] = None | ) -> Instance We take pre-tokenized input here, because we don't have a tokenizer in this class.","title":"text_to_instance"},{"location":"models/tagging/dataset_readers/conll2003/","text":"allennlp_models .tagging .dataset_readers .conll2003 [SOURCE]","title":"conll2003"},{"location":"models/tagging/dataset_readers/ontonotes_ner/","text":"allennlp_models .tagging .dataset_readers .ontonotes_ner [SOURCE] OntonotesNamedEntityRecognition # @DatasetReader . register ( \"ontonotes_ner\" ) class OntonotesNamedEntityRecognition ( DatasetReader ): | def __init__ ( | self , | token_indexers : Dict [ str , TokenIndexer ] = None , | domain_identifier : str = None , | coding_scheme : str = \"BIO\" , | ** kwargs | ) -> None This DatasetReader is designed to read in the English OntoNotes v5.0 data for fine-grained named entity recognition. It returns a dataset of instances with the following fields: tokens : TextField The tokens in the sentence. tags : SequenceLabelField A sequence of BIO tags for the NER classes. Note that the \"/pt/\" directory of the Onotonotes dataset representing annotations on the new and old testaments of the Bible are excluded, because they do not contain NER annotations. Parameters \u00b6 token_indexers : Dict[str, TokenIndexer] , optional We similarly use this for both the premise and the hypothesis. See TokenIndexer . Default is {\"tokens\": SingleIdTokenIndexer()} . domain_identifier : str , optional (default = None ) A string denoting a sub-domain of the Ontonotes 5.0 dataset to use. If present, only conll files under paths containing this domain identifier will be processed. coding_scheme : str , optional (default = None ) The coding scheme to use for the NER labels. Valid options are \"BIO\" or \"BIOUL\". Returns \u00b6 A Dataset of Instances for Fine-Grained NER. text_to_instance # class OntonotesNamedEntityRecognition ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | tokens : List [ Token ], | ner_tags : List [ str ] = None | ) -> Instance We take pre-tokenized input here, because we don't have a tokenizer in this class.","title":"ontonotes_ner"},{"location":"models/tagging/dataset_readers/ontonotes_ner/#ontonotesnamedentityrecognition","text":"@DatasetReader . register ( \"ontonotes_ner\" ) class OntonotesNamedEntityRecognition ( DatasetReader ): | def __init__ ( | self , | token_indexers : Dict [ str , TokenIndexer ] = None , | domain_identifier : str = None , | coding_scheme : str = \"BIO\" , | ** kwargs | ) -> None This DatasetReader is designed to read in the English OntoNotes v5.0 data for fine-grained named entity recognition. It returns a dataset of instances with the following fields: tokens : TextField The tokens in the sentence. tags : SequenceLabelField A sequence of BIO tags for the NER classes. Note that the \"/pt/\" directory of the Onotonotes dataset representing annotations on the new and old testaments of the Bible are excluded, because they do not contain NER annotations.","title":"OntonotesNamedEntityRecognition"},{"location":"models/tagging/dataset_readers/ontonotes_ner/#text_to_instance","text":"class OntonotesNamedEntityRecognition ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | tokens : List [ Token ], | ner_tags : List [ str ] = None | ) -> Instance We take pre-tokenized input here, because we don't have a tokenizer in this class.","title":"text_to_instance"},{"location":"models/tagging/models/crf_tagger/","text":"allennlp_models .tagging .models .crf_tagger [SOURCE] CrfTagger # @Model . register ( \"crf_tagger\" ) class CrfTagger ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | encoder : Seq2SeqEncoder , | label_namespace : str = \"labels\" , | feedforward : Optional [ FeedForward ] = None , | label_encoding : Optional [ str ] = None , | include_start_end_transitions : bool = True , | constrain_crf_decoding : bool = None , | calculate_span_f1 : bool = None , | dropout : Optional [ float ] = None , | verbose_metrics : bool = False , | initializer : InitializerApplicator = InitializerApplicator (), | top_k : int = 1 , | ignore_loss_on_o_tags : bool = False , | ** kwargs | ) -> None The CrfTagger encodes a sequence of text with a Seq2SeqEncoder , then uses a Conditional Random Field model to predict a tag for each token in the sequence. Registered as a Model with name \"crf_tagger\". Parameters \u00b6 vocab : Vocabulary A Vocabulary, required in order to compute sizes for input/output projections. text_field_embedder : TextFieldEmbedder Used to embed the tokens TextField we get as input to the model. encoder : Seq2SeqEncoder The encoder that we will use in between embedding tokens and predicting output tags. label_namespace : str , optional (default = labels ) This is needed to compute the SpanBasedF1Measure metric. Unless you did something unusual, the default value should be what you want. feedforward : FeedForward , optional (default = None ) An optional feedforward layer to apply after the encoder. label_encoding : str , optional (default = None ) Label encoding to use when calculating span f1 and constraining the CRF at decoding time . Valid options are \"BIO\", \"BIOUL\", \"IOB1\", \"BMES\". Required if calculate_span_f1 or constrain_crf_decoding is true. include_start_end_transitions : bool , optional (default = True ) Whether to include start and end transition parameters in the CRF. constrain_crf_decoding : bool , optional (default = None ) If True , the CRF is constrained at decoding time to produce valid sequences of tags. If this is True , then label_encoding is required. If None and label_encoding is specified, this is set to True . If None and label_encoding is not specified, it defaults to False . calculate_span_f1 : bool , optional (default = None ) Calculate span-level F1 metrics during training. If this is True , then label_encoding is required. If None and label_encoding is specified, this is set to True . If None and label_encoding is not specified, it defaults to False . dropout : float , optional (default = None ) Dropout probability. verbose_metrics : bool , optional (default = False ) If true, metrics will be returned per label class in addition to the overall statistics. initializer : InitializerApplicator , optional (default = InitializerApplicator() ) Used to initialize the model parameters. top_k : int , optional (default = 1 ) If provided, the number of parses to return from the crf in output_dict['top_k_tags']. Top k parses are returned as a list of dicts, where each dictionary is of the form: {\"tags\": List, \"score\": float}. The \"tags\" value for the first dict in the list for each data_item will be the top choice, and will equal the corresponding item in output_dict['tags'] ignore_loss_on_o_tags : bool , optional (default = False ) If True, we compute the loss only for actual spans in tags , and not on O tokens. This is useful for computing gradients of the loss on a single span , for interpretation / attacking. forward # class CrfTagger ( Model ): | ... | @overrides | def forward ( | self , | tokens : TextFieldTensors , | tags : torch . LongTensor = None , | metadata : List [ Dict [ str , Any ]] = None , | ignore_loss_on_o_tags : Optional [ bool ] = None , | ** kwargs | ) -> Dict [ str , torch . Tensor ] Parameters \u00b6 tokens : TextFieldTensors The output of TextField.as_array() , which should typically be passed directly to a TextFieldEmbedder . This output is a dictionary mapping keys to TokenIndexer tensors. At its most basic, using a SingleIdTokenIndexer this is : {\"tokens\": Tensor(batch_size, num_tokens)} . This dictionary will have the same keys as were used for the TokenIndexers when you created the TextField representing your sequence. The dictionary is designed to be passed directly to a TextFieldEmbedder , which knows how to combine different word representations into a single vector per token in your input. tags : torch.LongTensor , optional (default = None ) A torch tensor representing the sequence of integer gold class labels of shape (batch_size, num_tokens) . metadata : List[Dict[str, Any]] , optional (default = None ) metadata containg the original words in the sentence to be tagged under a 'words' key. ignore_loss_on_o_tags : Optional[bool] , optional (default = None ) If True, we compute the loss only for actual spans in tags , and not on O tokens. This is useful for computing gradients of the loss on a single span , for interpretation / attacking. If None , self.ignore_loss_on_o_tags is used instead. Returns \u00b6 An output dictionary consisting of: logits : torch.FloatTensor The logits that are the output of the tag_projection_layer mask : torch.BoolTensor The text field mask for the input tokens tags : List[List[int]] The predicted tags using the Viterbi algorithm. loss : torch.FloatTensor , optional A scalar loss to be optimised. Only computed if gold label tags are provided. make_output_human_readable # class CrfTagger ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Converts the tag ids to the actual tags. output_dict[\"tags\"] is a list of lists of tag_ids, so we use an ugly nested list comprehension. get_metrics # class CrfTagger ( Model ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] default_predictor # class CrfTagger ( Model ): | ... | default_predictor = \"sentence_tagger\"","title":"crf_tagger"},{"location":"models/tagging/models/crf_tagger/#crftagger","text":"@Model . register ( \"crf_tagger\" ) class CrfTagger ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | encoder : Seq2SeqEncoder , | label_namespace : str = \"labels\" , | feedforward : Optional [ FeedForward ] = None , | label_encoding : Optional [ str ] = None , | include_start_end_transitions : bool = True , | constrain_crf_decoding : bool = None , | calculate_span_f1 : bool = None , | dropout : Optional [ float ] = None , | verbose_metrics : bool = False , | initializer : InitializerApplicator = InitializerApplicator (), | top_k : int = 1 , | ignore_loss_on_o_tags : bool = False , | ** kwargs | ) -> None The CrfTagger encodes a sequence of text with a Seq2SeqEncoder , then uses a Conditional Random Field model to predict a tag for each token in the sequence. Registered as a Model with name \"crf_tagger\".","title":"CrfTagger"},{"location":"models/tagging/models/crf_tagger/#forward","text":"class CrfTagger ( Model ): | ... | @overrides | def forward ( | self , | tokens : TextFieldTensors , | tags : torch . LongTensor = None , | metadata : List [ Dict [ str , Any ]] = None , | ignore_loss_on_o_tags : Optional [ bool ] = None , | ** kwargs | ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"models/tagging/models/crf_tagger/#make_output_human_readable","text":"class CrfTagger ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Converts the tag ids to the actual tags. output_dict[\"tags\"] is a list of lists of tag_ids, so we use an ugly nested list comprehension.","title":"make_output_human_readable"},{"location":"models/tagging/models/crf_tagger/#get_metrics","text":"class CrfTagger ( Model ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"models/tagging/models/crf_tagger/#default_predictor","text":"class CrfTagger ( Model ): | ... | default_predictor = \"sentence_tagger\"","title":"default_predictor"},{"location":"models/tagging/predictors/sentence_tagger/","text":"allennlp_models .tagging .predictors .sentence_tagger [SOURCE]","title":"sentence_tagger"},{"location":"models/vision/dataset_readers/gqa/","text":"allennlp_models .vision .dataset_readers .gqa [SOURCE] GQAReader # @DatasetReader . register ( \"gqa\" ) class GQAReader ( VisionReader ): | def __init__ ( | self , | image_dir : Union [ str , PathLike ], | * , | image_loader : Optional [ ImageLoader ] = None , | image_featurizer : Optional [ Lazy [ GridEmbedder ]] = None , | region_detector : Optional [ Lazy [ RegionDetector ]] = None , | answer_vocab : Optional [ Union [ str , Vocabulary ]] = None , | feature_cache_dir : Optional [ Union [ str , PathLike ]] = None , | data_dir : Optional [ Union [ str , PathLike ]] = None , | tokenizer : Tokenizer = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | cuda_device : Optional [ Union [ int , torch . device ]] = None , | max_instances : Optional [ int ] = None , | image_processing_batch_size : int = 8 , | write_to_cache : bool = True | ) -> None Parametersimage_dir: `str` \u00b6 Path to directory containing `png` image files. image_loader : ImageLoader image_featurizer: Lazy[GridEmbedder] The backbone image processor (like a ResNet), whose output will be passed to the region detector for finding object boxes in the image. region_detector: Lazy[RegionDetector] For pulling out regions of the image (both coordinates and features) that will be used by downstream models. data_dir: str Path to directory containing text files for each dataset split. These files contain the sentences and metadata for each task instance. tokenizer: Tokenizer , optional token_indexers: Dict[str, TokenIndexer] text_to_instance # class GQAReader ( VisionReader ): | ... | @overrides | def text_to_instance ( | self , | question : str , | image : Optional [ Union [ str , Tuple [ Tensor , Tensor ]]], | answer : Optional [ Dict [ str , str ]] = None , | * , | use_cache : bool = True | ) -> Optional [ Instance ] apply_token_indexers # class GQAReader ( VisionReader ): | ... | @overrides | def apply_token_indexers ( self , instance : Instance ) -> None","title":"gqa"},{"location":"models/vision/dataset_readers/gqa/#gqareader","text":"@DatasetReader . register ( \"gqa\" ) class GQAReader ( VisionReader ): | def __init__ ( | self , | image_dir : Union [ str , PathLike ], | * , | image_loader : Optional [ ImageLoader ] = None , | image_featurizer : Optional [ Lazy [ GridEmbedder ]] = None , | region_detector : Optional [ Lazy [ RegionDetector ]] = None , | answer_vocab : Optional [ Union [ str , Vocabulary ]] = None , | feature_cache_dir : Optional [ Union [ str , PathLike ]] = None , | data_dir : Optional [ Union [ str , PathLike ]] = None , | tokenizer : Tokenizer = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | cuda_device : Optional [ Union [ int , torch . device ]] = None , | max_instances : Optional [ int ] = None , | image_processing_batch_size : int = 8 , | write_to_cache : bool = True | ) -> None","title":"GQAReader"},{"location":"models/vision/dataset_readers/gqa/#text_to_instance","text":"class GQAReader ( VisionReader ): | ... | @overrides | def text_to_instance ( | self , | question : str , | image : Optional [ Union [ str , Tuple [ Tensor , Tensor ]]], | answer : Optional [ Dict [ str , str ]] = None , | * , | use_cache : bool = True | ) -> Optional [ Instance ]","title":"text_to_instance"},{"location":"models/vision/dataset_readers/gqa/#apply_token_indexers","text":"class GQAReader ( VisionReader ): | ... | @overrides | def apply_token_indexers ( self , instance : Instance ) -> None","title":"apply_token_indexers"},{"location":"models/vision/dataset_readers/vision_reader/","text":"allennlp_models .vision .dataset_readers .vision_reader [SOURCE] VisionReader # class VisionReader ( DatasetReader ): | def __init__ ( | self , | image_dir : Optional [ Union [ str , PathLike ]], | * , | image_loader : Optional [ ImageLoader ] = None , | image_featurizer : Optional [ Lazy [ GridEmbedder ]] = None , | region_detector : Optional [ Lazy [ RegionDetector ]] = None , | feature_cache_dir : Optional [ Union [ str , PathLike ]] = None , | tokenizer : Optional [ Tokenizer ] = None , | token_indexers : Optional [ Dict [ str , TokenIndexer ]] = None , | cuda_device : Optional [ Union [ int , torch . device ]] = None , | max_instances : Optional [ int ] = None , | image_processing_batch_size : int = 8 , | write_to_cache : bool = True | ) -> None Base class for dataset readers for vision tasks. If you don't specify image_loader , image_featurizer , and region_detector , the reader assumes it can get all featurized images from the cache. If you don't specify feature_cache , the reader will featurize all images using the featurization components, and use an internal in-memory cache to catch duplicate images. If you don't specify either of these things, the reader will not produce featurized images at all. Parameters \u00b6 image_dir : str Path to directory containing image files. The structure of the directory doesn't matter. We find images by finding filenames that match *[image_id].jpg . image_loader : ImageLoader , optional The image loading component. image_featurizer : Lazy[GridEmbedder] , optional The backbone image processor (like a ResNet), whose output will be passed to the region detector for finding object boxes in the image. region_detector : Lazy[RegionDetector] , optional For pulling out regions of the image (both coordinates and features) that will be used by downstream models. tokenizer : Tokenizer , optional The Tokenizer to use to tokenize the text. By default, this uses the tokenizer for \"bert-base-uncased\" . token_indexers : Dict[str, TokenIndexer] , optional The TokenIndexer to use. By default, this uses the indexer for \"bert-base-uncased\" . cuda_device : Union[int, torch.device] , optional Either a torch device or a GPU number. This is the GPU we'll use to featurize the images. max_instances : int , optional For debugging, you can use this parameter to limit the number of instances the reader returns. image_processing_batch_size : int The number of images to process at one time while featurizing. Default is 8. write_to_cache : bool , optional (default = True ) Allows the reader to write to the cache. Disabling this is useful if you don't want to accidentally overwrite a cache you already have, or if you don't have write access to the cache you're using. image_featurizer # class VisionReader ( DatasetReader ): | ... | @property | def image_featurizer ( self ) -> Optional [ GridEmbedder ] region_detector # class VisionReader ( DatasetReader ): | ... | @property | def region_detector ( self ) -> Optional [ RegionDetector ]","title":"vision_reader"},{"location":"models/vision/dataset_readers/vision_reader/#visionreader","text":"class VisionReader ( DatasetReader ): | def __init__ ( | self , | image_dir : Optional [ Union [ str , PathLike ]], | * , | image_loader : Optional [ ImageLoader ] = None , | image_featurizer : Optional [ Lazy [ GridEmbedder ]] = None , | region_detector : Optional [ Lazy [ RegionDetector ]] = None , | feature_cache_dir : Optional [ Union [ str , PathLike ]] = None , | tokenizer : Optional [ Tokenizer ] = None , | token_indexers : Optional [ Dict [ str , TokenIndexer ]] = None , | cuda_device : Optional [ Union [ int , torch . device ]] = None , | max_instances : Optional [ int ] = None , | image_processing_batch_size : int = 8 , | write_to_cache : bool = True | ) -> None Base class for dataset readers for vision tasks. If you don't specify image_loader , image_featurizer , and region_detector , the reader assumes it can get all featurized images from the cache. If you don't specify feature_cache , the reader will featurize all images using the featurization components, and use an internal in-memory cache to catch duplicate images. If you don't specify either of these things, the reader will not produce featurized images at all.","title":"VisionReader"},{"location":"models/vision/dataset_readers/vision_reader/#image_featurizer","text":"class VisionReader ( DatasetReader ): | ... | @property | def image_featurizer ( self ) -> Optional [ GridEmbedder ]","title":"image_featurizer"},{"location":"models/vision/dataset_readers/vision_reader/#region_detector","text":"class VisionReader ( DatasetReader ): | ... | @property | def region_detector ( self ) -> Optional [ RegionDetector ]","title":"region_detector"},{"location":"models/vision/dataset_readers/visual_entailment/","text":"allennlp_models .vision .dataset_readers .visual_entailment [SOURCE] VisualEntailmentReader # @DatasetReader . register ( \"visual-entailment\" ) class VisualEntailmentReader ( VisionReader ) The dataset reader for visual entailment. text_to_instance # class VisualEntailmentReader ( VisionReader ): | ... | @overrides | def text_to_instance ( | self , | image : Union [ str , Tuple [ Tensor , Tensor ]], | hypothesis : str , | label : Optional [ str ] = None , | * , | use_cache : bool = True | ) -> Instance apply_token_indexers # class VisualEntailmentReader ( VisionReader ): | ... | @overrides | def apply_token_indexers ( self , instance : Instance ) -> None","title":"visual_entailment"},{"location":"models/vision/dataset_readers/visual_entailment/#visualentailmentreader","text":"@DatasetReader . register ( \"visual-entailment\" ) class VisualEntailmentReader ( VisionReader ) The dataset reader for visual entailment.","title":"VisualEntailmentReader"},{"location":"models/vision/dataset_readers/visual_entailment/#text_to_instance","text":"class VisualEntailmentReader ( VisionReader ): | ... | @overrides | def text_to_instance ( | self , | image : Union [ str , Tuple [ Tensor , Tensor ]], | hypothesis : str , | label : Optional [ str ] = None , | * , | use_cache : bool = True | ) -> Instance","title":"text_to_instance"},{"location":"models/vision/dataset_readers/visual_entailment/#apply_token_indexers","text":"class VisualEntailmentReader ( VisionReader ): | ... | @overrides | def apply_token_indexers ( self , instance : Instance ) -> None","title":"apply_token_indexers"},{"location":"models/vision/dataset_readers/vqav2/","text":"allennlp_models .vision .dataset_readers .vqav2 [SOURCE] contractions # contractions = { \"aint\" : \"ain't\" , \"arent\" : \"aren't\" , \"cant\" : \"can't\" , \"couldve\" : \"could've\" , \"c ... manual_map # manual_map = { \"none\" : \"0\" , \"zero\" : \"0\" , \"one\" : \"1\" , \"two\" : \"2\" , \"three\" : \"3\" , \"four\" : \"4 ... articles # articles = [ \"a\" , \"an\" , \"the\" ] period_strip # period_strip = re . compile ( r \"(?!<=\\d)(\\.)(?!\\d)\" ) comma_strip # comma_strip = re . compile ( r \"(\\d)(\\,)(\\d)\" ) punct # punct = [ \";\" , r \"/\" , \"[\" , \"]\" , '\"' , \"{\" , \"}\" , \"(\" , \")\" , \"=\" , \"+\" ... process_punctuation # def process_punctuation ( inText : str ) -> str process_digit_article # def process_digit_article ( input : str ) -> str preprocess_answer # @lru_cache ( maxsize = None ) def preprocess_answer ( answer : str ) -> str get_score # def get_score ( count : int ) -> float VQAv2Reader # @DatasetReader . register ( \"vqav2\" ) class VQAv2Reader ( VisionReader ): | def __init__ ( | self , | image_dir : Optional [ Union [ str , PathLike ]] = None , | * , | image_loader : Optional [ ImageLoader ] = None , | image_featurizer : Optional [ Lazy [ GridEmbedder ]] = None , | region_detector : Optional [ Lazy [ RegionDetector ]] = None , | answer_vocab : Optional [ Union [ Vocabulary , str ]] = None , | feature_cache_dir : Optional [ Union [ str , PathLike ]] = None , | tokenizer : Optional [ Tokenizer ] = None , | token_indexers : Optional [ Dict [ str , TokenIndexer ]] = None , | cuda_device : Optional [ Union [ int , torch . device ]] = None , | max_instances : Optional [ int ] = None , | image_processing_batch_size : int = 8 , | multiple_answers_per_question : bool = True , | write_to_cache : bool = True | ) -> None Parametersimage_dir: `str` \u00b6 Path to directory containing `png` image files. image_loader: ImageLoader The image loader component used to load the images. image_featurizer: Lazy[GridEmbedder] The backbone image processor (like a ResNet), whose output will be passed to the region detector for finding object boxes in the image. region_detector: Lazy[RegionDetector] For pulling out regions of the image (both coordinates and features) that will be used by downstream models. answer_vocab: Union[Vocabulary, str] , optional The vocabulary to use for answers. The reader will look into the \"answers\" namespace in the vocabulary to find possible answers. If this is given, the reader only outputs instances with answers contained in this vocab. If this is not given, the reader outputs all instances with all answers. If this is a URL or filename, we will download a previously saved vocabulary from there. feature_cache_dir: Union[str, PathLike] , optional An optional directory to cache the featurized images in. Featurizing images takes a long time, and many images are duplicated, so we highly recommend to use this cache. tokenizer: Tokenizer , optional The Tokenizer to use to tokenize the text. By default, this uses the tokenizer for \"bert-base-uncased\" . token_indexers: Dict[str, TokenIndexer] , optional The TokenIndexer to use. By default, this uses the indexer for \"bert-base-uncased\" . cuda_device: Union[int, torch.device] , optional Either a torch device or a GPU number. This is the GPU we'll use to featurize the images. max_instances: int , optional For debugging, you can use this parameter to limit the number of instances the reader returns. image_processing_batch_size: int The number of images to process at one time while featurizing. Default is 8. multiple_answers_per_question: bool VQA questions have multiple answers. By default, we use all of them, and give more points to the more common answer. But VQA also has a special answer, the so-called \"multiple choice answer\". If this is set to False , we only use that answer. text_to_instance # class VQAv2Reader ( VisionReader ): | ... | @overrides | def text_to_instance ( | self , | question : str , | image : Union [ str , Tuple [ Tensor , Tensor ]], | answer_counts : Optional [ MutableMapping [ str , int ]] = None , | * , | use_cache : bool = True | ) -> Optional [ Instance ] apply_token_indexers # class VQAv2Reader ( VisionReader ): | ... | @overrides | def apply_token_indexers ( self , instance : Instance ) -> None","title":"vqav2"},{"location":"models/vision/dataset_readers/vqav2/#contractions","text":"contractions = { \"aint\" : \"ain't\" , \"arent\" : \"aren't\" , \"cant\" : \"can't\" , \"couldve\" : \"could've\" , \"c ...","title":"contractions"},{"location":"models/vision/dataset_readers/vqav2/#manual_map","text":"manual_map = { \"none\" : \"0\" , \"zero\" : \"0\" , \"one\" : \"1\" , \"two\" : \"2\" , \"three\" : \"3\" , \"four\" : \"4 ...","title":"manual_map"},{"location":"models/vision/dataset_readers/vqav2/#articles","text":"articles = [ \"a\" , \"an\" , \"the\" ]","title":"articles"},{"location":"models/vision/dataset_readers/vqav2/#period_strip","text":"period_strip = re . compile ( r \"(?!<=\\d)(\\.)(?!\\d)\" )","title":"period_strip"},{"location":"models/vision/dataset_readers/vqav2/#comma_strip","text":"comma_strip = re . compile ( r \"(\\d)(\\,)(\\d)\" )","title":"comma_strip"},{"location":"models/vision/dataset_readers/vqav2/#punct","text":"punct = [ \";\" , r \"/\" , \"[\" , \"]\" , '\"' , \"{\" , \"}\" , \"(\" , \")\" , \"=\" , \"+\" ...","title":"punct"},{"location":"models/vision/dataset_readers/vqav2/#process_punctuation","text":"def process_punctuation ( inText : str ) -> str","title":"process_punctuation"},{"location":"models/vision/dataset_readers/vqav2/#process_digit_article","text":"def process_digit_article ( input : str ) -> str","title":"process_digit_article"},{"location":"models/vision/dataset_readers/vqav2/#preprocess_answer","text":"@lru_cache ( maxsize = None ) def preprocess_answer ( answer : str ) -> str","title":"preprocess_answer"},{"location":"models/vision/dataset_readers/vqav2/#get_score","text":"def get_score ( count : int ) -> float","title":"get_score"},{"location":"models/vision/dataset_readers/vqav2/#vqav2reader","text":"@DatasetReader . register ( \"vqav2\" ) class VQAv2Reader ( VisionReader ): | def __init__ ( | self , | image_dir : Optional [ Union [ str , PathLike ]] = None , | * , | image_loader : Optional [ ImageLoader ] = None , | image_featurizer : Optional [ Lazy [ GridEmbedder ]] = None , | region_detector : Optional [ Lazy [ RegionDetector ]] = None , | answer_vocab : Optional [ Union [ Vocabulary , str ]] = None , | feature_cache_dir : Optional [ Union [ str , PathLike ]] = None , | tokenizer : Optional [ Tokenizer ] = None , | token_indexers : Optional [ Dict [ str , TokenIndexer ]] = None , | cuda_device : Optional [ Union [ int , torch . device ]] = None , | max_instances : Optional [ int ] = None , | image_processing_batch_size : int = 8 , | multiple_answers_per_question : bool = True , | write_to_cache : bool = True | ) -> None","title":"VQAv2Reader"},{"location":"models/vision/dataset_readers/vqav2/#text_to_instance","text":"class VQAv2Reader ( VisionReader ): | ... | @overrides | def text_to_instance ( | self , | question : str , | image : Union [ str , Tuple [ Tensor , Tensor ]], | answer_counts : Optional [ MutableMapping [ str , int ]] = None , | * , | use_cache : bool = True | ) -> Optional [ Instance ]","title":"text_to_instance"},{"location":"models/vision/dataset_readers/vqav2/#apply_token_indexers","text":"class VQAv2Reader ( VisionReader ): | ... | @overrides | def apply_token_indexers ( self , instance : Instance ) -> None","title":"apply_token_indexers"},{"location":"models/vision/metrics/vqa/","text":"allennlp_models .vision .metrics .vqa [SOURCE] VqaMeasure # @Metric . register ( \"vqa\" ) class VqaMeasure ( Metric ): | def __init__ ( self ) -> None Compute the VQA metric, as described in https://www.semanticscholar.org/paper/VQA%3A-Visual-Question-Answering-Agrawal-Lu/97ad70a9fa3f99adf18030e5e38ebe3d90daa2db In VQA, we take the answer with the highest score, and then we find out how often humans decided this was the right answer. The accuracy score for an answer is min(1.0, human_count / 3) . This metric takes the logits from the models, i.e., a score for each possible answer, and the labels for the question, together with their weights. __call__ # class VqaMeasure ( Metric ): | ... | @overrides | def __call__ ( | self , | logits : torch . Tensor , | labels : torch . Tensor , | label_weights : torch . Tensor | ) Parameters \u00b6 logits : torch.Tensor A tensor of predictions of shape (batch_size, num_classes). labels : torch.Tensor A tensor of integer class label of shape (batch_size, num_labels). label_weights : torch.Tensor A tensor of floats of shape (batch_size, num_labels), giving a weight or score to every one of the labels. get_metric # class VqaMeasure ( Metric ): | ... | @overrides | def get_metric ( self , reset : bool = False ) reset # class VqaMeasure ( Metric ): | ... | @overrides | def reset ( self ) -> None","title":"vqa"},{"location":"models/vision/metrics/vqa/#vqameasure","text":"@Metric . register ( \"vqa\" ) class VqaMeasure ( Metric ): | def __init__ ( self ) -> None Compute the VQA metric, as described in https://www.semanticscholar.org/paper/VQA%3A-Visual-Question-Answering-Agrawal-Lu/97ad70a9fa3f99adf18030e5e38ebe3d90daa2db In VQA, we take the answer with the highest score, and then we find out how often humans decided this was the right answer. The accuracy score for an answer is min(1.0, human_count / 3) . This metric takes the logits from the models, i.e., a score for each possible answer, and the labels for the question, together with their weights.","title":"VqaMeasure"},{"location":"models/vision/metrics/vqa/#__call__","text":"class VqaMeasure ( Metric ): | ... | @overrides | def __call__ ( | self , | logits : torch . Tensor , | labels : torch . Tensor , | label_weights : torch . Tensor | )","title":"__call__"},{"location":"models/vision/metrics/vqa/#get_metric","text":"class VqaMeasure ( Metric ): | ... | @overrides | def get_metric ( self , reset : bool = False )","title":"get_metric"},{"location":"models/vision/metrics/vqa/#reset","text":"class VqaMeasure ( Metric ): | ... | @overrides | def reset ( self ) -> None","title":"reset"},{"location":"models/vision/models/vilbert_vqa/","text":"allennlp_models .vision .models .vilbert_vqa [SOURCE] VqaVilbert # @Model . register ( \"vqa_vilbert\" ) @Model . register ( \"vqa_vilbert_from_huggingface\" , constructor = \"from_huggingface_model_name\" ) class VqaVilbert ( VisionTextModel ): | def __init__ ( | self , | vocab : Vocabulary , | text_embeddings : TransformerEmbeddings , | image_embeddings : ImageFeatureEmbeddings , | encoder : BiModalEncoder , | pooled_output_dim : int , | fusion_method : str = \"sum\" , | dropout : float = 0.1 , | label_namespace : str = \"answers\" , | * , | ignore_text : bool = False , | ignore_image : bool = False | ) -> None Model for VQA task based on the VilBERT paper. Parameters \u00b6 vocab : Vocabulary text_embeddings : TransformerEmbeddings image_embeddings : ImageFeatureEmbeddings encoder : BiModalEncoder pooled_output_dim : int fusion_method : str , optional (default = \"sum\" ) dropout : float , optional (default = 0.1 ) label_namespace : str , optional (default = answers ) forward # class VqaVilbert ( VisionTextModel ): | ... | @overrides | def forward ( | self , | box_features : torch . Tensor , | box_coordinates : torch . Tensor , | box_mask : torch . Tensor , | question : TextFieldTensors , | labels : Optional [ torch . Tensor ] = None , | label_weights : Optional [ torch . Tensor ] = None | ) -> Dict [ str , torch . Tensor ] get_metrics # class VqaVilbert ( VisionTextModel ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] make_output_human_readable # class VqaVilbert ( VisionTextModel ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] default_predictor # class VqaVilbert ( VisionTextModel ): | ... | default_predictor = \"vilbert_vqa\"","title":"vilbert_vqa"},{"location":"models/vision/models/vilbert_vqa/#vqavilbert","text":"@Model . register ( \"vqa_vilbert\" ) @Model . register ( \"vqa_vilbert_from_huggingface\" , constructor = \"from_huggingface_model_name\" ) class VqaVilbert ( VisionTextModel ): | def __init__ ( | self , | vocab : Vocabulary , | text_embeddings : TransformerEmbeddings , | image_embeddings : ImageFeatureEmbeddings , | encoder : BiModalEncoder , | pooled_output_dim : int , | fusion_method : str = \"sum\" , | dropout : float = 0.1 , | label_namespace : str = \"answers\" , | * , | ignore_text : bool = False , | ignore_image : bool = False | ) -> None Model for VQA task based on the VilBERT paper.","title":"VqaVilbert"},{"location":"models/vision/models/vilbert_vqa/#forward","text":"class VqaVilbert ( VisionTextModel ): | ... | @overrides | def forward ( | self , | box_features : torch . Tensor , | box_coordinates : torch . Tensor , | box_mask : torch . Tensor , | question : TextFieldTensors , | labels : Optional [ torch . Tensor ] = None , | label_weights : Optional [ torch . Tensor ] = None | ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"models/vision/models/vilbert_vqa/#get_metrics","text":"class VqaVilbert ( VisionTextModel ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"models/vision/models/vilbert_vqa/#make_output_human_readable","text":"class VqaVilbert ( VisionTextModel ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ]","title":"make_output_human_readable"},{"location":"models/vision/models/vilbert_vqa/#default_predictor","text":"class VqaVilbert ( VisionTextModel ): | ... | default_predictor = \"vilbert_vqa\"","title":"default_predictor"},{"location":"models/vision/models/vision_text_model/","text":"allennlp_models .vision .models .vision_text_model [SOURCE] VisionTextModel # @Model . register ( \"vision_model\" ) class VisionTextModel ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_embeddings : TransformerEmbeddings , | image_embeddings : ImageFeatureEmbeddings , | encoder : BiModalEncoder , | pooled_output_dim : int , | fusion_method : str = \"sum\" , | dropout : float = 0.1 , | label_namespace : str = \"labels\" , | is_multilabel : bool = False , | * , | ignore_text : bool = False , | ignore_image : bool = False | ) -> None VisionTextModel takes as input a single text input and a single image input to produce some output. Example tasks include visual question-answering, visual entailment, etc. Parameters \u00b6 vocab : Vocabulary text_embeddings : TransformerEmbeddings image_embeddings : ImageFeatureEmbeddings encoder : BiModalEncoder pooled_output_dim : int fusion_method : str , optional (default = \"sum\" ) dropout : float , optional (default = 0.1 ) label_namespace : str , optional (default = \"labels\" ) is_multilabel : bool , optional (default = False ) Whether the output classification is multilabel. (i.e., can have multiple correct answers) from_huggingface_model_name # class VisionTextModel ( Model ): | ... | @classmethod | def from_huggingface_model_name ( | cls , | vocab : Vocabulary , | model_name : str , | image_feature_dim : int , | image_num_hidden_layers : int , | image_hidden_size : int , | image_num_attention_heads : int , | combined_hidden_size : int , | combined_num_attention_heads : int , | pooled_output_dim : int , | image_intermediate_size : int , | image_attention_dropout : float , | image_hidden_dropout : float , | image_biattention_id : List [ int ], | text_biattention_id : List [ int ], | text_fixed_layer : int , | image_fixed_layer : int , | pooled_dropout : float = 0.1 , | fusion_method : str = \"sum\" , | * , | ignore_text : bool = False , | ignore_image : bool = False | ) forward # class VisionTextModel ( Model ): | ... | @overrides | def forward ( | self , | box_features : torch . Tensor , | box_coordinates : torch . Tensor , | box_mask : torch . Tensor , | text : TextFieldTensors , | labels : Optional [ torch . Tensor ] = None , | label_weights : Optional [ torch . Tensor ] = None | ) -> Dict [ str , torch . Tensor ] Parameters \u00b6 box_features : Tensor Shape: (batch_size, num_boxes, feature_size) box_coordinates : Tensor Shape: (batch_size, num_boxes, 4) box_mask : Tensor A bool and 0-1 tensor of shape (batch_size, num_boxes) . text : TextFieldTensors label : Optional[Tensor] label_weights : Optional[Tensor] get_metrics # class VisionTextModel ( Model ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] make_output_human_readable # class VisionTextModel ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ]","title":"vision_text_model"},{"location":"models/vision/models/vision_text_model/#visiontextmodel","text":"@Model . register ( \"vision_model\" ) class VisionTextModel ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_embeddings : TransformerEmbeddings , | image_embeddings : ImageFeatureEmbeddings , | encoder : BiModalEncoder , | pooled_output_dim : int , | fusion_method : str = \"sum\" , | dropout : float = 0.1 , | label_namespace : str = \"labels\" , | is_multilabel : bool = False , | * , | ignore_text : bool = False , | ignore_image : bool = False | ) -> None VisionTextModel takes as input a single text input and a single image input to produce some output. Example tasks include visual question-answering, visual entailment, etc.","title":"VisionTextModel"},{"location":"models/vision/models/vision_text_model/#from_huggingface_model_name","text":"class VisionTextModel ( Model ): | ... | @classmethod | def from_huggingface_model_name ( | cls , | vocab : Vocabulary , | model_name : str , | image_feature_dim : int , | image_num_hidden_layers : int , | image_hidden_size : int , | image_num_attention_heads : int , | combined_hidden_size : int , | combined_num_attention_heads : int , | pooled_output_dim : int , | image_intermediate_size : int , | image_attention_dropout : float , | image_hidden_dropout : float , | image_biattention_id : List [ int ], | text_biattention_id : List [ int ], | text_fixed_layer : int , | image_fixed_layer : int , | pooled_dropout : float = 0.1 , | fusion_method : str = \"sum\" , | * , | ignore_text : bool = False , | ignore_image : bool = False | )","title":"from_huggingface_model_name"},{"location":"models/vision/models/vision_text_model/#forward","text":"class VisionTextModel ( Model ): | ... | @overrides | def forward ( | self , | box_features : torch . Tensor , | box_coordinates : torch . Tensor , | box_mask : torch . Tensor , | text : TextFieldTensors , | labels : Optional [ torch . Tensor ] = None , | label_weights : Optional [ torch . Tensor ] = None | ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"models/vision/models/vision_text_model/#get_metrics","text":"class VisionTextModel ( Model ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"models/vision/models/vision_text_model/#make_output_human_readable","text":"class VisionTextModel ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ]","title":"make_output_human_readable"},{"location":"models/vision/models/visual_entailment/","text":"allennlp_models .vision .models .visual_entailment [SOURCE] VisualEntailmentModel # @Model . register ( \"ve_vilbert\" ) @Model . register ( \"ve_vilbert_from_huggingface\" , constructor = \"from_huggingface_model_name\" ) class VisualEntailmentModel ( VisionTextModel ): | def __init__ ( | self , | vocab : Vocabulary , | text_embeddings : TransformerEmbeddings , | image_embeddings : ImageFeatureEmbeddings , | encoder : BiModalEncoder , | pooled_output_dim : int , | fusion_method : str = \"sum\" , | dropout : float = 0.1 , | label_namespace : str = \"labels\" , | * , | ignore_text : bool = False , | ignore_image : bool = False | ) -> None Model for visual entailment task based on the paper Visual Entailment: A Novel Task for Fine-Grained Image Understanding . Parameters \u00b6 vocab : Vocabulary text_embeddings : TransformerEmbeddings image_embeddings : ImageFeatureEmbeddings encoder : BiModalEncoder pooled_output_dim : int fusion_method : str , optional (default = \"sum\" ) dropout : float , optional (default = 0.1 ) label_namespace : str , optional (default = labels ) forward # class VisualEntailmentModel ( VisionTextModel ): | ... | @overrides | def forward ( | self , | box_features : torch . Tensor , | box_coordinates : torch . Tensor , | box_mask : torch . Tensor , | hypothesis : TextFieldTensors , | labels : Optional [ torch . Tensor ] = None | ) -> Dict [ str , torch . Tensor ] get_metrics # class VisualEntailmentModel ( VisionTextModel ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] make_output_human_readable # class VisualEntailmentModel ( VisionTextModel ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] default_predictor # class VisualEntailmentModel ( VisionTextModel ): | ... | default_predictor = \"vilbert_ve\"","title":"visual_entailment"},{"location":"models/vision/models/visual_entailment/#visualentailmentmodel","text":"@Model . register ( \"ve_vilbert\" ) @Model . register ( \"ve_vilbert_from_huggingface\" , constructor = \"from_huggingface_model_name\" ) class VisualEntailmentModel ( VisionTextModel ): | def __init__ ( | self , | vocab : Vocabulary , | text_embeddings : TransformerEmbeddings , | image_embeddings : ImageFeatureEmbeddings , | encoder : BiModalEncoder , | pooled_output_dim : int , | fusion_method : str = \"sum\" , | dropout : float = 0.1 , | label_namespace : str = \"labels\" , | * , | ignore_text : bool = False , | ignore_image : bool = False | ) -> None Model for visual entailment task based on the paper Visual Entailment: A Novel Task for Fine-Grained Image Understanding .","title":"VisualEntailmentModel"},{"location":"models/vision/models/visual_entailment/#forward","text":"class VisualEntailmentModel ( VisionTextModel ): | ... | @overrides | def forward ( | self , | box_features : torch . Tensor , | box_coordinates : torch . Tensor , | box_mask : torch . Tensor , | hypothesis : TextFieldTensors , | labels : Optional [ torch . Tensor ] = None | ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"models/vision/models/visual_entailment/#get_metrics","text":"class VisualEntailmentModel ( VisionTextModel ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"models/vision/models/visual_entailment/#make_output_human_readable","text":"class VisualEntailmentModel ( VisionTextModel ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ]","title":"make_output_human_readable"},{"location":"models/vision/models/visual_entailment/#default_predictor","text":"class VisualEntailmentModel ( VisionTextModel ): | ... | default_predictor = \"vilbert_ve\"","title":"default_predictor"},{"location":"models/vision/models/heads/visual_entailment_head/","text":"allennlp_models .vision .models .heads .visual_entailment_head [SOURCE] VisualEntailmentHead # @Head . register ( \"visual_entailment\" ) class VisualEntailmentHead ( Head ): | def __init__ ( | self , | vocab : Vocabulary , | embedding_dim : int , | label_namespace : str = \"labels\" | ) forward # class VisualEntailmentHead ( Head ): | ... | @overrides | def forward ( | self , | encoded_boxes : torch . Tensor , | encoded_boxes_mask : torch . Tensor , | encoded_boxes_pooled : torch . Tensor , | encoded_text : torch . Tensor , | encoded_text_mask : torch . Tensor , | encoded_text_pooled : torch . Tensor , | pooled_boxes_and_text : torch . Tensor , | labels : Optional [ torch . Tensor ] = None , | label_weights : Optional [ torch . Tensor ] = None | ) -> Dict [ str , torch . Tensor ] get_metrics # class VisualEntailmentHead ( Head ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] make_output_human_readable # class VisualEntailmentHead ( Head ): | ... | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] default_predictor # class VisualEntailmentHead ( Head ): | ... | default_predictor = \"vilbert_ve\"","title":"visual_entailment_head"},{"location":"models/vision/models/heads/visual_entailment_head/#visualentailmenthead","text":"@Head . register ( \"visual_entailment\" ) class VisualEntailmentHead ( Head ): | def __init__ ( | self , | vocab : Vocabulary , | embedding_dim : int , | label_namespace : str = \"labels\" | )","title":"VisualEntailmentHead"},{"location":"models/vision/models/heads/visual_entailment_head/#forward","text":"class VisualEntailmentHead ( Head ): | ... | @overrides | def forward ( | self , | encoded_boxes : torch . Tensor , | encoded_boxes_mask : torch . Tensor , | encoded_boxes_pooled : torch . Tensor , | encoded_text : torch . Tensor , | encoded_text_mask : torch . Tensor , | encoded_text_pooled : torch . Tensor , | pooled_boxes_and_text : torch . Tensor , | labels : Optional [ torch . Tensor ] = None , | label_weights : Optional [ torch . Tensor ] = None | ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"models/vision/models/heads/visual_entailment_head/#get_metrics","text":"class VisualEntailmentHead ( Head ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"models/vision/models/heads/visual_entailment_head/#make_output_human_readable","text":"class VisualEntailmentHead ( Head ): | ... | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ]","title":"make_output_human_readable"},{"location":"models/vision/models/heads/visual_entailment_head/#default_predictor","text":"class VisualEntailmentHead ( Head ): | ... | default_predictor = \"vilbert_ve\"","title":"default_predictor"},{"location":"models/vision/models/heads/vqa_head/","text":"allennlp_models .vision .models .heads .vqa_head [SOURCE] VqaHead # @Head . register ( \"vqa\" ) class VqaHead ( Head ): | def __init__ ( | self , | vocab : Vocabulary , | embedding_dim : int , | label_namespace : str = \"answers\" | ) forward # class VqaHead ( Head ): | ... | @overrides | def forward ( | self , | encoded_boxes : torch . Tensor , | encoded_boxes_mask : torch . Tensor , | encoded_boxes_pooled : torch . Tensor , | encoded_text : torch . Tensor , | encoded_text_mask : torch . Tensor , | encoded_text_pooled : torch . Tensor , | pooled_boxes_and_text : torch . Tensor , | labels : Optional [ torch . Tensor ] = None , | label_weights : Optional [ torch . Tensor ] = None | ) -> Dict [ str , torch . Tensor ] get_metrics # class VqaHead ( Head ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] make_output_human_readable # class VqaHead ( Head ): | ... | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] default_predictor # class VqaHead ( Head ): | ... | default_predictor = \"vilbert_vqa\"","title":"vqa_head"},{"location":"models/vision/models/heads/vqa_head/#vqahead","text":"@Head . register ( \"vqa\" ) class VqaHead ( Head ): | def __init__ ( | self , | vocab : Vocabulary , | embedding_dim : int , | label_namespace : str = \"answers\" | )","title":"VqaHead"},{"location":"models/vision/models/heads/vqa_head/#forward","text":"class VqaHead ( Head ): | ... | @overrides | def forward ( | self , | encoded_boxes : torch . Tensor , | encoded_boxes_mask : torch . Tensor , | encoded_boxes_pooled : torch . Tensor , | encoded_text : torch . Tensor , | encoded_text_mask : torch . Tensor , | encoded_text_pooled : torch . Tensor , | pooled_boxes_and_text : torch . Tensor , | labels : Optional [ torch . Tensor ] = None , | label_weights : Optional [ torch . Tensor ] = None | ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"models/vision/models/heads/vqa_head/#get_metrics","text":"class VqaHead ( Head ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"models/vision/models/heads/vqa_head/#make_output_human_readable","text":"class VqaHead ( Head ): | ... | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ]","title":"make_output_human_readable"},{"location":"models/vision/models/heads/vqa_head/#default_predictor","text":"class VqaHead ( Head ): | ... | default_predictor = \"vilbert_vqa\"","title":"default_predictor"},{"location":"models/vision/predictors/vilbert_vqa/","text":"allennlp_models .vision .predictors .vilbert_vqa [SOURCE] VilbertVqaPredictor # @Predictor . register ( \"vilbert_vqa\" ) class VilbertVqaPredictor ( Predictor ) predict # class VilbertVqaPredictor ( Predictor ): | ... | def predict ( self , image : str , question : str ) -> JsonDict predictions_to_labeled_instances # class VilbertVqaPredictor ( Predictor ): | ... | @overrides | def predictions_to_labeled_instances ( | self , | instance : Instance , | outputs : Dict [ str , numpy . ndarray ] | ) -> List [ Instance ]","title":"vilbert_vqa"},{"location":"models/vision/predictors/vilbert_vqa/#vilbertvqapredictor","text":"@Predictor . register ( \"vilbert_vqa\" ) class VilbertVqaPredictor ( Predictor )","title":"VilbertVqaPredictor"},{"location":"models/vision/predictors/vilbert_vqa/#predict","text":"class VilbertVqaPredictor ( Predictor ): | ... | def predict ( self , image : str , question : str ) -> JsonDict","title":"predict"},{"location":"models/vision/predictors/vilbert_vqa/#predictions_to_labeled_instances","text":"class VilbertVqaPredictor ( Predictor ): | ... | @overrides | def predictions_to_labeled_instances ( | self , | instance : Instance , | outputs : Dict [ str , numpy . ndarray ] | ) -> List [ Instance ]","title":"predictions_to_labeled_instances"},{"location":"models/vision/predictors/visual_entailment/","text":"allennlp_models .vision .predictors .visual_entailment [SOURCE] VisualEntailmentPredictor # @Predictor . register ( \"vilbert_ve\" ) class VisualEntailmentPredictor ( Predictor ) predict # class VisualEntailmentPredictor ( Predictor ): | ... | def predict ( self , image : str , hypothesis : str ) -> JsonDict predictions_to_labeled_instances # class VisualEntailmentPredictor ( Predictor ): | ... | @overrides | def predictions_to_labeled_instances ( | self , | instance : Instance , | outputs : Dict [ str , numpy . ndarray ] | ) -> List [ Instance ]","title":"visual_entailment"},{"location":"models/vision/predictors/visual_entailment/#visualentailmentpredictor","text":"@Predictor . register ( \"vilbert_ve\" ) class VisualEntailmentPredictor ( Predictor )","title":"VisualEntailmentPredictor"},{"location":"models/vision/predictors/visual_entailment/#predict","text":"class VisualEntailmentPredictor ( Predictor ): | ... | def predict ( self , image : str , hypothesis : str ) -> JsonDict","title":"predict"},{"location":"models/vision/predictors/visual_entailment/#predictions_to_labeled_instances","text":"class VisualEntailmentPredictor ( Predictor ): | ... | @overrides | def predictions_to_labeled_instances ( | self , | instance : Instance , | outputs : Dict [ str , numpy . ndarray ] | ) -> List [ Instance ]","title":"predictions_to_labeled_instances"}]}