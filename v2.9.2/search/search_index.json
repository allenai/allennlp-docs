{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"An Apache 2.0 NLP research library, built on PyTorch, for developing state-of-the-art deep learning models on a wide variety of linguistic tasks. Quick Links \u00b6 \u2197\ufe0f Website \ud83d\udd26 Guide \ud83d\uddbc Gallery \ud83d\udcbb Demo \ud83d\udcd3 Documentation ( latest | stable | commit ) \u2b06\ufe0f Upgrade Guide from 1.x to 2.0 \u2753 Stack Overflow \u270b Contributing Guidelines \ud83e\udd16 Officially Supported Models Pretrained Models Documentation ( latest | stable | commit ) \u2699\ufe0f Continuous Build \ud83c\udf19 Nightly Releases In this README \u00b6 Getting Started Using the Library Plugins Package Overview Installation Installing via pip Installing using Docker Installing from source Running AllenNLP Issues Contributions Citing Team Getting Started Using the Library \u00b6 If you're interested in using AllenNLP for model development, we recommend you check out the AllenNLP Guide for a thorough introduction to the library, followed by our more advanced guides on GitHub Discussions . When you're ready to start your project, we've created a couple of template repositories that you can use as a starting place: If you want to use allennlp train and config files to specify experiments, use this template . We recommend this approach. If you'd prefer to use python code to configure your experiments and run your training loop, use this template . There are a few things that are currently a little harder in this setup (loading a saved model, and using distributed training), but otherwise it's functionality equivalent to the config files setup. In addition, there are external tutorials: Hyperparameter optimization for AllenNLP using Optuna Training with multiple GPUs in AllenNLP Training on larger batches with less memory in AllenNLP How to upload transformer weights and tokenizers from AllenNLP to HuggingFace And others on the AI2 AllenNLP blog . Plugins \u00b6 AllenNLP supports loading \"plugins\" dynamically. A plugin is just a Python package that provides custom registered classes or additional allennlp subcommands. There is ecosystem of open source plugins, some of which are maintained by the AllenNLP team here at AI2, and some of which are maintained by the broader community. Plugin Maintainer CLI Description allennlp-models AI2 No A collection of state-of-the-art models allennlp-semparse AI2 No A framework for building semantic parsers allennlp-server AI2 Yes A simple demo server for serving models allennlp-optuna Makoto Hiramatsu Yes Optuna integration for hyperparameter optimization AllenNLP will automatically find any official AI2-maintained plugins that you have installed, but for AllenNLP to find personal or third-party plugins you've installed, you also have to create either a local plugins file named .allennlp_plugins in the directory where you run the allennlp command, or a global plugins file at ~/.allennlp/plugins . The file should list the plugin modules that you want to be loaded, one per line. To test that your plugins can be found and imported by AllenNLP, you can run the allennlp test-install command. Each discovered plugin will be logged to the terminal. For more information about plugins, see the plugins API docs . And for information on how to create a custom subcommand to distribute as a plugin, see the subcommand API docs . Package Overview \u00b6 allennlp An open-source NLP research library, built on PyTorch allennlp.commands Functionality for the CLI allennlp.common Utility modules that are used across the library allennlp.data A data processing module for loading datasets and encoding strings as integers for representation in matrices allennlp.fairness A module for bias mitigation and fairness algorithms and metrics allennlp.modules A collection of PyTorch modules for use with text allennlp.nn Tensor utility functions, such as initializers and activation functions allennlp.training Functionality for training models Installation \u00b6 AllenNLP requires Python 3.6.1 or later and PyTorch . We support AllenNLP on Mac and Linux environments. We presently do not support Windows but are open to contributions. Installing via conda-forge \u00b6 The simplest way to install AllenNLP is using conda (you can choose a different python version): conda install -c conda-forge python=3.8 allennlp To install optional packages, such as checklist , use conda install -c conda-forge allennlp-checklist or simply install allennlp-all directly. The plugins mentioned above are similarly installable, e.g. conda install -c conda-forge allennlp-models allennlp-semparse allennlp-server allennlp-optuna Installing via pip \u00b6 It's recommended that you install the PyTorch ecosystem before installing AllenNLP by following the instructions on pytorch.org . After that, just run pip install allennlp . \u26a0\ufe0f If you're using Python 3.7 or greater, you should ensure that you don't have the PyPI version of dataclasses installed after running the above command, as this could cause issues on certain platforms. You can quickly check this by running pip freeze | grep dataclasses . If you see something like dataclasses=0.6 in the output, then just run pip uninstall -y dataclasses . If you need pointers on setting up an appropriate Python environment or would like to install AllenNLP using a different method, see below. Setting up a virtual environment \u00b6 Conda can be used set up a virtual environment with the version of Python required for AllenNLP. If you already have a Python 3 environment you want to use, you can skip to the 'installing via pip' section. Download and install Conda . Create a Conda environment with Python 3.8 (3.7 or 3.9 would work as well): conda create -n allennlp_env python=3.8 Activate the Conda environment. You will need to activate the Conda environment in each terminal in which you want to use AllenNLP: conda activate allennlp_env Installing the library and dependencies \u00b6 Installing the library and dependencies is simple using pip . pip install allennlp To install the optional dependencies, such as checklist , run pip install allennlp [ checklist ] Or you can just install all optional dependencies with pip install allennlp[all] . Looking for bleeding edge features? You can install nightly releases directly from pypi AllenNLP installs a script when you install the python package, so you can run allennlp commands just by typing allennlp into a terminal. For example, you can now test your installation with allennlp test-install . You may also want to install allennlp-models , which contains the NLP constructs to train and run our officially supported models, many of which are hosted at https://demo.allennlp.org . pip install allennlp-models Installing using Docker \u00b6 Docker provides a virtual machine with everything set up to run AllenNLP-- whether you will leverage a GPU or just run on a CPU. Docker provides more isolation and consistency, and also makes it easy to distribute your environment to a compute cluster. AllenNLP provides official Docker images with the library and all of its dependencies installed. Once you have installed Docker , you should also install the NVIDIA Container Toolkit if you have GPUs available. Then run the following command to get an environment that will run on GPU: mkdir -p $HOME /.allennlp/ docker run --rm --gpus all -v $HOME /.allennlp:/root/.allennlp allennlp/allennlp:latest You can test the Docker environment with docker run --rm --gpus all -v $HOME /.allennlp:/root/.allennlp allennlp/allennlp:latest test-install If you don't have GPUs available, just omit the --gpus all flag. Building your own Docker image \u00b6 For various reasons you may need to create your own AllenNLP Docker image, such as if you need a different version of PyTorch. To do so, just run make docker-image from the root of your local clone of AllenNLP. By default this builds an image with the tag allennlp/allennlp , but you can change this to anything you want by setting the DOCKER_IMAGE_NAME flag when you call make . For example, make docker-image DOCKER_IMAGE_NAME=my-allennlp . If you want to use a different version of Python or PyTorch, set the flags DOCKER_PYTHON_VERSION and DOCKER_TORCH_VERSION to something like 3.9 and 1.9.0-cuda10.2 , respectively. These flags together determine the base image that is used. You can see the list of valid combinations in this GitHub Container Registry: github.com/allenai/docker-images/pkgs/container/pytorch . After building the image you should be able to see it listed by running docker images allennlp . REPOSITORY TAG IMAGE ID CREATED SIZE allennlp/allennlp latest b66aee6cb593 5 minutes ago 2.38GB Installing from source \u00b6 You can also install AllenNLP by cloning our git repository: git clone https://github.com/allenai/allennlp.git Create a Python 3.7 or 3.8 virtual environment, and install AllenNLP in editable mode by running: pip install -U pip setuptools wheel pip install --editable . [ dev,all ] This will make allennlp available on your system but it will use the sources from the local clone you made of the source repository. You can test your installation with allennlp test-install . See https://github.com/allenai/allennlp-models for instructions on installing allennlp-models from source. Running AllenNLP \u00b6 Once you've installed AllenNLP, you can run the command-line interface with the allennlp command (whether you installed from pip or from source). allennlp has various subcommands such as train , evaluate , and predict . To see the full usage information, run allennlp --help . You can test your installation by running allennlp test-install . Issues \u00b6 Everyone is welcome to file issues with either feature requests, bug reports, or general questions. As a small team with our own internal goals, we may ask for contributions if a prompt fix doesn't fit into our roadmap. To keep things tidy we will often close issues we think are answered, but don't hesitate to follow up if further discussion is needed. Contributions \u00b6 The AllenNLP team at AI2 ( @allenai ) welcomes contributions from the community. If you're a first time contributor, we recommend you start by reading our CONTRIBUTING.md guide. Then have a look at our issues with the tag Good First Issue . If you would like to contribute a larger feature, we recommend first creating an issue with a proposed design for discussion. This will prevent you from spending significant time on an implementation which has a technical limitation someone could have pointed out early on. Small contributions can be made directly in a pull request. Pull requests (PRs) must have one approving review and no requested changes before they are merged. As AllenNLP is primarily driven by AI2 we reserve the right to reject or revert contributions that we don't think are good additions. Citing \u00b6 If you use AllenNLP in your research, please cite AllenNLP: A Deep Semantic Natural Language Processing Platform . @inproceedings { Gardner2017AllenNLP , title = {AllenNLP: A Deep Semantic Natural Language Processing Platform} , author = {Matt Gardner and Joel Grus and Mark Neumann and Oyvind Tafjord and Pradeep Dasigi and Nelson F. Liu and Matthew Peters and Michael Schmitz and Luke S. Zettlemoyer} , year = {2017} , Eprint = {arXiv:1803.07640} , } Team \u00b6 AllenNLP is an open-source project backed by the Allen Institute for Artificial Intelligence (AI2) . AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering. To learn more about who specifically contributed to this codebase, see our contributors page.","title":"Home"},{"location":"#quick-links","text":"\u2197\ufe0f Website \ud83d\udd26 Guide \ud83d\uddbc Gallery \ud83d\udcbb Demo \ud83d\udcd3 Documentation ( latest | stable | commit ) \u2b06\ufe0f Upgrade Guide from 1.x to 2.0 \u2753 Stack Overflow \u270b Contributing Guidelines \ud83e\udd16 Officially Supported Models Pretrained Models Documentation ( latest | stable | commit ) \u2699\ufe0f Continuous Build \ud83c\udf19 Nightly Releases","title":"Quick Links"},{"location":"#in-this-readme","text":"Getting Started Using the Library Plugins Package Overview Installation Installing via pip Installing using Docker Installing from source Running AllenNLP Issues Contributions Citing Team","title":"In this README"},{"location":"#getting-started-using-the-library","text":"If you're interested in using AllenNLP for model development, we recommend you check out the AllenNLP Guide for a thorough introduction to the library, followed by our more advanced guides on GitHub Discussions . When you're ready to start your project, we've created a couple of template repositories that you can use as a starting place: If you want to use allennlp train and config files to specify experiments, use this template . We recommend this approach. If you'd prefer to use python code to configure your experiments and run your training loop, use this template . There are a few things that are currently a little harder in this setup (loading a saved model, and using distributed training), but otherwise it's functionality equivalent to the config files setup. In addition, there are external tutorials: Hyperparameter optimization for AllenNLP using Optuna Training with multiple GPUs in AllenNLP Training on larger batches with less memory in AllenNLP How to upload transformer weights and tokenizers from AllenNLP to HuggingFace And others on the AI2 AllenNLP blog .","title":"Getting Started Using the Library"},{"location":"#plugins","text":"AllenNLP supports loading \"plugins\" dynamically. A plugin is just a Python package that provides custom registered classes or additional allennlp subcommands. There is ecosystem of open source plugins, some of which are maintained by the AllenNLP team here at AI2, and some of which are maintained by the broader community. Plugin Maintainer CLI Description allennlp-models AI2 No A collection of state-of-the-art models allennlp-semparse AI2 No A framework for building semantic parsers allennlp-server AI2 Yes A simple demo server for serving models allennlp-optuna Makoto Hiramatsu Yes Optuna integration for hyperparameter optimization AllenNLP will automatically find any official AI2-maintained plugins that you have installed, but for AllenNLP to find personal or third-party plugins you've installed, you also have to create either a local plugins file named .allennlp_plugins in the directory where you run the allennlp command, or a global plugins file at ~/.allennlp/plugins . The file should list the plugin modules that you want to be loaded, one per line. To test that your plugins can be found and imported by AllenNLP, you can run the allennlp test-install command. Each discovered plugin will be logged to the terminal. For more information about plugins, see the plugins API docs . And for information on how to create a custom subcommand to distribute as a plugin, see the subcommand API docs .","title":"Plugins"},{"location":"#package-overview","text":"allennlp An open-source NLP research library, built on PyTorch allennlp.commands Functionality for the CLI allennlp.common Utility modules that are used across the library allennlp.data A data processing module for loading datasets and encoding strings as integers for representation in matrices allennlp.fairness A module for bias mitigation and fairness algorithms and metrics allennlp.modules A collection of PyTorch modules for use with text allennlp.nn Tensor utility functions, such as initializers and activation functions allennlp.training Functionality for training models","title":"Package Overview"},{"location":"#installation","text":"AllenNLP requires Python 3.6.1 or later and PyTorch . We support AllenNLP on Mac and Linux environments. We presently do not support Windows but are open to contributions.","title":"Installation"},{"location":"#installing-via-conda-forge","text":"The simplest way to install AllenNLP is using conda (you can choose a different python version): conda install -c conda-forge python=3.8 allennlp To install optional packages, such as checklist , use conda install -c conda-forge allennlp-checklist or simply install allennlp-all directly. The plugins mentioned above are similarly installable, e.g. conda install -c conda-forge allennlp-models allennlp-semparse allennlp-server allennlp-optuna","title":"Installing via conda-forge"},{"location":"#installing-via-pip","text":"It's recommended that you install the PyTorch ecosystem before installing AllenNLP by following the instructions on pytorch.org . After that, just run pip install allennlp . \u26a0\ufe0f If you're using Python 3.7 or greater, you should ensure that you don't have the PyPI version of dataclasses installed after running the above command, as this could cause issues on certain platforms. You can quickly check this by running pip freeze | grep dataclasses . If you see something like dataclasses=0.6 in the output, then just run pip uninstall -y dataclasses . If you need pointers on setting up an appropriate Python environment or would like to install AllenNLP using a different method, see below.","title":"Installing via pip"},{"location":"#installing-using-docker","text":"Docker provides a virtual machine with everything set up to run AllenNLP-- whether you will leverage a GPU or just run on a CPU. Docker provides more isolation and consistency, and also makes it easy to distribute your environment to a compute cluster. AllenNLP provides official Docker images with the library and all of its dependencies installed. Once you have installed Docker , you should also install the NVIDIA Container Toolkit if you have GPUs available. Then run the following command to get an environment that will run on GPU: mkdir -p $HOME /.allennlp/ docker run --rm --gpus all -v $HOME /.allennlp:/root/.allennlp allennlp/allennlp:latest You can test the Docker environment with docker run --rm --gpus all -v $HOME /.allennlp:/root/.allennlp allennlp/allennlp:latest test-install If you don't have GPUs available, just omit the --gpus all flag.","title":"Installing using Docker"},{"location":"#installing-from-source","text":"You can also install AllenNLP by cloning our git repository: git clone https://github.com/allenai/allennlp.git Create a Python 3.7 or 3.8 virtual environment, and install AllenNLP in editable mode by running: pip install -U pip setuptools wheel pip install --editable . [ dev,all ] This will make allennlp available on your system but it will use the sources from the local clone you made of the source repository. You can test your installation with allennlp test-install . See https://github.com/allenai/allennlp-models for instructions on installing allennlp-models from source.","title":"Installing from source"},{"location":"#running-allennlp","text":"Once you've installed AllenNLP, you can run the command-line interface with the allennlp command (whether you installed from pip or from source). allennlp has various subcommands such as train , evaluate , and predict . To see the full usage information, run allennlp --help . You can test your installation by running allennlp test-install .","title":"Running AllenNLP"},{"location":"#issues","text":"Everyone is welcome to file issues with either feature requests, bug reports, or general questions. As a small team with our own internal goals, we may ask for contributions if a prompt fix doesn't fit into our roadmap. To keep things tidy we will often close issues we think are answered, but don't hesitate to follow up if further discussion is needed.","title":"Issues"},{"location":"#contributions","text":"The AllenNLP team at AI2 ( @allenai ) welcomes contributions from the community. If you're a first time contributor, we recommend you start by reading our CONTRIBUTING.md guide. Then have a look at our issues with the tag Good First Issue . If you would like to contribute a larger feature, we recommend first creating an issue with a proposed design for discussion. This will prevent you from spending significant time on an implementation which has a technical limitation someone could have pointed out early on. Small contributions can be made directly in a pull request. Pull requests (PRs) must have one approving review and no requested changes before they are merged. As AllenNLP is primarily driven by AI2 we reserve the right to reject or revert contributions that we don't think are good additions.","title":"Contributions"},{"location":"#citing","text":"If you use AllenNLP in your research, please cite AllenNLP: A Deep Semantic Natural Language Processing Platform . @inproceedings { Gardner2017AllenNLP , title = {AllenNLP: A Deep Semantic Natural Language Processing Platform} , author = {Matt Gardner and Joel Grus and Mark Neumann and Oyvind Tafjord and Pradeep Dasigi and Nelson F. Liu and Matthew Peters and Michael Schmitz and Luke S. Zettlemoyer} , year = {2017} , Eprint = {arXiv:1803.07640} , }","title":"Citing"},{"location":"#team","text":"AllenNLP is an open-source project backed by the Allen Institute for Artificial Intelligence (AI2) . AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering. To learn more about who specifically contributed to this codebase, see our contributors page.","title":"Team"},{"location":"CHANGELOG/","text":"Changelog \u00b6 All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . Unreleased \u00b6 v2.9.2 - 2022-03-21 \u00b6 Fixed \u00b6 Removed unnecessary dependencies Restore functionality of CLI in absence of now-optional checklist-package v2.9.1 - 2022-03-09 \u00b6 Fixed \u00b6 Updated dependencies, especially around doc creation. Running the test suite out-of-tree (e.g. after installation) is now possible by pointing the environment variable ALLENNLP_SRC_DIR to the sources. Silenced a warning that happens when you inappropriately clone a tensor. Adding more clarification to the Vocabulary documentation around min_pretrained_embeddings and only_include_pretrained_words . Fixed bug with type mismatch caused by latest release of cached-path that now returns a Path instead of a str . Added \u00b6 We can now transparently read compressed input files during prediction. LZMA compression is now supported. Added a way to give JSON blobs as input to dataset readers in the evaluate command. Added the argument sub_module in PretrainedTransformerMismatchedEmbedder Updated the docs for PytorchSeq2VecWrapper to specify that mask is required rather than sequence lengths for clarity. Changed \u00b6 You can automatically include all words from a pretrained file when building a vocabulary by setting the value in min_pretrained_embeddings to -1 for that particular namespace. v2.9.0 - 2022-01-27 \u00b6 Added \u00b6 Added an Evaluator class to make comparing source, target, and predictions easier. Added a way to resize the vocabulary in the T5 module Added an argument reinit_modules to cached_transformers.get() that allows you to re-initialize the pretrained weights of a transformer model, using layer indices or regex strings. Added attribute _should_validate_this_epoch to GradientDescentTrainer that controls whether validation is run at the end of each epoch. Added ShouldValidateCallback that can be used to configure the frequency of validation during training. Added a MaxPoolingSpanExtractor . This SpanExtractor represents each span by a component wise max-pooling-operation. Added support for dist_metric kwarg in initializing fairness metrics, which allows optionally using wasserstein distance (previously only KL divergence was supported). Fixed \u00b6 Fixed the docstring information for the FBetaMultiLabelMeasure metric. Various fixes for Python 3.9 Fixed the name that the push-to-hf command uses to store weights. FBetaMultiLabelMeasure now works with multiple dimensions Support for inferior operating systems when making hardlinks Use , as a separator for filenames in the evaluate command, thus allowing for URLs (eg. gs://... ) as input files. Removed a spurious error message \"'torch.cuda' has no attribute '_check_driver'\" that would be appear in the logs when a ConfigurationError for missing GPU was raised. Load model on CPU post training to save GPU memory. Fixed a bug in ShouldValidateCallback that leads to validation occuring after the first epoch regardless of validation_start value. Fixed a bug in ShouldValidateCallback that leads to validation occuring every validation_interval + 1 epochs, instead of every validation_interval epochs. Fixed a bug in ShouldValidateCallback that leads to validation never occuring at the end of training. Removed \u00b6 Removed dependency on the overrides package Removed Tango components, since they now live at https://github.com/allenai/tango. Changed \u00b6 Make checklist an optional dependency. v2.8.0 - 2021-11-01 \u00b6 Added \u00b6 Added support to push models directly to the Hugging Face Hub with the command allennlp push-to-hf . More default tests for the TextualEntailmentSuite . Changed \u00b6 The behavior of --overrides has changed. Previously the final configuration params were simply taken as the union over the original params and the --overrides params. But now you can use --overrides to completely replace any part of the original config. For example, passing --overrides '{\"model\":{\"type\":\"foo\"}}' will completely replace the \"model\" part of the original config. However, when you just want to change a single field in the JSON structure without removing / replacing adjacent fields, you can still use the \"dot\" syntax. For example, --overrides '{\"model.num_layers\":3}' will only change the num_layers parameter to the \"model\" part of the config, leaving everything else unchanged. Integrated cached_path library to replace existing functionality in common.file_utils . This introduces some improvements without any breaking changes. Fixed \u00b6 Fixed the implementation of PairedPCABiasDirection in allennlp.fairness.bias_direction , where the difference vectors should not be centered when performing the PCA. Fixed the docstring of ExponentialMovingAverage , which was causing its argument descriptions to render inccorrectly in the docs. v2.7.0 - 2021-09-01 \u00b6 Added \u00b6 Added in a default behavior to the _to_params method of Registrable so that in the case it is not implemented by the child class, it will still produce a parameter dictionary . Added in _to_params implementations to all tokenizers. Added support to evaluate mutiple datasets and produce corresponding output files in the evaluate command. Added more documentation to the learning rate schedulers to include a sample config object for how to use it. Moved the pytorch learning rate schedulers wrappers to their own file called pytorch_lr_schedulers.py so that they will have their own documentation page. Added a module allennlp.nn.parallel with a new base class, DdpAccelerator , which generalizes PyTorch's DistributedDataParallel wrapper to support other implementations. Two implementations of this class are provided. The default is TorchDdpAccelerator (registered at \"torch\"), which is just a thin wrapper around DistributedDataParallel . The other is FairScaleFsdpAccelerator , which wraps FairScale's FullyShardedDataParallel . You can specify the DdpAccelerator in the \"distributed\" section of a configuration file under the key \"ddp_accelerator\". Added a module allennlp.nn.checkpoint with a new base class, CheckpointWrapper , for implementations of activation/gradient checkpointing. Two implentations are provided. The default implementation is TorchCheckpointWrapper (registered as \"torch\"), which exposes PyTorch's checkpoint functionality . The other is FairScaleCheckpointWrapper which exposes the more flexible checkpointing funtionality from FairScale . The Model base class now takes a ddp_accelerator parameter (an instance of DdpAccelerator ) which will be available as self.ddp_accelerator during distributed training. This is useful when, for example, instantiating submodules in your model's __init__() method by wrapping them with self.ddp_accelerator.wrap_module() . See the allennlp.modules.transformer.t5 for an example. We now log batch metrics to tensorboard and wandb. Added Tango components, to be explored in detail in a later post Added ScaledDotProductMatrixAttention , and converted the transformer toolkit to use it Added tests to ensure that all Attention and MatrixAttention implementations are interchangeable Added a way for AllenNLP Tango to read and write datasets lazily. Added a way to remix datasets flexibly Added from_pretrained_transformer_and_instances constructor to Vocabulary TransformerTextField now supports __len__ . Fixed \u00b6 Fixed a bug in ConditionalRandomField : transitions and tag_sequence tensors were not initialized on the desired device causing high CPU usage (see https://github.com/allenai/allennlp/issues/2884) Fixed a mispelling: the parameter contructor_extras in Lazy() is now correctly called constructor_extras . Fixed broken links in allennlp.nn.initializers docs. Fixed bug in BeamSearch where last_backpointers was not being passed to any Constraint s. TransformerTextField can now take tensors of shape (1, n) like the tensors produced from a HuggingFace tokenizer. tqdm lock is now set inside MultiProcessDataLoading when new workers are spawned to avoid contention when writing output. ConfigurationError is now pickleable. Checkpointer cleaning was fixed to work on Windows Paths Multitask models now support TextFieldTensor in heads, not just in the backbone. Fixed the signature of ScaledDotProductAttention to match the other Attention classes allennlp commands will now catch SIGTERM signals and handle them similar to SIGINT (keyboard interrupt). The MultiProcessDataLoader will properly shutdown its workers when a SIGTERM is received. Fixed the way names are applied to Tango Step instances. Fixed a bug in calculating loss in the distributed setting. Fixed a bug when extending a sparse sequence by 0 items. Changed \u00b6 The type of the grad_norm parameter of GradientDescentTrainer is now Union[float, bool] , with a default value of False . False means gradients are not rescaled and the gradient norm is never even calculated. True means the gradients are still not rescaled but the gradient norm is calculated and passed on to callbacks. A float value means gradients are rescaled. TensorCache now supports more concurrent readers and writers. We no longer log parameter statistics to tensorboard or wandb by default. v2.6.0 - 2021-07-19 \u00b6 Added \u00b6 Added on_backward training callback which allows for control over backpropagation and gradient manipulation. Added AdversarialBiasMitigator , a Model wrapper to adversarially mitigate biases in predictions produced by a pretrained model for a downstream task. Added which_loss parameter to ensure_model_can_train_save_and_load in ModelTestCase to specify which loss to test. Added **kwargs to Predictor.from_path() . These key-word argument will be passed on to the Predictor 's constructor. The activation layer in the transformer toolkit now can be queried for its output dimension. TransformerEmbeddings now takes, but ignores, a parameter for the attention mask. This is needed for compatibility with some other modules that get called the same way and use the mask. TransformerPooler can now be instantiated from a pretrained transformer module, just like the other modules in the transformer toolkit. TransformerTextField , for cases where you don't care about AllenNLP's advanced text handling capabilities. Added TransformerModule._post_load_pretrained_state_dict_hook() method. Can be used to modify missing_keys and unexpected_keys after loading a pretrained state dictionary. This is useful when tying weights, for example. Added an end-to-end test for the Transformer Toolkit. Added vocab argument to BeamSearch , which is passed to each contraint in constraints (if provided). Fixed \u00b6 Fixed missing device mapping in the allennlp.modules.conditional_random_field.py file. Fixed Broken link in allennlp.fairness.fairness_metrics.Separation docs Ensured all allennlp submodules are imported with allennlp.common.plugins.import_plugins() . Fixed IndexOutOfBoundsException in MultiOptimizer when checking if optimizer received any parameters. Removed confusing zero mask from VilBERT. Ensured ensure_model_can_train_save_and_load is consistently random. Fixed weight tying logic in T5 transformer module. Previously input/output embeddings were always tied. Now this is optional, and the default behavior is taken from the config.tie_word_embeddings value when instantiating from_pretrained_module() . Implemented slightly faster label smoothing. Fixed the docs for PytorchTransformerWrapper Fixed recovering training jobs with models that expect get_metrics() to not be called until they have seen at least one batch. Made the Transformer Toolkit compatible with transformers that don't start their positional embeddings at 0. Weights & Biases training callback (\"wandb\") now works when resuming training jobs. Changed \u00b6 Changed behavior of MultiOptimizer so that while a default optimizer is still required, an error is not thrown if the default optimizer receives no parameters. Made the epsilon parameter for the layer normalization in token embeddings configurable. Removed \u00b6 Removed TransformerModule._tied_weights . Weights should now just be tied directly in the __init__() method. You can also override TransformerModule._post_load_pretrained_state_dict_hook() to remove keys associated with tied weights from missing_keys after loading a pretrained state dictionary. v2.5.0 - 2021-06-03 \u00b6 Added \u00b6 Added TaskSuite base class and command line functionality for running checklist test suites, along with implementations for SentimentAnalysisSuite , QuestionAnsweringSuite , and TextualEntailmentSuite . These can be found in the allennlp.confidence_checks.task_checklists module. Added BiasMitigatorApplicator , which wraps any Model and mitigates biases by finetuning on a downstream task. Added allennlp diff command to compute a diff on model checkpoints, analogous to what git diff does on two files. Meta data defined by the class allennlp.common.meta.Meta is now saved in the serialization directory and archive file when training models from the command line. This is also now part of the Archive named tuple that's returned from load_archive() . Added nn.util.distributed_device() helper function. Added allennlp.nn.util.load_state_dict helper function. Added a way to avoid downloading and loading pretrained weights in modules that wrap transformers such as the PretrainedTransformerEmbedder and PretrainedTransformerMismatchedEmbedder . You can do this by setting the parameter load_weights to False . See PR #5172 for more details. Added SpanExtractorWithSpanWidthEmbedding , putting specific span embedding computations into the _embed_spans method and leaving the common code in SpanExtractorWithSpanWidthEmbedding to unify the arguments, and modified BidirectionalEndpointSpanExtractor , EndpointSpanExtractor and SelfAttentiveSpanExtractor accordingly. Now, SelfAttentiveSpanExtractor can also embed span widths. Added a min_steps parameter to BeamSearch to set a minimum length for the predicted sequences. Added the FinalSequenceScorer abstraction to calculate the final scores of the generated sequences in BeamSearch . Added shuffle argument to BucketBatchSampler which allows for disabling shuffling. Added allennlp.modules.transformer.attention_module which contains a generalized AttentionModule . SelfAttention and T5Attention both inherit from this. Added a Constraint abstract class to BeamSearch , which allows for incorporating constraints on the predictions found by BeamSearch , along with a RepeatedNGramBlockingConstraint constraint implementation, which allows for preventing repeated n-grams in the output from BeamSearch . Added DataCollator for dynamic operations for each batch. Changed \u00b6 Use dist_reduce_sum in distributed metrics. Allow Google Cloud Storage paths in cached_path (\"gs://...\"). Renamed nn.util.load_state_dict() to read_state_dict to avoid confusion with torch.nn.Module.load_state_dict() . TransformerModule.from_pretrained_module now only accepts a pretrained model ID (e.g. \"bert-base-case\") instead of an actual torch.nn.Module . Other parameters to this method have changed as well. Print the first batch to the console by default. Renamed sanity_checks to confidence_checks ( sanity_checks is deprecated and will be removed in AllenNLP 3.0). Trainer callbacks can now store and restore state in case a training run gets interrupted. VilBERT backbone now rolls and unrolls extra dimensions to handle input with > 3 dimensions. BeamSearch is now a Registrable class. Fixed \u00b6 When PretrainedTransformerIndexer folds long sequences, it no longer loses the information from token type ids. Fixed documentation for GradientDescentTrainer.cuda_device . Re-starting a training run from a checkpoint in the middle of an epoch now works correctly. When using the \"moving average\" weights smoothing feature of the trainer, training checkpoints would also get smoothed, with strange results for resuming a training job. This has been fixed. When re-starting an interrupted training job, the trainer will now read out the data loader even for epochs and batches that can be skipped. We do this to try to get any random number generators used by the reader or data loader into the same state as they were the first time the training job ran. Fixed the potential for a race condition with cached_path() when extracting archives. Although the race condition is still possible if used with force_extract=True . Fixed wandb callback to work in distributed training. Fixed tqdm logging into multiple files with allennlp-optuna . v2.4.0 - 2021-04-22 \u00b6 Added \u00b6 Added a T5 implementation to modules.transformers . Changed \u00b6 Weights & Biases callback can now work in anonymous mode (i.e. without the WANDB_API_KEY environment variable). Fixed \u00b6 The GradientDescentTrainer no longer leaves stray model checkpoints around when it runs out of patience. Fixed cached_path() for \"hf://\" files. Improved the error message for the PolynomialDecay LR scheduler when num_steps_per_epoch is missing. v2.3.1 - 2021-04-20 \u00b6 Added \u00b6 Added support for the HuggingFace Hub as an alternative way to handle loading files. Hub downloads should be made through the hf:// URL scheme. Add new dimension to the interpret module: influence functions via the InfluenceInterpreter base class, along with a concrete implementation: SimpleInfluence . Added a quiet parameter to the MultiProcessDataLoading that disables Tqdm progress bars. The test for distributed metrics now takes a parameter specifying how often you want to run it. Created the fairness module and added three fairness metrics: Independence , Separation , and Sufficiency . Added four bias metrics to the fairness module: WordEmbeddingAssociationTest , EmbeddingCoherenceTest , NaturalLanguageInference , and AssociationWithoutGroundTruth . Added four bias direction methods ( PCABiasDirection , PairedPCABiasDirection , TwoMeansBiasDirection , ClassificationNormalBiasDirection ) and four bias mitigation methods ( LinearBiasMitigator , HardBiasMitigator , INLPBiasMitigator , OSCaRBiasMitigator ). Changed \u00b6 Updated CONTRIBUTING.md to remind reader to upgrade pip setuptools to avoid spaCy installation issues. Fixed \u00b6 Fixed a bug with the ShardedDatasetReader when used with multi-process data loading (https://github.com/allenai/allennlp/issues/5132). v2.3.0 - 2021-04-14 \u00b6 Added \u00b6 Ported the following Huggingface LambdaLR -based schedulers: ConstantLearningRateScheduler , ConstantWithWarmupLearningRateScheduler , CosineWithWarmupLearningRateScheduler , CosineHardRestartsWithWarmupLearningRateScheduler . Added new sub_token_mode parameter to pretrained_transformer_mismatched_embedder class to support first sub-token embedding Added a way to run a multi task model with a dataset reader as part of allennlp predict . Added new eval_mode in PretrainedTransformerEmbedder . If it is set to True , the transformer is always run in evaluation mode, which, e.g., disables dropout and does not update batch normalization statistics. Added additional parameters to the W&B callback: entity , group , name , notes , and wandb_kwargs . Changed \u00b6 Sanity checks in the GradientDescentTrainer can now be turned off by setting the run_sanity_checks parameter to False . Allow the order of examples in the task cards to be specified explicitly histogram_interval parameter is now deprecated in TensorboardWriter , please use distribution_interval instead. Memory usage is not logged in tensorboard during training now. ConsoleLoggerCallback should be used instead. If you use the min_count parameter of the Vocabulary, but you specify a namespace that does not exist, the vocabulary creation will raise a ConfigurationError . Documentation updates made to SoftmaxLoss regarding padding and the expected shapes of the input and output tensors of forward . Moved the data preparation script for coref into allennlp-models. If a transformer is not in cache but has override weights, the transformer's pretrained weights are no longer downloaded, that is, only its config.json file is downloaded. SanityChecksCallback now raises SanityCheckError instead of AssertionError when a check fails. jsonpickle removed from dependencies. Improved the error message from Registrable.by_name() when the name passed does not match any registered subclassess. The error message will include a suggestion if there is a close match between the name passed and a registered name. Fixed \u00b6 Fixed a bug where some Activation implementations could not be pickled due to involving a lambda function. Fixed __str__() method on ModelCardInfo class. Fixed a stall when using distributed training and gradient accumulation at the same time Fixed an issue where using the from_pretrained_transformer Vocabulary constructor in distributed training via the allennlp train command would result in the data being iterated through unnecessarily. Fixed a bug regarding token indexers with the InterleavingDatasetReader when used with multi-process data loading. Fixed a warning from transformers when using max_length in the PretrainedTransformerTokenizer . Removed \u00b6 Removed the stride parameter to PretrainedTransformerTokenizer . This parameter had no effect. v2.2.0 - 2021-03-26 \u00b6 Added \u00b6 Add new method on Field class: .human_readable_repr() -> Any Add new method on Instance class: .human_readable_dict() -> JsonDict . Added WandBCallback class for Weights & Biases integration, registered as a callback under the name \"wandb\". Added TensorBoardCallback to replace the TensorBoardWriter . Registered as a callback under the name \"tensorboard\". Added NormalizationBiasVerification and SanityChecksCallback for model sanity checks. SanityChecksCallback runs by default from the allennlp train command. It can be turned off by setting trainer.enable_default_callbacks to false in your config. Changed \u00b6 Use attributes of ModelOutputs object in PretrainedTransformerEmbedder instead of indexing. Added support for PyTorch version 1.8 and torchvision version 0.9 . Model.get_parameters_for_histogram_tensorboard_logging is deprecated in favor of Model.get_parameters_for_histogram_logging . Fixed \u00b6 Makes sure tensors that are stored in TensorCache always live on CPUs Fixed a bug where FromParams objects wrapped in Lazy() couldn't be pickled. Fixed a bug where the ROUGE metric couldn't be picked. Fixed a bug reported by https://github.com/allenai/allennlp/issues/5036. We keeps our spacy POS tagger on. Removed \u00b6 Removed TensorBoardWriter . Please use the TensorBoardCallback instead. v2.1.0 - 2021-02-24 \u00b6 Changed \u00b6 coding_scheme parameter is now deprecated in Conll2003DatasetReader , please use convert_to_coding_scheme instead. Support spaCy v3 Added \u00b6 Added ModelUsage to ModelCard class. Added a way to specify extra parameters to the predictor in an allennlp predict call. Added a way to initialize a Vocabulary from transformers models. Added the ability to use Predictors with multitask models through the new MultiTaskPredictor . Added an example for fields of type ListField[TextField] to apply_token_indexers API docs. Added text_key and label_key parameters to TextClassificationJsonReader class. Added MultiOptimizer , which allows you to use different optimizers for different parts of your model. Added a clarification to predictions_to_labeled_instances API docs for attack from json Fixed \u00b6 @Registrable.register(...) decorator no longer masks the decorated class's annotations Ensured that MeanAbsoluteError always returns a float metric value instead of a Tensor . Learning rate schedulers that rely on metrics from the validation set were broken in v2.0.0. This brings that functionality back. Fixed a bug where the MultiProcessDataLoading would crash when num_workers > 0 , start_method = \"spawn\" , max_instances_in_memory not None , and batches_per_epoch not None . Fixed documentation and validation checks for FBetaMultiLabelMetric . Fixed handling of HTTP errors when fetching remote resources with cached_path() . Previously the content would be cached even when certain errors - like 404s - occurred. Now an HTTPError will be raised whenever the HTTP response is not OK. Fixed a bug where the MultiTaskDataLoader would crash when num_workers > 0 Fixed an import error that happens when PyTorch's distributed framework is unavailable on the system. v2.0.1 - 2021-01-29 \u00b6 Added \u00b6 Added tokenizer_kwargs and transformer_kwargs arguments to PretrainedTransformerBackbone Resize transformers word embeddings layer for additional_special_tokens Changed \u00b6 GradientDescentTrainer makes serialization_dir when it's instantiated, if it doesn't exist. Fixed \u00b6 common.util.sanitize now handles sets. v2.0.0 - 2021-01-27 \u00b6 Added \u00b6 The TrainerCallback constructor accepts serialization_dir provided by Trainer . This can be useful for Logger callbacks those need to store files in the run directory. The TrainerCallback.on_start() is fired at the start of the training. The TrainerCallback event methods now accept **kwargs . This may be useful to maintain backwards-compability of callbacks easier in the future. E.g. we may decide to pass the exception/traceback object in case of failure to on_end() and this older callbacks may simply ignore the argument instead of raising a TypeError . Added a TensorBoardCallback which wraps the TensorBoardWriter . Changed \u00b6 The TrainerCallack.on_epoch() does not fire with epoch=-1 at the start of the training. Instead, TrainerCallback.on_start() should be used for these cases. TensorBoardBatchMemoryUsage is converted from BatchCallback into TrainerCallback . TrackEpochCallback is converted from EpochCallback into TrainerCallback . Trainer can accept callbacks simply with name callbacks instead of trainer_callbacks . TensorboardWriter renamed to TensorBoardWriter , and removed as an argument to the GradientDescentTrainer . In order to enable TensorBoard logging during training, you should utilize the TensorBoardCallback instead. Removed \u00b6 Removed EpochCallback , BatchCallback in favour of TrainerCallback . The metaclass-wrapping implementation is removed as well. Removed the tensorboard_writer parameter to GradientDescentTrainer . You should use the TensorBoardCallback now instead. Fixed \u00b6 Now Trainer always fires TrainerCallback.on_end() so all the resources can be cleaned up properly. Fixed the misspelling, changed TensoboardBatchMemoryUsage to TensorBoardBatchMemoryUsage . We set a value to epoch so in case of firing TrainerCallback.on_end() the variable is bound. This could have lead to an error in case of trying to recover a run after it was finished training. v2.0.0rc1 - 2021-01-21 \u00b6 Added \u00b6 Added TensorCache class for caching tensors on disk Added abstraction and concrete implementation for image loading Added abstraction and concrete implementation for GridEmbedder Added abstraction and demo implementation for an image augmentation module. Added abstraction and concrete implementation for region detectors. A new high-performance default DataLoader : MultiProcessDataLoading . A MultiTaskModel and abstractions to use with it, including Backbone and Head . The MultiTaskModel first runs its inputs through the Backbone , then passes the result (and whatever other relevant inputs it got) to each Head that's in use. A MultiTaskDataLoader , with a corresponding MultiTaskDatasetReader , and a couple of new configuration objects: MultiTaskEpochSampler (for deciding what proportion to sample from each dataset at every epoch) and a MultiTaskScheduler (for ordering the instances within an epoch). Transformer toolkit to plug and play with modular components of transformer architectures. Added a command to count the number of instances we're going to be training with Added a FileLock class to common.file_utils . This is just like the FileLock from the filelock library, except that it adds an optional flag read_only_ok: bool , which when set to True changes the behavior so that a warning will be emitted instead of an exception when lacking write permissions on an existing file lock. This makes it possible to use the FileLock class on a read-only file system. Added a new learning rate scheduler: CombinedLearningRateScheduler . This can be used to combine different LR schedulers, using one after the other. Added an official CUDA 10.1 Docker image. Moving ModelCard and TaskCard abstractions into the main repository. Added a util function allennlp.nn.util.dist_reduce(...) for handling distributed reductions. This is especially useful when implementing a distributed Metric . Added a FileLock class to common.file_utils . This is just like the FileLock from the filelock library, except that it adds an optional flag read_only_ok: bool , which when set to True changes the behavior so that a warning will be emitted instead of an exception when lacking write permissions on an existing file lock. This makes it possible to use the FileLock class on a read-only file system. Added a new learning rate scheduler: CombinedLearningRateScheduler . This can be used to combine different LR schedulers, using one after the other. Moving ModelCard and TaskCard abstractions into the main repository. Changed \u00b6 DatasetReader s are now always lazy. This means there is no lazy parameter in the base class, and the _read() method should always be a generator. The DataLoader now decides whether to load instances lazily or not. With the PyTorchDataLoader this is controlled with the lazy parameter, but with the MultiProcessDataLoading this is controlled by the max_instances_in_memory setting. ArrayField is now called TensorField , and implemented in terms of torch tensors, not numpy. Improved nn.util.move_to_device function by avoiding an unnecessary recursive check for tensors and adding a non_blocking optional argument, which is the same argument as in torch.Tensor.to() . If you are trying to create a heterogeneous batch, you now get a better error message. Readers using the new vision features now explicitly log how they are featurizing images. master_addr and master_port renamed to primary_addr and primary_port , respectively. is_master parameter for training callbacks renamed to is_primary . master branch renamed to main Torch version bumped to 1.7.1 in Docker images. 'master' branch renamed to 'main' Torch version bumped to 1.7.1 in Docker images. Removed \u00b6 Removed nn.util.has_tensor . Fixed \u00b6 The build-vocab command no longer crashes when the resulting vocab file is in the current working directory. VQA models now use the vqa_score metric for early stopping. This results in much better scores. Fixed typo with LabelField string representation: removed trailing apostrophe. Vocabulary.from_files and cached_path will issue a warning, instead of failing, when a lock on an existing resource can't be acquired because the file system is read-only. TrackEpochCallback is now a EpochCallback . v1.3.0 - 2020-12-15 \u00b6 Added \u00b6 Added links to source code in docs. Added get_embedding_layer and get_text_field_embedder to the Predictor class; to specify embedding layers for non-AllenNLP models. Added Gaussian Error Linear Unit (GELU) as an Activation. Changed \u00b6 Renamed module allennlp.data.tokenizers.token to allennlp.data.tokenizers.token_class to avoid this bug . transformers dependency updated to version 4.0.1. BasicClassifier 's forward method now takes a metadata field. Fixed \u00b6 Fixed a lot of instances where tensors were first created and then sent to a device with .to(device) . Instead, these tensors are now created directly on the target device. Fixed issue with GradientDescentTrainer when constructed with validation_data_loader=None and learning_rate_scheduler!=None . Fixed a bug when removing all handlers in root logger. ShardedDatasetReader now inherits parameters from base_reader when required. Fixed an issue in FromParams where parameters in the params object used to a construct a class were not passed to the constructor if the value of the parameter was equal to the default value. This caused bugs in some edge cases where a subclass that takes **kwargs needs to inspect kwargs before passing them to its superclass. Improved the band-aid solution for segmentation faults and the \"ImportError: dlopen: cannot load any more object with static TLS\" by adding a transformers import. Added safety checks for extracting tar files Turned superfluous warning to info when extending the vocab in the embedding matrix, if no pretrained file was provided v1.2.2 - 2020-11-17 \u00b6 Added \u00b6 Added Docker builds for other torch-supported versions of CUDA. Adds allennlp-semparse as an official, default plugin. Fixed \u00b6 GumbelSampler now sorts the beams by their true log prob. v1.2.1 - 2020-11-10 \u00b6 Added \u00b6 Added an optional seed parameter to ModelTestCase.set_up_model which sets the random seed for random , numpy , and torch . Added support for a global plugins file at ~/.allennlp/plugins . Added more documentation about plugins. Added sampler class and parameter in beam search for non-deterministic search, with several implementations, including MultinomialSampler , TopKSampler , TopPSampler , and GumbelSampler . Utilizing GumbelSampler will give Stochastic Beam Search . Changed \u00b6 Pass batch metrics to BatchCallback . Fixed \u00b6 Fixed a bug where forward hooks were not cleaned up with saliency interpreters if there was an exception. Fixed the computation of saliency maps in the Interpret code when using mismatched indexing. Previously, we would compute gradients from the top of the transformer, after aggregation from wordpieces to tokens, which gives results that are not very informative. Now, we compute gradients with respect to the embedding layer, and aggregate wordpieces to tokens separately. Fixed the heuristics for finding embedding layers in the case of RoBERTa. An update in the transformers library broke our old heuristic. Fixed typo with registered name of ROUGE metric. Previously was rogue , fixed to rouge . Fixed default masks that were erroneously created on the CPU even when a GPU is available. Fixed pretrained embeddings for transformers that don't use end tokens. Fixed the transformer tokenizer cache when the tokenizers are initialized with custom kwargs. v1.2.0 - 2020-10-29 \u00b6 Changed \u00b6 Enforced stricter typing requirements around the use of Optional[T] types. Changed the behavior of Lazy types in from_params methods. Previously, if you defined a Lazy parameter like foo: Lazy[Foo] = None in a custom from_params classmethod, then foo would actually never be None . This behavior is now different. If no params were given for foo , it will be None . You can also now set default values for foo like foo: Lazy[Foo] = Lazy(Foo) . Or, if you want you want a default value but also want to allow for None values, you can write it like this: foo: Optional[Lazy[Foo]] = Lazy(Foo) . Added support for PyTorch version 1.7. Fixed \u00b6 Made it possible to instantiate TrainerCallback from config files. Fixed the remaining broken internal links in the API docs. Fixed a bug where Hotflip would crash with a model that had multiple TokenIndexers and the input used rare vocabulary items. Fixed a bug where BeamSearch would fail if max_steps was equal to 1. Fixed BasicTextFieldEmbedder to not raise ConfigurationError if it has embedders that are empty and not in input v1.2.0rc1 - 2020-10-22 \u00b6 Added \u00b6 Added a warning when batches_per_epoch for the validation data loader is inherited from the train data loader. Added a build-vocab subcommand that can be used to build a vocabulary from a training config file. Added tokenizer_kwargs argument to PretrainedTransformerMismatchedIndexer . Added tokenizer_kwargs and transformer_kwargs arguments to PretrainedTransformerMismatchedEmbedder . Added official support for Python 3.8. Added a script: scripts/release_notes.py , which automatically prepares markdown release notes from the CHANGELOG and commit history. Added a flag --predictions-output-file to the evaluate command, which tells AllenNLP to write the predictions from the given dataset to the file as JSON lines. Added the ability to ignore certain missing keys when loading a model from an archive. This is done by adding a class-level variable called authorized_missing_keys to any PyTorch module that a Model uses. If defined, authorized_missing_keys should be a list of regex string patterns. Added FBetaMultiLabelMeasure , a multi-label Fbeta metric. This is a subclass of the existing FBetaMeasure . Added ability to pass additional key word arguments to cached_transformers.get() , which will be passed on to AutoModel.from_pretrained() . Added an overrides argument to Predictor.from_path() . Added a cached-path command. Added a function inspect_cache to common.file_utils that prints useful information about the cache. This can also be used from the cached-path command with allennlp cached-path --inspect . Added a function remove_cache_entries to common.file_utils that removes any cache entries matching the given glob patterns. This can used from the cached-path command with allennlp cached-path --remove some-files-* . Added logging for the main process when running in distributed mode. Added a TrainerCallback object to support state sharing between batch and epoch-level training callbacks. Added support for .tar.gz in PretrainedModelInitializer. Made BeamSearch instantiable from_params . Pass serialization_dir to Model and DatasetReader . Added an optional include_in_archive parameter to the top-level of configuration files. When specified, include_in_archive should be a list of paths relative to the serialization directory which will be bundled up with the final archived model from a training run. Changed \u00b6 Subcommands that don't require plugins will no longer cause plugins to be loaded or have an --include-package flag. Allow overrides to be JSON string or dict . transformers dependency updated to version 3.1.0. When cached_path is called on a local archive with extract_archive=True , the archive is now extracted into a unique subdirectory of the cache root instead of a subdirectory of the archive's directory. The extraction directory is also unique to the modification time of the archive, so if the file changes, subsequent calls to cached_path will know to re-extract the archive. Removed the truncation_strategy parameter to PretrainedTransformerTokenizer . The way we're calling the tokenizer, the truncation strategy takes no effect anyways. Don't use initializers when loading a model, as it is not needed. Distributed training will now automatically search for a local open port if the master_port parameter is not provided. In training, save model weights before evaluation. allennlp.common.util.peak_memory_mb renamed to peak_cpu_memory , and allennlp.common.util.gpu_memory_mb renamed to peak_gpu_memory , and they both now return the results in bytes as integers. Also, the peak_gpu_memory function now utilizes PyTorch functions to find the memory usage instead of shelling out to the nvidia-smi command. This is more efficient and also more accurate because it only takes into account the tensor allocations of the current PyTorch process. Make sure weights are first loaded to the cpu when using PretrainedModelInitializer, preventing wasted GPU memory. Load dataset readers in load_archive . Updated AllenNlpTestCase docstring to remove reference to unittest.TestCase Removed \u00b6 Removed common.util.is_master function. Fixed \u00b6 Fix CUDA/CPU device mismatch bug during distributed training for categorical accuracy metric. Fixed a bug where the reported batch_loss metric was incorrect when training with gradient accumulation. Class decorators now displayed in API docs. Fixed up the documentation for the allennlp.nn.beam_search module. Ignore *args when constructing classes with FromParams . Ensured some consistency in the types of the values that metrics return. Fix a PyTorch warning by explicitly providing the as_tuple argument (leaving it as its default value of False ) to Tensor.nonzero() . Remove temporary directory when extracting model archive in load_archive at end of function rather than via atexit . Fixed a bug where using cached_path() offline could return a cached resource's lock file instead of the cache file. Fixed a bug where cached_path() would fail if passed a cache_dir with the user home shortcut ~/ . Fixed a bug in our doc building script where markdown links did not render properly if the \"href\" part of the link (the part inside the () ) was on a new line. Changed how gradients are zeroed out with an optimization. See this video from NVIDIA at around the 9 minute mark. Fixed a bug where parameters to a FromParams class that are dictionaries wouldn't get logged when an instance is instantiated from_params . Fixed a bug in distributed training where the vocab would be saved from every worker, when it should have been saved by only the local master process. Fixed a bug in the calculation of rouge metrics during distributed training where the total sequence count was not being aggregated across GPUs. Fixed allennlp.nn.util.add_sentence_boundary_token_ids() to use device parameter of input tensor. Be sure to close the TensorBoard writer even when training doesn't finish. Fixed the docstring for PyTorchSeq2VecWrapper . Fixed a bug in the cnn_encoder where activations involving masked tokens could be picked up by the max Fix intra word tokenization for PretrainedTransformerTokenizer when disabling fast tokenizer. v1.1.0 - 2020-09-08 \u00b6 Fixed \u00b6 Fixed handling of some edge cases when constructing classes with FromParams where the class accepts **kwargs . Fixed division by zero error when there are zero-length spans in the input to a PretrainedTransformerMismatchedIndexer . Improved robustness of cached_path when extracting archives so that the cache won't be corrupted if a failure occurs during extraction. Fixed a bug with the average and evalb_bracketing_score metrics in distributed training. Added \u00b6 Predictor.capture_model_internals() now accepts a regex specifying which modules to capture. v1.1.0rc4 - 2020-08-20 \u00b6 Added \u00b6 Added a workflow to GitHub Actions that will automatically close unassigned stale issues and ping the assignees of assigned stale issues. Fixed \u00b6 Fixed a bug in distributed metrics that caused nan values due to repeated addition of an accumulated variable. v1.1.0rc3 - 2020-08-12 \u00b6 Fixed \u00b6 Fixed how truncation was handled with PretrainedTransformerTokenizer . Previously, if max_length was set to None , the tokenizer would still do truncation if the transformer model had a default max length in its config. Also, when max_length was set to a non- None value, several warnings would appear for certain transformer models around the use of the truncation parameter. Fixed evaluation of all metrics when using distributed training. Added a py.typed marker. Fixed type annotations in allennlp.training.util . Fixed problem with automatically detecting whether tokenization is necessary. This affected primarily the Roberta SST model. Improved help text for using the --overrides command line flag. v1.1.0rc2 - 2020-07-31 \u00b6 Changed \u00b6 Upgraded PyTorch requirement to 1.6. Replaced the NVIDIA Apex AMP module with torch's native AMP module. The default trainer ( GradientDescentTrainer ) now takes a use_amp: bool parameter instead of the old opt_level: str parameter. Fixed \u00b6 Removed unnecessary warning about deadlocks in DataLoader . Fixed testing models that only return a loss when they are in training mode. Fixed a bug in FromParams that caused silent failure in case of the parameter type being Optional[Union[...]] . Fixed a bug where the program crashes if evaluation_data_loader is a AllennlpLazyDataset . Added \u00b6 Added the option to specify requires_grad: false within an optimizer's parameter groups. Added the file-friendly-logging flag back to the train command. Also added this flag to the predict , evaluate , and find-learning-rate commands. Added an EpochCallback to track current epoch as a model class member. Added the option to enable or disable gradient checkpointing for transformer token embedders via boolean parameter gradient_checkpointing . Removed \u00b6 Removed the opt_level parameter to Model.load and load_archive . In order to use AMP with a loaded model now, just run the model's forward pass within torch's autocast context. v1.1.0rc1 - 2020-07-14 \u00b6 Fixed \u00b6 Reduced the amount of log messages produced by allennlp.common.file_utils . Fixed a bug where PretrainedTransformerEmbedder parameters appeared to be trainable in the log output even when train_parameters was set to False . Fixed a bug with the sharded dataset reader where it would only read a fraction of the instances in distributed training. Fixed checking equality of TensorField s. Fixed a bug where NamespaceSwappingField did not work correctly with .empty_field() . Put more sensible defaults on the huggingface_adamw optimizer. Simplified logging so that all logging output always goes to one file. Fixed interaction with the python command line debugger. Log the grad norm properly even when we're not clipping it. Fixed a bug where PretrainedModelInitializer fails to initialize a model with a 0-dim tensor Fixed a bug with the layer unfreezing schedule of the SlantedTriangular learning rate scheduler. Fixed a regression with logging in the distributed setting. Only the main worker should write log output to the terminal. Pinned the version of boto3 for package managers (e.g. poetry). Fixed issue #4330 by updating the tokenizers dependency. Fixed a bug in TextClassificationPredictor so that it passes tokenized inputs to the DatasetReader in case it does not have a tokenizer. reg_loss is only now returned for models that have some regularization penalty configured. Fixed a bug that prevented cached_path from downloading assets from GitHub releases. Fixed a bug that erroneously increased last label's false positive count in calculating fbeta metrics. Tqdm output now looks much better when the output is being piped or redirected. Small improvements to how the API documentation is rendered. Only show validation progress bar from main process in distributed training. Added \u00b6 Adjust beam search to support multi-layer decoder. A method to ModelTestCase for running basic model tests when you aren't using config files. Added some convenience methods for reading files. Added an option to file_utils.cached_path to automatically extract archives. Added the ability to pass an archive file instead of a local directory to Vocab.from_files . Added the ability to pass an archive file instead of a glob to ShardedDatasetReader . Added a new \"linear_with_warmup\" learning rate scheduler. Added a check in ShardedDatasetReader that ensures the base reader doesn't implement manual distributed sharding itself. Added an option to PretrainedTransformerEmbedder and PretrainedTransformerMismatchedEmbedder to use a scalar mix of all hidden layers from the transformer model instead of just the last layer. To utilize this, just set last_layer_only to False . cached_path() can now read files inside of archives. Training metrics now include batch_loss and batch_reg_loss in addition to aggregate loss across number of batches. Changed \u00b6 Not specifying a cuda_device now automatically determines whether to use a GPU or not. Discovered plugins are logged so you can see what was loaded. allennlp.data.DataLoader is now an abstract registrable class. The default implementation remains the same, but was renamed to allennlp.data.PyTorchDataLoader . BertPooler can now unwrap and re-wrap extra dimensions if necessary. New transformers dependency. Only version >=3.0 now supported. v1.0.0 - 2020-06-16 \u00b6 Fixed \u00b6 Lazy dataset readers now work correctly with multi-process data loading. Fixed race conditions that could occur when using a dataset cache. Added \u00b6 A bug where where all datasets would be loaded for vocab creation even if not needed. A parameter to the DatasetReader class: manual_multi_process_sharding . This is similar to the manual_distributed_sharding parameter, but applies when using a multi-process DataLoader . v1.0.0rc6 - 2020-06-11 \u00b6 Fixed \u00b6 A bug where TextField s could not be duplicated since some tokenizers cannot be deep-copied. See https://github.com/allenai/allennlp/issues/4270. Our caching mechanism had the potential to introduce race conditions if multiple processes were attempting to cache the same file at once. This was fixed by using a lock file tied to each cached file. get_text_field_mask() now supports padding indices that are not 0 . A bug where predictor.get_gradients() would return an empty dictionary if an embedding layer had trainable set to false Fixes PretrainedTransformerMismatchedIndexer in the case where a token consists of zero word pieces. Fixes a bug when using a lazy dataset reader that results in a UserWarning from PyTorch being printed at every iteration during training. Predictor names were inconsistently switching between dashes and underscores. Now they all use underscores. Predictor.from_path now automatically loads plugins (unless you specify load_plugins=False ) so that you don't have to manually import a bunch of modules when instantiating predictors from an archive path. allennlp-server automatically found as a plugin once again. Added \u00b6 A duplicate() method on Instance s and Field s, to be used instead of copy.deepcopy() A batch sampler that makes sure each batch contains approximately the same number of tokens ( MaxTokensBatchSampler ) Functions to turn a sequence of token indices back into tokens The ability to use Huggingface encoder/decoder models as token embedders Improvements to beam search ROUGE metric Polynomial decay learning rate scheduler A BatchCallback for logging CPU and GPU memory usage to tensorboard. This is mainly for debugging because using it can cause a significant slowdown in training. Ability to run pretrained transformers as an embedder without training the weights Add Optuna Integrated badge to README.md Changed \u00b6 Similar to our caching mechanism, we introduced a lock file to the vocab to avoid race conditions when saving/loading the vocab from/to the same serialization directory in different processes. Changed the Token , Instance , and Batch classes along with all Field classes to \"slots\" classes. This dramatically reduces the size in memory of instances. SimpleTagger will no longer calculate span-based F1 metric when calculate_span_f1 is False . CPU memory for every worker is now reported in the logs and the metrics. Previously this was only reporting the CPU memory of the master process, and so it was only correct in the non-distributed setting. To be consistent with PyTorch IterableDataset , AllennlpLazyDataset no longer implements __len__() . Previously it would always return 1. Removed old tutorials, in favor of the new AllenNLP Guide Changed the vocabulary loading to consider new lines for Windows/Linux and Mac. v1.0.0rc5 - 2020-05-26 \u00b6 Fixed \u00b6 Fix bug where PretrainedTransformerTokenizer crashed with some transformers (#4267) Make cached_path work offline. Tons of docstring inconsistencies resolved. Nightly builds no longer run on forks. Distributed training now automatically figures out which worker should see which instances A race condition bug in distributed training caused from saving the vocab to file from the master process while other processing might be reading those files. Unused dependencies in setup.py removed. Added \u00b6 Additional CI checks to ensure docstrings are consistently formatted. Ability to train on CPU with multiple processes by setting cuda_devices to a list of negative integers in your training config. For example: \"distributed\": {\"cuda_devices\": [-1, -1]} . This is mainly to make it easier to test and debug distributed training code.. Documentation for when parameters don't need config file entries. Changed \u00b6 The allennlp test-install command now just ensures the core submodules can be imported successfully, and prints out some other useful information such as the version, PyTorch version, and the number of GPU devices available. All of the tests moved from allennlp/tests to tests at the root level, and allennlp/tests/fixtures moved to test_fixtures at the root level. The PyPI source and wheel distributions will no longer include tests and fixtures. v1.0.0rc4 - 2020-05-14 \u00b6 We first introduced this CHANGELOG after release v1.0.0rc4 , so please refer to the GitHub release notes for this and earlier releases.","title":"CHANGELOG"},{"location":"CHANGELOG/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"CHANGELOG/#unreleased","text":"","title":"Unreleased"},{"location":"CHANGELOG/#v292-2022-03-21","text":"","title":"v2.9.2 - 2022-03-21"},{"location":"CHANGELOG/#fixed","text":"Removed unnecessary dependencies Restore functionality of CLI in absence of now-optional checklist-package","title":"Fixed"},{"location":"CHANGELOG/#v291-2022-03-09","text":"","title":"v2.9.1 - 2022-03-09"},{"location":"CHANGELOG/#fixed_1","text":"Updated dependencies, especially around doc creation. Running the test suite out-of-tree (e.g. after installation) is now possible by pointing the environment variable ALLENNLP_SRC_DIR to the sources. Silenced a warning that happens when you inappropriately clone a tensor. Adding more clarification to the Vocabulary documentation around min_pretrained_embeddings and only_include_pretrained_words . Fixed bug with type mismatch caused by latest release of cached-path that now returns a Path instead of a str .","title":"Fixed"},{"location":"CHANGELOG/#added","text":"We can now transparently read compressed input files during prediction. LZMA compression is now supported. Added a way to give JSON blobs as input to dataset readers in the evaluate command. Added the argument sub_module in PretrainedTransformerMismatchedEmbedder Updated the docs for PytorchSeq2VecWrapper to specify that mask is required rather than sequence lengths for clarity.","title":"Added"},{"location":"CHANGELOG/#changed","text":"You can automatically include all words from a pretrained file when building a vocabulary by setting the value in min_pretrained_embeddings to -1 for that particular namespace.","title":"Changed"},{"location":"CHANGELOG/#v290-2022-01-27","text":"","title":"v2.9.0 - 2022-01-27"},{"location":"CHANGELOG/#added_1","text":"Added an Evaluator class to make comparing source, target, and predictions easier. Added a way to resize the vocabulary in the T5 module Added an argument reinit_modules to cached_transformers.get() that allows you to re-initialize the pretrained weights of a transformer model, using layer indices or regex strings. Added attribute _should_validate_this_epoch to GradientDescentTrainer that controls whether validation is run at the end of each epoch. Added ShouldValidateCallback that can be used to configure the frequency of validation during training. Added a MaxPoolingSpanExtractor . This SpanExtractor represents each span by a component wise max-pooling-operation. Added support for dist_metric kwarg in initializing fairness metrics, which allows optionally using wasserstein distance (previously only KL divergence was supported).","title":"Added"},{"location":"CHANGELOG/#fixed_2","text":"Fixed the docstring information for the FBetaMultiLabelMeasure metric. Various fixes for Python 3.9 Fixed the name that the push-to-hf command uses to store weights. FBetaMultiLabelMeasure now works with multiple dimensions Support for inferior operating systems when making hardlinks Use , as a separator for filenames in the evaluate command, thus allowing for URLs (eg. gs://... ) as input files. Removed a spurious error message \"'torch.cuda' has no attribute '_check_driver'\" that would be appear in the logs when a ConfigurationError for missing GPU was raised. Load model on CPU post training to save GPU memory. Fixed a bug in ShouldValidateCallback that leads to validation occuring after the first epoch regardless of validation_start value. Fixed a bug in ShouldValidateCallback that leads to validation occuring every validation_interval + 1 epochs, instead of every validation_interval epochs. Fixed a bug in ShouldValidateCallback that leads to validation never occuring at the end of training.","title":"Fixed"},{"location":"CHANGELOG/#removed","text":"Removed dependency on the overrides package Removed Tango components, since they now live at https://github.com/allenai/tango.","title":"Removed"},{"location":"CHANGELOG/#changed_1","text":"Make checklist an optional dependency.","title":"Changed"},{"location":"CHANGELOG/#v280-2021-11-01","text":"","title":"v2.8.0 - 2021-11-01"},{"location":"CHANGELOG/#added_2","text":"Added support to push models directly to the Hugging Face Hub with the command allennlp push-to-hf . More default tests for the TextualEntailmentSuite .","title":"Added"},{"location":"CHANGELOG/#changed_2","text":"The behavior of --overrides has changed. Previously the final configuration params were simply taken as the union over the original params and the --overrides params. But now you can use --overrides to completely replace any part of the original config. For example, passing --overrides '{\"model\":{\"type\":\"foo\"}}' will completely replace the \"model\" part of the original config. However, when you just want to change a single field in the JSON structure without removing / replacing adjacent fields, you can still use the \"dot\" syntax. For example, --overrides '{\"model.num_layers\":3}' will only change the num_layers parameter to the \"model\" part of the config, leaving everything else unchanged. Integrated cached_path library to replace existing functionality in common.file_utils . This introduces some improvements without any breaking changes.","title":"Changed"},{"location":"CHANGELOG/#fixed_3","text":"Fixed the implementation of PairedPCABiasDirection in allennlp.fairness.bias_direction , where the difference vectors should not be centered when performing the PCA. Fixed the docstring of ExponentialMovingAverage , which was causing its argument descriptions to render inccorrectly in the docs.","title":"Fixed"},{"location":"CHANGELOG/#v270-2021-09-01","text":"","title":"v2.7.0 - 2021-09-01"},{"location":"CHANGELOG/#added_3","text":"Added in a default behavior to the _to_params method of Registrable so that in the case it is not implemented by the child class, it will still produce a parameter dictionary . Added in _to_params implementations to all tokenizers. Added support to evaluate mutiple datasets and produce corresponding output files in the evaluate command. Added more documentation to the learning rate schedulers to include a sample config object for how to use it. Moved the pytorch learning rate schedulers wrappers to their own file called pytorch_lr_schedulers.py so that they will have their own documentation page. Added a module allennlp.nn.parallel with a new base class, DdpAccelerator , which generalizes PyTorch's DistributedDataParallel wrapper to support other implementations. Two implementations of this class are provided. The default is TorchDdpAccelerator (registered at \"torch\"), which is just a thin wrapper around DistributedDataParallel . The other is FairScaleFsdpAccelerator , which wraps FairScale's FullyShardedDataParallel . You can specify the DdpAccelerator in the \"distributed\" section of a configuration file under the key \"ddp_accelerator\". Added a module allennlp.nn.checkpoint with a new base class, CheckpointWrapper , for implementations of activation/gradient checkpointing. Two implentations are provided. The default implementation is TorchCheckpointWrapper (registered as \"torch\"), which exposes PyTorch's checkpoint functionality . The other is FairScaleCheckpointWrapper which exposes the more flexible checkpointing funtionality from FairScale . The Model base class now takes a ddp_accelerator parameter (an instance of DdpAccelerator ) which will be available as self.ddp_accelerator during distributed training. This is useful when, for example, instantiating submodules in your model's __init__() method by wrapping them with self.ddp_accelerator.wrap_module() . See the allennlp.modules.transformer.t5 for an example. We now log batch metrics to tensorboard and wandb. Added Tango components, to be explored in detail in a later post Added ScaledDotProductMatrixAttention , and converted the transformer toolkit to use it Added tests to ensure that all Attention and MatrixAttention implementations are interchangeable Added a way for AllenNLP Tango to read and write datasets lazily. Added a way to remix datasets flexibly Added from_pretrained_transformer_and_instances constructor to Vocabulary TransformerTextField now supports __len__ .","title":"Added"},{"location":"CHANGELOG/#fixed_4","text":"Fixed a bug in ConditionalRandomField : transitions and tag_sequence tensors were not initialized on the desired device causing high CPU usage (see https://github.com/allenai/allennlp/issues/2884) Fixed a mispelling: the parameter contructor_extras in Lazy() is now correctly called constructor_extras . Fixed broken links in allennlp.nn.initializers docs. Fixed bug in BeamSearch where last_backpointers was not being passed to any Constraint s. TransformerTextField can now take tensors of shape (1, n) like the tensors produced from a HuggingFace tokenizer. tqdm lock is now set inside MultiProcessDataLoading when new workers are spawned to avoid contention when writing output. ConfigurationError is now pickleable. Checkpointer cleaning was fixed to work on Windows Paths Multitask models now support TextFieldTensor in heads, not just in the backbone. Fixed the signature of ScaledDotProductAttention to match the other Attention classes allennlp commands will now catch SIGTERM signals and handle them similar to SIGINT (keyboard interrupt). The MultiProcessDataLoader will properly shutdown its workers when a SIGTERM is received. Fixed the way names are applied to Tango Step instances. Fixed a bug in calculating loss in the distributed setting. Fixed a bug when extending a sparse sequence by 0 items.","title":"Fixed"},{"location":"CHANGELOG/#changed_3","text":"The type of the grad_norm parameter of GradientDescentTrainer is now Union[float, bool] , with a default value of False . False means gradients are not rescaled and the gradient norm is never even calculated. True means the gradients are still not rescaled but the gradient norm is calculated and passed on to callbacks. A float value means gradients are rescaled. TensorCache now supports more concurrent readers and writers. We no longer log parameter statistics to tensorboard or wandb by default.","title":"Changed"},{"location":"CHANGELOG/#v260-2021-07-19","text":"","title":"v2.6.0 - 2021-07-19"},{"location":"CHANGELOG/#added_4","text":"Added on_backward training callback which allows for control over backpropagation and gradient manipulation. Added AdversarialBiasMitigator , a Model wrapper to adversarially mitigate biases in predictions produced by a pretrained model for a downstream task. Added which_loss parameter to ensure_model_can_train_save_and_load in ModelTestCase to specify which loss to test. Added **kwargs to Predictor.from_path() . These key-word argument will be passed on to the Predictor 's constructor. The activation layer in the transformer toolkit now can be queried for its output dimension. TransformerEmbeddings now takes, but ignores, a parameter for the attention mask. This is needed for compatibility with some other modules that get called the same way and use the mask. TransformerPooler can now be instantiated from a pretrained transformer module, just like the other modules in the transformer toolkit. TransformerTextField , for cases where you don't care about AllenNLP's advanced text handling capabilities. Added TransformerModule._post_load_pretrained_state_dict_hook() method. Can be used to modify missing_keys and unexpected_keys after loading a pretrained state dictionary. This is useful when tying weights, for example. Added an end-to-end test for the Transformer Toolkit. Added vocab argument to BeamSearch , which is passed to each contraint in constraints (if provided).","title":"Added"},{"location":"CHANGELOG/#fixed_5","text":"Fixed missing device mapping in the allennlp.modules.conditional_random_field.py file. Fixed Broken link in allennlp.fairness.fairness_metrics.Separation docs Ensured all allennlp submodules are imported with allennlp.common.plugins.import_plugins() . Fixed IndexOutOfBoundsException in MultiOptimizer when checking if optimizer received any parameters. Removed confusing zero mask from VilBERT. Ensured ensure_model_can_train_save_and_load is consistently random. Fixed weight tying logic in T5 transformer module. Previously input/output embeddings were always tied. Now this is optional, and the default behavior is taken from the config.tie_word_embeddings value when instantiating from_pretrained_module() . Implemented slightly faster label smoothing. Fixed the docs for PytorchTransformerWrapper Fixed recovering training jobs with models that expect get_metrics() to not be called until they have seen at least one batch. Made the Transformer Toolkit compatible with transformers that don't start their positional embeddings at 0. Weights & Biases training callback (\"wandb\") now works when resuming training jobs.","title":"Fixed"},{"location":"CHANGELOG/#changed_4","text":"Changed behavior of MultiOptimizer so that while a default optimizer is still required, an error is not thrown if the default optimizer receives no parameters. Made the epsilon parameter for the layer normalization in token embeddings configurable.","title":"Changed"},{"location":"CHANGELOG/#removed_1","text":"Removed TransformerModule._tied_weights . Weights should now just be tied directly in the __init__() method. You can also override TransformerModule._post_load_pretrained_state_dict_hook() to remove keys associated with tied weights from missing_keys after loading a pretrained state dictionary.","title":"Removed"},{"location":"CHANGELOG/#v250-2021-06-03","text":"","title":"v2.5.0 - 2021-06-03"},{"location":"CHANGELOG/#added_5","text":"Added TaskSuite base class and command line functionality for running checklist test suites, along with implementations for SentimentAnalysisSuite , QuestionAnsweringSuite , and TextualEntailmentSuite . These can be found in the allennlp.confidence_checks.task_checklists module. Added BiasMitigatorApplicator , which wraps any Model and mitigates biases by finetuning on a downstream task. Added allennlp diff command to compute a diff on model checkpoints, analogous to what git diff does on two files. Meta data defined by the class allennlp.common.meta.Meta is now saved in the serialization directory and archive file when training models from the command line. This is also now part of the Archive named tuple that's returned from load_archive() . Added nn.util.distributed_device() helper function. Added allennlp.nn.util.load_state_dict helper function. Added a way to avoid downloading and loading pretrained weights in modules that wrap transformers such as the PretrainedTransformerEmbedder and PretrainedTransformerMismatchedEmbedder . You can do this by setting the parameter load_weights to False . See PR #5172 for more details. Added SpanExtractorWithSpanWidthEmbedding , putting specific span embedding computations into the _embed_spans method and leaving the common code in SpanExtractorWithSpanWidthEmbedding to unify the arguments, and modified BidirectionalEndpointSpanExtractor , EndpointSpanExtractor and SelfAttentiveSpanExtractor accordingly. Now, SelfAttentiveSpanExtractor can also embed span widths. Added a min_steps parameter to BeamSearch to set a minimum length for the predicted sequences. Added the FinalSequenceScorer abstraction to calculate the final scores of the generated sequences in BeamSearch . Added shuffle argument to BucketBatchSampler which allows for disabling shuffling. Added allennlp.modules.transformer.attention_module which contains a generalized AttentionModule . SelfAttention and T5Attention both inherit from this. Added a Constraint abstract class to BeamSearch , which allows for incorporating constraints on the predictions found by BeamSearch , along with a RepeatedNGramBlockingConstraint constraint implementation, which allows for preventing repeated n-grams in the output from BeamSearch . Added DataCollator for dynamic operations for each batch.","title":"Added"},{"location":"CHANGELOG/#changed_5","text":"Use dist_reduce_sum in distributed metrics. Allow Google Cloud Storage paths in cached_path (\"gs://...\"). Renamed nn.util.load_state_dict() to read_state_dict to avoid confusion with torch.nn.Module.load_state_dict() . TransformerModule.from_pretrained_module now only accepts a pretrained model ID (e.g. \"bert-base-case\") instead of an actual torch.nn.Module . Other parameters to this method have changed as well. Print the first batch to the console by default. Renamed sanity_checks to confidence_checks ( sanity_checks is deprecated and will be removed in AllenNLP 3.0). Trainer callbacks can now store and restore state in case a training run gets interrupted. VilBERT backbone now rolls and unrolls extra dimensions to handle input with > 3 dimensions. BeamSearch is now a Registrable class.","title":"Changed"},{"location":"CHANGELOG/#fixed_6","text":"When PretrainedTransformerIndexer folds long sequences, it no longer loses the information from token type ids. Fixed documentation for GradientDescentTrainer.cuda_device . Re-starting a training run from a checkpoint in the middle of an epoch now works correctly. When using the \"moving average\" weights smoothing feature of the trainer, training checkpoints would also get smoothed, with strange results for resuming a training job. This has been fixed. When re-starting an interrupted training job, the trainer will now read out the data loader even for epochs and batches that can be skipped. We do this to try to get any random number generators used by the reader or data loader into the same state as they were the first time the training job ran. Fixed the potential for a race condition with cached_path() when extracting archives. Although the race condition is still possible if used with force_extract=True . Fixed wandb callback to work in distributed training. Fixed tqdm logging into multiple files with allennlp-optuna .","title":"Fixed"},{"location":"CHANGELOG/#v240-2021-04-22","text":"","title":"v2.4.0 - 2021-04-22"},{"location":"CHANGELOG/#added_6","text":"Added a T5 implementation to modules.transformers .","title":"Added"},{"location":"CHANGELOG/#changed_6","text":"Weights & Biases callback can now work in anonymous mode (i.e. without the WANDB_API_KEY environment variable).","title":"Changed"},{"location":"CHANGELOG/#fixed_7","text":"The GradientDescentTrainer no longer leaves stray model checkpoints around when it runs out of patience. Fixed cached_path() for \"hf://\" files. Improved the error message for the PolynomialDecay LR scheduler when num_steps_per_epoch is missing.","title":"Fixed"},{"location":"CHANGELOG/#v231-2021-04-20","text":"","title":"v2.3.1 - 2021-04-20"},{"location":"CHANGELOG/#added_7","text":"Added support for the HuggingFace Hub as an alternative way to handle loading files. Hub downloads should be made through the hf:// URL scheme. Add new dimension to the interpret module: influence functions via the InfluenceInterpreter base class, along with a concrete implementation: SimpleInfluence . Added a quiet parameter to the MultiProcessDataLoading that disables Tqdm progress bars. The test for distributed metrics now takes a parameter specifying how often you want to run it. Created the fairness module and added three fairness metrics: Independence , Separation , and Sufficiency . Added four bias metrics to the fairness module: WordEmbeddingAssociationTest , EmbeddingCoherenceTest , NaturalLanguageInference , and AssociationWithoutGroundTruth . Added four bias direction methods ( PCABiasDirection , PairedPCABiasDirection , TwoMeansBiasDirection , ClassificationNormalBiasDirection ) and four bias mitigation methods ( LinearBiasMitigator , HardBiasMitigator , INLPBiasMitigator , OSCaRBiasMitigator ).","title":"Added"},{"location":"CHANGELOG/#changed_7","text":"Updated CONTRIBUTING.md to remind reader to upgrade pip setuptools to avoid spaCy installation issues.","title":"Changed"},{"location":"CHANGELOG/#fixed_8","text":"Fixed a bug with the ShardedDatasetReader when used with multi-process data loading (https://github.com/allenai/allennlp/issues/5132).","title":"Fixed"},{"location":"CHANGELOG/#v230-2021-04-14","text":"","title":"v2.3.0 - 2021-04-14"},{"location":"CHANGELOG/#added_8","text":"Ported the following Huggingface LambdaLR -based schedulers: ConstantLearningRateScheduler , ConstantWithWarmupLearningRateScheduler , CosineWithWarmupLearningRateScheduler , CosineHardRestartsWithWarmupLearningRateScheduler . Added new sub_token_mode parameter to pretrained_transformer_mismatched_embedder class to support first sub-token embedding Added a way to run a multi task model with a dataset reader as part of allennlp predict . Added new eval_mode in PretrainedTransformerEmbedder . If it is set to True , the transformer is always run in evaluation mode, which, e.g., disables dropout and does not update batch normalization statistics. Added additional parameters to the W&B callback: entity , group , name , notes , and wandb_kwargs .","title":"Added"},{"location":"CHANGELOG/#changed_8","text":"Sanity checks in the GradientDescentTrainer can now be turned off by setting the run_sanity_checks parameter to False . Allow the order of examples in the task cards to be specified explicitly histogram_interval parameter is now deprecated in TensorboardWriter , please use distribution_interval instead. Memory usage is not logged in tensorboard during training now. ConsoleLoggerCallback should be used instead. If you use the min_count parameter of the Vocabulary, but you specify a namespace that does not exist, the vocabulary creation will raise a ConfigurationError . Documentation updates made to SoftmaxLoss regarding padding and the expected shapes of the input and output tensors of forward . Moved the data preparation script for coref into allennlp-models. If a transformer is not in cache but has override weights, the transformer's pretrained weights are no longer downloaded, that is, only its config.json file is downloaded. SanityChecksCallback now raises SanityCheckError instead of AssertionError when a check fails. jsonpickle removed from dependencies. Improved the error message from Registrable.by_name() when the name passed does not match any registered subclassess. The error message will include a suggestion if there is a close match between the name passed and a registered name.","title":"Changed"},{"location":"CHANGELOG/#fixed_9","text":"Fixed a bug where some Activation implementations could not be pickled due to involving a lambda function. Fixed __str__() method on ModelCardInfo class. Fixed a stall when using distributed training and gradient accumulation at the same time Fixed an issue where using the from_pretrained_transformer Vocabulary constructor in distributed training via the allennlp train command would result in the data being iterated through unnecessarily. Fixed a bug regarding token indexers with the InterleavingDatasetReader when used with multi-process data loading. Fixed a warning from transformers when using max_length in the PretrainedTransformerTokenizer .","title":"Fixed"},{"location":"CHANGELOG/#removed_2","text":"Removed the stride parameter to PretrainedTransformerTokenizer . This parameter had no effect.","title":"Removed"},{"location":"CHANGELOG/#v220-2021-03-26","text":"","title":"v2.2.0 - 2021-03-26"},{"location":"CHANGELOG/#added_9","text":"Add new method on Field class: .human_readable_repr() -> Any Add new method on Instance class: .human_readable_dict() -> JsonDict . Added WandBCallback class for Weights & Biases integration, registered as a callback under the name \"wandb\". Added TensorBoardCallback to replace the TensorBoardWriter . Registered as a callback under the name \"tensorboard\". Added NormalizationBiasVerification and SanityChecksCallback for model sanity checks. SanityChecksCallback runs by default from the allennlp train command. It can be turned off by setting trainer.enable_default_callbacks to false in your config.","title":"Added"},{"location":"CHANGELOG/#changed_9","text":"Use attributes of ModelOutputs object in PretrainedTransformerEmbedder instead of indexing. Added support for PyTorch version 1.8 and torchvision version 0.9 . Model.get_parameters_for_histogram_tensorboard_logging is deprecated in favor of Model.get_parameters_for_histogram_logging .","title":"Changed"},{"location":"CHANGELOG/#fixed_10","text":"Makes sure tensors that are stored in TensorCache always live on CPUs Fixed a bug where FromParams objects wrapped in Lazy() couldn't be pickled. Fixed a bug where the ROUGE metric couldn't be picked. Fixed a bug reported by https://github.com/allenai/allennlp/issues/5036. We keeps our spacy POS tagger on.","title":"Fixed"},{"location":"CHANGELOG/#removed_3","text":"Removed TensorBoardWriter . Please use the TensorBoardCallback instead.","title":"Removed"},{"location":"CHANGELOG/#v210-2021-02-24","text":"","title":"v2.1.0 - 2021-02-24"},{"location":"CHANGELOG/#changed_10","text":"coding_scheme parameter is now deprecated in Conll2003DatasetReader , please use convert_to_coding_scheme instead. Support spaCy v3","title":"Changed"},{"location":"CHANGELOG/#added_10","text":"Added ModelUsage to ModelCard class. Added a way to specify extra parameters to the predictor in an allennlp predict call. Added a way to initialize a Vocabulary from transformers models. Added the ability to use Predictors with multitask models through the new MultiTaskPredictor . Added an example for fields of type ListField[TextField] to apply_token_indexers API docs. Added text_key and label_key parameters to TextClassificationJsonReader class. Added MultiOptimizer , which allows you to use different optimizers for different parts of your model. Added a clarification to predictions_to_labeled_instances API docs for attack from json","title":"Added"},{"location":"CHANGELOG/#fixed_11","text":"@Registrable.register(...) decorator no longer masks the decorated class's annotations Ensured that MeanAbsoluteError always returns a float metric value instead of a Tensor . Learning rate schedulers that rely on metrics from the validation set were broken in v2.0.0. This brings that functionality back. Fixed a bug where the MultiProcessDataLoading would crash when num_workers > 0 , start_method = \"spawn\" , max_instances_in_memory not None , and batches_per_epoch not None . Fixed documentation and validation checks for FBetaMultiLabelMetric . Fixed handling of HTTP errors when fetching remote resources with cached_path() . Previously the content would be cached even when certain errors - like 404s - occurred. Now an HTTPError will be raised whenever the HTTP response is not OK. Fixed a bug where the MultiTaskDataLoader would crash when num_workers > 0 Fixed an import error that happens when PyTorch's distributed framework is unavailable on the system.","title":"Fixed"},{"location":"CHANGELOG/#v201-2021-01-29","text":"","title":"v2.0.1 - 2021-01-29"},{"location":"CHANGELOG/#added_11","text":"Added tokenizer_kwargs and transformer_kwargs arguments to PretrainedTransformerBackbone Resize transformers word embeddings layer for additional_special_tokens","title":"Added"},{"location":"CHANGELOG/#changed_11","text":"GradientDescentTrainer makes serialization_dir when it's instantiated, if it doesn't exist.","title":"Changed"},{"location":"CHANGELOG/#fixed_12","text":"common.util.sanitize now handles sets.","title":"Fixed"},{"location":"CHANGELOG/#v200-2021-01-27","text":"","title":"v2.0.0 - 2021-01-27"},{"location":"CHANGELOG/#added_12","text":"The TrainerCallback constructor accepts serialization_dir provided by Trainer . This can be useful for Logger callbacks those need to store files in the run directory. The TrainerCallback.on_start() is fired at the start of the training. The TrainerCallback event methods now accept **kwargs . This may be useful to maintain backwards-compability of callbacks easier in the future. E.g. we may decide to pass the exception/traceback object in case of failure to on_end() and this older callbacks may simply ignore the argument instead of raising a TypeError . Added a TensorBoardCallback which wraps the TensorBoardWriter .","title":"Added"},{"location":"CHANGELOG/#changed_12","text":"The TrainerCallack.on_epoch() does not fire with epoch=-1 at the start of the training. Instead, TrainerCallback.on_start() should be used for these cases. TensorBoardBatchMemoryUsage is converted from BatchCallback into TrainerCallback . TrackEpochCallback is converted from EpochCallback into TrainerCallback . Trainer can accept callbacks simply with name callbacks instead of trainer_callbacks . TensorboardWriter renamed to TensorBoardWriter , and removed as an argument to the GradientDescentTrainer . In order to enable TensorBoard logging during training, you should utilize the TensorBoardCallback instead.","title":"Changed"},{"location":"CHANGELOG/#removed_4","text":"Removed EpochCallback , BatchCallback in favour of TrainerCallback . The metaclass-wrapping implementation is removed as well. Removed the tensorboard_writer parameter to GradientDescentTrainer . You should use the TensorBoardCallback now instead.","title":"Removed"},{"location":"CHANGELOG/#fixed_13","text":"Now Trainer always fires TrainerCallback.on_end() so all the resources can be cleaned up properly. Fixed the misspelling, changed TensoboardBatchMemoryUsage to TensorBoardBatchMemoryUsage . We set a value to epoch so in case of firing TrainerCallback.on_end() the variable is bound. This could have lead to an error in case of trying to recover a run after it was finished training.","title":"Fixed"},{"location":"CHANGELOG/#v200rc1-2021-01-21","text":"","title":"v2.0.0rc1 - 2021-01-21"},{"location":"CHANGELOG/#added_13","text":"Added TensorCache class for caching tensors on disk Added abstraction and concrete implementation for image loading Added abstraction and concrete implementation for GridEmbedder Added abstraction and demo implementation for an image augmentation module. Added abstraction and concrete implementation for region detectors. A new high-performance default DataLoader : MultiProcessDataLoading . A MultiTaskModel and abstractions to use with it, including Backbone and Head . The MultiTaskModel first runs its inputs through the Backbone , then passes the result (and whatever other relevant inputs it got) to each Head that's in use. A MultiTaskDataLoader , with a corresponding MultiTaskDatasetReader , and a couple of new configuration objects: MultiTaskEpochSampler (for deciding what proportion to sample from each dataset at every epoch) and a MultiTaskScheduler (for ordering the instances within an epoch). Transformer toolkit to plug and play with modular components of transformer architectures. Added a command to count the number of instances we're going to be training with Added a FileLock class to common.file_utils . This is just like the FileLock from the filelock library, except that it adds an optional flag read_only_ok: bool , which when set to True changes the behavior so that a warning will be emitted instead of an exception when lacking write permissions on an existing file lock. This makes it possible to use the FileLock class on a read-only file system. Added a new learning rate scheduler: CombinedLearningRateScheduler . This can be used to combine different LR schedulers, using one after the other. Added an official CUDA 10.1 Docker image. Moving ModelCard and TaskCard abstractions into the main repository. Added a util function allennlp.nn.util.dist_reduce(...) for handling distributed reductions. This is especially useful when implementing a distributed Metric . Added a FileLock class to common.file_utils . This is just like the FileLock from the filelock library, except that it adds an optional flag read_only_ok: bool , which when set to True changes the behavior so that a warning will be emitted instead of an exception when lacking write permissions on an existing file lock. This makes it possible to use the FileLock class on a read-only file system. Added a new learning rate scheduler: CombinedLearningRateScheduler . This can be used to combine different LR schedulers, using one after the other. Moving ModelCard and TaskCard abstractions into the main repository.","title":"Added"},{"location":"CHANGELOG/#changed_13","text":"DatasetReader s are now always lazy. This means there is no lazy parameter in the base class, and the _read() method should always be a generator. The DataLoader now decides whether to load instances lazily or not. With the PyTorchDataLoader this is controlled with the lazy parameter, but with the MultiProcessDataLoading this is controlled by the max_instances_in_memory setting. ArrayField is now called TensorField , and implemented in terms of torch tensors, not numpy. Improved nn.util.move_to_device function by avoiding an unnecessary recursive check for tensors and adding a non_blocking optional argument, which is the same argument as in torch.Tensor.to() . If you are trying to create a heterogeneous batch, you now get a better error message. Readers using the new vision features now explicitly log how they are featurizing images. master_addr and master_port renamed to primary_addr and primary_port , respectively. is_master parameter for training callbacks renamed to is_primary . master branch renamed to main Torch version bumped to 1.7.1 in Docker images. 'master' branch renamed to 'main' Torch version bumped to 1.7.1 in Docker images.","title":"Changed"},{"location":"CHANGELOG/#removed_5","text":"Removed nn.util.has_tensor .","title":"Removed"},{"location":"CHANGELOG/#fixed_14","text":"The build-vocab command no longer crashes when the resulting vocab file is in the current working directory. VQA models now use the vqa_score metric for early stopping. This results in much better scores. Fixed typo with LabelField string representation: removed trailing apostrophe. Vocabulary.from_files and cached_path will issue a warning, instead of failing, when a lock on an existing resource can't be acquired because the file system is read-only. TrackEpochCallback is now a EpochCallback .","title":"Fixed"},{"location":"CHANGELOG/#v130-2020-12-15","text":"","title":"v1.3.0 - 2020-12-15"},{"location":"CHANGELOG/#added_14","text":"Added links to source code in docs. Added get_embedding_layer and get_text_field_embedder to the Predictor class; to specify embedding layers for non-AllenNLP models. Added Gaussian Error Linear Unit (GELU) as an Activation.","title":"Added"},{"location":"CHANGELOG/#changed_14","text":"Renamed module allennlp.data.tokenizers.token to allennlp.data.tokenizers.token_class to avoid this bug . transformers dependency updated to version 4.0.1. BasicClassifier 's forward method now takes a metadata field.","title":"Changed"},{"location":"CHANGELOG/#fixed_15","text":"Fixed a lot of instances where tensors were first created and then sent to a device with .to(device) . Instead, these tensors are now created directly on the target device. Fixed issue with GradientDescentTrainer when constructed with validation_data_loader=None and learning_rate_scheduler!=None . Fixed a bug when removing all handlers in root logger. ShardedDatasetReader now inherits parameters from base_reader when required. Fixed an issue in FromParams where parameters in the params object used to a construct a class were not passed to the constructor if the value of the parameter was equal to the default value. This caused bugs in some edge cases where a subclass that takes **kwargs needs to inspect kwargs before passing them to its superclass. Improved the band-aid solution for segmentation faults and the \"ImportError: dlopen: cannot load any more object with static TLS\" by adding a transformers import. Added safety checks for extracting tar files Turned superfluous warning to info when extending the vocab in the embedding matrix, if no pretrained file was provided","title":"Fixed"},{"location":"CHANGELOG/#v122-2020-11-17","text":"","title":"v1.2.2 - 2020-11-17"},{"location":"CHANGELOG/#added_15","text":"Added Docker builds for other torch-supported versions of CUDA. Adds allennlp-semparse as an official, default plugin.","title":"Added"},{"location":"CHANGELOG/#fixed_16","text":"GumbelSampler now sorts the beams by their true log prob.","title":"Fixed"},{"location":"CHANGELOG/#v121-2020-11-10","text":"","title":"v1.2.1 - 2020-11-10"},{"location":"CHANGELOG/#added_16","text":"Added an optional seed parameter to ModelTestCase.set_up_model which sets the random seed for random , numpy , and torch . Added support for a global plugins file at ~/.allennlp/plugins . Added more documentation about plugins. Added sampler class and parameter in beam search for non-deterministic search, with several implementations, including MultinomialSampler , TopKSampler , TopPSampler , and GumbelSampler . Utilizing GumbelSampler will give Stochastic Beam Search .","title":"Added"},{"location":"CHANGELOG/#changed_15","text":"Pass batch metrics to BatchCallback .","title":"Changed"},{"location":"CHANGELOG/#fixed_17","text":"Fixed a bug where forward hooks were not cleaned up with saliency interpreters if there was an exception. Fixed the computation of saliency maps in the Interpret code when using mismatched indexing. Previously, we would compute gradients from the top of the transformer, after aggregation from wordpieces to tokens, which gives results that are not very informative. Now, we compute gradients with respect to the embedding layer, and aggregate wordpieces to tokens separately. Fixed the heuristics for finding embedding layers in the case of RoBERTa. An update in the transformers library broke our old heuristic. Fixed typo with registered name of ROUGE metric. Previously was rogue , fixed to rouge . Fixed default masks that were erroneously created on the CPU even when a GPU is available. Fixed pretrained embeddings for transformers that don't use end tokens. Fixed the transformer tokenizer cache when the tokenizers are initialized with custom kwargs.","title":"Fixed"},{"location":"CHANGELOG/#v120-2020-10-29","text":"","title":"v1.2.0 - 2020-10-29"},{"location":"CHANGELOG/#changed_16","text":"Enforced stricter typing requirements around the use of Optional[T] types. Changed the behavior of Lazy types in from_params methods. Previously, if you defined a Lazy parameter like foo: Lazy[Foo] = None in a custom from_params classmethod, then foo would actually never be None . This behavior is now different. If no params were given for foo , it will be None . You can also now set default values for foo like foo: Lazy[Foo] = Lazy(Foo) . Or, if you want you want a default value but also want to allow for None values, you can write it like this: foo: Optional[Lazy[Foo]] = Lazy(Foo) . Added support for PyTorch version 1.7.","title":"Changed"},{"location":"CHANGELOG/#fixed_18","text":"Made it possible to instantiate TrainerCallback from config files. Fixed the remaining broken internal links in the API docs. Fixed a bug where Hotflip would crash with a model that had multiple TokenIndexers and the input used rare vocabulary items. Fixed a bug where BeamSearch would fail if max_steps was equal to 1. Fixed BasicTextFieldEmbedder to not raise ConfigurationError if it has embedders that are empty and not in input","title":"Fixed"},{"location":"CHANGELOG/#v120rc1-2020-10-22","text":"","title":"v1.2.0rc1 - 2020-10-22"},{"location":"CHANGELOG/#added_17","text":"Added a warning when batches_per_epoch for the validation data loader is inherited from the train data loader. Added a build-vocab subcommand that can be used to build a vocabulary from a training config file. Added tokenizer_kwargs argument to PretrainedTransformerMismatchedIndexer . Added tokenizer_kwargs and transformer_kwargs arguments to PretrainedTransformerMismatchedEmbedder . Added official support for Python 3.8. Added a script: scripts/release_notes.py , which automatically prepares markdown release notes from the CHANGELOG and commit history. Added a flag --predictions-output-file to the evaluate command, which tells AllenNLP to write the predictions from the given dataset to the file as JSON lines. Added the ability to ignore certain missing keys when loading a model from an archive. This is done by adding a class-level variable called authorized_missing_keys to any PyTorch module that a Model uses. If defined, authorized_missing_keys should be a list of regex string patterns. Added FBetaMultiLabelMeasure , a multi-label Fbeta metric. This is a subclass of the existing FBetaMeasure . Added ability to pass additional key word arguments to cached_transformers.get() , which will be passed on to AutoModel.from_pretrained() . Added an overrides argument to Predictor.from_path() . Added a cached-path command. Added a function inspect_cache to common.file_utils that prints useful information about the cache. This can also be used from the cached-path command with allennlp cached-path --inspect . Added a function remove_cache_entries to common.file_utils that removes any cache entries matching the given glob patterns. This can used from the cached-path command with allennlp cached-path --remove some-files-* . Added logging for the main process when running in distributed mode. Added a TrainerCallback object to support state sharing between batch and epoch-level training callbacks. Added support for .tar.gz in PretrainedModelInitializer. Made BeamSearch instantiable from_params . Pass serialization_dir to Model and DatasetReader . Added an optional include_in_archive parameter to the top-level of configuration files. When specified, include_in_archive should be a list of paths relative to the serialization directory which will be bundled up with the final archived model from a training run.","title":"Added"},{"location":"CHANGELOG/#changed_17","text":"Subcommands that don't require plugins will no longer cause plugins to be loaded or have an --include-package flag. Allow overrides to be JSON string or dict . transformers dependency updated to version 3.1.0. When cached_path is called on a local archive with extract_archive=True , the archive is now extracted into a unique subdirectory of the cache root instead of a subdirectory of the archive's directory. The extraction directory is also unique to the modification time of the archive, so if the file changes, subsequent calls to cached_path will know to re-extract the archive. Removed the truncation_strategy parameter to PretrainedTransformerTokenizer . The way we're calling the tokenizer, the truncation strategy takes no effect anyways. Don't use initializers when loading a model, as it is not needed. Distributed training will now automatically search for a local open port if the master_port parameter is not provided. In training, save model weights before evaluation. allennlp.common.util.peak_memory_mb renamed to peak_cpu_memory , and allennlp.common.util.gpu_memory_mb renamed to peak_gpu_memory , and they both now return the results in bytes as integers. Also, the peak_gpu_memory function now utilizes PyTorch functions to find the memory usage instead of shelling out to the nvidia-smi command. This is more efficient and also more accurate because it only takes into account the tensor allocations of the current PyTorch process. Make sure weights are first loaded to the cpu when using PretrainedModelInitializer, preventing wasted GPU memory. Load dataset readers in load_archive . Updated AllenNlpTestCase docstring to remove reference to unittest.TestCase","title":"Changed"},{"location":"CHANGELOG/#removed_6","text":"Removed common.util.is_master function.","title":"Removed"},{"location":"CHANGELOG/#fixed_19","text":"Fix CUDA/CPU device mismatch bug during distributed training for categorical accuracy metric. Fixed a bug where the reported batch_loss metric was incorrect when training with gradient accumulation. Class decorators now displayed in API docs. Fixed up the documentation for the allennlp.nn.beam_search module. Ignore *args when constructing classes with FromParams . Ensured some consistency in the types of the values that metrics return. Fix a PyTorch warning by explicitly providing the as_tuple argument (leaving it as its default value of False ) to Tensor.nonzero() . Remove temporary directory when extracting model archive in load_archive at end of function rather than via atexit . Fixed a bug where using cached_path() offline could return a cached resource's lock file instead of the cache file. Fixed a bug where cached_path() would fail if passed a cache_dir with the user home shortcut ~/ . Fixed a bug in our doc building script where markdown links did not render properly if the \"href\" part of the link (the part inside the () ) was on a new line. Changed how gradients are zeroed out with an optimization. See this video from NVIDIA at around the 9 minute mark. Fixed a bug where parameters to a FromParams class that are dictionaries wouldn't get logged when an instance is instantiated from_params . Fixed a bug in distributed training where the vocab would be saved from every worker, when it should have been saved by only the local master process. Fixed a bug in the calculation of rouge metrics during distributed training where the total sequence count was not being aggregated across GPUs. Fixed allennlp.nn.util.add_sentence_boundary_token_ids() to use device parameter of input tensor. Be sure to close the TensorBoard writer even when training doesn't finish. Fixed the docstring for PyTorchSeq2VecWrapper . Fixed a bug in the cnn_encoder where activations involving masked tokens could be picked up by the max Fix intra word tokenization for PretrainedTransformerTokenizer when disabling fast tokenizer.","title":"Fixed"},{"location":"CHANGELOG/#v110-2020-09-08","text":"","title":"v1.1.0 - 2020-09-08"},{"location":"CHANGELOG/#fixed_20","text":"Fixed handling of some edge cases when constructing classes with FromParams where the class accepts **kwargs . Fixed division by zero error when there are zero-length spans in the input to a PretrainedTransformerMismatchedIndexer . Improved robustness of cached_path when extracting archives so that the cache won't be corrupted if a failure occurs during extraction. Fixed a bug with the average and evalb_bracketing_score metrics in distributed training.","title":"Fixed"},{"location":"CHANGELOG/#added_18","text":"Predictor.capture_model_internals() now accepts a regex specifying which modules to capture.","title":"Added"},{"location":"CHANGELOG/#v110rc4-2020-08-20","text":"","title":"v1.1.0rc4 - 2020-08-20"},{"location":"CHANGELOG/#added_19","text":"Added a workflow to GitHub Actions that will automatically close unassigned stale issues and ping the assignees of assigned stale issues.","title":"Added"},{"location":"CHANGELOG/#fixed_21","text":"Fixed a bug in distributed metrics that caused nan values due to repeated addition of an accumulated variable.","title":"Fixed"},{"location":"CHANGELOG/#v110rc3-2020-08-12","text":"","title":"v1.1.0rc3 - 2020-08-12"},{"location":"CHANGELOG/#fixed_22","text":"Fixed how truncation was handled with PretrainedTransformerTokenizer . Previously, if max_length was set to None , the tokenizer would still do truncation if the transformer model had a default max length in its config. Also, when max_length was set to a non- None value, several warnings would appear for certain transformer models around the use of the truncation parameter. Fixed evaluation of all metrics when using distributed training. Added a py.typed marker. Fixed type annotations in allennlp.training.util . Fixed problem with automatically detecting whether tokenization is necessary. This affected primarily the Roberta SST model. Improved help text for using the --overrides command line flag.","title":"Fixed"},{"location":"CHANGELOG/#v110rc2-2020-07-31","text":"","title":"v1.1.0rc2 - 2020-07-31"},{"location":"CHANGELOG/#changed_18","text":"Upgraded PyTorch requirement to 1.6. Replaced the NVIDIA Apex AMP module with torch's native AMP module. The default trainer ( GradientDescentTrainer ) now takes a use_amp: bool parameter instead of the old opt_level: str parameter.","title":"Changed"},{"location":"CHANGELOG/#fixed_23","text":"Removed unnecessary warning about deadlocks in DataLoader . Fixed testing models that only return a loss when they are in training mode. Fixed a bug in FromParams that caused silent failure in case of the parameter type being Optional[Union[...]] . Fixed a bug where the program crashes if evaluation_data_loader is a AllennlpLazyDataset .","title":"Fixed"},{"location":"CHANGELOG/#added_20","text":"Added the option to specify requires_grad: false within an optimizer's parameter groups. Added the file-friendly-logging flag back to the train command. Also added this flag to the predict , evaluate , and find-learning-rate commands. Added an EpochCallback to track current epoch as a model class member. Added the option to enable or disable gradient checkpointing for transformer token embedders via boolean parameter gradient_checkpointing .","title":"Added"},{"location":"CHANGELOG/#removed_7","text":"Removed the opt_level parameter to Model.load and load_archive . In order to use AMP with a loaded model now, just run the model's forward pass within torch's autocast context.","title":"Removed"},{"location":"CHANGELOG/#v110rc1-2020-07-14","text":"","title":"v1.1.0rc1 - 2020-07-14"},{"location":"CHANGELOG/#fixed_24","text":"Reduced the amount of log messages produced by allennlp.common.file_utils . Fixed a bug where PretrainedTransformerEmbedder parameters appeared to be trainable in the log output even when train_parameters was set to False . Fixed a bug with the sharded dataset reader where it would only read a fraction of the instances in distributed training. Fixed checking equality of TensorField s. Fixed a bug where NamespaceSwappingField did not work correctly with .empty_field() . Put more sensible defaults on the huggingface_adamw optimizer. Simplified logging so that all logging output always goes to one file. Fixed interaction with the python command line debugger. Log the grad norm properly even when we're not clipping it. Fixed a bug where PretrainedModelInitializer fails to initialize a model with a 0-dim tensor Fixed a bug with the layer unfreezing schedule of the SlantedTriangular learning rate scheduler. Fixed a regression with logging in the distributed setting. Only the main worker should write log output to the terminal. Pinned the version of boto3 for package managers (e.g. poetry). Fixed issue #4330 by updating the tokenizers dependency. Fixed a bug in TextClassificationPredictor so that it passes tokenized inputs to the DatasetReader in case it does not have a tokenizer. reg_loss is only now returned for models that have some regularization penalty configured. Fixed a bug that prevented cached_path from downloading assets from GitHub releases. Fixed a bug that erroneously increased last label's false positive count in calculating fbeta metrics. Tqdm output now looks much better when the output is being piped or redirected. Small improvements to how the API documentation is rendered. Only show validation progress bar from main process in distributed training.","title":"Fixed"},{"location":"CHANGELOG/#added_21","text":"Adjust beam search to support multi-layer decoder. A method to ModelTestCase for running basic model tests when you aren't using config files. Added some convenience methods for reading files. Added an option to file_utils.cached_path to automatically extract archives. Added the ability to pass an archive file instead of a local directory to Vocab.from_files . Added the ability to pass an archive file instead of a glob to ShardedDatasetReader . Added a new \"linear_with_warmup\" learning rate scheduler. Added a check in ShardedDatasetReader that ensures the base reader doesn't implement manual distributed sharding itself. Added an option to PretrainedTransformerEmbedder and PretrainedTransformerMismatchedEmbedder to use a scalar mix of all hidden layers from the transformer model instead of just the last layer. To utilize this, just set last_layer_only to False . cached_path() can now read files inside of archives. Training metrics now include batch_loss and batch_reg_loss in addition to aggregate loss across number of batches.","title":"Added"},{"location":"CHANGELOG/#changed_19","text":"Not specifying a cuda_device now automatically determines whether to use a GPU or not. Discovered plugins are logged so you can see what was loaded. allennlp.data.DataLoader is now an abstract registrable class. The default implementation remains the same, but was renamed to allennlp.data.PyTorchDataLoader . BertPooler can now unwrap and re-wrap extra dimensions if necessary. New transformers dependency. Only version >=3.0 now supported.","title":"Changed"},{"location":"CHANGELOG/#v100-2020-06-16","text":"","title":"v1.0.0 - 2020-06-16"},{"location":"CHANGELOG/#fixed_25","text":"Lazy dataset readers now work correctly with multi-process data loading. Fixed race conditions that could occur when using a dataset cache.","title":"Fixed"},{"location":"CHANGELOG/#added_22","text":"A bug where where all datasets would be loaded for vocab creation even if not needed. A parameter to the DatasetReader class: manual_multi_process_sharding . This is similar to the manual_distributed_sharding parameter, but applies when using a multi-process DataLoader .","title":"Added"},{"location":"CHANGELOG/#v100rc6-2020-06-11","text":"","title":"v1.0.0rc6 - 2020-06-11"},{"location":"CHANGELOG/#fixed_26","text":"A bug where TextField s could not be duplicated since some tokenizers cannot be deep-copied. See https://github.com/allenai/allennlp/issues/4270. Our caching mechanism had the potential to introduce race conditions if multiple processes were attempting to cache the same file at once. This was fixed by using a lock file tied to each cached file. get_text_field_mask() now supports padding indices that are not 0 . A bug where predictor.get_gradients() would return an empty dictionary if an embedding layer had trainable set to false Fixes PretrainedTransformerMismatchedIndexer in the case where a token consists of zero word pieces. Fixes a bug when using a lazy dataset reader that results in a UserWarning from PyTorch being printed at every iteration during training. Predictor names were inconsistently switching between dashes and underscores. Now they all use underscores. Predictor.from_path now automatically loads plugins (unless you specify load_plugins=False ) so that you don't have to manually import a bunch of modules when instantiating predictors from an archive path. allennlp-server automatically found as a plugin once again.","title":"Fixed"},{"location":"CHANGELOG/#added_23","text":"A duplicate() method on Instance s and Field s, to be used instead of copy.deepcopy() A batch sampler that makes sure each batch contains approximately the same number of tokens ( MaxTokensBatchSampler ) Functions to turn a sequence of token indices back into tokens The ability to use Huggingface encoder/decoder models as token embedders Improvements to beam search ROUGE metric Polynomial decay learning rate scheduler A BatchCallback for logging CPU and GPU memory usage to tensorboard. This is mainly for debugging because using it can cause a significant slowdown in training. Ability to run pretrained transformers as an embedder without training the weights Add Optuna Integrated badge to README.md","title":"Added"},{"location":"CHANGELOG/#changed_20","text":"Similar to our caching mechanism, we introduced a lock file to the vocab to avoid race conditions when saving/loading the vocab from/to the same serialization directory in different processes. Changed the Token , Instance , and Batch classes along with all Field classes to \"slots\" classes. This dramatically reduces the size in memory of instances. SimpleTagger will no longer calculate span-based F1 metric when calculate_span_f1 is False . CPU memory for every worker is now reported in the logs and the metrics. Previously this was only reporting the CPU memory of the master process, and so it was only correct in the non-distributed setting. To be consistent with PyTorch IterableDataset , AllennlpLazyDataset no longer implements __len__() . Previously it would always return 1. Removed old tutorials, in favor of the new AllenNLP Guide Changed the vocabulary loading to consider new lines for Windows/Linux and Mac.","title":"Changed"},{"location":"CHANGELOG/#v100rc5-2020-05-26","text":"","title":"v1.0.0rc5 - 2020-05-26"},{"location":"CHANGELOG/#fixed_27","text":"Fix bug where PretrainedTransformerTokenizer crashed with some transformers (#4267) Make cached_path work offline. Tons of docstring inconsistencies resolved. Nightly builds no longer run on forks. Distributed training now automatically figures out which worker should see which instances A race condition bug in distributed training caused from saving the vocab to file from the master process while other processing might be reading those files. Unused dependencies in setup.py removed.","title":"Fixed"},{"location":"CHANGELOG/#added_24","text":"Additional CI checks to ensure docstrings are consistently formatted. Ability to train on CPU with multiple processes by setting cuda_devices to a list of negative integers in your training config. For example: \"distributed\": {\"cuda_devices\": [-1, -1]} . This is mainly to make it easier to test and debug distributed training code.. Documentation for when parameters don't need config file entries.","title":"Added"},{"location":"CHANGELOG/#changed_21","text":"The allennlp test-install command now just ensures the core submodules can be imported successfully, and prints out some other useful information such as the version, PyTorch version, and the number of GPU devices available. All of the tests moved from allennlp/tests to tests at the root level, and allennlp/tests/fixtures moved to test_fixtures at the root level. The PyPI source and wheel distributions will no longer include tests and fixtures.","title":"Changed"},{"location":"CHANGELOG/#v100rc4-2020-05-14","text":"We first introduced this CHANGELOG after release v1.0.0rc4 , so please refer to the GitHub release notes for this and earlier releases.","title":"v1.0.0rc4 - 2020-05-14"},{"location":"CONTRIBUTING/","text":"Contributing \u00b6 Thanks for considering contributing! We want AllenNLP to be the way to do cutting-edge NLP research, but we cannot get there without community support. How Can I Contribute? \u00b6 Bug fixes and new features \u00b6 Did you find a bug? First, do a quick search to see whether your issue has already been reported. If your issue has already been reported, please comment on the existing issue. Otherwise, open a new GitHub issue . Be sure to include a clear title and description. The description should include as much relevant information as possible. The description should explain how to reproduce the erroneous behavior as well as the behavior you expect to see. Ideally you would include a code sample or an executable test case demonstrating the expected behavior. Do you have a suggestion for an enhancement? We use GitHub issues to track enhancement requests. Before you create an enhancement request: Make sure you have a clear idea of the enhancement you would like. If you have a vague idea, consider discussing it first on a GitHub issue. Check the documentation to make sure your feature does not already exist. Do a quick search to see whether your enhancement has already been suggested. When creating your enhancement request, please: Provide a clear title and description. Explain why the enhancement would be useful. It may be helpful to highlight the feature in other libraries. Include code examples to demonstrate how the enhancement would be used. Making a pull request \u00b6 When you're ready to contribute code to address an open issue, please follow these guidelines to help us be able to review your pull request (PR) quickly. Initial setup (only do this once) Expand details \ud83d\udc47 If you haven't already done so, please fork this repository on GitHub. Then clone your fork locally with git clone https://github.com/USERNAME/allennlp.git or git clone git @github . com : USERNAME / allennlp . git At this point the local clone of your fork only knows that it came from your repo, github.com/USERNAME/allennlp.git, but doesn't know anything the main repo, https://github.com/allenai/allennlp.git . You can see this by running git remote -v which will output something like this: origin https://github.com/USERNAME/allennlp.git (fetch) origin https://github.com/USERNAME/allennlp.git (push) This means that your local clone can only track changes from your fork, but not from the main repo, and so you won't be able to keep your fork up-to-date with the main repo over time. Therefor you'll need to add another \"remote\" to your clone that points to https://github.com/allenai/allennlp.git . To do this, run the following: git remote add upstream https://github.com/allenai/allennlp.git Now if you do git remote -v again, you'll see origin https://github.com/USERNAME/allennlp.git (fetch) origin https://github.com/USERNAME/allennlp.git (push) upstream https://github.com/allenai/allennlp.git (fetch) upstream https://github.com/allenai/allennlp.git (push) Finally, you'll need to create a Python 3 virtual environment suitable for working on AllenNLP. There a number of tools out there that making working with virtual environments easier, but the most direct way is with the venv module in the standard library. Once your virtual environment is activated, you can install your local clone in \"editable mode\" with pip install - U pip setuptools wheel pip install - e . [ dev , all ] The \"editable mode\" comes from the -e argument to pip , and essential just creates a symbolic link from the site-packages directory of your virtual environment to the source code in your local clone. That way any changes you make will be immediately reflected in your virtual environment. Ensure your fork is up-to-date Expand details \ud83d\udc47 Once you've added an \"upstream\" remote pointing to https://github.com/allenai/allennlp.git , keeping your fork up-to-date is easy: git checkout main # if not already on main git pull -- rebase upstream main git push Create a new branch to work on your fix or enhancement Expand details \ud83d\udc47 Commiting directly to the main branch of your fork is not recommended. It will be easier to keep your fork clean if you work on a seperate branch for each contribution you intend to make. You can create a new branch with # replace BRANCH with whatever name you want to give it git checkout -b BRANCH git push -u origin BRANCH Test your changes Expand details \ud83d\udc47 Our continuous integration (CI) testing runs a number of checks for each pull request on GitHub Actions . You can run most of these tests locally, which is something you should do before opening a PR to help speed up the review process and make it easier for us. First, you should run black to make sure you code is formatted consistently. Many IDEs support code formatters as plugins, so you may be able to setup black to run automatically everytime you save. black.vim will give you this functionality in Vim, for example. But black is also easy to run directly from the command line. Just run this from the root of your clone: black . Our CI also uses flake8 to lint the code base and mypy for type-checking. You should run both of these next with flake8 . and make typecheck We also strive to maintain high test coverage, so most contributions should include additions to the unit tests . These tests are run with pytest , which you can use to locally run any test modules that you've added or changed. For example, if you've fixed a bug in allennlp/nn/util.py , you can run the tests specific to that module with pytest -v tests/nn/util_test.py Our CI will automatically check that test coverage stays above a certain threshold (around 90%). To check the coverage locally in this example, you could run pytest -v --cov allennlp.nn.util tests/nn/util_test.py If your contribution involves additions to any public part of the API, we require that you write docstrings for each function, method, class, or module that you add. See the Writing docstrings section below for details on the syntax. You should test to make sure the API documentation can build without errors by running make build-docs If the build fails, it's most likely due to small formatting issues. If the error message isn't clear, feel free to comment on this in your pull request. You can also serve and view the docs locally with make serve-docs And finally, please update the CHANGELOG with notes on your contribution in the \"Unreleased\" section at the top. After all of the above checks have passed, you can now open a new GitHub pull request . Make sure you have a clear description of the problem and the solution, and include a link to relevant issues. We look forward to reviewing your PR! Writing docstrings \u00b6 Our docstrings are written in a syntax that is essentially just Markdown with additional special syntax for writing parameter descriptions. Class docstrings should start with a description of the class, followed by a # Parameters section that lists the names, types, and purpose of all parameters to the class's __init__() method. Parameter descriptions should look like: name : `type` Description of the parameter, indented by four spaces. Optional parameters can also be written like this: name : `type`, optional (default = `default_value`) Description of the parameter, indented by four spaces. Sometimes you can omit the description if the parameter is self-explanatory. Method and function docstrings are similar, but should also include a # Returns section when the return value is not obvious. Other valid sections are # Attributes , for listing class attributes. These should be formatted in the same way as parameters. # Raises , for listing any errors that the function or method might intentionally raise. # Examples , where you can include code snippets. Here is an example of what the docstrings should look like in a class: class SentenceClassifier ( Model ): \"\"\" A model for classifying sentences. This is based on [this paper](link-to-paper). The input is a sentence and the output is a score for each target label. # Parameters vocab : `Vocabulary` text_field_embedder : `TextFieldEmbedder` The text field embedder that will be used to create a representation of the source tokens. seq2vec_encoder : `Seq2VeqEncoder` This encoder will take the embeddings from the `text_field_embedder` and encode them into a vector which represents the un-normalized scores for the target labels. dropout : `Optional[float]`, optional (default = `None`) Optional dropout to apply to the text field embeddings before passing through the `seq2vec_encoder`. \"\"\" def __init__ ( self , vocab : Vocabulary , text_field_embedder : TextFieldEmbedder , seq2vec_encoder : Seq2SeqEncoder , dropout : Optional [ float ] = None , ) -> None : pass def forward ( self , tokens : TextFieldTensors , labels : Optional [ Tensor ] = None , ) -> Dict [ str , Tensor ]: \"\"\" Runs a forward pass of the model, computing the predicted logits and also the loss when `labels` is provided. # Parameters tokens : `TextFieldTensors` The tokens corresponding to the source sequence. labels : `Optional[Tensor]`, optional (default = `None`) The target labels. # Returns `Dict[str, Tensor]` An output dictionary with keys for the `loss` and `logits`. \"\"\" pass New models \u00b6 Do you have a new state-of-the-art model? We are always looking for new models to add to our collection. The most popular models are usually added to the official AllenNLP Models repository, and in some cases to the AllenNLP Demo . If you think your model should be part of AllenNLP Models, please create a pull request in the models repo that includes: Any code changes needed to support your new model. A link to the model itself. Please do not check your model into the GitHub repository, but instead upload it in the PR conversation or provide a link to it at an external location. In the description of your PR, please clearly explain the task your model performs along with the relevant metrics on an established dataset.","title":"Contributing"},{"location":"CONTRIBUTING/#contributing","text":"Thanks for considering contributing! We want AllenNLP to be the way to do cutting-edge NLP research, but we cannot get there without community support.","title":"Contributing"},{"location":"CONTRIBUTING/#how-can-i-contribute","text":"","title":"How Can I Contribute?"},{"location":"CONTRIBUTING/#bug-fixes-and-new-features","text":"Did you find a bug? First, do a quick search to see whether your issue has already been reported. If your issue has already been reported, please comment on the existing issue. Otherwise, open a new GitHub issue . Be sure to include a clear title and description. The description should include as much relevant information as possible. The description should explain how to reproduce the erroneous behavior as well as the behavior you expect to see. Ideally you would include a code sample or an executable test case demonstrating the expected behavior. Do you have a suggestion for an enhancement? We use GitHub issues to track enhancement requests. Before you create an enhancement request: Make sure you have a clear idea of the enhancement you would like. If you have a vague idea, consider discussing it first on a GitHub issue. Check the documentation to make sure your feature does not already exist. Do a quick search to see whether your enhancement has already been suggested. When creating your enhancement request, please: Provide a clear title and description. Explain why the enhancement would be useful. It may be helpful to highlight the feature in other libraries. Include code examples to demonstrate how the enhancement would be used.","title":"Bug fixes and new features"},{"location":"CONTRIBUTING/#making-a-pull-request","text":"When you're ready to contribute code to address an open issue, please follow these guidelines to help us be able to review your pull request (PR) quickly. Initial setup (only do this once) Expand details \ud83d\udc47 If you haven't already done so, please fork this repository on GitHub. Then clone your fork locally with git clone https://github.com/USERNAME/allennlp.git or git clone git @github . com : USERNAME / allennlp . git At this point the local clone of your fork only knows that it came from your repo, github.com/USERNAME/allennlp.git, but doesn't know anything the main repo, https://github.com/allenai/allennlp.git . You can see this by running git remote -v which will output something like this: origin https://github.com/USERNAME/allennlp.git (fetch) origin https://github.com/USERNAME/allennlp.git (push) This means that your local clone can only track changes from your fork, but not from the main repo, and so you won't be able to keep your fork up-to-date with the main repo over time. Therefor you'll need to add another \"remote\" to your clone that points to https://github.com/allenai/allennlp.git . To do this, run the following: git remote add upstream https://github.com/allenai/allennlp.git Now if you do git remote -v again, you'll see origin https://github.com/USERNAME/allennlp.git (fetch) origin https://github.com/USERNAME/allennlp.git (push) upstream https://github.com/allenai/allennlp.git (fetch) upstream https://github.com/allenai/allennlp.git (push) Finally, you'll need to create a Python 3 virtual environment suitable for working on AllenNLP. There a number of tools out there that making working with virtual environments easier, but the most direct way is with the venv module in the standard library. Once your virtual environment is activated, you can install your local clone in \"editable mode\" with pip install - U pip setuptools wheel pip install - e . [ dev , all ] The \"editable mode\" comes from the -e argument to pip , and essential just creates a symbolic link from the site-packages directory of your virtual environment to the source code in your local clone. That way any changes you make will be immediately reflected in your virtual environment. Ensure your fork is up-to-date Expand details \ud83d\udc47 Once you've added an \"upstream\" remote pointing to https://github.com/allenai/allennlp.git , keeping your fork up-to-date is easy: git checkout main # if not already on main git pull -- rebase upstream main git push Create a new branch to work on your fix or enhancement Expand details \ud83d\udc47 Commiting directly to the main branch of your fork is not recommended. It will be easier to keep your fork clean if you work on a seperate branch for each contribution you intend to make. You can create a new branch with # replace BRANCH with whatever name you want to give it git checkout -b BRANCH git push -u origin BRANCH Test your changes Expand details \ud83d\udc47 Our continuous integration (CI) testing runs a number of checks for each pull request on GitHub Actions . You can run most of these tests locally, which is something you should do before opening a PR to help speed up the review process and make it easier for us. First, you should run black to make sure you code is formatted consistently. Many IDEs support code formatters as plugins, so you may be able to setup black to run automatically everytime you save. black.vim will give you this functionality in Vim, for example. But black is also easy to run directly from the command line. Just run this from the root of your clone: black . Our CI also uses flake8 to lint the code base and mypy for type-checking. You should run both of these next with flake8 . and make typecheck We also strive to maintain high test coverage, so most contributions should include additions to the unit tests . These tests are run with pytest , which you can use to locally run any test modules that you've added or changed. For example, if you've fixed a bug in allennlp/nn/util.py , you can run the tests specific to that module with pytest -v tests/nn/util_test.py Our CI will automatically check that test coverage stays above a certain threshold (around 90%). To check the coverage locally in this example, you could run pytest -v --cov allennlp.nn.util tests/nn/util_test.py If your contribution involves additions to any public part of the API, we require that you write docstrings for each function, method, class, or module that you add. See the Writing docstrings section below for details on the syntax. You should test to make sure the API documentation can build without errors by running make build-docs If the build fails, it's most likely due to small formatting issues. If the error message isn't clear, feel free to comment on this in your pull request. You can also serve and view the docs locally with make serve-docs And finally, please update the CHANGELOG with notes on your contribution in the \"Unreleased\" section at the top. After all of the above checks have passed, you can now open a new GitHub pull request . Make sure you have a clear description of the problem and the solution, and include a link to relevant issues. We look forward to reviewing your PR!","title":"Making a pull request"},{"location":"CONTRIBUTING/#writing-docstrings","text":"Our docstrings are written in a syntax that is essentially just Markdown with additional special syntax for writing parameter descriptions. Class docstrings should start with a description of the class, followed by a # Parameters section that lists the names, types, and purpose of all parameters to the class's __init__() method. Parameter descriptions should look like: name : `type` Description of the parameter, indented by four spaces. Optional parameters can also be written like this: name : `type`, optional (default = `default_value`) Description of the parameter, indented by four spaces. Sometimes you can omit the description if the parameter is self-explanatory. Method and function docstrings are similar, but should also include a # Returns section when the return value is not obvious. Other valid sections are # Attributes , for listing class attributes. These should be formatted in the same way as parameters. # Raises , for listing any errors that the function or method might intentionally raise. # Examples , where you can include code snippets. Here is an example of what the docstrings should look like in a class: class SentenceClassifier ( Model ): \"\"\" A model for classifying sentences. This is based on [this paper](link-to-paper). The input is a sentence and the output is a score for each target label. # Parameters vocab : `Vocabulary` text_field_embedder : `TextFieldEmbedder` The text field embedder that will be used to create a representation of the source tokens. seq2vec_encoder : `Seq2VeqEncoder` This encoder will take the embeddings from the `text_field_embedder` and encode them into a vector which represents the un-normalized scores for the target labels. dropout : `Optional[float]`, optional (default = `None`) Optional dropout to apply to the text field embeddings before passing through the `seq2vec_encoder`. \"\"\" def __init__ ( self , vocab : Vocabulary , text_field_embedder : TextFieldEmbedder , seq2vec_encoder : Seq2SeqEncoder , dropout : Optional [ float ] = None , ) -> None : pass def forward ( self , tokens : TextFieldTensors , labels : Optional [ Tensor ] = None , ) -> Dict [ str , Tensor ]: \"\"\" Runs a forward pass of the model, computing the predicted logits and also the loss when `labels` is provided. # Parameters tokens : `TextFieldTensors` The tokens corresponding to the source sequence. labels : `Optional[Tensor]`, optional (default = `None`) The target labels. # Returns `Dict[str, Tensor]` An output dictionary with keys for the `loss` and `logits`. \"\"\" pass","title":"Writing docstrings"},{"location":"CONTRIBUTING/#new-models","text":"Do you have a new state-of-the-art model? We are always looking for new models to add to our collection. The most popular models are usually added to the official AllenNLP Models repository, and in some cases to the AllenNLP Demo . If you think your model should be part of AllenNLP Models, please create a pull request in the models repo that includes: Any code changes needed to support your new model. A link to the model itself. Please do not check your model into the GitHub repository, but instead upload it in the PR conversation or provide a link to it at an external location. In the description of your PR, please clearly explain the task your model performs along with the relevant metrics on an established dataset.","title":"New models"},{"location":"api/commands/_checklist_internal/","text":"allennlp .commands ._checklist_internal [SOURCE] The checklist subcommand allows you to conduct behavioural testing for your model's predictions using a trained model and its Predictor wrapper. CheckList \u00b6 @Subcommand . register ( \"checklist\" ) class CheckList ( Subcommand ) add_subparser \u00b6 class CheckList ( Subcommand ): | ... | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser","title":"_checklist_internal"},{"location":"api/commands/_checklist_internal/#checklist","text":"@Subcommand . register ( \"checklist\" ) class CheckList ( Subcommand )","title":"CheckList"},{"location":"api/commands/_checklist_internal/#add_subparser","text":"class CheckList ( Subcommand ): | ... | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser","title":"add_subparser"},{"location":"api/commands/build_vocab/","text":"allennlp .commands .build_vocab [SOURCE] Subcommand for building a vocabulary from a training config. BuildVocab \u00b6 @Subcommand . register ( \"build-vocab\" ) class BuildVocab ( Subcommand ) add_subparser \u00b6 class BuildVocab ( Subcommand ): | ... | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser build_vocab_from_args \u00b6 def build_vocab_from_args ( args : argparse . Namespace )","title":"build_vocab"},{"location":"api/commands/build_vocab/#buildvocab","text":"@Subcommand . register ( \"build-vocab\" ) class BuildVocab ( Subcommand )","title":"BuildVocab"},{"location":"api/commands/build_vocab/#add_subparser","text":"class BuildVocab ( Subcommand ): | ... | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser","title":"add_subparser"},{"location":"api/commands/build_vocab/#build_vocab_from_args","text":"def build_vocab_from_args ( args : argparse . Namespace )","title":"build_vocab_from_args"},{"location":"api/commands/cached_path/","text":"allennlp .commands .cached_path [SOURCE] CLI to the the caching mechanism in common.file_utils . CachedPath \u00b6 @Subcommand . register ( \"cached-path\" ) class CachedPath ( Subcommand ) requires_plugins \u00b6 class CachedPath ( Subcommand ): | ... | requires_plugins : bool = False add_subparser \u00b6 class CachedPath ( Subcommand ): | ... | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser","title":"cached_path"},{"location":"api/commands/cached_path/#cachedpath","text":"@Subcommand . register ( \"cached-path\" ) class CachedPath ( Subcommand )","title":"CachedPath"},{"location":"api/commands/cached_path/#requires_plugins","text":"class CachedPath ( Subcommand ): | ... | requires_plugins : bool = False","title":"requires_plugins"},{"location":"api/commands/cached_path/#add_subparser","text":"class CachedPath ( Subcommand ): | ... | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser","title":"add_subparser"},{"location":"api/commands/checklist/","text":"allennlp .commands .checklist [SOURCE] The checklist subcommand allows you to conduct behavioural testing for your model's predictions using a trained model and its Predictor wrapper. It is based on the optional checklist package; if this is not available, the command will be replaced by a dummy.","title":"checklist"},{"location":"api/commands/count_instances/","text":"allennlp .commands .count_instances [SOURCE] Subcommand for counting the number of instances from a training config. CountInstances \u00b6 @Subcommand . register ( \"count-instances\" ) class CountInstances ( Subcommand ) add_subparser \u00b6 class CountInstances ( Subcommand ): | ... | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser count_instances_from_args \u00b6 def count_instances_from_args ( args : argparse . Namespace )","title":"count_instances"},{"location":"api/commands/count_instances/#countinstances","text":"@Subcommand . register ( \"count-instances\" ) class CountInstances ( Subcommand )","title":"CountInstances"},{"location":"api/commands/count_instances/#add_subparser","text":"class CountInstances ( Subcommand ): | ... | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser","title":"add_subparser"},{"location":"api/commands/count_instances/#count_instances_from_args","text":"def count_instances_from_args ( args : argparse . Namespace )","title":"count_instances_from_args"},{"location":"api/commands/diff/","text":"allennlp .commands .diff [SOURCE] Examples \u00b6 allennlp diff \\ hf://roberta-large/pytorch_model.bin \\ https://storage.googleapis.com/allennlp-public-models/transformer-qa-2020-10-03.tar.gz \\ --strip-prefix-1 'roberta.' \\ --strip-prefix-2 '_text_field_embedder.token_embedder_tokens.transformer_model.' Diff \u00b6 @Subcommand . register ( \"diff\" ) class Diff ( Subcommand ) requires_plugins \u00b6 class Diff ( Subcommand ): | ... | requires_plugins : bool = False add_subparser \u00b6 class Diff ( Subcommand ): | ... | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser Keep \u00b6 class Keep ( NamedTuple ) key \u00b6 class Keep ( NamedTuple ): | ... | key : str = None shape \u00b6 class Keep ( NamedTuple ): | ... | shape : Tuple [ int , ... ] = None display \u00b6 class Keep ( NamedTuple ): | ... | def display ( self ) Insert \u00b6 class Insert ( NamedTuple ) key \u00b6 class Insert ( NamedTuple ): | ... | key : str = None shape \u00b6 class Insert ( NamedTuple ): | ... | shape : Tuple [ int , ... ] = None display \u00b6 class Insert ( NamedTuple ): | ... | def display ( self ) Remove \u00b6 class Remove ( NamedTuple ) key \u00b6 class Remove ( NamedTuple ): | ... | key : str = None shape \u00b6 class Remove ( NamedTuple ): | ... | shape : Tuple [ int , ... ] = None display \u00b6 class Remove ( NamedTuple ): | ... | def display ( self ) Modify \u00b6 class Modify ( NamedTuple ) key \u00b6 class Modify ( NamedTuple ): | ... | key : str = None shape \u00b6 class Modify ( NamedTuple ): | ... | shape : Tuple [ int , ... ] = None distance \u00b6 class Modify ( NamedTuple ): | ... | distance : float = None display \u00b6 class Modify ( NamedTuple ): | ... | def display ( self ) checkpoint_diff \u00b6 def checkpoint_diff ( state_dict_a : Dict [ str , torch . Tensor ], state_dict_b : Dict [ str , torch . Tensor ], scale : float , threshold : float ) -> List [ Union [ Keep , Insert , Remove , Modify ]] Uses a modified version of the Myers diff algorithm to compute a representation of the diff between two model state dictionaries. The only difference is that in addition to the Keep , Insert , and Remove operations, we add Modify . This corresponds to keeping a parameter but changing its weights (not the shape). Adapted from this gist .","title":"diff"},{"location":"api/commands/diff/#diff","text":"@Subcommand . register ( \"diff\" ) class Diff ( Subcommand )","title":"Diff"},{"location":"api/commands/diff/#requires_plugins","text":"class Diff ( Subcommand ): | ... | requires_plugins : bool = False","title":"requires_plugins"},{"location":"api/commands/diff/#add_subparser","text":"class Diff ( Subcommand ): | ... | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser","title":"add_subparser"},{"location":"api/commands/diff/#keep","text":"class Keep ( NamedTuple )","title":"Keep"},{"location":"api/commands/diff/#key","text":"class Keep ( NamedTuple ): | ... | key : str = None","title":"key"},{"location":"api/commands/diff/#shape","text":"class Keep ( NamedTuple ): | ... | shape : Tuple [ int , ... ] = None","title":"shape"},{"location":"api/commands/diff/#display","text":"class Keep ( NamedTuple ): | ... | def display ( self )","title":"display"},{"location":"api/commands/diff/#insert","text":"class Insert ( NamedTuple )","title":"Insert"},{"location":"api/commands/diff/#key_1","text":"class Insert ( NamedTuple ): | ... | key : str = None","title":"key"},{"location":"api/commands/diff/#shape_1","text":"class Insert ( NamedTuple ): | ... | shape : Tuple [ int , ... ] = None","title":"shape"},{"location":"api/commands/diff/#display_1","text":"class Insert ( NamedTuple ): | ... | def display ( self )","title":"display"},{"location":"api/commands/diff/#remove","text":"class Remove ( NamedTuple )","title":"Remove"},{"location":"api/commands/diff/#key_2","text":"class Remove ( NamedTuple ): | ... | key : str = None","title":"key"},{"location":"api/commands/diff/#shape_2","text":"class Remove ( NamedTuple ): | ... | shape : Tuple [ int , ... ] = None","title":"shape"},{"location":"api/commands/diff/#display_2","text":"class Remove ( NamedTuple ): | ... | def display ( self )","title":"display"},{"location":"api/commands/diff/#modify","text":"class Modify ( NamedTuple )","title":"Modify"},{"location":"api/commands/diff/#key_3","text":"class Modify ( NamedTuple ): | ... | key : str = None","title":"key"},{"location":"api/commands/diff/#shape_3","text":"class Modify ( NamedTuple ): | ... | shape : Tuple [ int , ... ] = None","title":"shape"},{"location":"api/commands/diff/#distance","text":"class Modify ( NamedTuple ): | ... | distance : float = None","title":"distance"},{"location":"api/commands/diff/#display_3","text":"class Modify ( NamedTuple ): | ... | def display ( self )","title":"display"},{"location":"api/commands/diff/#checkpoint_diff","text":"def checkpoint_diff ( state_dict_a : Dict [ str , torch . Tensor ], state_dict_b : Dict [ str , torch . Tensor ], scale : float , threshold : float ) -> List [ Union [ Keep , Insert , Remove , Modify ]] Uses a modified version of the Myers diff algorithm to compute a representation of the diff between two model state dictionaries. The only difference is that in addition to the Keep , Insert , and Remove operations, we add Modify . This corresponds to keeping a parameter but changing its weights (not the shape). Adapted from this gist .","title":"checkpoint_diff"},{"location":"api/commands/evaluate/","text":"allennlp .commands .evaluate [SOURCE] The evaluate subcommand can be used to evaluate a trained model against a dataset and report any metrics calculated by the model. Evaluate \u00b6 @Subcommand . register ( \"evaluate\" ) class Evaluate ( Subcommand ) add_subparser \u00b6 class Evaluate ( Subcommand ): | ... | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser evaluate_from_args \u00b6 def evaluate_from_args ( args : argparse . Namespace ) -> Dict [ str , Any ] evaluate_from_archive \u00b6 def evaluate_from_archive ( archive_file : Union [ str , PathLike ], input_file : str , metrics_output_file : Optional [ str ] = None , predictions_output_file : Optional [ str ] = None , batch_size : Optional [ int ] = None , cmd_overrides : Union [ str , Dict [ str , Any ]] = \"\" , cuda_device : int = - 1 , embedding_sources_mapping : str = None , extend_vocab : bool = False , weights_file : str = None , file_friendly_logging : bool = False , batch_weight_key : str = None , auto_names : str = \"NONE\" ) -> Dict [ str , Any ] Parameters \u00b6 archive_file : Union[str, PathLike] Path to an archived trained model. input_file : str path to the file containing the evaluation data (for multiple files, put \":\" between filenames e.g., input1.txt:input2.txt) metrics_output_file : str , optional (default = None ) optional path to write the metrics to as JSON (for multiple files, put \":\" between filenames e.g., output1.txt:output2.txt) predictions_output_file : str , optional (default = None ) \"optional path to write the predictions to (for multiple files, put \":\" between filenames e.g., output1.jsonl:output2.jsonl) batch_size : int , optional (default = None ) If non-empty, the batch size to use during evaluation. cmd_overrides : str , optional (default = \"\" ) a json(net) structure used to override the experiment configuration, e.g., '{\\\"iterator.batch_size\\\": 16}'. Nested parameters can be specified either with nested dictionaries or with dot syntax. cuda_device : int , optional (default = -1 ) id of GPU to use (if any) embedding_sources_mapping : str , optional (default = None ) a JSON dict defining mapping from embedding module path to embedding pretrained-file used during training. If not passed, and embedding needs to be extended, we will try to use the original file paths used during training. If they are not available we will use random vectors for embedding extension. extend_vocab : bool , optional (default = False ) if specified, we will use the instances in your new dataset to extend your vocabulary. If pretrained-file was used to initialize embedding layers, you may also need to pass --embedding-sources-mapping. weights_file : str , optional (default = None ) A path that overrides which weights file to use file_friendly_logging : bool , optional (default = False ) If True , we add newlines to tqdm output, even on an interactive terminal, and we slow down tqdm's output to only once every 10 seconds. batch_weight_key : str , optional (default = None ) If non-empty, name of metric used to weight the loss on a per-batch basis. auto_names : str , optional (default = \"NONE\" ) Automatically create output names for each evaluation file. NONE will not automatically generate a file name for the neither the metrics nor the predictions. In this case you will need to pas in both metrics_output_file and predictions_output_file . METRICS will only automatically create a file name for the metrics file. PREDS will only automatically create a file name for the predictions outputs. ALL will create a filename for both the metrics and the predictions. Returns \u00b6 all_metrics : Dict[str, Any] The metrics from every evaluation file passed.","title":"evaluate"},{"location":"api/commands/evaluate/#evaluate","text":"@Subcommand . register ( \"evaluate\" ) class Evaluate ( Subcommand )","title":"Evaluate"},{"location":"api/commands/evaluate/#add_subparser","text":"class Evaluate ( Subcommand ): | ... | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser","title":"add_subparser"},{"location":"api/commands/evaluate/#evaluate_from_args","text":"def evaluate_from_args ( args : argparse . Namespace ) -> Dict [ str , Any ]","title":"evaluate_from_args"},{"location":"api/commands/evaluate/#evaluate_from_archive","text":"def evaluate_from_archive ( archive_file : Union [ str , PathLike ], input_file : str , metrics_output_file : Optional [ str ] = None , predictions_output_file : Optional [ str ] = None , batch_size : Optional [ int ] = None , cmd_overrides : Union [ str , Dict [ str , Any ]] = \"\" , cuda_device : int = - 1 , embedding_sources_mapping : str = None , extend_vocab : bool = False , weights_file : str = None , file_friendly_logging : bool = False , batch_weight_key : str = None , auto_names : str = \"NONE\" ) -> Dict [ str , Any ]","title":"evaluate_from_archive"},{"location":"api/commands/find_learning_rate/","text":"allennlp .commands .find_learning_rate [SOURCE] The find-lr subcommand can be used to find a good learning rate for a model. It requires a configuration file and a directory in which to write the results. FindLearningRate \u00b6 @Subcommand . register ( \"find-lr\" ) class FindLearningRate ( Subcommand ) add_subparser \u00b6 class FindLearningRate ( Subcommand ): | ... | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser find_learning_rate_from_args \u00b6 def find_learning_rate_from_args ( args : argparse . Namespace ) -> None Start learning rate finder for given args find_learning_rate_model \u00b6 def find_learning_rate_model ( params : Params , serialization_dir : str , start_lr : float = 1e-5 , end_lr : float = 10 , num_batches : int = 100 , linear_steps : bool = False , stopping_factor : float = None , force : bool = False ) -> None Runs learning rate search for given num_batches and saves the results in serialization_dir Parameters \u00b6 params : Params A parameter object specifying an AllenNLP Experiment. serialization_dir : str The directory in which to save results. start_lr : float Learning rate to start the search. end_lr : float Learning rate upto which search is done. num_batches : int Number of mini-batches to run Learning rate finder. linear_steps : bool Increase learning rate linearly if False exponentially. stopping_factor : float Stop the search when the current loss exceeds the best loss recorded by multiple of stopping factor. If None search proceeds till the end_lr force : bool If True and the serialization directory already exists, everything in it will be removed prior to finding the learning rate. search_learning_rate \u00b6 def search_learning_rate ( trainer : GradientDescentTrainer , start_lr : float = 1e-5 , end_lr : float = 10 , num_batches : int = 100 , linear_steps : bool = False , stopping_factor : float = None ) -> Tuple [ List [ float ], List [ float ]] Runs training loop on the model using GradientDescentTrainer increasing learning rate from start_lr to end_lr recording the losses. Parameters \u00b6 trainer : GradientDescentTrainer start_lr : float The learning rate to start the search. end_lr : float The learning rate upto which search is done. num_batches : int Number of batches to run the learning rate finder. linear_steps : bool Increase learning rate linearly if False exponentially. stopping_factor : float Stop the search when the current loss exceeds the best loss recorded by multiple of stopping factor. If None search proceeds till the end_lr Returns \u00b6 (learning_rates, losses) : Tuple[List[float], List[float]] Returns list of learning rates and corresponding losses. Note: The losses are recorded before applying the corresponding learning rate","title":"find_learning_rate"},{"location":"api/commands/find_learning_rate/#findlearningrate","text":"@Subcommand . register ( \"find-lr\" ) class FindLearningRate ( Subcommand )","title":"FindLearningRate"},{"location":"api/commands/find_learning_rate/#add_subparser","text":"class FindLearningRate ( Subcommand ): | ... | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser","title":"add_subparser"},{"location":"api/commands/find_learning_rate/#find_learning_rate_from_args","text":"def find_learning_rate_from_args ( args : argparse . Namespace ) -> None Start learning rate finder for given args","title":"find_learning_rate_from_args"},{"location":"api/commands/find_learning_rate/#find_learning_rate_model","text":"def find_learning_rate_model ( params : Params , serialization_dir : str , start_lr : float = 1e-5 , end_lr : float = 10 , num_batches : int = 100 , linear_steps : bool = False , stopping_factor : float = None , force : bool = False ) -> None Runs learning rate search for given num_batches and saves the results in serialization_dir","title":"find_learning_rate_model"},{"location":"api/commands/find_learning_rate/#search_learning_rate","text":"def search_learning_rate ( trainer : GradientDescentTrainer , start_lr : float = 1e-5 , end_lr : float = 10 , num_batches : int = 100 , linear_steps : bool = False , stopping_factor : float = None ) -> Tuple [ List [ float ], List [ float ]] Runs training loop on the model using GradientDescentTrainer increasing learning rate from start_lr to end_lr recording the losses.","title":"search_learning_rate"},{"location":"api/commands/predict/","text":"allennlp .commands .predict [SOURCE] The predict subcommand allows you to make bulk JSON-to-JSON or dataset to JSON predictions using a trained model and its Predictor wrapper. Predict \u00b6 @Subcommand . register ( \"predict\" ) class Predict ( Subcommand ) add_subparser \u00b6 class Predict ( Subcommand ): | ... | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser","title":"predict"},{"location":"api/commands/predict/#predict","text":"@Subcommand . register ( \"predict\" ) class Predict ( Subcommand )","title":"Predict"},{"location":"api/commands/predict/#add_subparser","text":"class Predict ( Subcommand ): | ... | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser","title":"add_subparser"},{"location":"api/commands/print_results/","text":"allennlp .commands .print_results [SOURCE] The print-results subcommand allows you to print results from multiple allennlp serialization directories to the console in a helpful csv format. PrintResults \u00b6 @Subcommand . register ( \"print-results\" ) class PrintResults ( Subcommand ) requires_plugins \u00b6 class PrintResults ( Subcommand ): | ... | requires_plugins : bool = False add_subparser \u00b6 class PrintResults ( Subcommand ): | ... | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser print_results_from_args \u00b6 def print_results_from_args ( args : argparse . Namespace ) Prints results from an argparse.Namespace object.","title":"print_results"},{"location":"api/commands/print_results/#printresults","text":"@Subcommand . register ( \"print-results\" ) class PrintResults ( Subcommand )","title":"PrintResults"},{"location":"api/commands/print_results/#requires_plugins","text":"class PrintResults ( Subcommand ): | ... | requires_plugins : bool = False","title":"requires_plugins"},{"location":"api/commands/print_results/#add_subparser","text":"class PrintResults ( Subcommand ): | ... | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser","title":"add_subparser"},{"location":"api/commands/print_results/#print_results_from_args","text":"def print_results_from_args ( args : argparse . Namespace ) Prints results from an argparse.Namespace object.","title":"print_results_from_args"},{"location":"api/commands/push_to_hf/","text":"allennlp .commands .push_to_hf [SOURCE] The push-to-hf subcommand can be used to push a trained model to the Hugging Face Hub ( hf.co ). PushToHf \u00b6 @Subcommand . register ( \"push-to-hf\" ) class PushToHf ( Subcommand ) add_subparser \u00b6 class PushToHf ( Subcommand ): | ... | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser push \u00b6 def push ( args : argparse . Namespace )","title":"push_to_hf"},{"location":"api/commands/push_to_hf/#pushtohf","text":"@Subcommand . register ( \"push-to-hf\" ) class PushToHf ( Subcommand )","title":"PushToHf"},{"location":"api/commands/push_to_hf/#add_subparser","text":"class PushToHf ( Subcommand ): | ... | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser","title":"add_subparser"},{"location":"api/commands/push_to_hf/#push","text":"def push ( args : argparse . Namespace )","title":"push"},{"location":"api/commands/subcommand/","text":"allennlp .commands .subcommand [SOURCE] Base class for subcommands under allennlp.run . T \u00b6 T = TypeVar ( \"T\" , bound = \"Subcommand\" ) Subcommand \u00b6 class Subcommand ( Registrable ) An abstract class representing subcommands for allennlp.run. If you wanted to (for example) create your own custom special-evaluate command to use like allennlp special-evaluate ... you would create a Subcommand subclass and then pass it as an override to main . requires_plugins \u00b6 class Subcommand ( Registrable ): | ... | requires_plugins : bool = True If True , the sub-command will trigger a call to import_plugins() (except for custom subcommands which come from plugins, since plugins will already have been imported by the time the subcommand is discovered), and will also have an additional --include-package flag. add_subparser \u00b6 class Subcommand ( Registrable ): | ... | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser register \u00b6 class Subcommand ( Registrable ): | ... | @classmethod | def register ( | cls : Type [ T ], | name : str , | constructor : Optional [ str ] = None , | exist_ok : bool = False | ) -> Callable [[ Type [ T ]], Type [ T ]] name \u00b6 class Subcommand ( Registrable ): | ... | @property | def name ( self ) -> str","title":"subcommand"},{"location":"api/commands/subcommand/#t","text":"T = TypeVar ( \"T\" , bound = \"Subcommand\" )","title":"T"},{"location":"api/commands/subcommand/#subcommand","text":"class Subcommand ( Registrable ) An abstract class representing subcommands for allennlp.run. If you wanted to (for example) create your own custom special-evaluate command to use like allennlp special-evaluate ... you would create a Subcommand subclass and then pass it as an override to main .","title":"Subcommand"},{"location":"api/commands/subcommand/#requires_plugins","text":"class Subcommand ( Registrable ): | ... | requires_plugins : bool = True If True , the sub-command will trigger a call to import_plugins() (except for custom subcommands which come from plugins, since plugins will already have been imported by the time the subcommand is discovered), and will also have an additional --include-package flag.","title":"requires_plugins"},{"location":"api/commands/subcommand/#add_subparser","text":"class Subcommand ( Registrable ): | ... | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser","title":"add_subparser"},{"location":"api/commands/subcommand/#register","text":"class Subcommand ( Registrable ): | ... | @classmethod | def register ( | cls : Type [ T ], | name : str , | constructor : Optional [ str ] = None , | exist_ok : bool = False | ) -> Callable [[ Type [ T ]], Type [ T ]]","title":"register"},{"location":"api/commands/subcommand/#name","text":"class Subcommand ( Registrable ): | ... | @property | def name ( self ) -> str","title":"name"},{"location":"api/commands/test_install/","text":"allennlp .commands .test_install [SOURCE] The test-install subcommand provides a programmatic way to verify that AllenNLP has been successfully installed. TestInstall \u00b6 @Subcommand . register ( \"test-install\" ) class TestInstall ( Subcommand ) add_subparser \u00b6 class TestInstall ( Subcommand ): | ... | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser","title":"test_install"},{"location":"api/commands/test_install/#testinstall","text":"@Subcommand . register ( \"test-install\" ) class TestInstall ( Subcommand )","title":"TestInstall"},{"location":"api/commands/test_install/#add_subparser","text":"class TestInstall ( Subcommand ): | ... | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser","title":"add_subparser"},{"location":"api/commands/train/","text":"allennlp .commands .train [SOURCE] The train subcommand can be used to train a model. It requires a configuration file and a directory in which to write the results. Train \u00b6 @Subcommand . register ( \"train\" ) class Train ( Subcommand ) add_subparser \u00b6 class Train ( Subcommand ): | ... | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser train_model_from_args \u00b6 def train_model_from_args ( args : argparse . Namespace ) Just converts from an argparse.Namespace object to string paths. train_model_from_file \u00b6 def train_model_from_file ( parameter_filename : Union [ str , PathLike ], serialization_dir : Union [ str , PathLike ], overrides : Union [ str , Dict [ str , Any ]] = \"\" , recover : bool = False , force : bool = False , node_rank : int = 0 , include_package : List [ str ] = None , dry_run : bool = False , file_friendly_logging : bool = False , return_model : Optional [ bool ] = None ) -> Optional [ Model ] A wrapper around train_model which loads the params from a file. Parameters \u00b6 parameter_filename : str A json parameter file specifying an AllenNLP experiment. serialization_dir : str The directory in which to save results and logs. We just pass this along to train_model . overrides : Union[str, Dict[str, Any]] , optional (default = \"\" ) A JSON string or a dict that we will use to override values in the input parameter file. recover : bool , optional (default = False ) If True , we will try to recover a training run from an existing serialization directory. This is only intended for use when something actually crashed during the middle of a run. For continuing training a model on new data, see Model.from_archive . force : bool , optional (default = False ) If True , we will overwrite the serialization directory if it already exists. node_rank : int , optional Rank of the current node in distributed training include_package : str , optional In distributed mode, extra packages mentioned will be imported in trainer workers. dry_run : bool , optional (default = False ) Do not train a model, but create a vocabulary, show dataset statistics and other training information. file_friendly_logging : bool , optional (default = False ) If True , we add newlines to tqdm output, even on an interactive terminal, and we slow down tqdm's output to only once every 10 seconds. return_model : Optional[bool] , optional (default = None ) Whether or not to return the final model. If not specified, this defaults to False for distributed training and True otherwise. Returns \u00b6 best_model : Optional[str] The path to the archived model with the best weights or None if in dry run. best_model : Optional[Model] The model with the best epoch weights or None , depending on the value of return_model and dry_run . train_model \u00b6 def train_model ( params : Params , serialization_dir : Union [ str , PathLike ], recover : bool = False , force : bool = False , node_rank : int = 0 , include_package : List [ str ] = None , dry_run : bool = False , file_friendly_logging : bool = False , return_model : Optional [ bool ] = None ) -> Optional [ Model ] Trains the model specified in the given Params object, using the data and training parameters also specified in that object, and saves the results in serialization_dir . Parameters \u00b6 params : Params A parameter object specifying an AllenNLP Experiment. serialization_dir : str The directory in which to save results and logs. recover : bool , optional (default = False ) If True , we will try to recover a training run from an existing serialization directory. This is only intended for use when something actually crashed during the middle of a run. For continuing training a model on new data, see Model.from_archive . force : bool , optional (default = False ) If True , we will overwrite the serialization directory if it already exists. node_rank : int , optional Rank of the current node in distributed training include_package : List[str] , optional In distributed mode, extra packages mentioned will be imported in trainer workers. dry_run : bool , optional (default = False ) Do not train a model, but create a vocabulary, show dataset statistics and other training information. file_friendly_logging : bool , optional (default = False ) If True , we add newlines to tqdm output, even on an interactive terminal, and we slow down tqdm's output to only once every 10 seconds. return_model : Optional[bool] , optional (default = None ) Whether or not to return the final model. If not specified, this defaults to False for distributed training and True otherwise. Returns \u00b6 best_model : Optional[Model] The model with the best epoch weights or None , depending on the value of return_model and dry_run . TrainModel \u00b6 class TrainModel ( Registrable ): | def __init__ ( | self , | serialization_dir : str , | model : Model , | trainer : Trainer , | evaluation_data_loader : DataLoader = None , | evaluate_on_test : bool = False , | batch_weight_key : str = \"\" | ) -> None This class exists so that we can easily read a configuration file with the allennlp train command. The basic logic is that we call train_loop = TrainModel.from_params(params_from_config_file) , then train_loop.run() . This class performs very little logic, pushing most of it to the Trainer that has a train() method. The point here is to construct all of the dependencies for the Trainer in a way that we can do it using from_params() , while having all of those dependencies transparently documented and not hidden in calls to params.pop() . If you are writing your own training loop, you almost certainly should not use this class, but you might look at the code for this class to see what we do, to make writing your training loop easier. In particular, if you are tempted to call the __init__ method of this class, you are probably doing something unnecessary. Literally all we do after __init__ is call trainer.train() . You can do that yourself, if you've constructed a Trainer already. What this class gives you is a way to construct the Trainer by means of a config file. The actual constructor that we use with from_params in this class is from_partial_objects . See that method for a description of all of the allowed top-level keys in a configuration file used with allennlp train . default_implementation \u00b6 class TrainModel ( Registrable ): | ... | default_implementation = \"default\" The default implementation is registered as 'default'. run \u00b6 class TrainModel ( Registrable ): | ... | def run ( self ) -> Dict [ str , Any ] finish \u00b6 class TrainModel ( Registrable ): | ... | def finish ( self , metrics : Dict [ str , Any ]) from_partial_objects \u00b6 class TrainModel ( Registrable ): | ... | @classmethod | def from_partial_objects ( | cls , | serialization_dir : str , | local_rank : int , | dataset_reader : DatasetReader , | train_data_path : Any , | model : Lazy [ Model ], | data_loader : Lazy [ DataLoader ], | trainer : Lazy [ Trainer ], | vocabulary : Lazy [ Vocabulary ] = Lazy ( Vocabulary ), | datasets_for_vocab_creation : List [ str ] = None , | validation_dataset_reader : DatasetReader = None , | validation_data_path : Any = None , | validation_data_loader : Lazy [ DataLoader ] = None , | test_data_path : Any = None , | evaluate_on_test : bool = False , | batch_weight_key : str = \"\" , | ddp_accelerator : Optional [ DdpAccelerator ] = None | ) -> \"TrainModel\" This method is intended for use with our FromParams logic, to construct a TrainModel object from a config file passed to the allennlp train command. The arguments to this method are the allowed top-level keys in a configuration file (except for the first three, which are obtained separately). You could use this outside of our FromParams logic if you really want to, but there might be easier ways to accomplish your goal than instantiating Lazy objects. If you are writing your own training loop, we recommend that you look at the implementation of this method for inspiration and possibly some utility functions you can call, but you very likely should not use this method directly. The Lazy type annotations here are a mechanism for building dependencies to an object sequentially - the TrainModel object needs data, a model, and a trainer, but the model needs to see the data before it's constructed (to create a vocabulary) and the trainer needs the data and the model before it's constructed. Objects that have sequential dependencies like this are labeled as Lazy in their type annotations, and we pass the missing dependencies when we call their construct() method, which you can see in the code below. Parameters \u00b6 serialization_dir : str The directory where logs and model archives will be saved. In a typical AllenNLP configuration file, this parameter does not get an entry as a top-level key, it gets passed in separately. local_rank : int The process index that is initialized using the GPU device id. In a typical AllenNLP configuration file, this parameter does not get an entry as a top-level key, it gets passed in separately. dataset_reader : DatasetReader The DatasetReader that will be used for training and (by default) for validation. train_data_path : str The file (or directory) that will be passed to dataset_reader.read() to construct the training data. model : Lazy[Model] The model that we will train. This is lazy because it depends on the Vocabulary ; after constructing the vocabulary we call model.construct(vocab=vocabulary) . data_loader : Lazy[DataLoader] The data_loader we use to batch instances from the dataset reader at training and (by default) validation time. This is lazy because it takes a dataset in it's constructor. trainer : Lazy[Trainer] The Trainer that actually implements the training loop. This is a lazy object because it depends on the model that's going to be trained. vocabulary : Lazy[Vocabulary] , optional (default = Lazy(Vocabulary) ) The Vocabulary that we will use to convert strings in the data to integer ids (and possibly set sizes of embedding matrices in the Model ). By default we construct the vocabulary from the instances that we read. datasets_for_vocab_creation : List[str] , optional (default = None ) If you pass in more than one dataset but don't want to use all of them to construct a vocabulary, you can pass in this key to limit it. Valid entries in the list are \"train\", \"validation\" and \"test\". validation_dataset_reader : DatasetReader , optional (default = None ) If given, we will use this dataset reader for the validation data instead of dataset_reader . validation_data_path : str , optional (default = None ) If given, we will use this data for computing validation metrics and early stopping. validation_data_loader : Lazy[DataLoader] , optional (default = None ) If given, the data_loader we use to batch instances from the dataset reader at validation and test time. This is lazy because it takes a dataset in it's constructor. test_data_path : str , optional (default = None ) If given, we will use this as test data. This makes it available for vocab creation by default, but nothing else. evaluate_on_test : bool , optional (default = False ) If given, we will evaluate the final model on this data at the end of training. Note that we do not recommend using this for actual test data in every-day experimentation; you should only very rarely evaluate your model on actual test data. batch_weight_key : str , optional (default = \"\" ) The name of metric used to weight the loss on a per-batch basis. This is only used during evaluation on final test data, if you've specified evaluate_on_test=True . ddp_accelerator : Optional[DdpAccelerator] , optional (default = None ) A DdpAccelerator to use in distributed trainer. Passed to the model and the trainer.","title":"train"},{"location":"api/commands/train/#train","text":"@Subcommand . register ( \"train\" ) class Train ( Subcommand )","title":"Train"},{"location":"api/commands/train/#add_subparser","text":"class Train ( Subcommand ): | ... | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser","title":"add_subparser"},{"location":"api/commands/train/#train_model_from_args","text":"def train_model_from_args ( args : argparse . Namespace ) Just converts from an argparse.Namespace object to string paths.","title":"train_model_from_args"},{"location":"api/commands/train/#train_model_from_file","text":"def train_model_from_file ( parameter_filename : Union [ str , PathLike ], serialization_dir : Union [ str , PathLike ], overrides : Union [ str , Dict [ str , Any ]] = \"\" , recover : bool = False , force : bool = False , node_rank : int = 0 , include_package : List [ str ] = None , dry_run : bool = False , file_friendly_logging : bool = False , return_model : Optional [ bool ] = None ) -> Optional [ Model ] A wrapper around train_model which loads the params from a file.","title":"train_model_from_file"},{"location":"api/commands/train/#train_model","text":"def train_model ( params : Params , serialization_dir : Union [ str , PathLike ], recover : bool = False , force : bool = False , node_rank : int = 0 , include_package : List [ str ] = None , dry_run : bool = False , file_friendly_logging : bool = False , return_model : Optional [ bool ] = None ) -> Optional [ Model ] Trains the model specified in the given Params object, using the data and training parameters also specified in that object, and saves the results in serialization_dir .","title":"train_model"},{"location":"api/commands/train/#trainmodel","text":"class TrainModel ( Registrable ): | def __init__ ( | self , | serialization_dir : str , | model : Model , | trainer : Trainer , | evaluation_data_loader : DataLoader = None , | evaluate_on_test : bool = False , | batch_weight_key : str = \"\" | ) -> None This class exists so that we can easily read a configuration file with the allennlp train command. The basic logic is that we call train_loop = TrainModel.from_params(params_from_config_file) , then train_loop.run() . This class performs very little logic, pushing most of it to the Trainer that has a train() method. The point here is to construct all of the dependencies for the Trainer in a way that we can do it using from_params() , while having all of those dependencies transparently documented and not hidden in calls to params.pop() . If you are writing your own training loop, you almost certainly should not use this class, but you might look at the code for this class to see what we do, to make writing your training loop easier. In particular, if you are tempted to call the __init__ method of this class, you are probably doing something unnecessary. Literally all we do after __init__ is call trainer.train() . You can do that yourself, if you've constructed a Trainer already. What this class gives you is a way to construct the Trainer by means of a config file. The actual constructor that we use with from_params in this class is from_partial_objects . See that method for a description of all of the allowed top-level keys in a configuration file used with allennlp train .","title":"TrainModel"},{"location":"api/commands/train/#default_implementation","text":"class TrainModel ( Registrable ): | ... | default_implementation = \"default\" The default implementation is registered as 'default'.","title":"default_implementation"},{"location":"api/commands/train/#run","text":"class TrainModel ( Registrable ): | ... | def run ( self ) -> Dict [ str , Any ]","title":"run"},{"location":"api/commands/train/#finish","text":"class TrainModel ( Registrable ): | ... | def finish ( self , metrics : Dict [ str , Any ])","title":"finish"},{"location":"api/commands/train/#from_partial_objects","text":"class TrainModel ( Registrable ): | ... | @classmethod | def from_partial_objects ( | cls , | serialization_dir : str , | local_rank : int , | dataset_reader : DatasetReader , | train_data_path : Any , | model : Lazy [ Model ], | data_loader : Lazy [ DataLoader ], | trainer : Lazy [ Trainer ], | vocabulary : Lazy [ Vocabulary ] = Lazy ( Vocabulary ), | datasets_for_vocab_creation : List [ str ] = None , | validation_dataset_reader : DatasetReader = None , | validation_data_path : Any = None , | validation_data_loader : Lazy [ DataLoader ] = None , | test_data_path : Any = None , | evaluate_on_test : bool = False , | batch_weight_key : str = \"\" , | ddp_accelerator : Optional [ DdpAccelerator ] = None | ) -> \"TrainModel\" This method is intended for use with our FromParams logic, to construct a TrainModel object from a config file passed to the allennlp train command. The arguments to this method are the allowed top-level keys in a configuration file (except for the first three, which are obtained separately). You could use this outside of our FromParams logic if you really want to, but there might be easier ways to accomplish your goal than instantiating Lazy objects. If you are writing your own training loop, we recommend that you look at the implementation of this method for inspiration and possibly some utility functions you can call, but you very likely should not use this method directly. The Lazy type annotations here are a mechanism for building dependencies to an object sequentially - the TrainModel object needs data, a model, and a trainer, but the model needs to see the data before it's constructed (to create a vocabulary) and the trainer needs the data and the model before it's constructed. Objects that have sequential dependencies like this are labeled as Lazy in their type annotations, and we pass the missing dependencies when we call their construct() method, which you can see in the code below.","title":"from_partial_objects"},{"location":"api/common/cached_transformers/","text":"allennlp .common .cached_transformers [SOURCE] TransformerSpec \u00b6 class TransformerSpec ( NamedTuple ) model_name \u00b6 class TransformerSpec ( NamedTuple ): | ... | model_name : str = None override_weights_file \u00b6 class TransformerSpec ( NamedTuple ): | ... | override_weights_file : Optional [ str ] = None override_weights_strip_prefix \u00b6 class TransformerSpec ( NamedTuple ): | ... | override_weights_strip_prefix : Optional [ str ] = None reinit_modules \u00b6 class TransformerSpec ( NamedTuple ): | ... | reinit_modules : Optional [ Union [ int , Tuple [ int , ... ], Tuple [ str , ... ]]] = None get \u00b6 def get ( model_name : str , make_copy : bool , override_weights_file : Optional [ str ] = None , override_weights_strip_prefix : Optional [ str ] = None , reinit_modules : Optional [ Union [ int , Tuple [ int , ... ], Tuple [ str , ... ]]] = None , load_weights : bool = True , ** kwargs ) -> transformers . PreTrainedModel Returns a transformer model from the cache. Parameters \u00b6 model_name : str The name of the transformer, for example \"bert-base-cased\" make_copy : bool If this is True , return a copy of the model instead of the cached model itself. If you want to modify the parameters of the model, set this to True . If you want only part of the model, set this to False , but make sure to copy.deepcopy() the bits you are keeping. override_weights_file : str , optional (default = None ) If set, this specifies a file from which to load alternate weights that override the weights from huggingface. The file is expected to contain a PyTorch state_dict , created with torch.save() . override_weights_strip_prefix : str , optional (default = None ) If set, strip the given prefix from the state dict when loading it. reinit_modules : Optional[Union[int, Tuple[int, ...], Tuple[str, ...]]] , optional (default = None ) If this is an integer, the last reinit_modules layers of the transformer will be re-initialized. If this is a tuple of integers, the layers indexed by reinit_modules will be re-initialized. Note, because the module structure of the transformer model_name can differ, we cannot guarantee that providing an integer or tuple of integers will work. If this fails, you can instead provide a tuple of strings, which will be treated as regexes and any module with a name matching the regex will be re-initialized. Re-initializing the last few layers of a pretrained transformer can reduce the instability of fine-tuning on small datasets and may improve performance (https://arxiv.org/abs/2006.05987v3). Has no effect if load_weights is False or override_weights_file is not None . load_weights : bool , optional (default = True ) If set to False , no weights will be loaded. This is helpful when you only want to initialize the architecture, like when you've already fine-tuned a model and are going to load the weights from a state dict elsewhere. get_tokenizer \u00b6 def get_tokenizer ( model_name : str , ** kwargs ) -> transformers . PreTrainedTokenizer","title":"cached_transformers"},{"location":"api/common/cached_transformers/#transformerspec","text":"class TransformerSpec ( NamedTuple )","title":"TransformerSpec"},{"location":"api/common/cached_transformers/#model_name","text":"class TransformerSpec ( NamedTuple ): | ... | model_name : str = None","title":"model_name"},{"location":"api/common/cached_transformers/#override_weights_file","text":"class TransformerSpec ( NamedTuple ): | ... | override_weights_file : Optional [ str ] = None","title":"override_weights_file"},{"location":"api/common/cached_transformers/#override_weights_strip_prefix","text":"class TransformerSpec ( NamedTuple ): | ... | override_weights_strip_prefix : Optional [ str ] = None","title":"override_weights_strip_prefix"},{"location":"api/common/cached_transformers/#reinit_modules","text":"class TransformerSpec ( NamedTuple ): | ... | reinit_modules : Optional [ Union [ int , Tuple [ int , ... ], Tuple [ str , ... ]]] = None","title":"reinit_modules"},{"location":"api/common/cached_transformers/#get","text":"def get ( model_name : str , make_copy : bool , override_weights_file : Optional [ str ] = None , override_weights_strip_prefix : Optional [ str ] = None , reinit_modules : Optional [ Union [ int , Tuple [ int , ... ], Tuple [ str , ... ]]] = None , load_weights : bool = True , ** kwargs ) -> transformers . PreTrainedModel Returns a transformer model from the cache.","title":"get"},{"location":"api/common/cached_transformers/#get_tokenizer","text":"def get_tokenizer ( model_name : str , ** kwargs ) -> transformers . PreTrainedTokenizer","title":"get_tokenizer"},{"location":"api/common/checks/","text":"allennlp .common .checks [SOURCE] Functions and exceptions for checking that AllenNLP and its models are configured correctly. ConfigurationError \u00b6 class ConfigurationError ( Exception ): | def __init__ ( self , message : str ) The exception raised by any AllenNLP object when it's misconfigured (e.g. missing properties, invalid properties, unknown properties). ExperimentalFeatureWarning \u00b6 class ExperimentalFeatureWarning ( RuntimeWarning ) A warning that you are using an experimental feature that may change or be deleted. log_pytorch_version_info \u00b6 def log_pytorch_version_info () check_dimensions_match \u00b6 def check_dimensions_match ( dimension_1 : int , dimension_2 : int , dim_1_name : str , dim_2_name : str ) -> None parse_cuda_device \u00b6 def parse_cuda_device ( cuda_device : Union [ str , int , List [ int ]]) -> int Disambiguates single GPU and multiple GPU settings for cuda_device param. check_for_gpu \u00b6 def check_for_gpu ( device : Union [ int , torch . device , List [ Union [ int , torch . device ]]] ) check_for_java \u00b6 def check_for_java () -> bool","title":"checks"},{"location":"api/common/checks/#configurationerror","text":"class ConfigurationError ( Exception ): | def __init__ ( self , message : str ) The exception raised by any AllenNLP object when it's misconfigured (e.g. missing properties, invalid properties, unknown properties).","title":"ConfigurationError"},{"location":"api/common/checks/#experimentalfeaturewarning","text":"class ExperimentalFeatureWarning ( RuntimeWarning ) A warning that you are using an experimental feature that may change or be deleted.","title":"ExperimentalFeatureWarning"},{"location":"api/common/checks/#log_pytorch_version_info","text":"def log_pytorch_version_info ()","title":"log_pytorch_version_info"},{"location":"api/common/checks/#check_dimensions_match","text":"def check_dimensions_match ( dimension_1 : int , dimension_2 : int , dim_1_name : str , dim_2_name : str ) -> None","title":"check_dimensions_match"},{"location":"api/common/checks/#parse_cuda_device","text":"def parse_cuda_device ( cuda_device : Union [ str , int , List [ int ]]) -> int Disambiguates single GPU and multiple GPU settings for cuda_device param.","title":"parse_cuda_device"},{"location":"api/common/checks/#check_for_gpu","text":"def check_for_gpu ( device : Union [ int , torch . device , List [ Union [ int , torch . device ]]] )","title":"check_for_gpu"},{"location":"api/common/checks/#check_for_java","text":"def check_for_java () -> bool","title":"check_for_java"},{"location":"api/common/file_utils/","text":"allennlp .common .file_utils [SOURCE] Utilities for working with the local dataset cache. CACHE_ROOT \u00b6 CACHE_ROOT = Path ( os . getenv ( \"ALLENNLP_CACHE_ROOT\" , Path . home () / \".allennlp\" )) CACHE_DIRECTORY \u00b6 CACHE_DIRECTORY = str ( CACHE_ROOT / \"cache\" ) DEPRECATED_CACHE_DIRECTORY \u00b6 DEPRECATED_CACHE_DIRECTORY = str ( CACHE_ROOT / \"datasets\" ) DATASET_CACHE \u00b6 DATASET_CACHE = CACHE_DIRECTORY filename_to_url \u00b6 def filename_to_url ( filename : str , cache_dir : Union [ str , Path ] = None ) -> Tuple [ str , str ] Return the url and etag (which may be None ) stored for filename . Raise FileNotFoundError if filename or its stored metadata do not exist. cached_path \u00b6 def cached_path ( url_or_filename : Union [ str , PathLike ], cache_dir : Union [ str , Path ] = None , extract_archive : bool = False , force_extract : bool = False ) -> str Given something that might be a URL or local path, determine which. If it's a remote resource, download the file and cache it, and then return the path to the cached file. If it's already a local path, make sure the file exists and return the path. For URLs, \"http://\", \"https://\", \"s3://\", \"gs://\", and \"hf://\" are all supported. The latter corresponds to the HuggingFace Hub. For example, to download the PyTorch weights for the model epwalsh/bert-xsmall-dummy on HuggingFace, you could do: cached_path ( \"hf://epwalsh/bert-xsmall-dummy/pytorch_model.bin\" ) For paths or URLs that point to a tarfile or zipfile, you can also add a path to a specific file to the url_or_filename preceeded by a \"!\", and the archive will be automatically extracted (provided you set extract_archive to True ), returning the local path to the specific file. For example: cached_path ( \"model.tar.gz!weights.th\" , extract_archive = True ) Parameters \u00b6 url_or_filename : Union[str, Path] A URL or path to parse and possibly download. cache_dir : Union[str, Path] , optional (default = None ) The directory to cache downloads. extract_archive : bool , optional (default = False ) If True , then zip or tar.gz archives will be automatically extracted. In which case the directory is returned. force_extract : bool , optional (default = False ) If True and the file is an archive file, it will be extracted regardless of whether or not the extracted directory already exists. Warning Use this flag with caution! This can lead to race conditions if used from multiple processes on the same file. TensorCache \u00b6 class TensorCache ( MutableMapping [ str , Tensor ], ABC ): | def __init__ ( | self , | filename : Union [ str , PathLike ], | * , map_size : int = 1024 * 1024 * 1024 * 1024 , | * , read_only : bool = False | ) -> None This is a key-value store, mapping strings to tensors. The data is kept on disk, making this class useful as a cache for storing tensors. TensorCache is also safe to access from multiple processes at the same time, so you can use it in distributed training situations, or from multiple training runs at the same time. read_only \u00b6 class TensorCache ( MutableMapping [ str , Tensor ], ABC ): | ... | @property | def read_only ( self ) -> bool __iter__ \u00b6 class TensorCache ( MutableMapping [ str , Tensor ], ABC ): | ... | def __iter__ ( self ) LocalCacheResource \u00b6 class LocalCacheResource : | def __init__ ( | self , | resource_name : str , | version : str , | cache_dir : str = CACHE_DIRECTORY | ) -> None This is a context manager that can be used to fetch and cache arbitrary resources locally using the same mechanisms that cached_path uses for remote resources. It can be used, for example, when you want to cache the result of an expensive computation. Examples \u00b6 with LocalCacheResource ( \"long-computation\" , \"v1\" ) as cache : if cache . cached (): with cache . reader () as f : # read from cache else : with cache . writer () as f : # do the computation # ... # write to cache cached \u00b6 class LocalCacheResource : | ... | def cached ( self ) -> bool writer \u00b6 class LocalCacheResource : | ... | @contextmanager | def writer ( self , mode = \"w\" ) reader \u00b6 class LocalCacheResource : | ... | @contextmanager | def reader ( self , mode = \"r\" ) read_set_from_file \u00b6 def read_set_from_file ( filename : str ) -> Set [ str ] Extract a de-duped collection (set) of text from a file. Expected file format is one item per line. get_file_extension \u00b6 def get_file_extension ( path : str , dot = True , lower : bool = True ) open_compressed \u00b6 def open_compressed ( filename : Union [ str , PathLike ], mode : str = \"rt\" , encoding : Optional [ str ] = \"UTF-8\" , ** kwargs ) text_lines_from_file \u00b6 def text_lines_from_file ( filename : Union [ str , PathLike ], strip_lines : bool = True ) -> Iterator [ str ] json_lines_from_file \u00b6 def json_lines_from_file ( filename : Union [ str , PathLike ] ) -> Iterable [ Union [ list , dict ]] remove_cache_entries \u00b6 def remove_cache_entries ( patterns : List [ str ], cache_dir : Union [ str , Path ] = None ) -> int Remove cache entries matching the given patterns. Returns the total reclaimed space in bytes. inspect_cache \u00b6 def inspect_cache ( patterns : List [ str ] = None , cache_dir : Union [ str , Path ] = None ) Print out useful information about the cache directory. hardlink_or_copy \u00b6 def hardlink_or_copy ( source : PathOrStr , dest : PathOrStr )","title":"file_utils"},{"location":"api/common/file_utils/#cache_root","text":"CACHE_ROOT = Path ( os . getenv ( \"ALLENNLP_CACHE_ROOT\" , Path . home () / \".allennlp\" ))","title":"CACHE_ROOT"},{"location":"api/common/file_utils/#cache_directory","text":"CACHE_DIRECTORY = str ( CACHE_ROOT / \"cache\" )","title":"CACHE_DIRECTORY"},{"location":"api/common/file_utils/#deprecated_cache_directory","text":"DEPRECATED_CACHE_DIRECTORY = str ( CACHE_ROOT / \"datasets\" )","title":"DEPRECATED_CACHE_DIRECTORY"},{"location":"api/common/file_utils/#dataset_cache","text":"DATASET_CACHE = CACHE_DIRECTORY","title":"DATASET_CACHE"},{"location":"api/common/file_utils/#filename_to_url","text":"def filename_to_url ( filename : str , cache_dir : Union [ str , Path ] = None ) -> Tuple [ str , str ] Return the url and etag (which may be None ) stored for filename . Raise FileNotFoundError if filename or its stored metadata do not exist.","title":"filename_to_url"},{"location":"api/common/file_utils/#cached_path","text":"def cached_path ( url_or_filename : Union [ str , PathLike ], cache_dir : Union [ str , Path ] = None , extract_archive : bool = False , force_extract : bool = False ) -> str Given something that might be a URL or local path, determine which. If it's a remote resource, download the file and cache it, and then return the path to the cached file. If it's already a local path, make sure the file exists and return the path. For URLs, \"http://\", \"https://\", \"s3://\", \"gs://\", and \"hf://\" are all supported. The latter corresponds to the HuggingFace Hub. For example, to download the PyTorch weights for the model epwalsh/bert-xsmall-dummy on HuggingFace, you could do: cached_path ( \"hf://epwalsh/bert-xsmall-dummy/pytorch_model.bin\" ) For paths or URLs that point to a tarfile or zipfile, you can also add a path to a specific file to the url_or_filename preceeded by a \"!\", and the archive will be automatically extracted (provided you set extract_archive to True ), returning the local path to the specific file. For example: cached_path ( \"model.tar.gz!weights.th\" , extract_archive = True )","title":"cached_path"},{"location":"api/common/file_utils/#tensorcache","text":"class TensorCache ( MutableMapping [ str , Tensor ], ABC ): | def __init__ ( | self , | filename : Union [ str , PathLike ], | * , map_size : int = 1024 * 1024 * 1024 * 1024 , | * , read_only : bool = False | ) -> None This is a key-value store, mapping strings to tensors. The data is kept on disk, making this class useful as a cache for storing tensors. TensorCache is also safe to access from multiple processes at the same time, so you can use it in distributed training situations, or from multiple training runs at the same time.","title":"TensorCache"},{"location":"api/common/file_utils/#read_only","text":"class TensorCache ( MutableMapping [ str , Tensor ], ABC ): | ... | @property | def read_only ( self ) -> bool","title":"read_only"},{"location":"api/common/file_utils/#__iter__","text":"class TensorCache ( MutableMapping [ str , Tensor ], ABC ): | ... | def __iter__ ( self )","title":"__iter__"},{"location":"api/common/file_utils/#localcacheresource","text":"class LocalCacheResource : | def __init__ ( | self , | resource_name : str , | version : str , | cache_dir : str = CACHE_DIRECTORY | ) -> None This is a context manager that can be used to fetch and cache arbitrary resources locally using the same mechanisms that cached_path uses for remote resources. It can be used, for example, when you want to cache the result of an expensive computation.","title":"LocalCacheResource"},{"location":"api/common/file_utils/#cached","text":"class LocalCacheResource : | ... | def cached ( self ) -> bool","title":"cached"},{"location":"api/common/file_utils/#writer","text":"class LocalCacheResource : | ... | @contextmanager | def writer ( self , mode = \"w\" )","title":"writer"},{"location":"api/common/file_utils/#reader","text":"class LocalCacheResource : | ... | @contextmanager | def reader ( self , mode = \"r\" )","title":"reader"},{"location":"api/common/file_utils/#read_set_from_file","text":"def read_set_from_file ( filename : str ) -> Set [ str ] Extract a de-duped collection (set) of text from a file. Expected file format is one item per line.","title":"read_set_from_file"},{"location":"api/common/file_utils/#get_file_extension","text":"def get_file_extension ( path : str , dot = True , lower : bool = True )","title":"get_file_extension"},{"location":"api/common/file_utils/#open_compressed","text":"def open_compressed ( filename : Union [ str , PathLike ], mode : str = \"rt\" , encoding : Optional [ str ] = \"UTF-8\" , ** kwargs )","title":"open_compressed"},{"location":"api/common/file_utils/#text_lines_from_file","text":"def text_lines_from_file ( filename : Union [ str , PathLike ], strip_lines : bool = True ) -> Iterator [ str ]","title":"text_lines_from_file"},{"location":"api/common/file_utils/#json_lines_from_file","text":"def json_lines_from_file ( filename : Union [ str , PathLike ] ) -> Iterable [ Union [ list , dict ]]","title":"json_lines_from_file"},{"location":"api/common/file_utils/#remove_cache_entries","text":"def remove_cache_entries ( patterns : List [ str ], cache_dir : Union [ str , Path ] = None ) -> int Remove cache entries matching the given patterns. Returns the total reclaimed space in bytes.","title":"remove_cache_entries"},{"location":"api/common/file_utils/#inspect_cache","text":"def inspect_cache ( patterns : List [ str ] = None , cache_dir : Union [ str , Path ] = None ) Print out useful information about the cache directory.","title":"inspect_cache"},{"location":"api/common/file_utils/#hardlink_or_copy","text":"def hardlink_or_copy ( source : PathOrStr , dest : PathOrStr )","title":"hardlink_or_copy"},{"location":"api/common/from_params/","text":"allennlp .common .from_params [SOURCE] T \u00b6 T = TypeVar ( \"T\" , bound = \"FromParams\" ) takes_arg \u00b6 def takes_arg ( obj , arg : str ) -> bool Checks whether the provided obj takes a certain arg. If it's a class, we're really checking whether its constructor does. If it's a function or method, we're checking the object itself. Otherwise, we raise an error. takes_kwargs \u00b6 def takes_kwargs ( obj ) -> bool Checks whether a provided object takes in any positional arguments. Similar to takes_arg, we do this for both the init function of the class or a function / method Otherwise, we raise an error can_construct_from_params \u00b6 def can_construct_from_params ( type_ : Type ) -> bool is_base_registrable \u00b6 def is_base_registrable ( cls ) -> bool Checks whether this is a class that directly inherits from Registrable, or is a subclass of such a class. remove_optional \u00b6 def remove_optional ( annotation : type ) Optional[X] annotations are actually represented as Union[X, NoneType]. For our purposes, the \"Optional\" part is not interesting, so here we throw it away. infer_constructor_params \u00b6 def infer_constructor_params ( cls : Type [ T ], constructor : Union [ Callable [ ... , T ], Callable [[ T ], None ]] = None ) -> Dict [ str , inspect . Parameter ] infer_params \u00b6 infer_params = infer_constructor_params infer_method_params \u00b6 def infer_method_params ( cls : Type [ T ], method : Callable ) -> Dict [ str , inspect . Parameter ] create_kwargs \u00b6 def create_kwargs ( constructor : Callable [ ... , T ], cls : Type [ T ], params : Params , ** extras ) -> Dict [ str , Any ] Given some class, a Params object, and potentially other keyword arguments, create a dict of keyword args suitable for passing to the class's constructor. The function does this by finding the class's constructor, matching the constructor arguments to entries in the params object, and instantiating values for the parameters using the type annotation and possibly a from_params method. Any values that are provided in the extras will just be used as is. For instance, you might provide an existing Vocabulary this way. create_extras \u00b6 def create_extras ( cls : Type [ T ], extras : Dict [ str , Any ] ) -> Dict [ str , Any ] Given a dictionary of extra arguments, returns a dictionary of kwargs that actually are a part of the signature of the cls.from_params (or cls) method. pop_and_construct_arg \u00b6 def pop_and_construct_arg ( class_name : str , argument_name : str , annotation : Type , default : Any , params : Params , ** extras ) -> Any Does the work of actually constructing an individual argument for create_kwargs . Here we're in the inner loop of iterating over the parameters to a particular constructor, trying to construct just one of them. The information we get for that parameter is its name, its type annotation, and its default value; we also get the full set of Params for constructing the object (which we may mutate), and any extras that the constructor might need. We take the type annotation and default value here separately, instead of using an inspect.Parameter object directly, so that we can handle Union types using recursion on this method, trying the different annotation types in the union in turn. construct_arg \u00b6 def construct_arg ( class_name : str , argument_name : str , popped_params : Params , annotation : Type , default : Any , ** extras ) -> Any The first two parameters here are only used for logging if we encounter an error. FromParams \u00b6 class FromParams Mixin to give a from_params method to classes. We create a distinct base class for this because sometimes we want non-Registrable classes to be instantiatable from_params. from_params \u00b6 class FromParams : | ... | @classmethod | def from_params ( | cls : Type [ T ], | params : Params , | constructor_to_call : Callable [ ... , T ] = None , | constructor_to_inspect : Union [ Callable [ ... , T ], Callable [[ T ], None ]] = None , | ** extras | ) -> T This is the automatic implementation of from_params . Any class that subclasses FromParams (or Registrable , which itself subclasses FromParams ) gets this implementation for free. If you want your class to be instantiated from params in the \"obvious\" way -- pop off parameters and hand them to your constructor with the same names -- this provides that functionality. If you need more complex logic in your from from_params method, you'll have to implement your own method that overrides this one. The constructor_to_call and constructor_to_inspect arguments deal with a bit of redirection that we do. We allow you to register particular @classmethods on a class as the constructor to use for a registered name. This lets you, e.g., have a single Vocabulary class that can be constructed in two different ways, with different names registered to each constructor. In order to handle this, we need to know not just the class we're trying to construct ( cls ), but also what method we should inspect to find its arguments ( constructor_to_inspect ), and what method to call when we're done constructing arguments ( constructor_to_call ). These two methods are the same when you've used a @classmethod as your constructor, but they are different when you use the default constructor (because you inspect __init__ , but call cls() ). to_params \u00b6 class FromParams : | ... | def to_params ( self ) -> Params Returns a Params object that can be used with .from_params() to recreate an object just like it. This relies on _to_params() . If you need this in your custom FromParams class, override _to_params() , not this method.","title":"from_params"},{"location":"api/common/from_params/#t","text":"T = TypeVar ( \"T\" , bound = \"FromParams\" )","title":"T"},{"location":"api/common/from_params/#takes_arg","text":"def takes_arg ( obj , arg : str ) -> bool Checks whether the provided obj takes a certain arg. If it's a class, we're really checking whether its constructor does. If it's a function or method, we're checking the object itself. Otherwise, we raise an error.","title":"takes_arg"},{"location":"api/common/from_params/#takes_kwargs","text":"def takes_kwargs ( obj ) -> bool Checks whether a provided object takes in any positional arguments. Similar to takes_arg, we do this for both the init function of the class or a function / method Otherwise, we raise an error","title":"takes_kwargs"},{"location":"api/common/from_params/#can_construct_from_params","text":"def can_construct_from_params ( type_ : Type ) -> bool","title":"can_construct_from_params"},{"location":"api/common/from_params/#is_base_registrable","text":"def is_base_registrable ( cls ) -> bool Checks whether this is a class that directly inherits from Registrable, or is a subclass of such a class.","title":"is_base_registrable"},{"location":"api/common/from_params/#remove_optional","text":"def remove_optional ( annotation : type ) Optional[X] annotations are actually represented as Union[X, NoneType]. For our purposes, the \"Optional\" part is not interesting, so here we throw it away.","title":"remove_optional"},{"location":"api/common/from_params/#infer_constructor_params","text":"def infer_constructor_params ( cls : Type [ T ], constructor : Union [ Callable [ ... , T ], Callable [[ T ], None ]] = None ) -> Dict [ str , inspect . Parameter ]","title":"infer_constructor_params"},{"location":"api/common/from_params/#infer_params","text":"infer_params = infer_constructor_params","title":"infer_params"},{"location":"api/common/from_params/#infer_method_params","text":"def infer_method_params ( cls : Type [ T ], method : Callable ) -> Dict [ str , inspect . Parameter ]","title":"infer_method_params"},{"location":"api/common/from_params/#create_kwargs","text":"def create_kwargs ( constructor : Callable [ ... , T ], cls : Type [ T ], params : Params , ** extras ) -> Dict [ str , Any ] Given some class, a Params object, and potentially other keyword arguments, create a dict of keyword args suitable for passing to the class's constructor. The function does this by finding the class's constructor, matching the constructor arguments to entries in the params object, and instantiating values for the parameters using the type annotation and possibly a from_params method. Any values that are provided in the extras will just be used as is. For instance, you might provide an existing Vocabulary this way.","title":"create_kwargs"},{"location":"api/common/from_params/#create_extras","text":"def create_extras ( cls : Type [ T ], extras : Dict [ str , Any ] ) -> Dict [ str , Any ] Given a dictionary of extra arguments, returns a dictionary of kwargs that actually are a part of the signature of the cls.from_params (or cls) method.","title":"create_extras"},{"location":"api/common/from_params/#pop_and_construct_arg","text":"def pop_and_construct_arg ( class_name : str , argument_name : str , annotation : Type , default : Any , params : Params , ** extras ) -> Any Does the work of actually constructing an individual argument for create_kwargs . Here we're in the inner loop of iterating over the parameters to a particular constructor, trying to construct just one of them. The information we get for that parameter is its name, its type annotation, and its default value; we also get the full set of Params for constructing the object (which we may mutate), and any extras that the constructor might need. We take the type annotation and default value here separately, instead of using an inspect.Parameter object directly, so that we can handle Union types using recursion on this method, trying the different annotation types in the union in turn.","title":"pop_and_construct_arg"},{"location":"api/common/from_params/#construct_arg","text":"def construct_arg ( class_name : str , argument_name : str , popped_params : Params , annotation : Type , default : Any , ** extras ) -> Any The first two parameters here are only used for logging if we encounter an error.","title":"construct_arg"},{"location":"api/common/from_params/#fromparams","text":"class FromParams Mixin to give a from_params method to classes. We create a distinct base class for this because sometimes we want non-Registrable classes to be instantiatable from_params.","title":"FromParams"},{"location":"api/common/from_params/#from_params","text":"class FromParams : | ... | @classmethod | def from_params ( | cls : Type [ T ], | params : Params , | constructor_to_call : Callable [ ... , T ] = None , | constructor_to_inspect : Union [ Callable [ ... , T ], Callable [[ T ], None ]] = None , | ** extras | ) -> T This is the automatic implementation of from_params . Any class that subclasses FromParams (or Registrable , which itself subclasses FromParams ) gets this implementation for free. If you want your class to be instantiated from params in the \"obvious\" way -- pop off parameters and hand them to your constructor with the same names -- this provides that functionality. If you need more complex logic in your from from_params method, you'll have to implement your own method that overrides this one. The constructor_to_call and constructor_to_inspect arguments deal with a bit of redirection that we do. We allow you to register particular @classmethods on a class as the constructor to use for a registered name. This lets you, e.g., have a single Vocabulary class that can be constructed in two different ways, with different names registered to each constructor. In order to handle this, we need to know not just the class we're trying to construct ( cls ), but also what method we should inspect to find its arguments ( constructor_to_inspect ), and what method to call when we're done constructing arguments ( constructor_to_call ). These two methods are the same when you've used a @classmethod as your constructor, but they are different when you use the default constructor (because you inspect __init__ , but call cls() ).","title":"from_params"},{"location":"api/common/from_params/#to_params","text":"class FromParams : | ... | def to_params ( self ) -> Params Returns a Params object that can be used with .from_params() to recreate an object just like it. This relies on _to_params() . If you need this in your custom FromParams class, override _to_params() , not this method.","title":"to_params"},{"location":"api/common/lazy/","text":"allennlp .common .lazy [SOURCE] T \u00b6 T = TypeVar ( \"T\" ) Lazy \u00b6 class Lazy ( Generic [ T ]): | def __init__ ( | self , | constructor : Union [ Type [ T ], Callable [ ... , T ]], | params : Optional [ Params ] = None , | constructor_extras : Optional [ Dict [ str , Any ]] = None , | ** kwargs | ) -> None This class is for use when constructing objects using FromParams , when an argument to a constructor has a sequential dependency with another argument to the same constructor. For example, in a Trainer class you might want to take a Model and an Optimizer as arguments, but the Optimizer needs to be constructed using the parameters from the Model . You can give the type annotation Lazy[Optimizer] to the optimizer argument, then inside the constructor call optimizer.construct(parameters=model.parameters) . This is only recommended for use when you have registered a @classmethod as the constructor for your class, instead of using __init__ . Having a Lazy[] type annotation on an argument to an __init__ method makes your class completely dependent on being constructed using the FromParams pipeline, which is not a good idea. The actual implementation here is incredibly simple; the logic that handles the lazy construction is actually found in FromParams , where we have a special case for a Lazy type annotation. @classmethod def my_constructor ( cls , some_object : Lazy [ MyObject ], optional_object : Lazy [ MyObject ] = None , # or: # optional_object: Optional[Lazy[MyObject]] = None, optional_object_with_default : Optional [ Lazy [ MyObject ]] = Lazy ( MyObjectDefault ), required_object_with_default : Lazy [ MyObject ] = Lazy ( MyObjectDefault ), ) -> MyClass : obj1 = some_object . construct () obj2 = None if optional_object is None else optional_object . construct () obj3 = None optional_object_with_default is None else optional_object_with_default . construct () obj4 = required_object_with_default . construct () constructor \u00b6 class Lazy ( Generic [ T ]): | ... | @property | def constructor ( self ) -> Callable [ ... , T ] construct \u00b6 class Lazy ( Generic [ T ]): | ... | def construct ( self , ** kwargs ) -> T Call the constructor to create an instance of T .","title":"lazy"},{"location":"api/common/lazy/#t","text":"T = TypeVar ( \"T\" )","title":"T"},{"location":"api/common/lazy/#lazy","text":"class Lazy ( Generic [ T ]): | def __init__ ( | self , | constructor : Union [ Type [ T ], Callable [ ... , T ]], | params : Optional [ Params ] = None , | constructor_extras : Optional [ Dict [ str , Any ]] = None , | ** kwargs | ) -> None This class is for use when constructing objects using FromParams , when an argument to a constructor has a sequential dependency with another argument to the same constructor. For example, in a Trainer class you might want to take a Model and an Optimizer as arguments, but the Optimizer needs to be constructed using the parameters from the Model . You can give the type annotation Lazy[Optimizer] to the optimizer argument, then inside the constructor call optimizer.construct(parameters=model.parameters) . This is only recommended for use when you have registered a @classmethod as the constructor for your class, instead of using __init__ . Having a Lazy[] type annotation on an argument to an __init__ method makes your class completely dependent on being constructed using the FromParams pipeline, which is not a good idea. The actual implementation here is incredibly simple; the logic that handles the lazy construction is actually found in FromParams , where we have a special case for a Lazy type annotation. @classmethod def my_constructor ( cls , some_object : Lazy [ MyObject ], optional_object : Lazy [ MyObject ] = None , # or: # optional_object: Optional[Lazy[MyObject]] = None, optional_object_with_default : Optional [ Lazy [ MyObject ]] = Lazy ( MyObjectDefault ), required_object_with_default : Lazy [ MyObject ] = Lazy ( MyObjectDefault ), ) -> MyClass : obj1 = some_object . construct () obj2 = None if optional_object is None else optional_object . construct () obj3 = None optional_object_with_default is None else optional_object_with_default . construct () obj4 = required_object_with_default . construct ()","title":"Lazy"},{"location":"api/common/lazy/#constructor","text":"class Lazy ( Generic [ T ]): | ... | @property | def constructor ( self ) -> Callable [ ... , T ]","title":"constructor"},{"location":"api/common/lazy/#construct","text":"class Lazy ( Generic [ T ]): | ... | def construct ( self , ** kwargs ) -> T Call the constructor to create an instance of T .","title":"construct"},{"location":"api/common/logging/","text":"allennlp .common .logging [SOURCE] AllenNlpLogger \u00b6 class AllenNlpLogger ( logging . Logger ): | def __init__ ( self , name ) A custom subclass of 'logging.Logger' that keeps a set of messages to implement {debug,info,etc.}_once() methods. debug_once \u00b6 class AllenNlpLogger ( logging . Logger ): | ... | def debug_once ( self , msg , * args , ** kwargs ) info_once \u00b6 class AllenNlpLogger ( logging . Logger ): | ... | def info_once ( self , msg , * args , ** kwargs ) warning_once \u00b6 class AllenNlpLogger ( logging . Logger ): | ... | def warning_once ( self , msg , * args , ** kwargs ) error_once \u00b6 class AllenNlpLogger ( logging . Logger ): | ... | def error_once ( self , msg , * args , ** kwargs ) critical_once \u00b6 class AllenNlpLogger ( logging . Logger ): | ... | def critical_once ( self , msg , * args , ** kwargs ) FILE_FRIENDLY_LOGGING \u00b6 FILE_FRIENDLY_LOGGING : bool = False If this flag is set to True , we add newlines to tqdm output, even on an interactive terminal, and we slow down tqdm's output to only once every 10 seconds. By default, it is set to False . ErrorFilter \u00b6 class ErrorFilter ( Filter ) Filters out everything that is at the ERROR level or higher. This is meant to be used with a stdout handler when a stderr handler is also configured. That way ERROR messages aren't duplicated. filter \u00b6 class ErrorFilter ( Filter ): | ... | def filter ( self , record ) prepare_global_logging \u00b6 def prepare_global_logging ( serialization_dir : Union [ str , PathLike ], rank : int = 0 , world_size : int = 1 ) -> None","title":"logging"},{"location":"api/common/logging/#allennlplogger","text":"class AllenNlpLogger ( logging . Logger ): | def __init__ ( self , name ) A custom subclass of 'logging.Logger' that keeps a set of messages to implement {debug,info,etc.}_once() methods.","title":"AllenNlpLogger"},{"location":"api/common/logging/#debug_once","text":"class AllenNlpLogger ( logging . Logger ): | ... | def debug_once ( self , msg , * args , ** kwargs )","title":"debug_once"},{"location":"api/common/logging/#info_once","text":"class AllenNlpLogger ( logging . Logger ): | ... | def info_once ( self , msg , * args , ** kwargs )","title":"info_once"},{"location":"api/common/logging/#warning_once","text":"class AllenNlpLogger ( logging . Logger ): | ... | def warning_once ( self , msg , * args , ** kwargs )","title":"warning_once"},{"location":"api/common/logging/#error_once","text":"class AllenNlpLogger ( logging . Logger ): | ... | def error_once ( self , msg , * args , ** kwargs )","title":"error_once"},{"location":"api/common/logging/#critical_once","text":"class AllenNlpLogger ( logging . Logger ): | ... | def critical_once ( self , msg , * args , ** kwargs )","title":"critical_once"},{"location":"api/common/logging/#file_friendly_logging","text":"FILE_FRIENDLY_LOGGING : bool = False If this flag is set to True , we add newlines to tqdm output, even on an interactive terminal, and we slow down tqdm's output to only once every 10 seconds. By default, it is set to False .","title":"FILE_FRIENDLY_LOGGING"},{"location":"api/common/logging/#errorfilter","text":"class ErrorFilter ( Filter ) Filters out everything that is at the ERROR level or higher. This is meant to be used with a stdout handler when a stderr handler is also configured. That way ERROR messages aren't duplicated.","title":"ErrorFilter"},{"location":"api/common/logging/#filter","text":"class ErrorFilter ( Filter ): | ... | def filter ( self , record )","title":"filter"},{"location":"api/common/logging/#prepare_global_logging","text":"def prepare_global_logging ( serialization_dir : Union [ str , PathLike ], rank : int = 0 , world_size : int = 1 ) -> None","title":"prepare_global_logging"},{"location":"api/common/meta/","text":"allennlp .common .meta [SOURCE] META_NAME \u00b6 META_NAME = \"meta.json\" Meta \u00b6 @dataclass class Meta Defines the meta data that's saved in a serialization directory and archive when training an AllenNLP model. version \u00b6 class Meta : | ... | version : str = None new \u00b6 class Meta : | ... | @classmethod | def new ( cls ) -> \"Meta\" to_file \u00b6 class Meta : | ... | def to_file ( self , path : Union [ PathLike , str ]) -> None from_path \u00b6 class Meta : | ... | @classmethod | def from_path ( cls , path : Union [ PathLike , str ]) -> \"Meta\"","title":"meta"},{"location":"api/common/meta/#meta_name","text":"META_NAME = \"meta.json\"","title":"META_NAME"},{"location":"api/common/meta/#meta","text":"@dataclass class Meta Defines the meta data that's saved in a serialization directory and archive when training an AllenNLP model.","title":"Meta"},{"location":"api/common/meta/#version","text":"class Meta : | ... | version : str = None","title":"version"},{"location":"api/common/meta/#new","text":"class Meta : | ... | @classmethod | def new ( cls ) -> \"Meta\"","title":"new"},{"location":"api/common/meta/#to_file","text":"class Meta : | ... | def to_file ( self , path : Union [ PathLike , str ]) -> None","title":"to_file"},{"location":"api/common/meta/#from_path","text":"class Meta : | ... | @classmethod | def from_path ( cls , path : Union [ PathLike , str ]) -> \"Meta\"","title":"from_path"},{"location":"api/common/model_card/","text":"allennlp .common .model_card [SOURCE] A specification for defining model cards as described in Model Cards for Model Reporting (Mitchell et al, 2019) The descriptions of the fields and some examples are taken from the paper. The specification is provided to prompt model developers to think about the various aspects that should ideally be reported. The information filled should adhere to the spirit of transparency rather than the letter; i.e., it should not be filled for the sake of being filled. If the information cannot be inferred, it should be left empty. get_description \u00b6 def get_description ( model_class ) Returns the model's description from the docstring. ModelCardInfo \u00b6 class ModelCardInfo ( FromParams ) to_dict \u00b6 class ModelCardInfo ( FromParams ): | ... | def to_dict ( self ) Only the non-empty attributes are returned, to minimize empty values. Paper \u00b6 @dataclass ( frozen = True ) class Paper ( ModelCardInfo ) This provides information about the paper. Parameters \u00b6 title : str The name of the paper. url : str A web link to the paper. citation : str The BibTex for the paper. title \u00b6 class Paper ( ModelCardInfo ): | ... | title : Optional [ str ] = None url \u00b6 class Paper ( ModelCardInfo ): | ... | url : Optional [ str ] = None citation \u00b6 class Paper ( ModelCardInfo ): | ... | citation : Optional [ str ] = None ModelDetails \u00b6 class ModelDetails ( ModelCardInfo ): | def __init__ ( | self , | description : Optional [ str ] = None , | short_description : Optional [ str ] = None , | developed_by : Optional [ str ] = None , | contributed_by : Optional [ str ] = None , | date : Optional [ str ] = None , | version : Optional [ str ] = None , | model_type : Optional [ str ] = None , | paper : Optional [ Union [ str , Dict , Paper ]] = None , | license : Optional [ str ] = None , | contact : Optional [ str ] = None | ) This provides the basic information about the model. Parameters \u00b6 description : str A high-level overview of the model. Eg. The model implements a reading comprehension model patterned after the proposed model in Devlin et al, 2018 , with improvements borrowed from the SQuAD model in the transformers project. It predicts start tokens and end tokens with a linear layer on top of word piece embeddings. short_description : str A one-line description of the model. Eg. A reading comprehension model patterned after RoBERTa, with improvements borrowed from the SQuAD model in the transformers project. developed_by : str Person/organization that developed the model. This can be used by all stakeholders to infer details pertaining to model development and potential conflicts of interest. contributed_by : str Person that contributed the model to the repository. date : str The date on which the model was contributed. This is useful for all stakeholders to become further informed on what techniques and data sources were likely to be available during model development. Format example: 2020-09-23 version : str The version of the model, and how it differs from previous versions. This is useful for all stakeholders to track whether the model is the latest version, associate known bugs to the correct model versions, and aid in model comparisons. model_type : str The type of the model; the basic architecture. This is likely to be particularly relevant for software and model developers, as well as individuals knowledgeable about machine learning, to highlight what kinds of assumptions are encoded in the system. Eg. Naive Bayes Classifier. paper : Union[str, Dict, Paper] The paper on which the model is based. Format example: { \"title\": \"Model Cards for Model Reporting (Mitchell et al, 2019)\", \"url\": \"https://api.semanticscholar.org/CorpusID:52946140\", \"citation\": \" \", } license : str License information for the model. contact : str The email address to reach out to the relevant developers/contributors for questions/feedback about the model. IntendedUse \u00b6 @dataclass ( frozen = True ) class IntendedUse ( ModelCardInfo ) This determines what the model should and should not be used for. Parameters \u00b6 primary_uses : str Details the primary intended uses of the model; whether it was developed for general or specific tasks. Eg. The toxic text identifier model was developed to identify toxic comments on online platforms. An example use case is to provide feedback to comment authors. primary_users : str The primary intended users. For example, was the model developed for entertainment purposes, for hobbyists, or enterprise solutions? This helps users gain insight into how robust the model may be to different kinds of inputs. out_of_scope_use_cases : str Highlights the technology that the model might easily be confused with, or related contexts that users could try to apply the model to. Eg. the toxic text identifier model is not intended for fully automated moderation, or to make judgements about specific individuals. Also recommends a related or similar model that was designed to better meet a particular need, where possible. Eg. not for use on text examples longer than 100 tokens; please use the bigger-toxic-text-identifier instead. primary_uses \u00b6 class IntendedUse ( ModelCardInfo ): | ... | primary_uses : Optional [ str ] = None primary_users \u00b6 class IntendedUse ( ModelCardInfo ): | ... | primary_users : Optional [ str ] = None out_of_scope_use_cases \u00b6 class IntendedUse ( ModelCardInfo ): | ... | out_of_scope_use_cases : Optional [ str ] = None Factors \u00b6 @dataclass ( frozen = True ) class Factors ( ModelCardInfo ) This provides a summary of relevant factors such as demographics, instrumentation used, etc. for which the model performance may vary. Parameters \u00b6 relevant_factors : str The foreseeable salient factors for which model performance may vary, and how these were determined. Eg. the model performance may vary for variations in dialects of English. evaluation_factors : str Mentions the factors that are being reported, and the reasons for why they were chosen. Also includes the reasons for choosing different evaluation factors than relevant factors. Eg. While dialect variation is a relevant factor, dialect-specific annotations were not available, and hence, the performance was not evaluated on different dialects. relevant_factors \u00b6 class Factors ( ModelCardInfo ): | ... | relevant_factors : Optional [ str ] = None evaluation_factors \u00b6 class Factors ( ModelCardInfo ): | ... | evaluation_factors : Optional [ str ] = None Metrics \u00b6 @dataclass ( frozen = True ) class Metrics ( ModelCardInfo ) This lists the reported metrics and the reasons for choosing them. Parameters \u00b6 model_performance_measures : str Which model performance measures were selected and the reasons for selecting them. decision_thresholds : str If decision thresholds are used, what are they, and the reasons for choosing them. variation_approaches : str How are the measurements and estimations of these metrics calculated? Eg. standard deviation, variance, confidence intervals, KL divergence. Details of how these values are approximated should also be included. Eg. average of 5 runs, 10-fold cross-validation, etc. model_performance_measures \u00b6 class Metrics ( ModelCardInfo ): | ... | model_performance_measures : Optional [ str ] = None decision_thresholds \u00b6 class Metrics ( ModelCardInfo ): | ... | decision_thresholds : Optional [ str ] = None variation_approaches \u00b6 class Metrics ( ModelCardInfo ): | ... | variation_approaches : Optional [ str ] = None Dataset \u00b6 @dataclass ( frozen = True ) class Dataset ( ModelCardInfo ) This provides basic information about the dataset. Parameters \u00b6 name : str The name of the dataset. url : str A web link to the dataset information/datasheet. processed_url : str A web link to a downloadable/directly usable version of the dataset, if available. notes : str Any other notes on downloading/processing the data. name \u00b6 class Dataset ( ModelCardInfo ): | ... | name : Optional [ str ] = None url \u00b6 class Dataset ( ModelCardInfo ): | ... | url : Optional [ str ] = None processed_url \u00b6 class Dataset ( ModelCardInfo ): | ... | processed_url : Optional [ str ] = None notes \u00b6 class Dataset ( ModelCardInfo ): | ... | notes : Optional [ str ] = None EvaluationData \u00b6 class EvaluationData ( ModelCardInfo ): | def __init__ ( | self , | dataset : Optional [ Union [ str , Dict , Dataset ]] = None , | motivation : Optional [ str ] = None , | preprocessing : Optional [ str ] = None | ) This provides information about the evaluation data. Parameters \u00b6 dataset : Union[str, Dict, Dataset] The name(s) (and link(s), if available) of the dataset(s) used to evaluate the model. Optionally, provide a link to the relevant datasheet(s) as well. motivation : str The reasons for selecting the dataset(s). Eg. For the BERT model, document-level corpora were used rather than a shuffled sentence-level corpus in order to extract long contiguous sequences. preprocessing : str How was the data preprocessed for evaluation? Eg. tokenization of sentences, filtering of paragraphs by length, etc. to_dict \u00b6 class EvaluationData ( ModelCardInfo ): | ... | def to_dict ( self ) TrainingData \u00b6 class TrainingData ( ModelCardInfo ): | def __init__ ( | self , | dataset : Optional [ Union [ str , Dict , Dataset ]] = None , | motivation : Optional [ str ] = None , | preprocessing : Optional [ str ] = None | ) This provides information about the training data. If the model was initialized from pretrained weights, a link to the pretrained model's model card/training data can additionally be provided, if available. Any relevant definitions should also be included. Parameters \u00b6 dataset : Union[str, Dict, Dataset] The name(s) (and link(s), if available) of the dataset(s) used to train the model. Optionally, provide a link to the relevant datasheet(s) as well. Eg. * Proprietary data from Perspective API; includes comments from online forums such as Wikipedia and New York Times, with crowdsourced labels of whether the comment is \"toxic\". * \"Toxic\" is defined as \"a rude, disrespectful, or unreasonable comment that is likely to make you leave a discussion.\" motivation : str The reasons for selecting the dataset(s). Eg. For the BERT model, document-level corpora were used rather than a shuffled sentence-level corpus in order to extract long contiguous sequences. preprocessing : str Eg. Only the text passages were extracted from English Wikipedia; lists, tables, and headers were ignored. to_dict \u00b6 class TrainingData ( ModelCardInfo ): | ... | def to_dict ( self ) QuantitativeAnalyses \u00b6 @dataclass ( frozen = True ) class QuantitativeAnalyses ( ModelCardInfo ) This provides disaggregated evaluation of how the model performed based on chosen metrics, with confidence intervals, if possible. Links to plots/figures showing the metrics can also be provided. Parameters \u00b6 unitary_results : str The performance of the model with respect to each chosen factor. intersectional_results : str The performance of the model with respect to the intersection of the evaluated factors. unitary_results \u00b6 class QuantitativeAnalyses ( ModelCardInfo ): | ... | unitary_results : Optional [ str ] = None intersectional_results \u00b6 class QuantitativeAnalyses ( ModelCardInfo ): | ... | intersectional_results : Optional [ str ] = None ModelEthicalConsiderations \u00b6 @dataclass ( frozen = True ) class ModelEthicalConsiderations ( ModelCardInfo ) This highlights any ethical considerations to keep in mind when using the model. Eg. Is the model intended to be used for informing decisions on human life? Does it use sensitive data? What kind of risks are possible, and what mitigation strategies were used to address them? Eg. The model does not take into account user history when making judgments about toxicity, due to privacy concerns. ethical_considerations \u00b6 class ModelEthicalConsiderations ( ModelCardInfo ): | ... | ethical_considerations : Optional [ str ] = None ModelCaveatsAndRecommendations \u00b6 @dataclass ( frozen = True ) class ModelCaveatsAndRecommendations ( ModelCardInfo ) This lists any additional concerns. For instance, were any relevant groups not present in the evaluation data? Eg. The evaluation data is synthetically designed to be representative of common use cases and concerns, but may not be comprehensive. caveats_and_recommendations \u00b6 class ModelCaveatsAndRecommendations ( ModelCardInfo ): | ... | caveats_and_recommendations : Optional [ str ] = None ModelUsage \u00b6 class ModelUsage ( ModelCardInfo ): | def __init__ ( | self , | archive_file : Optional [ str ] = None , | training_config : Optional [ str ] = None , | install_instructions : Optional [ str ] = None , | overrides : Optional [ Dict ] = None | ) archive_file : str , optional The location of model's pretrained weights. training_config : str , optional A url to the training config. install_instructions : str , optional Any additional instructions for installations. overrides : Dict , optional Optional overrides for the model's architecture. ModelCard \u00b6 class ModelCard ( ModelCardInfo ): | def __init__ ( | self , | id : str , | registered_model_name : Optional [ str ] = None , | model_class : Optional [ Callable [ ... , Model ]] = None , | registered_predictor_name : Optional [ str ] = None , | display_name : Optional [ str ] = None , | task_id : Optional [ str ] = None , | model_usage : Optional [ Union [ str , ModelUsage ]] = None , | model_details : Optional [ Union [ str , ModelDetails ]] = None , | intended_use : Optional [ Union [ str , IntendedUse ]] = None , | factors : Optional [ Union [ str , Factors ]] = None , | metrics : Optional [ Union [ str , Metrics ]] = None , | evaluation_data : Optional [ Union [ str , EvaluationData ]] = None , | training_data : Optional [ Union [ str , TrainingData ]] = None , | quantitative_analyses : Optional [ Union [ str , QuantitativeAnalyses ]] = None , | model_ethical_considerations : Optional [ Union [ str , ModelEthicalConsiderations ]] = None , | model_caveats_and_recommendations : Optional [ | Union [ str , ModelCaveatsAndRecommendations ] | ] = None | ) The model card stores the recommended attributes for model reporting. Parameters \u00b6 id : str Model's id, following the convention of task-model-relevant-details. Example: rc-bidaf-elmo for a reading comprehension BiDAF model using ELMo embeddings. registered_model_name : str , optional The model's registered name. If model_class is not given, this will be used to find any available Model registered with this name. model_class : type , optional If given, the ModelCard will pull some default information from the class. registered_predictor_name : str , optional The registered name of the corresponding predictor. display_name : str , optional The pretrained model's display name. task_id : str , optional The id of the task for which the model was built. model_usage : Union[ModelUsage, str] , optional model_details : Union[ModelDetails, str] , optional intended_use : Union[IntendedUse, str] , optional factors : Union[Factors, str] , optional metrics : Union[Metrics, str] , optional evaluation_data : Union[EvaluationData, str] , optional quantitative_analyses : Union[QuantitativeAnalyses, str] , optional ethical_considerations : Union[ModelEthicalConsiderations, str] , optional caveats_and_recommendations : Union[ModelCaveatsAndRecommendations, str] , optional Note For all the fields that are Union[ModelCardInfo, str] , a str input will be treated as the first argument of the relevant constructor. to_dict \u00b6 class ModelCard ( ModelCardInfo ): | ... | def to_dict ( self ) -> Dict [ str , Any ] Converts the ModelCard to a flat dictionary object. This can be converted to json and passed to any front-end.","title":"model_card"},{"location":"api/common/model_card/#get_description","text":"def get_description ( model_class ) Returns the model's description from the docstring.","title":"get_description"},{"location":"api/common/model_card/#modelcardinfo","text":"class ModelCardInfo ( FromParams )","title":"ModelCardInfo"},{"location":"api/common/model_card/#to_dict","text":"class ModelCardInfo ( FromParams ): | ... | def to_dict ( self ) Only the non-empty attributes are returned, to minimize empty values.","title":"to_dict"},{"location":"api/common/model_card/#paper","text":"@dataclass ( frozen = True ) class Paper ( ModelCardInfo ) This provides information about the paper.","title":"Paper"},{"location":"api/common/model_card/#title","text":"class Paper ( ModelCardInfo ): | ... | title : Optional [ str ] = None","title":"title"},{"location":"api/common/model_card/#url","text":"class Paper ( ModelCardInfo ): | ... | url : Optional [ str ] = None","title":"url"},{"location":"api/common/model_card/#citation","text":"class Paper ( ModelCardInfo ): | ... | citation : Optional [ str ] = None","title":"citation"},{"location":"api/common/model_card/#modeldetails","text":"class ModelDetails ( ModelCardInfo ): | def __init__ ( | self , | description : Optional [ str ] = None , | short_description : Optional [ str ] = None , | developed_by : Optional [ str ] = None , | contributed_by : Optional [ str ] = None , | date : Optional [ str ] = None , | version : Optional [ str ] = None , | model_type : Optional [ str ] = None , | paper : Optional [ Union [ str , Dict , Paper ]] = None , | license : Optional [ str ] = None , | contact : Optional [ str ] = None | ) This provides the basic information about the model.","title":"ModelDetails"},{"location":"api/common/model_card/#intendeduse","text":"@dataclass ( frozen = True ) class IntendedUse ( ModelCardInfo ) This determines what the model should and should not be used for.","title":"IntendedUse"},{"location":"api/common/model_card/#primary_uses","text":"class IntendedUse ( ModelCardInfo ): | ... | primary_uses : Optional [ str ] = None","title":"primary_uses"},{"location":"api/common/model_card/#primary_users","text":"class IntendedUse ( ModelCardInfo ): | ... | primary_users : Optional [ str ] = None","title":"primary_users"},{"location":"api/common/model_card/#out_of_scope_use_cases","text":"class IntendedUse ( ModelCardInfo ): | ... | out_of_scope_use_cases : Optional [ str ] = None","title":"out_of_scope_use_cases"},{"location":"api/common/model_card/#factors","text":"@dataclass ( frozen = True ) class Factors ( ModelCardInfo ) This provides a summary of relevant factors such as demographics, instrumentation used, etc. for which the model performance may vary.","title":"Factors"},{"location":"api/common/model_card/#relevant_factors","text":"class Factors ( ModelCardInfo ): | ... | relevant_factors : Optional [ str ] = None","title":"relevant_factors"},{"location":"api/common/model_card/#evaluation_factors","text":"class Factors ( ModelCardInfo ): | ... | evaluation_factors : Optional [ str ] = None","title":"evaluation_factors"},{"location":"api/common/model_card/#metrics","text":"@dataclass ( frozen = True ) class Metrics ( ModelCardInfo ) This lists the reported metrics and the reasons for choosing them.","title":"Metrics"},{"location":"api/common/model_card/#model_performance_measures","text":"class Metrics ( ModelCardInfo ): | ... | model_performance_measures : Optional [ str ] = None","title":"model_performance_measures"},{"location":"api/common/model_card/#decision_thresholds","text":"class Metrics ( ModelCardInfo ): | ... | decision_thresholds : Optional [ str ] = None","title":"decision_thresholds"},{"location":"api/common/model_card/#variation_approaches","text":"class Metrics ( ModelCardInfo ): | ... | variation_approaches : Optional [ str ] = None","title":"variation_approaches"},{"location":"api/common/model_card/#dataset","text":"@dataclass ( frozen = True ) class Dataset ( ModelCardInfo ) This provides basic information about the dataset.","title":"Dataset"},{"location":"api/common/model_card/#name","text":"class Dataset ( ModelCardInfo ): | ... | name : Optional [ str ] = None","title":"name"},{"location":"api/common/model_card/#url_1","text":"class Dataset ( ModelCardInfo ): | ... | url : Optional [ str ] = None","title":"url"},{"location":"api/common/model_card/#processed_url","text":"class Dataset ( ModelCardInfo ): | ... | processed_url : Optional [ str ] = None","title":"processed_url"},{"location":"api/common/model_card/#notes","text":"class Dataset ( ModelCardInfo ): | ... | notes : Optional [ str ] = None","title":"notes"},{"location":"api/common/model_card/#evaluationdata","text":"class EvaluationData ( ModelCardInfo ): | def __init__ ( | self , | dataset : Optional [ Union [ str , Dict , Dataset ]] = None , | motivation : Optional [ str ] = None , | preprocessing : Optional [ str ] = None | ) This provides information about the evaluation data.","title":"EvaluationData"},{"location":"api/common/model_card/#to_dict_1","text":"class EvaluationData ( ModelCardInfo ): | ... | def to_dict ( self )","title":"to_dict"},{"location":"api/common/model_card/#trainingdata","text":"class TrainingData ( ModelCardInfo ): | def __init__ ( | self , | dataset : Optional [ Union [ str , Dict , Dataset ]] = None , | motivation : Optional [ str ] = None , | preprocessing : Optional [ str ] = None | ) This provides information about the training data. If the model was initialized from pretrained weights, a link to the pretrained model's model card/training data can additionally be provided, if available. Any relevant definitions should also be included.","title":"TrainingData"},{"location":"api/common/model_card/#to_dict_2","text":"class TrainingData ( ModelCardInfo ): | ... | def to_dict ( self )","title":"to_dict"},{"location":"api/common/model_card/#quantitativeanalyses","text":"@dataclass ( frozen = True ) class QuantitativeAnalyses ( ModelCardInfo ) This provides disaggregated evaluation of how the model performed based on chosen metrics, with confidence intervals, if possible. Links to plots/figures showing the metrics can also be provided.","title":"QuantitativeAnalyses"},{"location":"api/common/model_card/#unitary_results","text":"class QuantitativeAnalyses ( ModelCardInfo ): | ... | unitary_results : Optional [ str ] = None","title":"unitary_results"},{"location":"api/common/model_card/#intersectional_results","text":"class QuantitativeAnalyses ( ModelCardInfo ): | ... | intersectional_results : Optional [ str ] = None","title":"intersectional_results"},{"location":"api/common/model_card/#modelethicalconsiderations","text":"@dataclass ( frozen = True ) class ModelEthicalConsiderations ( ModelCardInfo ) This highlights any ethical considerations to keep in mind when using the model. Eg. Is the model intended to be used for informing decisions on human life? Does it use sensitive data? What kind of risks are possible, and what mitigation strategies were used to address them? Eg. The model does not take into account user history when making judgments about toxicity, due to privacy concerns.","title":"ModelEthicalConsiderations"},{"location":"api/common/model_card/#ethical_considerations","text":"class ModelEthicalConsiderations ( ModelCardInfo ): | ... | ethical_considerations : Optional [ str ] = None","title":"ethical_considerations"},{"location":"api/common/model_card/#modelcaveatsandrecommendations","text":"@dataclass ( frozen = True ) class ModelCaveatsAndRecommendations ( ModelCardInfo ) This lists any additional concerns. For instance, were any relevant groups not present in the evaluation data? Eg. The evaluation data is synthetically designed to be representative of common use cases and concerns, but may not be comprehensive.","title":"ModelCaveatsAndRecommendations"},{"location":"api/common/model_card/#caveats_and_recommendations","text":"class ModelCaveatsAndRecommendations ( ModelCardInfo ): | ... | caveats_and_recommendations : Optional [ str ] = None","title":"caveats_and_recommendations"},{"location":"api/common/model_card/#modelusage","text":"class ModelUsage ( ModelCardInfo ): | def __init__ ( | self , | archive_file : Optional [ str ] = None , | training_config : Optional [ str ] = None , | install_instructions : Optional [ str ] = None , | overrides : Optional [ Dict ] = None | ) archive_file : str , optional The location of model's pretrained weights. training_config : str , optional A url to the training config. install_instructions : str , optional Any additional instructions for installations. overrides : Dict , optional Optional overrides for the model's architecture.","title":"ModelUsage"},{"location":"api/common/model_card/#modelcard","text":"class ModelCard ( ModelCardInfo ): | def __init__ ( | self , | id : str , | registered_model_name : Optional [ str ] = None , | model_class : Optional [ Callable [ ... , Model ]] = None , | registered_predictor_name : Optional [ str ] = None , | display_name : Optional [ str ] = None , | task_id : Optional [ str ] = None , | model_usage : Optional [ Union [ str , ModelUsage ]] = None , | model_details : Optional [ Union [ str , ModelDetails ]] = None , | intended_use : Optional [ Union [ str , IntendedUse ]] = None , | factors : Optional [ Union [ str , Factors ]] = None , | metrics : Optional [ Union [ str , Metrics ]] = None , | evaluation_data : Optional [ Union [ str , EvaluationData ]] = None , | training_data : Optional [ Union [ str , TrainingData ]] = None , | quantitative_analyses : Optional [ Union [ str , QuantitativeAnalyses ]] = None , | model_ethical_considerations : Optional [ Union [ str , ModelEthicalConsiderations ]] = None , | model_caveats_and_recommendations : Optional [ | Union [ str , ModelCaveatsAndRecommendations ] | ] = None | ) The model card stores the recommended attributes for model reporting.","title":"ModelCard"},{"location":"api/common/model_card/#to_dict_3","text":"class ModelCard ( ModelCardInfo ): | ... | def to_dict ( self ) -> Dict [ str , Any ] Converts the ModelCard to a flat dictionary object. This can be converted to json and passed to any front-end.","title":"to_dict"},{"location":"api/common/params/","text":"allennlp .common .params [SOURCE] infer_and_cast \u00b6 def infer_and_cast ( value : Any ) In some cases we'll be feeding params dicts to functions we don't own; for example, PyTorch optimizers. In that case we can't use pop_int or similar to force casts (which means you can't specify int parameters using environment variables). This function takes something that looks JSON-like and recursively casts things that look like (bool, int, float) to (bool, int, float). T \u00b6 T = TypeVar ( \"T\" , dict , list ) with_overrides \u00b6 def with_overrides ( original : T , overrides_dict : Dict [ str , Any ], prefix : str = \"\" ) -> T parse_overrides \u00b6 def parse_overrides ( serialized_overrides : str , ext_vars : Optional [ Dict [ str , Any ]] = None ) -> Dict [ str , Any ] Params \u00b6 class Params ( MutableMapping ): | def __init__ ( self , params : Dict [ str , Any ], history : str = \"\" ) -> None Represents a parameter dictionary with a history, and contains other functionality around parameter passing and validation for AllenNLP. There are currently two benefits of a Params object over a plain dictionary for parameter passing: We handle a few kinds of parameter validation, including making sure that parameters representing discrete choices actually have acceptable values, and making sure no extra parameters are passed. We log all parameter reads, including default values. This gives a more complete specification of the actual parameters used than is given in a JSON file, because those may not specify what default values were used, whereas this will log them. Consumption The convention for using a Params object in AllenNLP is that you will consume the parameters as you read them, so that there are none left when you've read everything you expect. This lets us easily validate that you didn't pass in any extra parameters, just by making sure that the parameter dictionary is empty. You should do this when you're done handling parameters, by calling Params.assert_empty . DEFAULT \u00b6 class Params ( MutableMapping ): | ... | DEFAULT = object () pop \u00b6 class Params ( MutableMapping ): | ... | def pop ( | self , | key : str , | default : Any = DEFAULT , | keep_as_dict : bool = False | ) -> Any Performs the functionality associated with dict.pop(key), along with checking for returned dictionaries, replacing them with Param objects with an updated history (unless keep_as_dict is True, in which case we leave them as dictionaries). If key is not present in the dictionary, and no default was specified, we raise a ConfigurationError , instead of the typical KeyError . pop_int \u00b6 class Params ( MutableMapping ): | ... | def pop_int ( self , key : str , default : Any = DEFAULT ) -> Optional [ int ] Performs a pop and coerces to an int. pop_float \u00b6 class Params ( MutableMapping ): | ... | def pop_float ( | self , | key : str , | default : Any = DEFAULT | ) -> Optional [ float ] Performs a pop and coerces to a float. pop_bool \u00b6 class Params ( MutableMapping ): | ... | def pop_bool ( self , key : str , default : Any = DEFAULT ) -> Optional [ bool ] Performs a pop and coerces to a bool. get \u00b6 class Params ( MutableMapping ): | ... | def get ( self , key : str , default : Any = DEFAULT ) Performs the functionality associated with dict.get(key) but also checks for returned dicts and returns a Params object in their place with an updated history. pop_choice \u00b6 class Params ( MutableMapping ): | ... | def pop_choice ( | self , | key : str , | choices : List [ Any ], | default_to_first_choice : bool = False , | allow_class_names : bool = True | ) -> Any Gets the value of key in the params dictionary, ensuring that the value is one of the given choices. Note that this pops the key from params, modifying the dictionary, consistent with how parameters are processed in this codebase. Parameters \u00b6 key : str Key to get the value from in the param dictionary choices : List[Any] A list of valid options for values corresponding to key . For example, if you're specifying the type of encoder to use for some part of your model, the choices might be the list of encoder classes we know about and can instantiate. If the value we find in the param dictionary is not in choices , we raise a ConfigurationError , because the user specified an invalid value in their parameter file. default_to_first_choice : bool , optional (default = False ) If this is True , we allow the key to not be present in the parameter dictionary. If the key is not present, we will use the return as the value the first choice in the choices list. If this is False , we raise a ConfigurationError , because specifying the key is required (e.g., you have to specify your model class when running an experiment, but you can feel free to use default settings for encoders if you want). allow_class_names : bool , optional (default = True ) If this is True , then we allow unknown choices that look like fully-qualified class names. This is to allow e.g. specifying a model type as my_library.my_model.MyModel and importing it on the fly. Our check for \"looks like\" is extremely lenient and consists of checking that the value contains a '.'. as_dict \u00b6 class Params ( MutableMapping ): | ... | def as_dict ( | self , | quiet : bool = False , | infer_type_and_cast : bool = False | ) Sometimes we need to just represent the parameters as a dict, for instance when we pass them to PyTorch code. Parameters \u00b6 quiet : bool , optional (default = False ) Whether to log the parameters before returning them as a dict. infer_type_and_cast : bool , optional (default = False ) If True, we infer types and cast (e.g. things that look like floats to floats). as_flat_dict \u00b6 class Params ( MutableMapping ): | ... | def as_flat_dict ( self ) -> Dict [ str , Any ] Returns the parameters of a flat dictionary from keys to values. Nested structure is collapsed with periods. duplicate \u00b6 class Params ( MutableMapping ): | ... | def duplicate ( self ) -> \"Params\" Uses copy.deepcopy() to create a duplicate (but fully distinct) copy of these Params. assert_empty \u00b6 class Params ( MutableMapping ): | ... | def assert_empty ( self , class_name : str ) Raises a ConfigurationError if self.params is not empty. We take class_name as an argument so that the error message gives some idea of where an error happened, if there was one. class_name should be the name of the calling class, the one that got extra parameters (if there are any). __iter__ \u00b6 class Params ( MutableMapping ): | ... | def __iter__ ( self ) from_file \u00b6 class Params ( MutableMapping ): | ... | @classmethod | def from_file ( | cls , | params_file : Union [ str , PathLike ], | params_overrides : Union [ str , Dict [ str , Any ]] = \"\" , | ext_vars : dict = None | ) -> \"Params\" Load a Params object from a configuration file. Parameters \u00b6 params_file : str The path to the configuration file to load. params_overrides : Union[str, Dict[str, Any]] , optional (default = \"\" ) A dict of overrides that can be applied to final object. e.g. {\"model.embedding_dim\": 10} will change the value of \"embedding_dim\" within the \"model\" object of the config to 10. If you wanted to override the entire \"model\" object of the config, you could do {\"model\": {\"type\": \"other_type\", ...}} . ext_vars : dict , optional Our config files are Jsonnet, which allows specifying external variables for later substitution. Typically we substitute these using environment variables; however, you can also specify them here, in which case they take priority over environment variables. e.g. {\"HOME_DIR\": \"/Users/allennlp/home\"} to_file \u00b6 class Params ( MutableMapping ): | ... | def to_file ( | self , | params_file : str , | preference_orders : List [ List [ str ]] = None | ) -> None as_ordered_dict \u00b6 class Params ( MutableMapping ): | ... | def as_ordered_dict ( | self , | preference_orders : List [ List [ str ]] = None | ) -> OrderedDict Returns Ordered Dict of Params from list of partial order preferences. Parameters \u00b6 preference_orders : List[List[str]] , optional preference_orders is list of partial preference orders. [\"A\", \"B\", \"C\"] means \"A\" > \"B\" > \"C\". For multiple preference_orders first will be considered first. Keys not found, will have last but alphabetical preference. Default Preferences: [[\"dataset_reader\", \"iterator\", \"model\", \"train_data_path\", \"validation_data_path\", \"test_data_path\", \"trainer\", \"vocabulary\"], [\"type\"]] get_hash \u00b6 class Params ( MutableMapping ): | ... | def get_hash ( self ) -> str Returns a hash code representing the current state of this Params object. We don't want to implement __hash__ because that has deeper python implications (and this is a mutable object), but this will give you a representation of the current state. We use zlib.adler32 instead of Python's builtin hash because the random seed for the latter is reset on each new program invocation, as discussed here: https://stackoverflow.com/questions/27954892/deterministic-hashing-in-python-3. pop_choice \u00b6 def pop_choice ( params : Dict [ str , Any ], key : str , choices : List [ Any ], default_to_first_choice : bool = False , history : str = \"?.\" , allow_class_names : bool = True ) -> Any Performs the same function as Params.pop_choice , but is required in order to deal with places that the Params object is not welcome, such as inside Keras layers. See the docstring of that method for more detail on how this function works. This method adds a history parameter, in the off-chance that you know it, so that we can reproduce Params.pop_choice exactly. We default to using \"?.\" if you don't know the history, so you'll have to fix that in the log if you want to actually recover the logged parameters. remove_keys_from_params \u00b6 def remove_keys_from_params ( params : Params , keys : List [ str ] = [ \"pretrained_file\" , \"initializer\" ] )","title":"params"},{"location":"api/common/params/#infer_and_cast","text":"def infer_and_cast ( value : Any ) In some cases we'll be feeding params dicts to functions we don't own; for example, PyTorch optimizers. In that case we can't use pop_int or similar to force casts (which means you can't specify int parameters using environment variables). This function takes something that looks JSON-like and recursively casts things that look like (bool, int, float) to (bool, int, float).","title":"infer_and_cast"},{"location":"api/common/params/#t","text":"T = TypeVar ( \"T\" , dict , list )","title":"T"},{"location":"api/common/params/#with_overrides","text":"def with_overrides ( original : T , overrides_dict : Dict [ str , Any ], prefix : str = \"\" ) -> T","title":"with_overrides"},{"location":"api/common/params/#parse_overrides","text":"def parse_overrides ( serialized_overrides : str , ext_vars : Optional [ Dict [ str , Any ]] = None ) -> Dict [ str , Any ]","title":"parse_overrides"},{"location":"api/common/params/#params","text":"class Params ( MutableMapping ): | def __init__ ( self , params : Dict [ str , Any ], history : str = \"\" ) -> None Represents a parameter dictionary with a history, and contains other functionality around parameter passing and validation for AllenNLP. There are currently two benefits of a Params object over a plain dictionary for parameter passing: We handle a few kinds of parameter validation, including making sure that parameters representing discrete choices actually have acceptable values, and making sure no extra parameters are passed. We log all parameter reads, including default values. This gives a more complete specification of the actual parameters used than is given in a JSON file, because those may not specify what default values were used, whereas this will log them. Consumption The convention for using a Params object in AllenNLP is that you will consume the parameters as you read them, so that there are none left when you've read everything you expect. This lets us easily validate that you didn't pass in any extra parameters, just by making sure that the parameter dictionary is empty. You should do this when you're done handling parameters, by calling Params.assert_empty .","title":"Params"},{"location":"api/common/params/#default","text":"class Params ( MutableMapping ): | ... | DEFAULT = object ()","title":"DEFAULT"},{"location":"api/common/params/#pop","text":"class Params ( MutableMapping ): | ... | def pop ( | self , | key : str , | default : Any = DEFAULT , | keep_as_dict : bool = False | ) -> Any Performs the functionality associated with dict.pop(key), along with checking for returned dictionaries, replacing them with Param objects with an updated history (unless keep_as_dict is True, in which case we leave them as dictionaries). If key is not present in the dictionary, and no default was specified, we raise a ConfigurationError , instead of the typical KeyError .","title":"pop"},{"location":"api/common/params/#pop_int","text":"class Params ( MutableMapping ): | ... | def pop_int ( self , key : str , default : Any = DEFAULT ) -> Optional [ int ] Performs a pop and coerces to an int.","title":"pop_int"},{"location":"api/common/params/#pop_float","text":"class Params ( MutableMapping ): | ... | def pop_float ( | self , | key : str , | default : Any = DEFAULT | ) -> Optional [ float ] Performs a pop and coerces to a float.","title":"pop_float"},{"location":"api/common/params/#pop_bool","text":"class Params ( MutableMapping ): | ... | def pop_bool ( self , key : str , default : Any = DEFAULT ) -> Optional [ bool ] Performs a pop and coerces to a bool.","title":"pop_bool"},{"location":"api/common/params/#get","text":"class Params ( MutableMapping ): | ... | def get ( self , key : str , default : Any = DEFAULT ) Performs the functionality associated with dict.get(key) but also checks for returned dicts and returns a Params object in their place with an updated history.","title":"get"},{"location":"api/common/params/#pop_choice","text":"class Params ( MutableMapping ): | ... | def pop_choice ( | self , | key : str , | choices : List [ Any ], | default_to_first_choice : bool = False , | allow_class_names : bool = True | ) -> Any Gets the value of key in the params dictionary, ensuring that the value is one of the given choices. Note that this pops the key from params, modifying the dictionary, consistent with how parameters are processed in this codebase.","title":"pop_choice"},{"location":"api/common/params/#as_dict","text":"class Params ( MutableMapping ): | ... | def as_dict ( | self , | quiet : bool = False , | infer_type_and_cast : bool = False | ) Sometimes we need to just represent the parameters as a dict, for instance when we pass them to PyTorch code.","title":"as_dict"},{"location":"api/common/params/#as_flat_dict","text":"class Params ( MutableMapping ): | ... | def as_flat_dict ( self ) -> Dict [ str , Any ] Returns the parameters of a flat dictionary from keys to values. Nested structure is collapsed with periods.","title":"as_flat_dict"},{"location":"api/common/params/#duplicate","text":"class Params ( MutableMapping ): | ... | def duplicate ( self ) -> \"Params\" Uses copy.deepcopy() to create a duplicate (but fully distinct) copy of these Params.","title":"duplicate"},{"location":"api/common/params/#assert_empty","text":"class Params ( MutableMapping ): | ... | def assert_empty ( self , class_name : str ) Raises a ConfigurationError if self.params is not empty. We take class_name as an argument so that the error message gives some idea of where an error happened, if there was one. class_name should be the name of the calling class, the one that got extra parameters (if there are any).","title":"assert_empty"},{"location":"api/common/params/#__iter__","text":"class Params ( MutableMapping ): | ... | def __iter__ ( self )","title":"__iter__"},{"location":"api/common/params/#from_file","text":"class Params ( MutableMapping ): | ... | @classmethod | def from_file ( | cls , | params_file : Union [ str , PathLike ], | params_overrides : Union [ str , Dict [ str , Any ]] = \"\" , | ext_vars : dict = None | ) -> \"Params\" Load a Params object from a configuration file.","title":"from_file"},{"location":"api/common/params/#to_file","text":"class Params ( MutableMapping ): | ... | def to_file ( | self , | params_file : str , | preference_orders : List [ List [ str ]] = None | ) -> None","title":"to_file"},{"location":"api/common/params/#as_ordered_dict","text":"class Params ( MutableMapping ): | ... | def as_ordered_dict ( | self , | preference_orders : List [ List [ str ]] = None | ) -> OrderedDict Returns Ordered Dict of Params from list of partial order preferences.","title":"as_ordered_dict"},{"location":"api/common/params/#get_hash","text":"class Params ( MutableMapping ): | ... | def get_hash ( self ) -> str Returns a hash code representing the current state of this Params object. We don't want to implement __hash__ because that has deeper python implications (and this is a mutable object), but this will give you a representation of the current state. We use zlib.adler32 instead of Python's builtin hash because the random seed for the latter is reset on each new program invocation, as discussed here: https://stackoverflow.com/questions/27954892/deterministic-hashing-in-python-3.","title":"get_hash"},{"location":"api/common/params/#pop_choice_1","text":"def pop_choice ( params : Dict [ str , Any ], key : str , choices : List [ Any ], default_to_first_choice : bool = False , history : str = \"?.\" , allow_class_names : bool = True ) -> Any Performs the same function as Params.pop_choice , but is required in order to deal with places that the Params object is not welcome, such as inside Keras layers. See the docstring of that method for more detail on how this function works. This method adds a history parameter, in the off-chance that you know it, so that we can reproduce Params.pop_choice exactly. We default to using \"?.\" if you don't know the history, so you'll have to fix that in the log if you want to actually recover the logged parameters.","title":"pop_choice"},{"location":"api/common/params/#remove_keys_from_params","text":"def remove_keys_from_params ( params : Params , keys : List [ str ] = [ \"pretrained_file\" , \"initializer\" ] )","title":"remove_keys_from_params"},{"location":"api/common/plugins/","text":"allennlp .common .plugins [SOURCE] Plugin management. \u00b6 AllenNLP supports loading \"plugins\" dynamically. A plugin is just a Python package that provides custom registered classes or additional allennlp subcommands. In order for AllenNLP to find your plugins, you have to create either a local plugins file named .allennlp_plugins in the directory where the allennlp command is run, or a global plugins file at ~/.allennlp/plugins . The file should list the plugin modules that you want to be loaded, one per line. LOCAL_PLUGINS_FILENAME \u00b6 LOCAL_PLUGINS_FILENAME = \".allennlp_plugins\" Local plugin files should have this name. GLOBAL_PLUGINS_FILENAME \u00b6 GLOBAL_PLUGINS_FILENAME = str ( Path . home () / \".allennlp\" / \"plugins\" ) The global plugins file will be found here. DEFAULT_PLUGINS \u00b6 DEFAULT_PLUGINS = ( \"allennlp_models\" , \"allennlp_semparse\" , \"allennlp_server\" ) Default plugins do not need to be declared in a plugins file. They will always be imported when they are installed in the current Python environment. discover_file_plugins \u00b6 def discover_file_plugins ( plugins_filename : str = LOCAL_PLUGINS_FILENAME ) -> Iterable [ str ] Returns an iterable of the plugins found, declared within a file whose path is plugins_filename . discover_plugins \u00b6 def discover_plugins () -> Iterable [ str ] Returns an iterable of the plugins found. import_plugins \u00b6 def import_plugins () -> None Imports the plugins found with discover_plugins() .","title":"plugins"},{"location":"api/common/plugins/#local_plugins_filename","text":"LOCAL_PLUGINS_FILENAME = \".allennlp_plugins\" Local plugin files should have this name.","title":"LOCAL_PLUGINS_FILENAME"},{"location":"api/common/plugins/#global_plugins_filename","text":"GLOBAL_PLUGINS_FILENAME = str ( Path . home () / \".allennlp\" / \"plugins\" ) The global plugins file will be found here.","title":"GLOBAL_PLUGINS_FILENAME"},{"location":"api/common/plugins/#default_plugins","text":"DEFAULT_PLUGINS = ( \"allennlp_models\" , \"allennlp_semparse\" , \"allennlp_server\" ) Default plugins do not need to be declared in a plugins file. They will always be imported when they are installed in the current Python environment.","title":"DEFAULT_PLUGINS"},{"location":"api/common/plugins/#discover_file_plugins","text":"def discover_file_plugins ( plugins_filename : str = LOCAL_PLUGINS_FILENAME ) -> Iterable [ str ] Returns an iterable of the plugins found, declared within a file whose path is plugins_filename .","title":"discover_file_plugins"},{"location":"api/common/plugins/#discover_plugins","text":"def discover_plugins () -> Iterable [ str ] Returns an iterable of the plugins found.","title":"discover_plugins"},{"location":"api/common/plugins/#import_plugins","text":"def import_plugins () -> None Imports the plugins found with discover_plugins() .","title":"import_plugins"},{"location":"api/common/push_to_hf/","text":"allennlp .common .push_to_hf [SOURCE] Utilities for pushing models to the Hugging Face Hub ( hf.co ). README_TEMPLATE \u00b6 README_TEMPLATE = \"\"\"--- tags: - allennlp --- # TODO: Fill this model card \"\"\" push_to_hf \u00b6 def push_to_hf ( repo_name : str , serialization_dir : Optional [ Union [ str , PathLike ]] = None , archive_path : Optional [ Union [ str , PathLike ]] = None , organization : Optional [ str ] = None , commit_message : str = \"Update repository\" , local_repo_path : Union [ str , PathLike ] = \"hub\" , use_auth_token : Union [ bool , str ] = True ) -> str Pushes model and related files to the Hugging Face Hub ( hf.co ) Parameters \u00b6 repo_name : str Name of the repository in the Hugging Face Hub. serialization_dir : Union[str, PathLike] , optional (default = None ) Full path to a directory with the serialized model. archive_path : Union[str, PathLike] , optional (default = None ) Full path to the zipped model (e.g. model/model.tar.gz). Use serialization_dir if possible. organization : Optional[str] , optional (default = None ) Name of organization to which the model should be uploaded. commit_message : str , optional (default = Update repository ) Commit message to use for the push. local_repo_path : Union[str, Path] , optional (default = hub ) Local directory where the repository will be saved. use_auth_token ( str or bool , optional , defaults True ): huggingface_token can be extract from HfApi().login(username, password) and is used to authenticate against the Hugging Face Hub (useful from Google Colab for instance). It's automatically retrieved if you've done huggingface-cli login before.","title":"push_to_hf"},{"location":"api/common/push_to_hf/#readme_template","text":"README_TEMPLATE = \"\"\"--- tags: - allennlp --- # TODO: Fill this model card \"\"\"","title":"README_TEMPLATE"},{"location":"api/common/push_to_hf/#push_to_hf","text":"def push_to_hf ( repo_name : str , serialization_dir : Optional [ Union [ str , PathLike ]] = None , archive_path : Optional [ Union [ str , PathLike ]] = None , organization : Optional [ str ] = None , commit_message : str = \"Update repository\" , local_repo_path : Union [ str , PathLike ] = \"hub\" , use_auth_token : Union [ bool , str ] = True ) -> str Pushes model and related files to the Hugging Face Hub ( hf.co )","title":"push_to_hf"},{"location":"api/common/registrable/","text":"allennlp .common .registrable [SOURCE] allennlp.common.registrable.Registrable is a \"mixin\" for endowing any base class with a named registry for its subclasses and a decorator for registering them. Registrable \u00b6 class Registrable ( FromParams ) Any class that inherits from Registrable gains access to a named registry for its subclasses. To register them, just decorate them with the classmethod @BaseClass.register(name) . After which you can call BaseClass.list_available() to get the keys for the registered subclasses, and BaseClass.by_name(name) to get the corresponding subclass. Note that the registry stores the subclasses themselves; not class instances. In most cases you would then call from_params(params) on the returned subclass. You can specify a default by setting BaseClass.default_implementation . If it is set, it will be the first element of list_available() . Note that if you use this class to implement a new Registrable abstract class, you must ensure that all subclasses of the abstract class are loaded when the module is loaded, because the subclasses register themselves in their respective files. You can achieve this by having the abstract class and all subclasses in the init .py of the module in which they reside (as this causes any import of either the abstract class or a subclass to load all other subclasses and the abstract class). default_implementation \u00b6 class Registrable ( FromParams ): | ... | default_implementation : Optional [ str ] = None register \u00b6 class Registrable ( FromParams ): | ... | @classmethod | def register ( | cls , | name : str , | constructor : Optional [ str ] = None , | exist_ok : bool = False | ) -> Callable [[ Type [ _T ]], Type [ _T ]] Register a class under a particular name. Parameters \u00b6 name : str The name to register the class under. constructor : str , optional (default = None ) The name of the method to use on the class to construct the object. If this is given, we will use this method (which must be a @classmethod ) instead of the default constructor. exist_ok : bool , optional (default = False ) If True, overwrites any existing models registered under name . Else, throws an error if a model is already registered under name . Examples \u00b6 To use this class, you would typically have a base class that inherits from Registrable : class Vocabulary ( Registrable ): ... Then, if you want to register a subclass, you decorate it like this: @Vocabulary . register ( \"my-vocabulary\" ) class MyVocabulary ( Vocabulary ): def __init__ ( self , param1 : int , param2 : str ): ... Registering a class like this will let you instantiate a class from a config file, where you give \"type\": \"my-vocabulary\" , and keys corresponding to the parameters of the __init__ method (note that for this to work, those parameters must have type annotations). If you want to have the instantiation from a config file call a method other than the constructor, either because you have several different construction paths that could be taken for the same object (as we do in Vocabulary ) or because you have logic you want to happen before you get to the constructor (as we do in Embedding ), you can register a specific @classmethod as the constructor to use, like this: @Vocabulary . register ( \"my-vocabulary-from-instances\" , constructor = \"from_instances\" ) @Vocabulary . register ( \"my-vocabulary-from-files\" , constructor = \"from_files\" ) class MyVocabulary ( Vocabulary ): def __init__ ( self , some_params ): ... @classmethod def from_instances ( cls , some_other_params ) -> MyVocabulary : ... # construct some_params from instances return cls ( some_params ) @classmethod def from_files ( cls , still_other_params ) -> MyVocabulary : ... # construct some_params from files return cls ( some_params ) by_name \u00b6 class Registrable ( FromParams ): | ... | @classmethod | def by_name ( | cls : Type [ _RegistrableT ], | name : str | ) -> Callable [ ... , _RegistrableT ] Returns a callable function that constructs an argument of the registered class. Because you can register particular functions as constructors for specific names, this isn't necessarily the __init__ method of some class. resolve_class_name \u00b6 class Registrable ( FromParams ): | ... | @classmethod | def resolve_class_name ( | cls : Type [ _RegistrableT ], | name : str | ) -> Tuple [ Type [ _RegistrableT ], Optional [ str ]] Returns the subclass that corresponds to the given name , along with the name of the method that was registered as a constructor for that name , if any. This method also allows name to be a fully-specified module name, instead of a name that was already added to the Registry . In that case, you cannot use a separate function as a constructor (as you need to call cls.register() in order to tell us what separate function to use). list_available \u00b6 class Registrable ( FromParams ): | ... | @classmethod | def list_available ( cls ) -> List [ str ] List default first if it exists","title":"registrable"},{"location":"api/common/registrable/#registrable","text":"class Registrable ( FromParams ) Any class that inherits from Registrable gains access to a named registry for its subclasses. To register them, just decorate them with the classmethod @BaseClass.register(name) . After which you can call BaseClass.list_available() to get the keys for the registered subclasses, and BaseClass.by_name(name) to get the corresponding subclass. Note that the registry stores the subclasses themselves; not class instances. In most cases you would then call from_params(params) on the returned subclass. You can specify a default by setting BaseClass.default_implementation . If it is set, it will be the first element of list_available() . Note that if you use this class to implement a new Registrable abstract class, you must ensure that all subclasses of the abstract class are loaded when the module is loaded, because the subclasses register themselves in their respective files. You can achieve this by having the abstract class and all subclasses in the init .py of the module in which they reside (as this causes any import of either the abstract class or a subclass to load all other subclasses and the abstract class).","title":"Registrable"},{"location":"api/common/registrable/#default_implementation","text":"class Registrable ( FromParams ): | ... | default_implementation : Optional [ str ] = None","title":"default_implementation"},{"location":"api/common/registrable/#register","text":"class Registrable ( FromParams ): | ... | @classmethod | def register ( | cls , | name : str , | constructor : Optional [ str ] = None , | exist_ok : bool = False | ) -> Callable [[ Type [ _T ]], Type [ _T ]] Register a class under a particular name.","title":"register"},{"location":"api/common/registrable/#by_name","text":"class Registrable ( FromParams ): | ... | @classmethod | def by_name ( | cls : Type [ _RegistrableT ], | name : str | ) -> Callable [ ... , _RegistrableT ] Returns a callable function that constructs an argument of the registered class. Because you can register particular functions as constructors for specific names, this isn't necessarily the __init__ method of some class.","title":"by_name"},{"location":"api/common/registrable/#resolve_class_name","text":"class Registrable ( FromParams ): | ... | @classmethod | def resolve_class_name ( | cls : Type [ _RegistrableT ], | name : str | ) -> Tuple [ Type [ _RegistrableT ], Optional [ str ]] Returns the subclass that corresponds to the given name , along with the name of the method that was registered as a constructor for that name , if any. This method also allows name to be a fully-specified module name, instead of a name that was already added to the Registry . In that case, you cannot use a separate function as a constructor (as you need to call cls.register() in order to tell us what separate function to use).","title":"resolve_class_name"},{"location":"api/common/registrable/#list_available","text":"class Registrable ( FromParams ): | ... | @classmethod | def list_available ( cls ) -> List [ str ] List default first if it exists","title":"list_available"},{"location":"api/common/sequences/","text":"allennlp .common .sequences [SOURCE] ShuffledSequence \u00b6 class ShuffledSequence ( abc . Sequence ): | def __init__ ( | self , | inner_sequence : Sequence , | indices : Optional [ Sequence [ int ]] = None | ) Produces a shuffled view of a sequence, such as a list. This assumes that the inner sequence never changes. If it does, the results are undefined. SlicedSequence \u00b6 class SlicedSequence ( ShuffledSequence ): | def __init__ ( self , inner_sequence : Sequence , s : slice ) Produces a sequence that's a slice into another sequence, without copying the elements. This assumes that the inner sequence never changes. If it does, the results are undefined. ConcatenatedSequence \u00b6 class ConcatenatedSequence ( abc . Sequence ): | def __init__ ( self , * sequences : Sequence ) Produces a sequence that's the concatenation of multiple other sequences, without copying the elements. This assumes that the inner sequence never changes. If it does, the results are undefined.","title":"sequences"},{"location":"api/common/sequences/#shuffledsequence","text":"class ShuffledSequence ( abc . Sequence ): | def __init__ ( | self , | inner_sequence : Sequence , | indices : Optional [ Sequence [ int ]] = None | ) Produces a shuffled view of a sequence, such as a list. This assumes that the inner sequence never changes. If it does, the results are undefined.","title":"ShuffledSequence"},{"location":"api/common/sequences/#slicedsequence","text":"class SlicedSequence ( ShuffledSequence ): | def __init__ ( self , inner_sequence : Sequence , s : slice ) Produces a sequence that's a slice into another sequence, without copying the elements. This assumes that the inner sequence never changes. If it does, the results are undefined.","title":"SlicedSequence"},{"location":"api/common/sequences/#concatenatedsequence","text":"class ConcatenatedSequence ( abc . Sequence ): | def __init__ ( self , * sequences : Sequence ) Produces a sequence that's the concatenation of multiple other sequences, without copying the elements. This assumes that the inner sequence never changes. If it does, the results are undefined.","title":"ConcatenatedSequence"},{"location":"api/common/task_card/","text":"allennlp .common .task_card [SOURCE] A specification for defining task cards (derived from model cards). Motivation: A model's capabilities and limitations are dependent on the task definition. Thus, it is helpful to separate the information in the model card that comes from specifically the task itself. ExampleCategory \u00b6 @dataclass ( frozen = True ) class ExampleCategory ( FromParams ) category \u00b6 class ExampleCategory ( FromParams ): | ... | category : str = None examples \u00b6 class ExampleCategory ( FromParams ): | ... | examples : List [ Dict [ str , str ]] = None TaskCard \u00b6 @dataclass ( frozen = True ) class TaskCard ( FromParams ) The TaskCard stores information about the task. It is modeled after the ModelCard . Parameters \u00b6 id : str The task id. Example: \"rc\" for reading comprehension. name : str , optional The (display) name of the task. description : str , optional Description of the task. Example: \"Textual Entailment (TE) is the task of predicting whether, for a pair of sentences, the facts in the first sentence necessarily imply the facts in the second.\" expected_inputs : str , optional All expected inputs and their format. Example: (For a reading comprehension task) Passage (text string), Question (text string) expected_outputs : str , optional All expected outputs and their format. Example: (For a reading comprehension task) Answer span (start token position and end token position). examples : Union[List[Dict[str, str]], Dict[str, List[Dict[str, str]]]] , optional List of examples for the task. Each example dict should contain as keys the expected_inputs . Example: (For textual entailment) [{\"premise\": \"A handmade djembe was on display at the Smithsonian.\", \"hypothesis\": \"Visitors could see the djembe.\"}] scope_and_limitations : str , optional This discusses the scope of the task based on how it is defined, and any limitations. Example: \"The Textual Entailment task is in some sense \"NLP-complete\", and you should not expect any current model to cover every possible aspect of entailment. Instead, you should think about what the model was trained on to see whether it could reasonably capture the phenomena that you are querying it with.\" id \u00b6 class TaskCard ( FromParams ): | ... | id : str = None name \u00b6 class TaskCard ( FromParams ): | ... | name : Optional [ str ] = None description \u00b6 class TaskCard ( FromParams ): | ... | description : Optional [ str ] = None expected_inputs \u00b6 class TaskCard ( FromParams ): | ... | expected_inputs : Optional [ str ] = None expected_outputs \u00b6 class TaskCard ( FromParams ): | ... | expected_outputs : Optional [ str ] = None scope_and_limitations \u00b6 class TaskCard ( FromParams ): | ... | scope_and_limitations : Optional [ str ] = None examples \u00b6 class TaskCard ( FromParams ): | ... | examples : Optional [ Union [ List [ Dict [ str , str ]], List [ ExampleCategory ]]] = None","title":"task_card"},{"location":"api/common/task_card/#examplecategory","text":"@dataclass ( frozen = True ) class ExampleCategory ( FromParams )","title":"ExampleCategory"},{"location":"api/common/task_card/#category","text":"class ExampleCategory ( FromParams ): | ... | category : str = None","title":"category"},{"location":"api/common/task_card/#examples","text":"class ExampleCategory ( FromParams ): | ... | examples : List [ Dict [ str , str ]] = None","title":"examples"},{"location":"api/common/task_card/#taskcard","text":"@dataclass ( frozen = True ) class TaskCard ( FromParams ) The TaskCard stores information about the task. It is modeled after the ModelCard .","title":"TaskCard"},{"location":"api/common/task_card/#id","text":"class TaskCard ( FromParams ): | ... | id : str = None","title":"id"},{"location":"api/common/task_card/#name","text":"class TaskCard ( FromParams ): | ... | name : Optional [ str ] = None","title":"name"},{"location":"api/common/task_card/#description","text":"class TaskCard ( FromParams ): | ... | description : Optional [ str ] = None","title":"description"},{"location":"api/common/task_card/#expected_inputs","text":"class TaskCard ( FromParams ): | ... | expected_inputs : Optional [ str ] = None","title":"expected_inputs"},{"location":"api/common/task_card/#expected_outputs","text":"class TaskCard ( FromParams ): | ... | expected_outputs : Optional [ str ] = None","title":"expected_outputs"},{"location":"api/common/task_card/#scope_and_limitations","text":"class TaskCard ( FromParams ): | ... | scope_and_limitations : Optional [ str ] = None","title":"scope_and_limitations"},{"location":"api/common/task_card/#examples_1","text":"class TaskCard ( FromParams ): | ... | examples : Optional [ Union [ List [ Dict [ str , str ]], List [ ExampleCategory ]]] = None","title":"examples"},{"location":"api/common/tqdm/","text":"allennlp .common .tqdm [SOURCE] allennlp.common.tqdm.Tqdm wraps tqdm so we can add configurable global defaults for certain tqdm parameters. logger.propagate \u00b6 logger . propagate = False replace_cr_with_newline \u00b6 def replace_cr_with_newline ( message : str ) -> str TQDM and requests use carriage returns to get the training line to update for each batch without adding more lines to the terminal output. Displaying those in a file won't work correctly, so we'll just make sure that each batch shows up on its one line. TqdmToLogsWriter \u00b6 class TqdmToLogsWriter ( object ): | def __init__ ( self ) write \u00b6 class TqdmToLogsWriter ( object ): | ... | def write ( self , message ) flush \u00b6 class TqdmToLogsWriter ( object ): | ... | def flush ( self ) Tqdm \u00b6 class Tqdm tqdm \u00b6 class Tqdm : | ... | @staticmethod | def tqdm ( * args , ** kwargs ) set_lock \u00b6 class Tqdm : | ... | @staticmethod | def set_lock ( lock ) get_lock \u00b6 class Tqdm : | ... | @staticmethod | def get_lock ()","title":"tqdm"},{"location":"api/common/tqdm/#loggerpropagate","text":"logger . propagate = False","title":"logger.propagate"},{"location":"api/common/tqdm/#replace_cr_with_newline","text":"def replace_cr_with_newline ( message : str ) -> str TQDM and requests use carriage returns to get the training line to update for each batch without adding more lines to the terminal output. Displaying those in a file won't work correctly, so we'll just make sure that each batch shows up on its one line.","title":"replace_cr_with_newline"},{"location":"api/common/tqdm/#tqdmtologswriter","text":"class TqdmToLogsWriter ( object ): | def __init__ ( self )","title":"TqdmToLogsWriter"},{"location":"api/common/tqdm/#write","text":"class TqdmToLogsWriter ( object ): | ... | def write ( self , message )","title":"write"},{"location":"api/common/tqdm/#flush","text":"class TqdmToLogsWriter ( object ): | ... | def flush ( self )","title":"flush"},{"location":"api/common/tqdm/#tqdm","text":"class Tqdm","title":"Tqdm"},{"location":"api/common/tqdm/#tqdm_1","text":"class Tqdm : | ... | @staticmethod | def tqdm ( * args , ** kwargs )","title":"tqdm"},{"location":"api/common/tqdm/#set_lock","text":"class Tqdm : | ... | @staticmethod | def set_lock ( lock )","title":"set_lock"},{"location":"api/common/tqdm/#get_lock","text":"class Tqdm : | ... | @staticmethod | def get_lock ()","title":"get_lock"},{"location":"api/common/util/","text":"allennlp .common .util [SOURCE] Various utilities that don't fit anywhere else. JsonDict \u00b6 JsonDict = Dict [ str , Any ] START_SYMBOL \u00b6 START_SYMBOL = \"@start@\" END_SYMBOL \u00b6 END_SYMBOL = \"@end@\" PathType \u00b6 PathType = Union [ os . PathLike , str ] T \u00b6 T = TypeVar ( \"T\" ) ContextManagerFunctionReturnType \u00b6 ContextManagerFunctionReturnType = Generator [ T , None , None ] sanitize \u00b6 def sanitize ( x : Any ) -> Any Sanitize turns PyTorch and Numpy types into basic Python types so they can be serialized into JSON. group_by_count \u00b6 def group_by_count ( iterable : List [ Any ], count : int , default_value : Any ) -> List [ List [ Any ]] Takes a list and groups it into sublists of size count , using default_value to pad the list at the end if the list is not divisable by count . For example: >>> group_by_count([1, 2, 3, 4, 5, 6, 7], 3, 0) [[1, 2, 3], [4, 5, 6], [7, 0, 0]] This is a short method, but it's complicated and hard to remember as a one-liner, so we just make a function out of it. A \u00b6 A = TypeVar ( \"A\" ) lazy_groups_of \u00b6 def lazy_groups_of ( iterable : Iterable [ A ], group_size : int ) -> Iterator [ List [ A ]] Takes an iterable and batches the individual instances into lists of the specified size. The last list may be smaller if there are instances left over. pad_sequence_to_length \u00b6 def pad_sequence_to_length ( sequence : Sequence , desired_length : int , default_value : Callable [[], Any ] = lambda : 0 , padding_on_right : bool = True ) -> List Take a list of objects and pads it to the desired length, returning the padded list. The original list is not modified. Parameters \u00b6 sequence : List A list of objects to be padded. desired_length : int Maximum length of each sequence. Longer sequences are truncated to this length, and shorter ones are padded to it. default_value : Callable , optional (default = lambda: 0 ) Callable that outputs a default value (of any type) to use as padding values. This is a lambda to avoid using the same object when the default value is more complex, like a list. padding_on_right : bool , optional (default = True ) When we add padding tokens (or truncate the sequence), should we do it on the right or the left? Returns \u00b6 padded_sequence : List add_noise_to_dict_values \u00b6 def add_noise_to_dict_values ( dictionary : Dict [ A , float ], noise_param : float ) -> Dict [ A , float ] Returns a new dictionary with noise added to every key in dictionary . The noise is uniformly distributed within noise_param percent of the value for every value in the dictionary. namespace_match \u00b6 def namespace_match ( pattern : str , namespace : str ) Matches a namespace pattern against a namespace string. For example, *tags matches passage_tags and question_tags and tokens matches tokens but not stemmed_tokens . prepare_environment \u00b6 def prepare_environment ( params : Params ) Sets random seeds for reproducible experiments. This may not work as expected if you use this from within a python project in which you have already imported Pytorch. If you use the scripts/run_model.py entry point to training models with this library, your experiments should be reasonably reproducible. If you are using this from your own project, you will want to call this function before importing Pytorch. Complete determinism is very difficult to achieve with libraries doing optimized linear algebra due to massively parallel execution, which is exacerbated by using GPUs. Parameters \u00b6 params : Params A Params object or dict holding the json parameters. LOADED_SPACY_MODELS \u00b6 LOADED_SPACY_MODELS : Dict [ Tuple [ str , bool , bool , bool ], SpacyModelType ] = {} get_spacy_model \u00b6 def get_spacy_model ( spacy_model_name : str , pos_tags : bool = True , parse : bool = False , ner : bool = False ) -> SpacyModelType In order to avoid loading spacy models a whole bunch of times, we'll save references to them, keyed by the options we used to create the spacy model, so any particular configuration only gets loaded once. pushd \u00b6 @contextmanager def pushd ( new_dir : PathType , verbose : bool = False ) -> ContextManagerFunctionReturnType [ None ] Changes the current directory to the given path and prepends it to sys.path . This method is intended to use with with , so after its usage, the current directory will be set to the previous value. push_python_path \u00b6 @contextmanager def push_python_path ( path : PathType ) -> ContextManagerFunctionReturnType [ None ] Prepends the given path to sys.path . This method is intended to use with with , so after its usage, its value willbe removed from sys.path . import_module_and_submodules \u00b6 def import_module_and_submodules ( package_name : str , exclude : Optional [ Set [ str ]] = None ) -> None Import all public submodules under the given package. Primarily useful so that people using AllenNLP as a library can specify their own custom packages and have their custom classes get loaded and registered. peak_cpu_memory \u00b6 def peak_cpu_memory () -> Dict [ int , int ] Get peak memory usage for each worker, as measured by max-resident-set size: https://unix.stackexchange.com/questions/30940/getrusage-system-call-what-is-maximum-resident-set-size Only works on OSX and Linux, otherwise the result will be 0.0 for every worker. peak_gpu_memory \u00b6 def peak_gpu_memory () -> Dict [ int , int ] Get the peak GPU memory usage in bytes by device. Returns \u00b6 Dict[int, int] Keys are device ids as integers. Values are memory usage as integers in bytes. Returns an empty dict if GPUs are not available. ensure_list \u00b6 def ensure_list ( iterable : Iterable [ A ]) -> List [ A ] An Iterable may be a list or a generator. This ensures we get a list without making an unnecessary copy. is_lazy \u00b6 def is_lazy ( iterable : Iterable [ A ]) -> bool Checks if the given iterable is lazy, which here just means it's not a list. int_to_device \u00b6 def int_to_device ( device : Union [ int , torch . device ]) -> torch . device log_frozen_and_tunable_parameter_names \u00b6 def log_frozen_and_tunable_parameter_names ( model : torch . nn . Module ) -> None get_frozen_and_tunable_parameter_names \u00b6 def get_frozen_and_tunable_parameter_names ( model : torch . nn . Module ) -> Tuple [ Iterable [ str ], Iterable [ str ]] dump_metrics \u00b6 def dump_metrics ( file_path : Optional [ str ], metrics : Dict [ str , Any ], log : bool = False ) -> None flatten_filename \u00b6 def flatten_filename ( file_path : str ) -> str is_distributed \u00b6 def is_distributed () -> bool Checks if the distributed process group is available and has been initialized is_global_primary \u00b6 def is_global_primary () -> bool Checks if the distributed process group is the global primary (rank = 0). If the distributed process group is not available or has not been initialized, this trivially returns True . sanitize_wordpiece \u00b6 def sanitize_wordpiece ( wordpiece : str ) -> str Sanitizes wordpieces from BERT, RoBERTa or ALBERT tokenizers. sanitize_ptb_tokenized_string \u00b6 def sanitize_ptb_tokenized_string ( text : str ) -> str Sanitizes string that was tokenized using PTBTokenizer find_open_port \u00b6 def find_open_port () -> int Find a random open port on local host. format_timedelta \u00b6 def format_timedelta ( td : timedelta ) -> str Format a timedelta for humans. format_size \u00b6 def format_size ( size : int ) -> str Format a size (in bytes) for humans. nan_safe_tensor_divide \u00b6 def nan_safe_tensor_divide ( numerator , denominator ) Performs division and handles divide-by-zero. On zero-division, sets the corresponding result elements to zero. shuffle_iterable \u00b6 def shuffle_iterable ( i : Iterable [ T ], pool_size : int = 1024 ) -> Iterable [ T ] cycle_iterator_function \u00b6 def cycle_iterator_function ( iterator_function : Callable [[], Iterable [ T ]] ) -> Iterator [ T ] Functionally equivalent to itertools.cycle(iterator_function()) , but this function does not cache the result of calling the iterator like cycle does. Instead, we just call iterator_function() again whenever we get a StopIteration . This should only be preferred over itertools.cycle in cases where you're sure you don't want the caching behavior that's done in itertools.cycle . hash_object \u00b6 def hash_object ( o : Any ) -> str Returns a character hash code of arbitrary Python objects. SigTermReceived \u00b6 class SigTermReceived ( Exception ) install_sigterm_handler \u00b6 def install_sigterm_handler ()","title":"util"},{"location":"api/common/util/#jsondict","text":"JsonDict = Dict [ str , Any ]","title":"JsonDict"},{"location":"api/common/util/#start_symbol","text":"START_SYMBOL = \"@start@\"","title":"START_SYMBOL"},{"location":"api/common/util/#end_symbol","text":"END_SYMBOL = \"@end@\"","title":"END_SYMBOL"},{"location":"api/common/util/#pathtype","text":"PathType = Union [ os . PathLike , str ]","title":"PathType"},{"location":"api/common/util/#t","text":"T = TypeVar ( \"T\" )","title":"T"},{"location":"api/common/util/#contextmanagerfunctionreturntype","text":"ContextManagerFunctionReturnType = Generator [ T , None , None ]","title":"ContextManagerFunctionReturnType"},{"location":"api/common/util/#sanitize","text":"def sanitize ( x : Any ) -> Any Sanitize turns PyTorch and Numpy types into basic Python types so they can be serialized into JSON.","title":"sanitize"},{"location":"api/common/util/#group_by_count","text":"def group_by_count ( iterable : List [ Any ], count : int , default_value : Any ) -> List [ List [ Any ]] Takes a list and groups it into sublists of size count , using default_value to pad the list at the end if the list is not divisable by count . For example: >>> group_by_count([1, 2, 3, 4, 5, 6, 7], 3, 0) [[1, 2, 3], [4, 5, 6], [7, 0, 0]] This is a short method, but it's complicated and hard to remember as a one-liner, so we just make a function out of it.","title":"group_by_count"},{"location":"api/common/util/#a","text":"A = TypeVar ( \"A\" )","title":"A"},{"location":"api/common/util/#lazy_groups_of","text":"def lazy_groups_of ( iterable : Iterable [ A ], group_size : int ) -> Iterator [ List [ A ]] Takes an iterable and batches the individual instances into lists of the specified size. The last list may be smaller if there are instances left over.","title":"lazy_groups_of"},{"location":"api/common/util/#pad_sequence_to_length","text":"def pad_sequence_to_length ( sequence : Sequence , desired_length : int , default_value : Callable [[], Any ] = lambda : 0 , padding_on_right : bool = True ) -> List Take a list of objects and pads it to the desired length, returning the padded list. The original list is not modified.","title":"pad_sequence_to_length"},{"location":"api/common/util/#add_noise_to_dict_values","text":"def add_noise_to_dict_values ( dictionary : Dict [ A , float ], noise_param : float ) -> Dict [ A , float ] Returns a new dictionary with noise added to every key in dictionary . The noise is uniformly distributed within noise_param percent of the value for every value in the dictionary.","title":"add_noise_to_dict_values"},{"location":"api/common/util/#namespace_match","text":"def namespace_match ( pattern : str , namespace : str ) Matches a namespace pattern against a namespace string. For example, *tags matches passage_tags and question_tags and tokens matches tokens but not stemmed_tokens .","title":"namespace_match"},{"location":"api/common/util/#prepare_environment","text":"def prepare_environment ( params : Params ) Sets random seeds for reproducible experiments. This may not work as expected if you use this from within a python project in which you have already imported Pytorch. If you use the scripts/run_model.py entry point to training models with this library, your experiments should be reasonably reproducible. If you are using this from your own project, you will want to call this function before importing Pytorch. Complete determinism is very difficult to achieve with libraries doing optimized linear algebra due to massively parallel execution, which is exacerbated by using GPUs.","title":"prepare_environment"},{"location":"api/common/util/#loaded_spacy_models","text":"LOADED_SPACY_MODELS : Dict [ Tuple [ str , bool , bool , bool ], SpacyModelType ] = {}","title":"LOADED_SPACY_MODELS"},{"location":"api/common/util/#get_spacy_model","text":"def get_spacy_model ( spacy_model_name : str , pos_tags : bool = True , parse : bool = False , ner : bool = False ) -> SpacyModelType In order to avoid loading spacy models a whole bunch of times, we'll save references to them, keyed by the options we used to create the spacy model, so any particular configuration only gets loaded once.","title":"get_spacy_model"},{"location":"api/common/util/#pushd","text":"@contextmanager def pushd ( new_dir : PathType , verbose : bool = False ) -> ContextManagerFunctionReturnType [ None ] Changes the current directory to the given path and prepends it to sys.path . This method is intended to use with with , so after its usage, the current directory will be set to the previous value.","title":"pushd"},{"location":"api/common/util/#push_python_path","text":"@contextmanager def push_python_path ( path : PathType ) -> ContextManagerFunctionReturnType [ None ] Prepends the given path to sys.path . This method is intended to use with with , so after its usage, its value willbe removed from sys.path .","title":"push_python_path"},{"location":"api/common/util/#import_module_and_submodules","text":"def import_module_and_submodules ( package_name : str , exclude : Optional [ Set [ str ]] = None ) -> None Import all public submodules under the given package. Primarily useful so that people using AllenNLP as a library can specify their own custom packages and have their custom classes get loaded and registered.","title":"import_module_and_submodules"},{"location":"api/common/util/#peak_cpu_memory","text":"def peak_cpu_memory () -> Dict [ int , int ] Get peak memory usage for each worker, as measured by max-resident-set size: https://unix.stackexchange.com/questions/30940/getrusage-system-call-what-is-maximum-resident-set-size Only works on OSX and Linux, otherwise the result will be 0.0 for every worker.","title":"peak_cpu_memory"},{"location":"api/common/util/#peak_gpu_memory","text":"def peak_gpu_memory () -> Dict [ int , int ] Get the peak GPU memory usage in bytes by device.","title":"peak_gpu_memory"},{"location":"api/common/util/#ensure_list","text":"def ensure_list ( iterable : Iterable [ A ]) -> List [ A ] An Iterable may be a list or a generator. This ensures we get a list without making an unnecessary copy.","title":"ensure_list"},{"location":"api/common/util/#is_lazy","text":"def is_lazy ( iterable : Iterable [ A ]) -> bool Checks if the given iterable is lazy, which here just means it's not a list.","title":"is_lazy"},{"location":"api/common/util/#int_to_device","text":"def int_to_device ( device : Union [ int , torch . device ]) -> torch . device","title":"int_to_device"},{"location":"api/common/util/#log_frozen_and_tunable_parameter_names","text":"def log_frozen_and_tunable_parameter_names ( model : torch . nn . Module ) -> None","title":"log_frozen_and_tunable_parameter_names"},{"location":"api/common/util/#get_frozen_and_tunable_parameter_names","text":"def get_frozen_and_tunable_parameter_names ( model : torch . nn . Module ) -> Tuple [ Iterable [ str ], Iterable [ str ]]","title":"get_frozen_and_tunable_parameter_names"},{"location":"api/common/util/#dump_metrics","text":"def dump_metrics ( file_path : Optional [ str ], metrics : Dict [ str , Any ], log : bool = False ) -> None","title":"dump_metrics"},{"location":"api/common/util/#flatten_filename","text":"def flatten_filename ( file_path : str ) -> str","title":"flatten_filename"},{"location":"api/common/util/#is_distributed","text":"def is_distributed () -> bool Checks if the distributed process group is available and has been initialized","title":"is_distributed"},{"location":"api/common/util/#is_global_primary","text":"def is_global_primary () -> bool Checks if the distributed process group is the global primary (rank = 0). If the distributed process group is not available or has not been initialized, this trivially returns True .","title":"is_global_primary"},{"location":"api/common/util/#sanitize_wordpiece","text":"def sanitize_wordpiece ( wordpiece : str ) -> str Sanitizes wordpieces from BERT, RoBERTa or ALBERT tokenizers.","title":"sanitize_wordpiece"},{"location":"api/common/util/#sanitize_ptb_tokenized_string","text":"def sanitize_ptb_tokenized_string ( text : str ) -> str Sanitizes string that was tokenized using PTBTokenizer","title":"sanitize_ptb_tokenized_string"},{"location":"api/common/util/#find_open_port","text":"def find_open_port () -> int Find a random open port on local host.","title":"find_open_port"},{"location":"api/common/util/#format_timedelta","text":"def format_timedelta ( td : timedelta ) -> str Format a timedelta for humans.","title":"format_timedelta"},{"location":"api/common/util/#format_size","text":"def format_size ( size : int ) -> str Format a size (in bytes) for humans.","title":"format_size"},{"location":"api/common/util/#nan_safe_tensor_divide","text":"def nan_safe_tensor_divide ( numerator , denominator ) Performs division and handles divide-by-zero. On zero-division, sets the corresponding result elements to zero.","title":"nan_safe_tensor_divide"},{"location":"api/common/util/#shuffle_iterable","text":"def shuffle_iterable ( i : Iterable [ T ], pool_size : int = 1024 ) -> Iterable [ T ]","title":"shuffle_iterable"},{"location":"api/common/util/#cycle_iterator_function","text":"def cycle_iterator_function ( iterator_function : Callable [[], Iterable [ T ]] ) -> Iterator [ T ] Functionally equivalent to itertools.cycle(iterator_function()) , but this function does not cache the result of calling the iterator like cycle does. Instead, we just call iterator_function() again whenever we get a StopIteration . This should only be preferred over itertools.cycle in cases where you're sure you don't want the caching behavior that's done in itertools.cycle .","title":"cycle_iterator_function"},{"location":"api/common/util/#hash_object","text":"def hash_object ( o : Any ) -> str Returns a character hash code of arbitrary Python objects.","title":"hash_object"},{"location":"api/common/util/#sigtermreceived","text":"class SigTermReceived ( Exception )","title":"SigTermReceived"},{"location":"api/common/util/#install_sigterm_handler","text":"def install_sigterm_handler ()","title":"install_sigterm_handler"},{"location":"api/common/testing/checklist_test/","text":"allennlp .common .testing .checklist_test [SOURCE] FakeTaskSuite \u00b6 @TaskSuite . register ( \"fake-task-suite\" ) class FakeTaskSuite ( TaskSuite ): | def __init__ ( | self , | suite : Optional [ TestSuite ] = None , | fake_arg1 : Optional [ int ] = None , | fake_arg2 : Optional [ int ] = None | ) Fake checklist suite for testing purpose.","title":"checklist_test"},{"location":"api/common/testing/checklist_test/#faketasksuite","text":"@TaskSuite . register ( \"fake-task-suite\" ) class FakeTaskSuite ( TaskSuite ): | def __init__ ( | self , | suite : Optional [ TestSuite ] = None , | fake_arg1 : Optional [ int ] = None , | fake_arg2 : Optional [ int ] = None | ) Fake checklist suite for testing purpose.","title":"FakeTaskSuite"},{"location":"api/common/testing/confidence_check_test/","text":"allennlp .common .testing .confidence_check_test [SOURCE] FakeModelForTestingNormalizationBiasVerification \u00b6 class FakeModelForTestingNormalizationBiasVerification ( Model ): | def __init__ ( self , use_bias = True ) forward \u00b6 class FakeModelForTestingNormalizationBiasVerification ( Model ): | ... | def forward ( self , x )","title":"confidence_check_test"},{"location":"api/common/testing/confidence_check_test/#fakemodelfortestingnormalizationbiasverification","text":"class FakeModelForTestingNormalizationBiasVerification ( Model ): | def __init__ ( self , use_bias = True )","title":"FakeModelForTestingNormalizationBiasVerification"},{"location":"api/common/testing/confidence_check_test/#forward","text":"class FakeModelForTestingNormalizationBiasVerification ( Model ): | ... | def forward ( self , x )","title":"forward"},{"location":"api/common/testing/distributed_test/","text":"allennlp .common .testing .distributed_test [SOURCE] init_process \u00b6 def init_process ( process_rank : int , world_size : int , distributed_device_ids : List [ int ], func : Callable , func_args : Tuple = None , func_kwargs : Dict [ str , Any ] = None , primary_addr : str = \"127.0.0.1\" , primary_port : int = 29500 ) run_distributed_test \u00b6 def run_distributed_test ( device_ids : List [ int ] = None , func : Callable = None , * args , ** kwargs , * , , ) This runs the func in a simulated distributed environment. Parameters \u00b6 device_ids : List[int] List of devices. There need to be at least 2 devices. Default is [-1, -1]. func : Callable func needs to be global for spawning the processes, so that it can be pickled. start_method : Optional[str] , optional (default = None ) The start method to use for starting the workers. Defaults to \"spawn\" for GPU processes and fork otherwise.","title":"distributed_test"},{"location":"api/common/testing/distributed_test/#init_process","text":"def init_process ( process_rank : int , world_size : int , distributed_device_ids : List [ int ], func : Callable , func_args : Tuple = None , func_kwargs : Dict [ str , Any ] = None , primary_addr : str = \"127.0.0.1\" , primary_port : int = 29500 )","title":"init_process"},{"location":"api/common/testing/distributed_test/#run_distributed_test","text":"def run_distributed_test ( device_ids : List [ int ] = None , func : Callable = None , * args , ** kwargs , * , , ) This runs the func in a simulated distributed environment.","title":"run_distributed_test"},{"location":"api/common/testing/interpret_test/","text":"allennlp .common .testing .interpret_test [SOURCE] FakeModelForTestingInterpret \u00b6 class FakeModelForTestingInterpret ( Model ): | def __init__ ( self , vocab , max_tokens = 7 , num_labels = 2 ) forward \u00b6 class FakeModelForTestingInterpret ( Model ): | ... | def forward ( self , tokens , label = None ) make_output_human_readable \u00b6 class FakeModelForTestingInterpret ( Model ): | ... | def make_output_human_readable ( self , output_dict ) FakePredictorForTestingInterpret \u00b6 class FakePredictorForTestingInterpret ( TextClassifierPredictor ) get_interpretable_layer \u00b6 class FakePredictorForTestingInterpret ( TextClassifierPredictor ): | ... | def get_interpretable_layer ( self ) get_interpretable_text_field_embedder \u00b6 class FakePredictorForTestingInterpret ( TextClassifierPredictor ): | ... | def get_interpretable_text_field_embedder ( self )","title":"interpret_test"},{"location":"api/common/testing/interpret_test/#fakemodelfortestinginterpret","text":"class FakeModelForTestingInterpret ( Model ): | def __init__ ( self , vocab , max_tokens = 7 , num_labels = 2 )","title":"FakeModelForTestingInterpret"},{"location":"api/common/testing/interpret_test/#forward","text":"class FakeModelForTestingInterpret ( Model ): | ... | def forward ( self , tokens , label = None )","title":"forward"},{"location":"api/common/testing/interpret_test/#make_output_human_readable","text":"class FakeModelForTestingInterpret ( Model ): | ... | def make_output_human_readable ( self , output_dict )","title":"make_output_human_readable"},{"location":"api/common/testing/interpret_test/#fakepredictorfortestinginterpret","text":"class FakePredictorForTestingInterpret ( TextClassifierPredictor )","title":"FakePredictorForTestingInterpret"},{"location":"api/common/testing/interpret_test/#get_interpretable_layer","text":"class FakePredictorForTestingInterpret ( TextClassifierPredictor ): | ... | def get_interpretable_layer ( self )","title":"get_interpretable_layer"},{"location":"api/common/testing/interpret_test/#get_interpretable_text_field_embedder","text":"class FakePredictorForTestingInterpret ( TextClassifierPredictor ): | ... | def get_interpretable_text_field_embedder ( self )","title":"get_interpretable_text_field_embedder"},{"location":"api/common/testing/model_test_case/","text":"allennlp .common .testing .model_test_case [SOURCE] ModelTestCase \u00b6 class ModelTestCase ( AllenNlpTestCase ) A subclass of AllenNlpTestCase with added methods for testing Model subclasses. set_up_model \u00b6 class ModelTestCase ( AllenNlpTestCase ): | ... | def set_up_model ( | self , | param_file : PathLike , | dataset_file : PathLike , | serialization_dir : PathLike = None , | seed : int = None | ) test_model_batch_norm_verification \u00b6 class ModelTestCase ( AllenNlpTestCase ): | ... | def test_model_batch_norm_verification ( self ) ensure_model_can_train_save_and_load \u00b6 class ModelTestCase ( AllenNlpTestCase ): | ... | def ensure_model_can_train_save_and_load ( | self , | param_file : Union [ PathLike , str ], | tolerance : float = 1e-4 , | cuda_device : int = - 1 , | gradients_to_ignore : Set [ str ] = None , | overrides : str = \"\" , | metric_to_check : str = None , | metric_terminal_value : float = None , | metric_tolerance : float = 1e-4 , | disable_dropout : bool = True , | which_loss : str = \"loss\" , | seed : int = None | ) Parameters \u00b6 param_file : str Path to a training configuration file that we will use to train the model for this test. tolerance : float , optional (default = 1e-4 ) When comparing model predictions between the originally-trained model and the model after saving and loading, we will use this tolerance value (passed as rtol to numpy.testing.assert_allclose ). cuda_device : int , optional (default = -1 ) The device to run the test on. gradients_to_ignore : Set[str] , optional (default = None ) This test runs a gradient check to make sure that we're actually computing gradients for all of the parameters in the model. If you really want to ignore certain parameters when doing that check, you can pass their names here. This is not recommended unless you're really sure you don't need to have non-zero gradients for those parameters (e.g., some of the beam search / state machine models have infrequently-used parameters that are hard to force the model to use in a small test). overrides : str , optional (default = \"\" ) A JSON string that we will use to override values in the input parameter file. metric_to_check : str , optional (default = None ) We may want to automatically perform a check that model reaches given metric when training (on validation set, if it is specified). It may be useful in CI, for example. You can pass any metric that is in your model returned metrics. metric_terminal_value : str , optional (default = None ) When you set metric_to_check , you need to set the value this metric must converge to metric_tolerance : float , optional (default = 1e-4 ) Tolerance to check you model metric against metric terminal value. One can expect some variance in model metrics when the training process is highly stochastic. disable_dropout : bool , optional (default = True ) If True we will set all dropout to 0 before checking gradients. (Otherwise, with small datasets, you may get zero gradients because of unlucky dropout.) which_loss : str , optional (default = \"loss\" ) Specifies which loss to test. For example, which_loss may be \"adversary_loss\" for adversarial_bias_mitigator . ensure_model_can_train \u00b6 class ModelTestCase ( AllenNlpTestCase ): | ... | def ensure_model_can_train ( | self , | trainer : GradientDescentTrainer , | gradients_to_ignore : Set [ str ] = None , | metric_to_check : str = None , | metric_terminal_value : float = None , | metric_tolerance : float = 1e-4 , | disable_dropout : bool = True | ) A simple test for model training behavior when you are not using configuration files. In this case, we don't have a story around saving and loading models (you need to handle that yourself), so we don't have tests for that. We just test that the model can train, and that it computes gradients for all parameters. Because the Trainer already has a reference to a model and to a data loader, we just take the Trainer object itself, and grab the Model and other necessary objects from there. Parameters \u00b6 trainer : GradientDescentTrainer The Trainer to use for the test, which already has references to a Model and a DataLoader , which we will use in the test. gradients_to_ignore : Set[str] , optional (default = None ) This test runs a gradient check to make sure that we're actually computing gradients for all of the parameters in the model. If you really want to ignore certain parameters when doing that check, you can pass their names here. This is not recommended unless you're really sure you don't need to have non-zero gradients for those parameters (e.g., some of the beam search / state machine models have infrequently-used parameters that are hard to force the model to use in a small test). metric_to_check : str , optional (default = None ) We may want to automatically perform a check that model reaches given metric when training (on validation set, if it is specified). It may be useful in CI, for example. You can pass any metric that is in your model returned metrics. metric_terminal_value : str , optional (default = None ) When you set metric_to_check , you need to set the value this metric must converge to metric_tolerance : float , optional (default = 1e-4 ) Tolerance to check you model metric against metric terminal value. One can expect some variance in model metrics when the training process is highly stochastic. disable_dropout : bool , optional (default = True ) If True we will set all dropout to 0 before checking gradients. (Otherwise, with small datasets, you may get zero gradients because of unlucky dropout.) assert_fields_equal \u00b6 class ModelTestCase ( AllenNlpTestCase ): | ... | def assert_fields_equal ( | self , | field1 , | field2 , | name : str , | tolerance : float = 1e-6 | ) -> None check_model_computes_gradients_correctly \u00b6 class ModelTestCase ( AllenNlpTestCase ): | ... | @staticmethod | def check_model_computes_gradients_correctly ( | model : Model , | model_batch : Dict [ str , Union [ Any , Dict [ str , Any ]]], | params_to_ignore : Set [ str ] = None , | disable_dropout : bool = True , | which_loss : str = \"loss\" | ) ensure_batch_predictions_are_consistent \u00b6 class ModelTestCase ( AllenNlpTestCase ): | ... | def ensure_batch_predictions_are_consistent ( | self , | keys_to_ignore : Iterable [ str ] = () | ) Ensures that the model performs the same on a batch of instances as on individual instances. Ignores metrics matching the regexp . loss. and those specified explicitly. Parameters \u00b6 keys_to_ignore : Iterable[str] , optional (default = () ) Names of metrics that should not be taken into account, e.g. \"batch_weight\".","title":"model_test_case"},{"location":"api/common/testing/model_test_case/#modeltestcase","text":"class ModelTestCase ( AllenNlpTestCase ) A subclass of AllenNlpTestCase with added methods for testing Model subclasses.","title":"ModelTestCase"},{"location":"api/common/testing/model_test_case/#set_up_model","text":"class ModelTestCase ( AllenNlpTestCase ): | ... | def set_up_model ( | self , | param_file : PathLike , | dataset_file : PathLike , | serialization_dir : PathLike = None , | seed : int = None | )","title":"set_up_model"},{"location":"api/common/testing/model_test_case/#test_model_batch_norm_verification","text":"class ModelTestCase ( AllenNlpTestCase ): | ... | def test_model_batch_norm_verification ( self )","title":"test_model_batch_norm_verification"},{"location":"api/common/testing/model_test_case/#ensure_model_can_train_save_and_load","text":"class ModelTestCase ( AllenNlpTestCase ): | ... | def ensure_model_can_train_save_and_load ( | self , | param_file : Union [ PathLike , str ], | tolerance : float = 1e-4 , | cuda_device : int = - 1 , | gradients_to_ignore : Set [ str ] = None , | overrides : str = \"\" , | metric_to_check : str = None , | metric_terminal_value : float = None , | metric_tolerance : float = 1e-4 , | disable_dropout : bool = True , | which_loss : str = \"loss\" , | seed : int = None | )","title":"ensure_model_can_train_save_and_load"},{"location":"api/common/testing/model_test_case/#ensure_model_can_train","text":"class ModelTestCase ( AllenNlpTestCase ): | ... | def ensure_model_can_train ( | self , | trainer : GradientDescentTrainer , | gradients_to_ignore : Set [ str ] = None , | metric_to_check : str = None , | metric_terminal_value : float = None , | metric_tolerance : float = 1e-4 , | disable_dropout : bool = True | ) A simple test for model training behavior when you are not using configuration files. In this case, we don't have a story around saving and loading models (you need to handle that yourself), so we don't have tests for that. We just test that the model can train, and that it computes gradients for all parameters. Because the Trainer already has a reference to a model and to a data loader, we just take the Trainer object itself, and grab the Model and other necessary objects from there.","title":"ensure_model_can_train"},{"location":"api/common/testing/model_test_case/#assert_fields_equal","text":"class ModelTestCase ( AllenNlpTestCase ): | ... | def assert_fields_equal ( | self , | field1 , | field2 , | name : str , | tolerance : float = 1e-6 | ) -> None","title":"assert_fields_equal"},{"location":"api/common/testing/model_test_case/#check_model_computes_gradients_correctly","text":"class ModelTestCase ( AllenNlpTestCase ): | ... | @staticmethod | def check_model_computes_gradients_correctly ( | model : Model , | model_batch : Dict [ str , Union [ Any , Dict [ str , Any ]]], | params_to_ignore : Set [ str ] = None , | disable_dropout : bool = True , | which_loss : str = \"loss\" | )","title":"check_model_computes_gradients_correctly"},{"location":"api/common/testing/model_test_case/#ensure_batch_predictions_are_consistent","text":"class ModelTestCase ( AllenNlpTestCase ): | ... | def ensure_batch_predictions_are_consistent ( | self , | keys_to_ignore : Iterable [ str ] = () | ) Ensures that the model performs the same on a batch of instances as on individual instances. Ignores metrics matching the regexp . loss. and those specified explicitly.","title":"ensure_batch_predictions_are_consistent"},{"location":"api/common/testing/test_case/","text":"allennlp .common .testing .test_case [SOURCE] TEST_DIR \u00b6 TEST_DIR = tempfile . mkdtemp ( prefix = \"allennlp_tests\" ) AllenNlpTestCase \u00b6 class AllenNlpTestCase A custom testing class that disables some of the more verbose AllenNLP logging and that creates and destroys a temp directory as a test fixture. PROJECT_ROOT \u00b6 class AllenNlpTestCase : | ... | PROJECT_ROOT = ( pathlib . Path ( __file__ ) . parent / \"..\" / \"..\" / \"..\" ) . resolve () MODULE_ROOT \u00b6 class AllenNlpTestCase : | ... | MODULE_ROOT = PROJECT_ROOT / \"allennlp\" TOOLS_ROOT \u00b6 class AllenNlpTestCase : | ... | TOOLS_ROOT = MODULE_ROOT / \"tools\" PROJECT_ROOT_FALLBACK \u00b6 class AllenNlpTestCase : | ... | PROJECT_ROOT_FALLBACK = ( # users wanting to run test suite for installed package pathlib . Path ( os . environ [ \"A ... TESTS_ROOT \u00b6 class AllenNlpTestCase : | ... | TESTS_ROOT = PROJECT_ROOT_FALLBACK / \"tests\" FIXTURES_ROOT \u00b6 class AllenNlpTestCase : | ... | FIXTURES_ROOT = PROJECT_ROOT_FALLBACK / \"test_fixtures\" setup_method \u00b6 class AllenNlpTestCase : | ... | def setup_method ( self ) teardown_method \u00b6 class AllenNlpTestCase : | ... | def teardown_method ( self )","title":"test_case"},{"location":"api/common/testing/test_case/#test_dir","text":"TEST_DIR = tempfile . mkdtemp ( prefix = \"allennlp_tests\" )","title":"TEST_DIR"},{"location":"api/common/testing/test_case/#allennlptestcase","text":"class AllenNlpTestCase A custom testing class that disables some of the more verbose AllenNLP logging and that creates and destroys a temp directory as a test fixture.","title":"AllenNlpTestCase"},{"location":"api/common/testing/test_case/#project_root","text":"class AllenNlpTestCase : | ... | PROJECT_ROOT = ( pathlib . Path ( __file__ ) . parent / \"..\" / \"..\" / \"..\" ) . resolve ()","title":"PROJECT_ROOT"},{"location":"api/common/testing/test_case/#module_root","text":"class AllenNlpTestCase : | ... | MODULE_ROOT = PROJECT_ROOT / \"allennlp\"","title":"MODULE_ROOT"},{"location":"api/common/testing/test_case/#tools_root","text":"class AllenNlpTestCase : | ... | TOOLS_ROOT = MODULE_ROOT / \"tools\"","title":"TOOLS_ROOT"},{"location":"api/common/testing/test_case/#project_root_fallback","text":"class AllenNlpTestCase : | ... | PROJECT_ROOT_FALLBACK = ( # users wanting to run test suite for installed package pathlib . Path ( os . environ [ \"A ...","title":"PROJECT_ROOT_FALLBACK"},{"location":"api/common/testing/test_case/#tests_root","text":"class AllenNlpTestCase : | ... | TESTS_ROOT = PROJECT_ROOT_FALLBACK / \"tests\"","title":"TESTS_ROOT"},{"location":"api/common/testing/test_case/#fixtures_root","text":"class AllenNlpTestCase : | ... | FIXTURES_ROOT = PROJECT_ROOT_FALLBACK / \"test_fixtures\"","title":"FIXTURES_ROOT"},{"location":"api/common/testing/test_case/#setup_method","text":"class AllenNlpTestCase : | ... | def setup_method ( self )","title":"setup_method"},{"location":"api/common/testing/test_case/#teardown_method","text":"class AllenNlpTestCase : | ... | def teardown_method ( self )","title":"teardown_method"},{"location":"api/confidence_checks/normalization_bias_verification/","text":"allennlp .confidence_checks .normalization_bias_verification [SOURCE] Code based almost entirely from the pytorch-lightning-snippets repository. NormalizationBiasVerification \u00b6 class NormalizationBiasVerification ( VerificationBase ): | def __init__ ( self , model : nn . Module ) Network layers with biases should not be combined with normalization layers, as the bias makes normalization ineffective and can lead to unstable training. This verification detects such combinations. normalization_layers \u00b6 class NormalizationBiasVerification ( VerificationBase ): | ... | normalization_layers = ( nn . BatchNorm1d , nn . BatchNorm2d , nn . BatchNorm3d , nn . SyncBatchNorm , ... detected_pairs \u00b6 class NormalizationBiasVerification ( VerificationBase ): | ... | @property | def detected_pairs ( self ) -> List [ Tuple ] check \u00b6 class NormalizationBiasVerification ( VerificationBase ): | ... | def check ( self , inputs ) -> bool collect_detections \u00b6 class NormalizationBiasVerification ( VerificationBase ): | ... | def collect_detections ( self ) register_hooks \u00b6 class NormalizationBiasVerification ( VerificationBase ): | ... | def register_hooks ( self ) destroy_hooks \u00b6 class NormalizationBiasVerification ( VerificationBase ): | ... | def destroy_hooks ( self )","title":"normalization_bias_verification"},{"location":"api/confidence_checks/normalization_bias_verification/#normalizationbiasverification","text":"class NormalizationBiasVerification ( VerificationBase ): | def __init__ ( self , model : nn . Module ) Network layers with biases should not be combined with normalization layers, as the bias makes normalization ineffective and can lead to unstable training. This verification detects such combinations.","title":"NormalizationBiasVerification"},{"location":"api/confidence_checks/normalization_bias_verification/#normalization_layers","text":"class NormalizationBiasVerification ( VerificationBase ): | ... | normalization_layers = ( nn . BatchNorm1d , nn . BatchNorm2d , nn . BatchNorm3d , nn . SyncBatchNorm , ...","title":"normalization_layers"},{"location":"api/confidence_checks/normalization_bias_verification/#detected_pairs","text":"class NormalizationBiasVerification ( VerificationBase ): | ... | @property | def detected_pairs ( self ) -> List [ Tuple ]","title":"detected_pairs"},{"location":"api/confidence_checks/normalization_bias_verification/#check","text":"class NormalizationBiasVerification ( VerificationBase ): | ... | def check ( self , inputs ) -> bool","title":"check"},{"location":"api/confidence_checks/normalization_bias_verification/#collect_detections","text":"class NormalizationBiasVerification ( VerificationBase ): | ... | def collect_detections ( self )","title":"collect_detections"},{"location":"api/confidence_checks/normalization_bias_verification/#register_hooks","text":"class NormalizationBiasVerification ( VerificationBase ): | ... | def register_hooks ( self )","title":"register_hooks"},{"location":"api/confidence_checks/normalization_bias_verification/#destroy_hooks","text":"class NormalizationBiasVerification ( VerificationBase ): | ... | def destroy_hooks ( self )","title":"destroy_hooks"},{"location":"api/confidence_checks/verification_base/","text":"allennlp .confidence_checks .verification_base [SOURCE] Code based almost entirely on https://github.com/awaelchli/pytorch-lightning-snippets/commit/7db53f774715d635c59ef56f21a17634d246b2c5 VerificationBase \u00b6 class VerificationBase : | def __init__ ( self , model : nn . Module ) Base class for model verification. All verifications should run with any torch.nn.Module unless otherwise stated. check \u00b6 class VerificationBase : | ... | @abstractmethod | def check ( self , * args , ** kwargs ) -> bool Runs the actual test on the model. All verification classes must implement this. Arguments: args: Any positional arguments that are needed to run the test kwargs: Keyword arguments that are needed to run the test Returns: True if the test passes, and False otherwise. Some verifications can only be performed with a heuristic accuracy, thus the return value may not always reflect the true state of the system in these cases.","title":"verification_base"},{"location":"api/confidence_checks/verification_base/#verificationbase","text":"class VerificationBase : | def __init__ ( self , model : nn . Module ) Base class for model verification. All verifications should run with any torch.nn.Module unless otherwise stated.","title":"VerificationBase"},{"location":"api/confidence_checks/verification_base/#check","text":"class VerificationBase : | ... | @abstractmethod | def check ( self , * args , ** kwargs ) -> bool Runs the actual test on the model. All verification classes must implement this. Arguments: args: Any positional arguments that are needed to run the test kwargs: Keyword arguments that are needed to run the test Returns: True if the test passes, and False otherwise. Some verifications can only be performed with a heuristic accuracy, thus the return value may not always reflect the true state of the system in these cases.","title":"check"},{"location":"api/confidence_checks/task_checklists/question_answering_suite/","text":"allennlp .confidence_checks .task_checklists .question_answering_suite [SOURCE] QuestionAnsweringSuite \u00b6 @TaskSuite . register ( \"question-answering\" ) class QuestionAnsweringSuite ( TaskSuite ): | def __init__ ( | self , | suite : Optional [ TestSuite ] = None , | context_key : str = \"context\" , | question_key : str = \"question\" , | answer_key : str = \"best_span_str\" , | ** kwargs | ) contractions \u00b6 class QuestionAnsweringSuite ( TaskSuite ): | ... | @classmethod | def contractions ( cls ) typos \u00b6 class QuestionAnsweringSuite ( TaskSuite ): | ... | @classmethod | def typos ( cls ) punctuation \u00b6 class QuestionAnsweringSuite ( TaskSuite ): | ... | @classmethod | def punctuation ( cls )","title":"question_answering_suite"},{"location":"api/confidence_checks/task_checklists/question_answering_suite/#questionansweringsuite","text":"@TaskSuite . register ( \"question-answering\" ) class QuestionAnsweringSuite ( TaskSuite ): | def __init__ ( | self , | suite : Optional [ TestSuite ] = None , | context_key : str = \"context\" , | question_key : str = \"question\" , | answer_key : str = \"best_span_str\" , | ** kwargs | )","title":"QuestionAnsweringSuite"},{"location":"api/confidence_checks/task_checklists/question_answering_suite/#contractions","text":"class QuestionAnsweringSuite ( TaskSuite ): | ... | @classmethod | def contractions ( cls )","title":"contractions"},{"location":"api/confidence_checks/task_checklists/question_answering_suite/#typos","text":"class QuestionAnsweringSuite ( TaskSuite ): | ... | @classmethod | def typos ( cls )","title":"typos"},{"location":"api/confidence_checks/task_checklists/question_answering_suite/#punctuation","text":"class QuestionAnsweringSuite ( TaskSuite ): | ... | @classmethod | def punctuation ( cls )","title":"punctuation"},{"location":"api/confidence_checks/task_checklists/sentiment_analysis_suite/","text":"allennlp .confidence_checks .task_checklists .sentiment_analysis_suite [SOURCE] SentimentAnalysisSuite \u00b6 @TaskSuite . register ( \"sentiment-analysis\" ) class SentimentAnalysisSuite ( TaskSuite ): | def __init__ ( | self , | suite : Optional [ TestSuite ] = None , | positive : Optional [ int ] = 0 , | negative : Optional [ int ] = 1 , | ** kwargs | ) This suite was built using the checklist process with the self.editor suggestions. Users are encouraged to add/modify as they see fit. Note: editor.suggest(...) can be slow as it runs a language model.","title":"sentiment_analysis_suite"},{"location":"api/confidence_checks/task_checklists/sentiment_analysis_suite/#sentimentanalysissuite","text":"@TaskSuite . register ( \"sentiment-analysis\" ) class SentimentAnalysisSuite ( TaskSuite ): | def __init__ ( | self , | suite : Optional [ TestSuite ] = None , | positive : Optional [ int ] = 0 , | negative : Optional [ int ] = 1 , | ** kwargs | ) This suite was built using the checklist process with the self.editor suggestions. Users are encouraged to add/modify as they see fit. Note: editor.suggest(...) can be slow as it runs a language model.","title":"SentimentAnalysisSuite"},{"location":"api/confidence_checks/task_checklists/task_suite/","text":"allennlp .confidence_checks .task_checklists .task_suite [SOURCE] TaskSuite \u00b6 class TaskSuite ( Registrable ): | def __init__ ( | self , | suite : Optional [ TestSuite ] = None , | add_default_tests : bool = True , | data : Optional [ List [ Any ]] = None , | num_test_cases : int = 100 , | ** kwargs | ) Base class for various task test suites. This is a wrapper class around the CheckList toolkit introduced in the paper Beyond Accuracy: Behavioral Testing of NLP models with CheckList (Ribeiro et al) . Note To use the checklist integration you should install allennlp with the \"checklist\" extra (e.g. conda install allennlp-checklist , pip install allennlp[checklist] or just install checklist after the fact). Task suites are intended to be used as a form of behavioral testing for NLP models to check for robustness across several general linguistic capabilities; eg. Vocabulary, SRL, Negation, etc. An example of the entire checklist process can be found at: https://github.com/marcotcr/checklist/blob/master/notebooks/tutorials/ . A task suite should contain tests that check general capabilities, including but not limited to: Vocabulary + POS : Important words/word types for the task Taxonomy : Synonyms/antonyms, etc. Robustness : To typos, irrelevant changes, etc. NER : Appropriately understanding named entities. Temporal : Understanding the order of events. Negation Coreference Semantic Role Labeling : Understanding roles such as agents and objects. Logic : Ability to handle symmetry, consistency, and conjunctions. Fairness Parameters \u00b6 suite : checklist.test_suite.TestSuite , optional (default = None ) Pass in an existing test suite. add_default_tests : bool , optional (default = False ) Whether to add default checklist tests for the task. data : List[Any] , optional (default = None ) If the data is provided, and add_default_tests is True , tests that perturb the data are also added. For instance, if the task is sentiment analysis, and the a list of sentences is passed, it will add tests that check a model's robustness to typos, etc. describe \u00b6 class TaskSuite ( Registrable ): | ... | def describe ( self ) Gives a description of the test suite. This is intended as a utility for examining the test suite. summary \u00b6 class TaskSuite ( Registrable ): | ... | def summary ( | self , | capabilities : Optional [ List [ str ]] = None , | file : TextIO = sys . stdout , | ** kwargs | ) Prints a summary of the test results. Parameters \u00b6 capabilities : List[str] , optional (default = None ) If not None, will only show tests with these capabilities. **kwargs : type Will be passed as arguments to each test.summary() run \u00b6 class TaskSuite ( Registrable ): | ... | def run ( | self , | predictor : Predictor , | capabilities : Optional [ List [ str ]] = None , | max_examples : Optional [ int ] = None | ) Runs the predictor on the test suite data. Parameters \u00b6 predictor : Predictor The predictor object. capabilities : List[str] , optional (default = None ) If not None, will only run tests with these capabilities. max_examples : int , optional (default = None ) Maximum number of examples to run. If None, all examples will be run. constructor \u00b6 class TaskSuite ( Registrable ): | ... | @classmethod | def constructor ( | cls , | name : Optional [ str ] = None , | suite_file : Optional [ str ] = None , | extra_args : Optional [ Dict [ str , Any ]] = None | ) -> \"TaskSuite\" save_suite \u00b6 class TaskSuite ( Registrable ): | ... | def save_suite ( self , suite_file : str ) Saves the suite to a file. contractions \u00b6 class TaskSuite ( Registrable ): | ... | @classmethod | def contractions ( cls ) -> Callable This returns a function which adds/removes contractions in relevant str inputs of a task's inputs. For instance, \"isn't\" will be changed to \"is not\", and \"will not\" will be changed to \"won't\". Expected arguments for this function: (example, **args, **kwargs) where the example is an instance of some task. It can be of any type. For example, for a sentiment analysis task, it will be a a str (the sentence for which we want to predict the sentiment). For a textual entailment task, it can be a tuple or a Dict, etc. Expected output of this function is a list of instances for the task, of the same type as example . typos \u00b6 class TaskSuite ( Registrable ): | ... | @classmethod | def typos ( cls ) -> Callable This returns a function which adds simple typos in relevant str inputs of a task's inputs. Expected arguments for this function: (example, **args, **kwargs) where the example is an instance of some task. It can be of any type. For example, for a sentiment analysis task, it will be a a str (the sentence for which we want to predict the sentiment). For a textual entailment task, it can be a tuple or a Dict, etc. Expected output of this function is a list of instances for the task, of the same type as example . punctuation \u00b6 class TaskSuite ( Registrable ): | ... | @classmethod | def punctuation ( cls ) -> Callable This returns a function which adds/removes punctuations in relevant str inputs of a task's inputs. For instance, \"isn't\" will be changed to \"is not\", and \"will not\" will be changed to \"won't\". Expected arguments for this function: (example, **args, **kwargs) where the example is an instance of some task. It can be of any type. For example, for a sentiment analysis task, it will be a a str (the sentence for which we want to predict the sentiment). For a textual entailment task, it can be a tuple or a Dict, etc. Expected output of this function is a list of instances for the task, of the same type as example . add_test \u00b6 class TaskSuite ( Registrable ): | ... | def add_test ( self , test : Union [ MFT , INV , DIR ]) Adds a fully specified checklist test to the suite. The tests can be of the following types: MFT: A minimum functionality test. It checks if the predicted output matches the expected output. For example, for a sentiment analysis task, a simple MFT can check if the model always predicts a positive sentiment for very positive words. The test's data contains the input and the expected output. INV: An invariance test. It checks if the predicted output is invariant to some change in the input. For example, for a sentiment analysis task, an INV test can check if the prediction stays consistent if simple typos are added. The test's data contains the pairs (input, modified input). DIR: A directional expectation test. It checks if the predicted output changes in some specific way in response to the change in input. For example, for a sentiment analysis task, a DIR test can check if adding a reducer (eg. \"good\" -> \"somewhat good\") causes the prediction's positive confidence score to decrease (or at least not increase). The test's data contains single inputs or pairs (input, modified input). Please refer to the paper for more details and examples. Note: test needs to be fully specified; with name, capability and description.","title":"task_suite"},{"location":"api/confidence_checks/task_checklists/task_suite/#tasksuite","text":"class TaskSuite ( Registrable ): | def __init__ ( | self , | suite : Optional [ TestSuite ] = None , | add_default_tests : bool = True , | data : Optional [ List [ Any ]] = None , | num_test_cases : int = 100 , | ** kwargs | ) Base class for various task test suites. This is a wrapper class around the CheckList toolkit introduced in the paper Beyond Accuracy: Behavioral Testing of NLP models with CheckList (Ribeiro et al) . Note To use the checklist integration you should install allennlp with the \"checklist\" extra (e.g. conda install allennlp-checklist , pip install allennlp[checklist] or just install checklist after the fact). Task suites are intended to be used as a form of behavioral testing for NLP models to check for robustness across several general linguistic capabilities; eg. Vocabulary, SRL, Negation, etc. An example of the entire checklist process can be found at: https://github.com/marcotcr/checklist/blob/master/notebooks/tutorials/ . A task suite should contain tests that check general capabilities, including but not limited to: Vocabulary + POS : Important words/word types for the task Taxonomy : Synonyms/antonyms, etc. Robustness : To typos, irrelevant changes, etc. NER : Appropriately understanding named entities. Temporal : Understanding the order of events. Negation Coreference Semantic Role Labeling : Understanding roles such as agents and objects. Logic : Ability to handle symmetry, consistency, and conjunctions. Fairness","title":"TaskSuite"},{"location":"api/confidence_checks/task_checklists/task_suite/#describe","text":"class TaskSuite ( Registrable ): | ... | def describe ( self ) Gives a description of the test suite. This is intended as a utility for examining the test suite.","title":"describe"},{"location":"api/confidence_checks/task_checklists/task_suite/#summary","text":"class TaskSuite ( Registrable ): | ... | def summary ( | self , | capabilities : Optional [ List [ str ]] = None , | file : TextIO = sys . stdout , | ** kwargs | ) Prints a summary of the test results.","title":"summary"},{"location":"api/confidence_checks/task_checklists/task_suite/#run","text":"class TaskSuite ( Registrable ): | ... | def run ( | self , | predictor : Predictor , | capabilities : Optional [ List [ str ]] = None , | max_examples : Optional [ int ] = None | ) Runs the predictor on the test suite data.","title":"run"},{"location":"api/confidence_checks/task_checklists/task_suite/#constructor","text":"class TaskSuite ( Registrable ): | ... | @classmethod | def constructor ( | cls , | name : Optional [ str ] = None , | suite_file : Optional [ str ] = None , | extra_args : Optional [ Dict [ str , Any ]] = None | ) -> \"TaskSuite\"","title":"constructor"},{"location":"api/confidence_checks/task_checklists/task_suite/#save_suite","text":"class TaskSuite ( Registrable ): | ... | def save_suite ( self , suite_file : str ) Saves the suite to a file.","title":"save_suite"},{"location":"api/confidence_checks/task_checklists/task_suite/#contractions","text":"class TaskSuite ( Registrable ): | ... | @classmethod | def contractions ( cls ) -> Callable This returns a function which adds/removes contractions in relevant str inputs of a task's inputs. For instance, \"isn't\" will be changed to \"is not\", and \"will not\" will be changed to \"won't\". Expected arguments for this function: (example, **args, **kwargs) where the example is an instance of some task. It can be of any type. For example, for a sentiment analysis task, it will be a a str (the sentence for which we want to predict the sentiment). For a textual entailment task, it can be a tuple or a Dict, etc. Expected output of this function is a list of instances for the task, of the same type as example .","title":"contractions"},{"location":"api/confidence_checks/task_checklists/task_suite/#typos","text":"class TaskSuite ( Registrable ): | ... | @classmethod | def typos ( cls ) -> Callable This returns a function which adds simple typos in relevant str inputs of a task's inputs. Expected arguments for this function: (example, **args, **kwargs) where the example is an instance of some task. It can be of any type. For example, for a sentiment analysis task, it will be a a str (the sentence for which we want to predict the sentiment). For a textual entailment task, it can be a tuple or a Dict, etc. Expected output of this function is a list of instances for the task, of the same type as example .","title":"typos"},{"location":"api/confidence_checks/task_checklists/task_suite/#punctuation","text":"class TaskSuite ( Registrable ): | ... | @classmethod | def punctuation ( cls ) -> Callable This returns a function which adds/removes punctuations in relevant str inputs of a task's inputs. For instance, \"isn't\" will be changed to \"is not\", and \"will not\" will be changed to \"won't\". Expected arguments for this function: (example, **args, **kwargs) where the example is an instance of some task. It can be of any type. For example, for a sentiment analysis task, it will be a a str (the sentence for which we want to predict the sentiment). For a textual entailment task, it can be a tuple or a Dict, etc. Expected output of this function is a list of instances for the task, of the same type as example .","title":"punctuation"},{"location":"api/confidence_checks/task_checklists/task_suite/#add_test","text":"class TaskSuite ( Registrable ): | ... | def add_test ( self , test : Union [ MFT , INV , DIR ]) Adds a fully specified checklist test to the suite. The tests can be of the following types: MFT: A minimum functionality test. It checks if the predicted output matches the expected output. For example, for a sentiment analysis task, a simple MFT can check if the model always predicts a positive sentiment for very positive words. The test's data contains the input and the expected output. INV: An invariance test. It checks if the predicted output is invariant to some change in the input. For example, for a sentiment analysis task, an INV test can check if the prediction stays consistent if simple typos are added. The test's data contains the pairs (input, modified input). DIR: A directional expectation test. It checks if the predicted output changes in some specific way in response to the change in input. For example, for a sentiment analysis task, a DIR test can check if adding a reducer (eg. \"good\" -> \"somewhat good\") causes the prediction's positive confidence score to decrease (or at least not increase). The test's data contains single inputs or pairs (input, modified input). Please refer to the paper for more details and examples. Note: test needs to be fully specified; with name, capability and description.","title":"add_test"},{"location":"api/confidence_checks/task_checklists/textual_entailment_suite/","text":"allennlp .confidence_checks .task_checklists .textual_entailment_suite [SOURCE] TextualEntailmentSuite \u00b6 @TaskSuite . register ( \"textual-entailment\" ) class TextualEntailmentSuite ( TaskSuite ): | def __init__ ( | self , | suite : Optional [ TestSuite ] = None , | entails : int = 0 , | contradicts : int = 1 , | neutral : int = 2 , | premise : str = \"premise\" , | hypothesis : str = \"hypothesis\" , | probs_key : str = \"probs\" , | ** kwargs | ) contractions \u00b6 class TextualEntailmentSuite ( TaskSuite ): | ... | @classmethod | def contractions ( cls ) typos \u00b6 class TextualEntailmentSuite ( TaskSuite ): | ... | @classmethod | def typos ( cls ) punctuation \u00b6 class TextualEntailmentSuite ( TaskSuite ): | ... | @classmethod | def punctuation ( cls )","title":"textual_entailment_suite"},{"location":"api/confidence_checks/task_checklists/textual_entailment_suite/#textualentailmentsuite","text":"@TaskSuite . register ( \"textual-entailment\" ) class TextualEntailmentSuite ( TaskSuite ): | def __init__ ( | self , | suite : Optional [ TestSuite ] = None , | entails : int = 0 , | contradicts : int = 1 , | neutral : int = 2 , | premise : str = \"premise\" , | hypothesis : str = \"hypothesis\" , | probs_key : str = \"probs\" , | ** kwargs | )","title":"TextualEntailmentSuite"},{"location":"api/confidence_checks/task_checklists/textual_entailment_suite/#contractions","text":"class TextualEntailmentSuite ( TaskSuite ): | ... | @classmethod | def contractions ( cls )","title":"contractions"},{"location":"api/confidence_checks/task_checklists/textual_entailment_suite/#typos","text":"class TextualEntailmentSuite ( TaskSuite ): | ... | @classmethod | def typos ( cls )","title":"typos"},{"location":"api/confidence_checks/task_checklists/textual_entailment_suite/#punctuation","text":"class TextualEntailmentSuite ( TaskSuite ): | ... | @classmethod | def punctuation ( cls )","title":"punctuation"},{"location":"api/confidence_checks/task_checklists/utils/","text":"allennlp .confidence_checks .task_checklists .utils [SOURCE] add_common_lexicons \u00b6 def add_common_lexicons ( editor : Editor ) Add commonly used lexicons to the editor object. These can be used in all the task suites. Note: Updates the editor object in place. spacy_wrap \u00b6 def spacy_wrap ( fn : Callable , language : str = \"en_core_web_sm\" , ** kwargs ) -> Callable Wrap the function so that it runs the input text data through a spacy model before the function call. strip_punctuation \u00b6 def strip_punctuation ( data : Union [ str , spacy . tokens . doc . Doc ]) -> str Removes all punctuation from data . toggle_punctuation \u00b6 def toggle_punctuation ( data : str ) -> List [ str ] If data contains any punctuation, it is removed. Otherwise, a . is added to the string. Returns a list of strings. Eg. data = \"This was great!\" Returns [\"This was great\", \"This was great.\"] data = \"The movie was good\" Returns [\"The movie was good.\"] random_string \u00b6 def random_string ( n : int ) -> str Returns a random alphanumeric string of length n . random_url \u00b6 def random_url ( n : int = 6 ) -> str Returns a random url of length n . random_handle \u00b6 def random_handle ( n : int = 6 ) -> str Returns a random handle of length n . Eg. \"@randomstr23` add_random_strings \u00b6 def add_random_strings ( data : str ) -> List [ str ] Adds random strings to the start and end of the string data . Returns a list of strings.","title":"utils"},{"location":"api/confidence_checks/task_checklists/utils/#add_common_lexicons","text":"def add_common_lexicons ( editor : Editor ) Add commonly used lexicons to the editor object. These can be used in all the task suites. Note: Updates the editor object in place.","title":"add_common_lexicons"},{"location":"api/confidence_checks/task_checklists/utils/#spacy_wrap","text":"def spacy_wrap ( fn : Callable , language : str = \"en_core_web_sm\" , ** kwargs ) -> Callable Wrap the function so that it runs the input text data through a spacy model before the function call.","title":"spacy_wrap"},{"location":"api/confidence_checks/task_checklists/utils/#strip_punctuation","text":"def strip_punctuation ( data : Union [ str , spacy . tokens . doc . Doc ]) -> str Removes all punctuation from data .","title":"strip_punctuation"},{"location":"api/confidence_checks/task_checklists/utils/#toggle_punctuation","text":"def toggle_punctuation ( data : str ) -> List [ str ] If data contains any punctuation, it is removed. Otherwise, a . is added to the string. Returns a list of strings. Eg. data = \"This was great!\" Returns [\"This was great\", \"This was great.\"] data = \"The movie was good\" Returns [\"The movie was good.\"]","title":"toggle_punctuation"},{"location":"api/confidence_checks/task_checklists/utils/#random_string","text":"def random_string ( n : int ) -> str Returns a random alphanumeric string of length n .","title":"random_string"},{"location":"api/confidence_checks/task_checklists/utils/#random_url","text":"def random_url ( n : int = 6 ) -> str Returns a random url of length n .","title":"random_url"},{"location":"api/confidence_checks/task_checklists/utils/#random_handle","text":"def random_handle ( n : int = 6 ) -> str Returns a random handle of length n . Eg. \"@randomstr23`","title":"random_handle"},{"location":"api/confidence_checks/task_checklists/utils/#add_random_strings","text":"def add_random_strings ( data : str ) -> List [ str ] Adds random strings to the start and end of the string data . Returns a list of strings.","title":"add_random_strings"},{"location":"api/data/batch/","text":"allennlp .data .batch [SOURCE] A Batch represents a collection of Instance s to be fed through a model. Batch \u00b6 class Batch ( Iterable ): | def __init__ ( self , instances : Iterable [ Instance ]) -> None A batch of Instances. In addition to containing the instances themselves, it contains helper functions for converting the data into tensors. A Batch just takes an iterable of instances in its constructor and hangs onto them in a list. get_padding_lengths \u00b6 class Batch ( Iterable ): | ... | def get_padding_lengths ( self ) -> Dict [ str , Dict [ str , int ]] Gets the maximum padding lengths from all Instances in this batch. Each Instance has multiple Fields , and each Field could have multiple things that need padding. We look at all fields in all instances, and find the max values for each (field_name, padding_key) pair, returning them in a dictionary. This can then be used to convert this batch into arrays of consistent length, or to set model parameters, etc. as_tensor_dict \u00b6 class Batch ( Iterable ): | ... | def as_tensor_dict ( | self , | padding_lengths : Dict [ str , Dict [ str , int ]] = None , | verbose : bool = False | ) -> Dict [ str , Union [ torch . Tensor , Dict [ str , torch . Tensor ]]] This method converts this Batch into a set of pytorch Tensors that can be passed through a model. In order for the tensors to be valid tensors, all Instances in this batch need to be padded to the same lengths wherever padding is necessary, so we do that first, then we combine all of the tensors for each field in each instance into a set of batched tensors for each field. Parameters \u00b6 padding_lengths : Dict[str, Dict[str, int]] If a key is present in this dictionary with a non- None value, we will pad to that length instead of the length calculated from the data. This lets you, e.g., set a maximum value for sentence length if you want to throw out long sequences. Entries in this dictionary are keyed first by field name (e.g., \"question\"), then by padding key (e.g., \"num_tokens\"). verbose : bool , optional (default = False ) Should we output logging information when we're doing this padding? If the batch is large, this is nice to have, because padding a large batch could take a long time. But if you're doing this inside of a data generator, having all of this output per batch is a bit obnoxious (and really slow). Returns \u00b6 tensors : Dict[str, DataArray] A dictionary of tensors, keyed by field name, suitable for passing as input to a model. This is a batch of instances, so, e.g., if the instances have a \"question\" field and an \"answer\" field, the \"question\" fields for all of the instances will be grouped together into a single tensor, and the \"answer\" fields for all instances will be similarly grouped in a parallel set of tensors, for batched computation. Additionally, for complex Fields , the value of the dictionary key is not necessarily a single tensor. For example, with the TextField , the output is a dictionary mapping TokenIndexer keys to tensors. The number of elements in this sub-dictionary therefore corresponds to the number of TokenIndexers used to index the TextField . Each Field class is responsible for batching its own output. __iter__ \u00b6 class Batch ( Iterable ): | ... | def __iter__ ( self ) -> Iterator [ Instance ] index_instances \u00b6 class Batch ( Iterable ): | ... | def index_instances ( self , vocab : Vocabulary ) -> None print_statistics \u00b6 class Batch ( Iterable ): | ... | def print_statistics ( self ) -> None","title":"batch"},{"location":"api/data/batch/#batch","text":"class Batch ( Iterable ): | def __init__ ( self , instances : Iterable [ Instance ]) -> None A batch of Instances. In addition to containing the instances themselves, it contains helper functions for converting the data into tensors. A Batch just takes an iterable of instances in its constructor and hangs onto them in a list.","title":"Batch"},{"location":"api/data/batch/#get_padding_lengths","text":"class Batch ( Iterable ): | ... | def get_padding_lengths ( self ) -> Dict [ str , Dict [ str , int ]] Gets the maximum padding lengths from all Instances in this batch. Each Instance has multiple Fields , and each Field could have multiple things that need padding. We look at all fields in all instances, and find the max values for each (field_name, padding_key) pair, returning them in a dictionary. This can then be used to convert this batch into arrays of consistent length, or to set model parameters, etc.","title":"get_padding_lengths"},{"location":"api/data/batch/#as_tensor_dict","text":"class Batch ( Iterable ): | ... | def as_tensor_dict ( | self , | padding_lengths : Dict [ str , Dict [ str , int ]] = None , | verbose : bool = False | ) -> Dict [ str , Union [ torch . Tensor , Dict [ str , torch . Tensor ]]] This method converts this Batch into a set of pytorch Tensors that can be passed through a model. In order for the tensors to be valid tensors, all Instances in this batch need to be padded to the same lengths wherever padding is necessary, so we do that first, then we combine all of the tensors for each field in each instance into a set of batched tensors for each field.","title":"as_tensor_dict"},{"location":"api/data/batch/#__iter__","text":"class Batch ( Iterable ): | ... | def __iter__ ( self ) -> Iterator [ Instance ]","title":"__iter__"},{"location":"api/data/batch/#index_instances","text":"class Batch ( Iterable ): | ... | def index_instances ( self , vocab : Vocabulary ) -> None","title":"index_instances"},{"location":"api/data/batch/#print_statistics","text":"class Batch ( Iterable ): | ... | def print_statistics ( self ) -> None","title":"print_statistics"},{"location":"api/data/image_loader/","text":"allennlp .data .image_loader [SOURCE] OnePath \u00b6 OnePath = Union [ str , PathLike ] ManyPaths \u00b6 ManyPaths = Sequence [ OnePath ] ImagesWithSize \u00b6 ImagesWithSize = Tuple [ FloatTensor , IntTensor ] ImageLoader \u00b6 class ImageLoader ( Registrable ): | def __init__ ( | self , | * , size_divisibility : int = 0 , | * , pad_value : float = 0.0 , | * , device : Union [ str , torch . device ] = \"cpu\" | ) -> None An ImageLoader is a callable that takes as input one or more filenames, and outputs two tensors: one representing the images themselves, and one that just holds the sizes of each image. The first tensor is the images and is of shape (batch_size, color_channels, height, width) . The second tensor is the sizes and is of shape (batch_size, 2) , where the last dimension contains the height and width, respectively. If only a single image is passed (as a Path or str , instead of a list) then the batch dimension will be removed. Subclasses only need to implement the load() method, which should load a single image from a path. Parameters \u00b6 size_divisibility : int , optional (default = 0 ) If set to a positive number, padding will be added so that the height and width dimensions are divisible by size_divisibility . Certain models may require this. pad_value : float , optional (default = 0.0 ) The value to use for padding. device : Union[str, torch.device] , optional (default = \"cpu\" ) A torch device identifier to put the image and size tensors on. default_implementation \u00b6 class ImageLoader ( Registrable ): | ... | default_implementation = \"torch\" __call__ \u00b6 class ImageLoader ( Registrable ): | ... | def __call__ ( | self , | filename_or_filenames : Union [ OnePath , ManyPaths ] | ) -> ImagesWithSize load \u00b6 class ImageLoader ( Registrable ): | ... | def load ( self , filename : OnePath ) -> FloatTensor TorchImageLoader \u00b6 @ImageLoader . register ( \"torch\" ) class TorchImageLoader ( ImageLoader ): | def __init__ ( | self , | * , image_backend : str = None , | * , resize : bool = True , | * , normalize : bool = True , | * , min_size : int = 800 , | * , max_size : int = 1333 , | * , pixel_mean : Tuple [ float , float , float ] = ( 0.485 , 0.456 , 0.406 ), | * , pixel_std : Tuple [ float , float , float ] = ( 0.229 , 0.224 , 0.225 ), | * , size_divisibility : int = 32 , | ** kwargs , | * , , | ) -> None This is just a wrapper around the default image loader from torchvision . Parameters \u00b6 image_backend : Optional[str] , optional (default = None ) Set the image backend. Can be one of \"PIL\" or \"accimage\" . resize : bool , optional (default = True ) If True (the default), images will be resized when necessary according to the values of min_size and max_size . normalize : bool , optional (default = True ) If True (the default), images will be normalized according to the values of pixel_mean and pixel_std . min_size : int , optional (default = 800 ) If resize is True , images smaller than this will be resized up to min_size . max_size : int , optional (default = 1333 ) If resize is True , images larger than this will be resized down to max_size . pixel_mean : Tuple[float, float, float] , optional (default = (0.485, 0.456, 0.406) ) Mean values for image normalization. The defaults are reasonable for most models from torchvision . pixel_std : Tuple[float, float, float] , optional (default = (0.229, 0.224, 0.225) ) Standard deviation for image normalization. The defaults are reasonable for most models from torchvision . size_divisibility : int , optional (default = 32 ) Same parameter as with the ImageLoader base class, but the default here is different. load \u00b6 class TorchImageLoader ( ImageLoader ): | ... | def load ( self , filename : OnePath ) -> FloatTensor","title":"image_loader"},{"location":"api/data/image_loader/#onepath","text":"OnePath = Union [ str , PathLike ]","title":"OnePath"},{"location":"api/data/image_loader/#manypaths","text":"ManyPaths = Sequence [ OnePath ]","title":"ManyPaths"},{"location":"api/data/image_loader/#imageswithsize","text":"ImagesWithSize = Tuple [ FloatTensor , IntTensor ]","title":"ImagesWithSize"},{"location":"api/data/image_loader/#imageloader","text":"class ImageLoader ( Registrable ): | def __init__ ( | self , | * , size_divisibility : int = 0 , | * , pad_value : float = 0.0 , | * , device : Union [ str , torch . device ] = \"cpu\" | ) -> None An ImageLoader is a callable that takes as input one or more filenames, and outputs two tensors: one representing the images themselves, and one that just holds the sizes of each image. The first tensor is the images and is of shape (batch_size, color_channels, height, width) . The second tensor is the sizes and is of shape (batch_size, 2) , where the last dimension contains the height and width, respectively. If only a single image is passed (as a Path or str , instead of a list) then the batch dimension will be removed. Subclasses only need to implement the load() method, which should load a single image from a path.","title":"ImageLoader"},{"location":"api/data/image_loader/#default_implementation","text":"class ImageLoader ( Registrable ): | ... | default_implementation = \"torch\"","title":"default_implementation"},{"location":"api/data/image_loader/#__call__","text":"class ImageLoader ( Registrable ): | ... | def __call__ ( | self , | filename_or_filenames : Union [ OnePath , ManyPaths ] | ) -> ImagesWithSize","title":"__call__"},{"location":"api/data/image_loader/#load","text":"class ImageLoader ( Registrable ): | ... | def load ( self , filename : OnePath ) -> FloatTensor","title":"load"},{"location":"api/data/image_loader/#torchimageloader","text":"@ImageLoader . register ( \"torch\" ) class TorchImageLoader ( ImageLoader ): | def __init__ ( | self , | * , image_backend : str = None , | * , resize : bool = True , | * , normalize : bool = True , | * , min_size : int = 800 , | * , max_size : int = 1333 , | * , pixel_mean : Tuple [ float , float , float ] = ( 0.485 , 0.456 , 0.406 ), | * , pixel_std : Tuple [ float , float , float ] = ( 0.229 , 0.224 , 0.225 ), | * , size_divisibility : int = 32 , | ** kwargs , | * , , | ) -> None This is just a wrapper around the default image loader from torchvision .","title":"TorchImageLoader"},{"location":"api/data/image_loader/#load_1","text":"class TorchImageLoader ( ImageLoader ): | ... | def load ( self , filename : OnePath ) -> FloatTensor","title":"load"},{"location":"api/data/instance/","text":"allennlp .data .instance [SOURCE] Instance \u00b6 class Instance ( Mapping [ str , Field ]): | def __init__ ( self , fields : MutableMapping [ str , Field ]) -> None An Instance is a collection of Field objects, specifying the inputs and outputs to some model. We don't make a distinction between inputs and outputs here, though - all operations are done on all fields, and when we return arrays, we return them as dictionaries keyed by field name. A model can then decide which fields it wants to use as inputs as which as outputs. The Fields in an Instance can start out either indexed or un-indexed. During the data processing pipeline, all fields will be indexed, after which multiple instances can be combined into a Batch and then converted into padded arrays. Parameters \u00b6 fields : Dict[str, Field] The Field objects that will be used to produce data arrays for this instance. __iter__ \u00b6 class Instance ( Mapping [ str , Field ]): | ... | def __iter__ ( self ) add_field \u00b6 class Instance ( Mapping [ str , Field ]): | ... | def add_field ( | self , | field_name : str , | field : Field , | vocab : Vocabulary = None | ) -> None Add the field to the existing fields mapping. If we have already indexed the Instance, then we also index field , so it is necessary to supply the vocab. count_vocab_items \u00b6 class Instance ( Mapping [ str , Field ]): | ... | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]]) Increments counts in the given counter for all of the vocabulary items in all of the Fields in this Instance . index_fields \u00b6 class Instance ( Mapping [ str , Field ]): | ... | def index_fields ( self , vocab : Vocabulary ) -> None Indexes all fields in this Instance using the provided Vocabulary . This mutates the current object, it does not return a new Instance . A DataLoader will call this on each pass through a dataset; we use the indexed flag to make sure that indexing only happens once. This means that if for some reason you modify your vocabulary after you've indexed your instances, you might get unexpected behavior. get_padding_lengths \u00b6 class Instance ( Mapping [ str , Field ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , Dict [ str , int ]] Returns a dictionary of padding lengths, keyed by field name. Each Field returns a mapping from padding keys to actual lengths, and we just key that dictionary by field name. as_tensor_dict \u00b6 class Instance ( Mapping [ str , Field ]): | ... | def as_tensor_dict ( | self , | padding_lengths : Dict [ str , Dict [ str , int ]] = None | ) -> Dict [ str , DataArray ] Pads each Field in this instance to the lengths given in padding_lengths (which is keyed by field name, then by padding key, the same as the return value in get_padding_lengths ), returning a list of torch tensors for each field. If padding_lengths is omitted, we will call self.get_padding_lengths() to get the sizes of the tensors to create. duplicate \u00b6 class Instance ( Mapping [ str , Field ]): | ... | def duplicate ( self ) -> \"Instance\" human_readable_dict \u00b6 class Instance ( Mapping [ str , Field ]): | ... | def human_readable_dict ( self ) -> JsonDict This function help to output instances to json files or print for human readability. Use case includes example-based explanation, where it's better to have a output file or rather than printing or logging.","title":"instance"},{"location":"api/data/instance/#instance","text":"class Instance ( Mapping [ str , Field ]): | def __init__ ( self , fields : MutableMapping [ str , Field ]) -> None An Instance is a collection of Field objects, specifying the inputs and outputs to some model. We don't make a distinction between inputs and outputs here, though - all operations are done on all fields, and when we return arrays, we return them as dictionaries keyed by field name. A model can then decide which fields it wants to use as inputs as which as outputs. The Fields in an Instance can start out either indexed or un-indexed. During the data processing pipeline, all fields will be indexed, after which multiple instances can be combined into a Batch and then converted into padded arrays.","title":"Instance"},{"location":"api/data/instance/#__iter__","text":"class Instance ( Mapping [ str , Field ]): | ... | def __iter__ ( self )","title":"__iter__"},{"location":"api/data/instance/#add_field","text":"class Instance ( Mapping [ str , Field ]): | ... | def add_field ( | self , | field_name : str , | field : Field , | vocab : Vocabulary = None | ) -> None Add the field to the existing fields mapping. If we have already indexed the Instance, then we also index field , so it is necessary to supply the vocab.","title":"add_field"},{"location":"api/data/instance/#count_vocab_items","text":"class Instance ( Mapping [ str , Field ]): | ... | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]]) Increments counts in the given counter for all of the vocabulary items in all of the Fields in this Instance .","title":"count_vocab_items"},{"location":"api/data/instance/#index_fields","text":"class Instance ( Mapping [ str , Field ]): | ... | def index_fields ( self , vocab : Vocabulary ) -> None Indexes all fields in this Instance using the provided Vocabulary . This mutates the current object, it does not return a new Instance . A DataLoader will call this on each pass through a dataset; we use the indexed flag to make sure that indexing only happens once. This means that if for some reason you modify your vocabulary after you've indexed your instances, you might get unexpected behavior.","title":"index_fields"},{"location":"api/data/instance/#get_padding_lengths","text":"class Instance ( Mapping [ str , Field ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , Dict [ str , int ]] Returns a dictionary of padding lengths, keyed by field name. Each Field returns a mapping from padding keys to actual lengths, and we just key that dictionary by field name.","title":"get_padding_lengths"},{"location":"api/data/instance/#as_tensor_dict","text":"class Instance ( Mapping [ str , Field ]): | ... | def as_tensor_dict ( | self , | padding_lengths : Dict [ str , Dict [ str , int ]] = None | ) -> Dict [ str , DataArray ] Pads each Field in this instance to the lengths given in padding_lengths (which is keyed by field name, then by padding key, the same as the return value in get_padding_lengths ), returning a list of torch tensors for each field. If padding_lengths is omitted, we will call self.get_padding_lengths() to get the sizes of the tensors to create.","title":"as_tensor_dict"},{"location":"api/data/instance/#duplicate","text":"class Instance ( Mapping [ str , Field ]): | ... | def duplicate ( self ) -> \"Instance\"","title":"duplicate"},{"location":"api/data/instance/#human_readable_dict","text":"class Instance ( Mapping [ str , Field ]): | ... | def human_readable_dict ( self ) -> JsonDict This function help to output instances to json files or print for human readability. Use case includes example-based explanation, where it's better to have a output file or rather than printing or logging.","title":"human_readable_dict"},{"location":"api/data/vocabulary/","text":"allennlp .data .vocabulary [SOURCE] A Vocabulary maps strings to integers, allowing for strings to be mapped to an out-of-vocabulary token. DEFAULT_NON_PADDED_NAMESPACES \u00b6 DEFAULT_NON_PADDED_NAMESPACES = ( \"*tags\" , \"*labels\" ) DEFAULT_PADDING_TOKEN \u00b6 DEFAULT_PADDING_TOKEN = \"@@PADDING@@\" DEFAULT_OOV_TOKEN \u00b6 DEFAULT_OOV_TOKEN = \"@@UNKNOWN@@\" NAMESPACE_PADDING_FILE \u00b6 NAMESPACE_PADDING_FILE = \"non_padded_namespaces.txt\" Vocabulary \u00b6 class Vocabulary ( Registrable ): | def __init__ ( | self , | counter : Dict [ str , Dict [ str , int ]] = None , | min_count : Dict [ str , int ] = None , | max_vocab_size : Union [ int , Dict [ str , int ]] = None , | non_padded_namespaces : Iterable [ str ] = DEFAULT_NON_PADDED_NAMESPACES , | pretrained_files : Optional [ Dict [ str , str ]] = None , | only_include_pretrained_words : bool = False , | tokens_to_add : Dict [ str , List [ str ]] = None , | min_pretrained_embeddings : Dict [ str , int ] = None , | padding_token : Optional [ str ] = DEFAULT_PADDING_TOKEN , | oov_token : Optional [ str ] = DEFAULT_OOV_TOKEN | ) -> None A Vocabulary maps strings to integers, allowing for strings to be mapped to an out-of-vocabulary token. Vocabularies are fit to a particular dataset, which we use to decide which tokens are in-vocabulary. Vocabularies also allow for several different namespaces, so you can have separate indices for 'a' as a word, and 'a' as a character, for instance, and so we can use this object to also map tag and label strings to indices, for a unified .fields.field.Field API. Most of the methods on this class allow you to pass in a namespace; by default we use the 'tokens' namespace, and you can omit the namespace argument everywhere and just use the default. This class is registered as a Vocabulary with four different names, which all point to different @classmethod constructors found in this class. from_instances is registered as \"from_instances\", from_files is registered as \"from_files\", from_files_and_instances is registered as \"extend\", and empty is registered as \"empty\". If you are using a configuration file to construct a vocabulary, you can use any of those strings as the \"type\" key in the configuration file to use the corresponding @classmethod to construct the object. \"from_instances\" is the default. Look at the docstring for the @classmethod to see what keys are allowed in the configuration file (when there is an instances argument to the @classmethod , it will be passed in separately and does not need a corresponding key in the configuration file). Parameters \u00b6 counter : Dict[str, Dict[str, int]] , optional (default = None ) A collection of counts from which to initialize this vocabulary. We will examine the counts and, together with the other parameters to this class, use them to decide which words are in-vocabulary. If this is None , we just won't initialize the vocabulary with anything. min_count : Dict[str, int] , optional (default = None ) When initializing the vocab from a counter, you can specify a minimum count, and every token with a count less than this will not be added to the dictionary. These minimum counts are namespace-specific , so you can specify different minimums for labels versus words tokens, for example. If a namespace does not have a key in the given dictionary, we will add all seen tokens to that namespace. max_vocab_size : Union[int, Dict[str, int]] , optional (default = None ) If you want to cap the number of tokens in your vocabulary, you can do so with this parameter. If you specify a single integer, every namespace will have its vocabulary fixed to be no larger than this. If you specify a dictionary, then each namespace in the counter can have a separate maximum vocabulary size. Any missing key will have a value of None , which means no cap on the vocabulary size. non_padded_namespaces : Iterable[str] , optional By default, we assume you are mapping word / character tokens to integers, and so you want to reserve word indices for padding and out-of-vocabulary tokens. However, if you are mapping NER or SRL tags, or class labels, to integers, you probably do not want to reserve indices for padding and out-of-vocabulary tokens. Use this field to specify which namespaces should not have padding and OOV tokens added. The format of each element of this is either a string, which must match field names exactly, or * followed by a string, which we match as a suffix against field names. We try to make the default here reasonable, so that you don't have to think about this. The default is (\"*tags\", \"*labels\") , so as long as your namespace ends in \"tags\" or \"labels\" (which is true by default for all tag and label fields in this code), you don't have to specify anything here. pretrained_files : Dict[str, str] , optional If provided, this map specifies the path to optional pretrained embedding files for each namespace. This can be used to either restrict the vocabulary to only words which appear in this file, or to ensure that any words in this file are included in the vocabulary regardless of their count, depending on the value of only_include_pretrained_words . Words which appear in the pretrained embedding file but not in the data are NOT included in the Vocabulary. min_pretrained_embeddings : Dict[str, int] , optional Specifies for each namespace a minimum number of lines (typically the most common words) to keep from pretrained embedding files, even for words not appearing in the data. By default the minimum number of lines to keep is 0. You can automatically include all lines for a namespace by setting the minimum number of lines to -1 . only_include_pretrained_words : bool , optional (default = False ) This defines the strategy for using any pretrained embedding files which may have been specified in pretrained_files . If False , we use an inclusive strategy and include both words in the counter that have a count of at least min_count and words from the pretrained file that are within the first N lines defined by min_pretrained_embeddings . If True , we use an exclusive strategy where words are only included in the Vocabulary if they are in the pretrained embedding file. Their count must also be at least min_count or they must be listed in the embedding file within the first N lines defined by min_pretrained_embeddings . tokens_to_add : Dict[str, List[str]] , optional (default = None ) If given, this is a list of tokens to add to the vocabulary, keyed by the namespace to add the tokens to. This is a way to be sure that certain items appear in your vocabulary, regardless of any other vocabulary computation. padding_token : str , optional (default = DEFAULT_PADDING_TOKEN ) If given, this the string used for padding. oov_token : str , optional (default = DEFAULT_OOV_TOKEN ) If given, this the string used for the out of vocabulary (OOVs) tokens. default_implementation \u00b6 class Vocabulary ( Registrable ): | ... | default_implementation = \"from_instances\" from_pretrained_transformer \u00b6 class Vocabulary ( Registrable ): | ... | @classmethod | def from_pretrained_transformer ( | cls , | model_name : str , | namespace : str = \"tokens\" , | oov_token : Optional [ str ] = None | ) -> \"Vocabulary\" Initialize a vocabulary from the vocabulary of a pretrained transformer model. If oov_token is not given, we will try to infer it from the transformer tokenizer. from_instances \u00b6 class Vocabulary ( Registrable ): | ... | @classmethod | def from_instances ( | cls , | instances : Iterable [ \"adi.Instance\" ], | min_count : Dict [ str , int ] = None , | max_vocab_size : Union [ int , Dict [ str , int ]] = None , | non_padded_namespaces : Iterable [ str ] = DEFAULT_NON_PADDED_NAMESPACES , | pretrained_files : Optional [ Dict [ str , str ]] = None , | only_include_pretrained_words : bool = False , | tokens_to_add : Dict [ str , List [ str ]] = None , | min_pretrained_embeddings : Dict [ str , int ] = None , | padding_token : Optional [ str ] = DEFAULT_PADDING_TOKEN , | oov_token : Optional [ str ] = DEFAULT_OOV_TOKEN | ) -> \"Vocabulary\" Constructs a vocabulary given a collection of Instances and some parameters. We count all of the vocabulary items in the instances, then pass those counts and the other parameters, to __init__ . See that method for a description of what the other parameters do. The instances parameter does not get an entry in a typical AllenNLP configuration file, but the other parameters do (if you want non-default parameters). from_files \u00b6 class Vocabulary ( Registrable ): | ... | @classmethod | def from_files ( | cls , | directory : Union [ str , os . PathLike ], | padding_token : Optional [ str ] = DEFAULT_PADDING_TOKEN , | oov_token : Optional [ str ] = DEFAULT_OOV_TOKEN | ) -> \"Vocabulary\" Loads a Vocabulary that was serialized either using save_to_files or inside a model archive file. Parameters \u00b6 directory : str The directory or archive file containing the serialized vocabulary. from_files_and_instances \u00b6 class Vocabulary ( Registrable ): | ... | @classmethod | def from_files_and_instances ( | cls , | instances : Iterable [ \"adi.Instance\" ], | directory : str , | padding_token : Optional [ str ] = DEFAULT_PADDING_TOKEN , | oov_token : Optional [ str ] = DEFAULT_OOV_TOKEN , | min_count : Dict [ str , int ] = None , | max_vocab_size : Union [ int , Dict [ str , int ]] = None , | non_padded_namespaces : Iterable [ str ] = DEFAULT_NON_PADDED_NAMESPACES , | pretrained_files : Optional [ Dict [ str , str ]] = None , | only_include_pretrained_words : bool = False , | tokens_to_add : Dict [ str , List [ str ]] = None , | min_pretrained_embeddings : Dict [ str , int ] = None | ) -> \"Vocabulary\" Extends an already generated vocabulary using a collection of instances. The instances parameter does not get an entry in a typical AllenNLP configuration file, but the other parameters do (if you want non-default parameters). See __init__ for a description of what the other parameters mean. from_pretrained_transformer_and_instances \u00b6 class Vocabulary ( Registrable ): | ... | @classmethod | def from_pretrained_transformer_and_instances ( | cls , | instances : Iterable [ \"adi.Instance\" ], | transformers : Dict [ str , str ], | min_count : Dict [ str , int ] = None , | max_vocab_size : Union [ int , Dict [ str , int ]] = None , | non_padded_namespaces : Iterable [ str ] = DEFAULT_NON_PADDED_NAMESPACES , | pretrained_files : Optional [ Dict [ str , str ]] = None , | only_include_pretrained_words : bool = False , | tokens_to_add : Dict [ str , List [ str ]] = None , | min_pretrained_embeddings : Dict [ str , int ] = None , | padding_token : Optional [ str ] = DEFAULT_PADDING_TOKEN , | oov_token : Optional [ str ] = DEFAULT_OOV_TOKEN | ) -> \"Vocabulary\" Construct a vocabulary given a collection of Instance 's and some parameters. Then extends it with generated vocabularies from pretrained transformers. Vocabulary from instances is constructed by passing parameters to from_instances , and then updated by including merging in vocabularies from from_pretrained_transformer . See other methods for full descriptions for what the other parameters do. The instances parameters does not get an entry in a typical AllenNLP configuration file, other parameters do (if you want non-default parameters). Parameters \u00b6 transformers : Dict[str, str] Dictionary mapping the vocab namespaces (keys) to a transformer model name (value). Namespaces not included will be ignored. Examples \u00b6 You can use this constructor by modifying the following example within your training configuration. { vocabulary: { type: 'from_pretrained_transformer_and_instances', transformers: { 'namespace1': 'bert-base-cased', 'namespace2': 'roberta-base', }, } } empty \u00b6 class Vocabulary ( Registrable ): | ... | @classmethod | def empty ( cls ) -> \"Vocabulary\" This method returns a bare vocabulary instantiated with cls() (so, Vocabulary() if you haven't made a subclass of this object). The only reason to call Vocabulary.empty() instead of Vocabulary() is if you are instantiating this object from a config file. We register this constructor with the key \"empty\", so if you know that you don't need to compute a vocabulary (either because you're loading a pre-trained model from an archive file, you're using a pre-trained transformer that has its own vocabulary, or something else), you can use this to avoid having the default vocabulary construction code iterate through the data. add_transformer_vocab \u00b6 class Vocabulary ( Registrable ): | ... | def add_transformer_vocab ( | self , | tokenizer : PreTrainedTokenizer , | namespace : str = \"tokens\" | ) -> None Copies tokens from a transformer tokenizer's vocab into the given namespace. set_from_file \u00b6 class Vocabulary ( Registrable ): | ... | def set_from_file ( | self , | filename : str , | is_padded : bool = True , | oov_token : str = DEFAULT_OOV_TOKEN , | namespace : str = \"tokens\" | ) If you already have a vocabulary file for a trained model somewhere, and you really want to use that vocabulary file instead of just setting the vocabulary from a dataset, for whatever reason, you can do that with this method. You must specify the namespace to use, and we assume that you want to use padding and OOV tokens for this. Parameters \u00b6 filename : str The file containing the vocabulary to load. It should be formatted as one token per line, with nothing else in the line. The index we assign to the token is the line number in the file (1-indexed if is_padded , 0-indexed otherwise). Note that this file should contain the OOV token string! is_padded : bool , optional (default = True ) Is this vocabulary padded? For token / word / character vocabularies, this should be True ; while for tag or label vocabularies, this should typically be False . If True , we add a padding token with index 0, and we enforce that the oov_token is present in the file. oov_token : str , optional (default = DEFAULT_OOV_TOKEN ) What token does this vocabulary use to represent out-of-vocabulary characters? This must show up as a line in the vocabulary file. When we find it, we replace oov_token with self._oov_token , because we only use one OOV token across namespaces. namespace : str , optional (default = \"tokens\" ) What namespace should we overwrite with this vocab file? extend_from_instances \u00b6 class Vocabulary ( Registrable ): | ... | def extend_from_instances ( | self , | instances : Iterable [ \"adi.Instance\" ] | ) -> None extend_from_vocab \u00b6 class Vocabulary ( Registrable ): | ... | def extend_from_vocab ( self , vocab : \"Vocabulary\" ) -> None Adds all vocabulary items from all namespaces in the given vocabulary to this vocabulary. Useful if you want to load a model and extends its vocabulary from new instances. We also add all non-padded namespaces from the given vocabulary to this vocabulary. save_to_files \u00b6 class Vocabulary ( Registrable ): | ... | def save_to_files ( self , directory : str ) -> None Persist this Vocabulary to files so it can be reloaded later. Each namespace corresponds to one file. Parameters \u00b6 directory : str The directory where we save the serialized vocabulary. is_padded \u00b6 class Vocabulary ( Registrable ): | ... | def is_padded ( self , namespace : str ) -> bool Returns whether or not there are padding and OOV tokens added to the given namespace. add_token_to_namespace \u00b6 class Vocabulary ( Registrable ): | ... | def add_token_to_namespace ( | self , | token : str , | namespace : str = \"tokens\" | ) -> int Adds token to the index, if it is not already present. Either way, we return the index of the token. add_tokens_to_namespace \u00b6 class Vocabulary ( Registrable ): | ... | def add_tokens_to_namespace ( | self , | tokens : List [ str ], | namespace : str = \"tokens\" | ) -> List [ int ] Adds tokens to the index, if they are not already present. Either way, we return the indices of the tokens in the order that they were given. get_index_to_token_vocabulary \u00b6 class Vocabulary ( Registrable ): | ... | def get_index_to_token_vocabulary ( | self , | namespace : str = \"tokens\" | ) -> Dict [ int , str ] get_token_to_index_vocabulary \u00b6 class Vocabulary ( Registrable ): | ... | def get_token_to_index_vocabulary ( | self , | namespace : str = \"tokens\" | ) -> Dict [ str , int ] get_token_index \u00b6 class Vocabulary ( Registrable ): | ... | def get_token_index ( | self , | token : str , | namespace : str = \"tokens\" | ) -> int get_token_from_index \u00b6 class Vocabulary ( Registrable ): | ... | def get_token_from_index ( | self , | index : int , | namespace : str = \"tokens\" | ) -> str get_vocab_size \u00b6 class Vocabulary ( Registrable ): | ... | def get_vocab_size ( self , namespace : str = \"tokens\" ) -> int get_namespaces \u00b6 class Vocabulary ( Registrable ): | ... | def get_namespaces ( self ) -> Set [ str ] print_statistics \u00b6 class Vocabulary ( Registrable ): | ... | def print_statistics ( self ) -> None","title":"vocabulary"},{"location":"api/data/vocabulary/#default_non_padded_namespaces","text":"DEFAULT_NON_PADDED_NAMESPACES = ( \"*tags\" , \"*labels\" )","title":"DEFAULT_NON_PADDED_NAMESPACES"},{"location":"api/data/vocabulary/#default_padding_token","text":"DEFAULT_PADDING_TOKEN = \"@@PADDING@@\"","title":"DEFAULT_PADDING_TOKEN"},{"location":"api/data/vocabulary/#default_oov_token","text":"DEFAULT_OOV_TOKEN = \"@@UNKNOWN@@\"","title":"DEFAULT_OOV_TOKEN"},{"location":"api/data/vocabulary/#namespace_padding_file","text":"NAMESPACE_PADDING_FILE = \"non_padded_namespaces.txt\"","title":"NAMESPACE_PADDING_FILE"},{"location":"api/data/vocabulary/#vocabulary","text":"class Vocabulary ( Registrable ): | def __init__ ( | self , | counter : Dict [ str , Dict [ str , int ]] = None , | min_count : Dict [ str , int ] = None , | max_vocab_size : Union [ int , Dict [ str , int ]] = None , | non_padded_namespaces : Iterable [ str ] = DEFAULT_NON_PADDED_NAMESPACES , | pretrained_files : Optional [ Dict [ str , str ]] = None , | only_include_pretrained_words : bool = False , | tokens_to_add : Dict [ str , List [ str ]] = None , | min_pretrained_embeddings : Dict [ str , int ] = None , | padding_token : Optional [ str ] = DEFAULT_PADDING_TOKEN , | oov_token : Optional [ str ] = DEFAULT_OOV_TOKEN | ) -> None A Vocabulary maps strings to integers, allowing for strings to be mapped to an out-of-vocabulary token. Vocabularies are fit to a particular dataset, which we use to decide which tokens are in-vocabulary. Vocabularies also allow for several different namespaces, so you can have separate indices for 'a' as a word, and 'a' as a character, for instance, and so we can use this object to also map tag and label strings to indices, for a unified .fields.field.Field API. Most of the methods on this class allow you to pass in a namespace; by default we use the 'tokens' namespace, and you can omit the namespace argument everywhere and just use the default. This class is registered as a Vocabulary with four different names, which all point to different @classmethod constructors found in this class. from_instances is registered as \"from_instances\", from_files is registered as \"from_files\", from_files_and_instances is registered as \"extend\", and empty is registered as \"empty\". If you are using a configuration file to construct a vocabulary, you can use any of those strings as the \"type\" key in the configuration file to use the corresponding @classmethod to construct the object. \"from_instances\" is the default. Look at the docstring for the @classmethod to see what keys are allowed in the configuration file (when there is an instances argument to the @classmethod , it will be passed in separately and does not need a corresponding key in the configuration file).","title":"Vocabulary"},{"location":"api/data/vocabulary/#default_implementation","text":"class Vocabulary ( Registrable ): | ... | default_implementation = \"from_instances\"","title":"default_implementation"},{"location":"api/data/vocabulary/#from_pretrained_transformer","text":"class Vocabulary ( Registrable ): | ... | @classmethod | def from_pretrained_transformer ( | cls , | model_name : str , | namespace : str = \"tokens\" , | oov_token : Optional [ str ] = None | ) -> \"Vocabulary\" Initialize a vocabulary from the vocabulary of a pretrained transformer model. If oov_token is not given, we will try to infer it from the transformer tokenizer.","title":"from_pretrained_transformer"},{"location":"api/data/vocabulary/#from_instances","text":"class Vocabulary ( Registrable ): | ... | @classmethod | def from_instances ( | cls , | instances : Iterable [ \"adi.Instance\" ], | min_count : Dict [ str , int ] = None , | max_vocab_size : Union [ int , Dict [ str , int ]] = None , | non_padded_namespaces : Iterable [ str ] = DEFAULT_NON_PADDED_NAMESPACES , | pretrained_files : Optional [ Dict [ str , str ]] = None , | only_include_pretrained_words : bool = False , | tokens_to_add : Dict [ str , List [ str ]] = None , | min_pretrained_embeddings : Dict [ str , int ] = None , | padding_token : Optional [ str ] = DEFAULT_PADDING_TOKEN , | oov_token : Optional [ str ] = DEFAULT_OOV_TOKEN | ) -> \"Vocabulary\" Constructs a vocabulary given a collection of Instances and some parameters. We count all of the vocabulary items in the instances, then pass those counts and the other parameters, to __init__ . See that method for a description of what the other parameters do. The instances parameter does not get an entry in a typical AllenNLP configuration file, but the other parameters do (if you want non-default parameters).","title":"from_instances"},{"location":"api/data/vocabulary/#from_files","text":"class Vocabulary ( Registrable ): | ... | @classmethod | def from_files ( | cls , | directory : Union [ str , os . PathLike ], | padding_token : Optional [ str ] = DEFAULT_PADDING_TOKEN , | oov_token : Optional [ str ] = DEFAULT_OOV_TOKEN | ) -> \"Vocabulary\" Loads a Vocabulary that was serialized either using save_to_files or inside a model archive file.","title":"from_files"},{"location":"api/data/vocabulary/#from_files_and_instances","text":"class Vocabulary ( Registrable ): | ... | @classmethod | def from_files_and_instances ( | cls , | instances : Iterable [ \"adi.Instance\" ], | directory : str , | padding_token : Optional [ str ] = DEFAULT_PADDING_TOKEN , | oov_token : Optional [ str ] = DEFAULT_OOV_TOKEN , | min_count : Dict [ str , int ] = None , | max_vocab_size : Union [ int , Dict [ str , int ]] = None , | non_padded_namespaces : Iterable [ str ] = DEFAULT_NON_PADDED_NAMESPACES , | pretrained_files : Optional [ Dict [ str , str ]] = None , | only_include_pretrained_words : bool = False , | tokens_to_add : Dict [ str , List [ str ]] = None , | min_pretrained_embeddings : Dict [ str , int ] = None | ) -> \"Vocabulary\" Extends an already generated vocabulary using a collection of instances. The instances parameter does not get an entry in a typical AllenNLP configuration file, but the other parameters do (if you want non-default parameters). See __init__ for a description of what the other parameters mean.","title":"from_files_and_instances"},{"location":"api/data/vocabulary/#from_pretrained_transformer_and_instances","text":"class Vocabulary ( Registrable ): | ... | @classmethod | def from_pretrained_transformer_and_instances ( | cls , | instances : Iterable [ \"adi.Instance\" ], | transformers : Dict [ str , str ], | min_count : Dict [ str , int ] = None , | max_vocab_size : Union [ int , Dict [ str , int ]] = None , | non_padded_namespaces : Iterable [ str ] = DEFAULT_NON_PADDED_NAMESPACES , | pretrained_files : Optional [ Dict [ str , str ]] = None , | only_include_pretrained_words : bool = False , | tokens_to_add : Dict [ str , List [ str ]] = None , | min_pretrained_embeddings : Dict [ str , int ] = None , | padding_token : Optional [ str ] = DEFAULT_PADDING_TOKEN , | oov_token : Optional [ str ] = DEFAULT_OOV_TOKEN | ) -> \"Vocabulary\" Construct a vocabulary given a collection of Instance 's and some parameters. Then extends it with generated vocabularies from pretrained transformers. Vocabulary from instances is constructed by passing parameters to from_instances , and then updated by including merging in vocabularies from from_pretrained_transformer . See other methods for full descriptions for what the other parameters do. The instances parameters does not get an entry in a typical AllenNLP configuration file, other parameters do (if you want non-default parameters).","title":"from_pretrained_transformer_and_instances"},{"location":"api/data/vocabulary/#empty","text":"class Vocabulary ( Registrable ): | ... | @classmethod | def empty ( cls ) -> \"Vocabulary\" This method returns a bare vocabulary instantiated with cls() (so, Vocabulary() if you haven't made a subclass of this object). The only reason to call Vocabulary.empty() instead of Vocabulary() is if you are instantiating this object from a config file. We register this constructor with the key \"empty\", so if you know that you don't need to compute a vocabulary (either because you're loading a pre-trained model from an archive file, you're using a pre-trained transformer that has its own vocabulary, or something else), you can use this to avoid having the default vocabulary construction code iterate through the data.","title":"empty"},{"location":"api/data/vocabulary/#add_transformer_vocab","text":"class Vocabulary ( Registrable ): | ... | def add_transformer_vocab ( | self , | tokenizer : PreTrainedTokenizer , | namespace : str = \"tokens\" | ) -> None Copies tokens from a transformer tokenizer's vocab into the given namespace.","title":"add_transformer_vocab"},{"location":"api/data/vocabulary/#set_from_file","text":"class Vocabulary ( Registrable ): | ... | def set_from_file ( | self , | filename : str , | is_padded : bool = True , | oov_token : str = DEFAULT_OOV_TOKEN , | namespace : str = \"tokens\" | ) If you already have a vocabulary file for a trained model somewhere, and you really want to use that vocabulary file instead of just setting the vocabulary from a dataset, for whatever reason, you can do that with this method. You must specify the namespace to use, and we assume that you want to use padding and OOV tokens for this.","title":"set_from_file"},{"location":"api/data/vocabulary/#extend_from_instances","text":"class Vocabulary ( Registrable ): | ... | def extend_from_instances ( | self , | instances : Iterable [ \"adi.Instance\" ] | ) -> None","title":"extend_from_instances"},{"location":"api/data/vocabulary/#extend_from_vocab","text":"class Vocabulary ( Registrable ): | ... | def extend_from_vocab ( self , vocab : \"Vocabulary\" ) -> None Adds all vocabulary items from all namespaces in the given vocabulary to this vocabulary. Useful if you want to load a model and extends its vocabulary from new instances. We also add all non-padded namespaces from the given vocabulary to this vocabulary.","title":"extend_from_vocab"},{"location":"api/data/vocabulary/#save_to_files","text":"class Vocabulary ( Registrable ): | ... | def save_to_files ( self , directory : str ) -> None Persist this Vocabulary to files so it can be reloaded later. Each namespace corresponds to one file.","title":"save_to_files"},{"location":"api/data/vocabulary/#is_padded","text":"class Vocabulary ( Registrable ): | ... | def is_padded ( self , namespace : str ) -> bool Returns whether or not there are padding and OOV tokens added to the given namespace.","title":"is_padded"},{"location":"api/data/vocabulary/#add_token_to_namespace","text":"class Vocabulary ( Registrable ): | ... | def add_token_to_namespace ( | self , | token : str , | namespace : str = \"tokens\" | ) -> int Adds token to the index, if it is not already present. Either way, we return the index of the token.","title":"add_token_to_namespace"},{"location":"api/data/vocabulary/#add_tokens_to_namespace","text":"class Vocabulary ( Registrable ): | ... | def add_tokens_to_namespace ( | self , | tokens : List [ str ], | namespace : str = \"tokens\" | ) -> List [ int ] Adds tokens to the index, if they are not already present. Either way, we return the indices of the tokens in the order that they were given.","title":"add_tokens_to_namespace"},{"location":"api/data/vocabulary/#get_index_to_token_vocabulary","text":"class Vocabulary ( Registrable ): | ... | def get_index_to_token_vocabulary ( | self , | namespace : str = \"tokens\" | ) -> Dict [ int , str ]","title":"get_index_to_token_vocabulary"},{"location":"api/data/vocabulary/#get_token_to_index_vocabulary","text":"class Vocabulary ( Registrable ): | ... | def get_token_to_index_vocabulary ( | self , | namespace : str = \"tokens\" | ) -> Dict [ str , int ]","title":"get_token_to_index_vocabulary"},{"location":"api/data/vocabulary/#get_token_index","text":"class Vocabulary ( Registrable ): | ... | def get_token_index ( | self , | token : str , | namespace : str = \"tokens\" | ) -> int","title":"get_token_index"},{"location":"api/data/vocabulary/#get_token_from_index","text":"class Vocabulary ( Registrable ): | ... | def get_token_from_index ( | self , | index : int , | namespace : str = \"tokens\" | ) -> str","title":"get_token_from_index"},{"location":"api/data/vocabulary/#get_vocab_size","text":"class Vocabulary ( Registrable ): | ... | def get_vocab_size ( self , namespace : str = \"tokens\" ) -> int","title":"get_vocab_size"},{"location":"api/data/vocabulary/#get_namespaces","text":"class Vocabulary ( Registrable ): | ... | def get_namespaces ( self ) -> Set [ str ]","title":"get_namespaces"},{"location":"api/data/vocabulary/#print_statistics","text":"class Vocabulary ( Registrable ): | ... | def print_statistics ( self ) -> None","title":"print_statistics"},{"location":"api/data/data_loaders/data_collator/","text":"allennlp .data .data_loaders .data_collator [SOURCE] allennlp_collate \u00b6 def allennlp_collate ( instances : List [ Instance ]) -> TensorDict This is the default function used to turn a list of Instance s into a TensorDict batch. DataCollator \u00b6 class DataCollator ( Registrable ) This class is similar with DataCollator in Transformers Allow to do some dynamic operations for tensor in different batches Cause this method run before each epoch to convert List[Instance] to TensorDict default_implementation \u00b6 class DataCollator ( Registrable ): | ... | default_implementation = \"allennlp\" __call__ \u00b6 class DataCollator ( Registrable ): | ... | def __call__ ( self , instances : List [ Instance ]) -> TensorDict DefaultDataCollator \u00b6 @DataCollator . register ( \"allennlp\" ) class DefaultDataCollator ( DataCollator ) __call__ \u00b6 class DefaultDataCollator ( DataCollator ): | ... | def __call__ ( self , instances : List [ Instance ]) -> TensorDict LanguageModelingDataCollator \u00b6 @DataCollator . register ( \"language_model\" ) class LanguageModelingDataCollator ( DataCollator ): | def __init__ ( | self , | model_name : str , | mlm : bool = True , | mlm_probability : float = 0.15 , | filed_name : str = \"source\" , | namespace : str = \"tokens\" | ) Register as an DataCollator with name LanguageModelingDataCollator Used for language modeling. __call__ \u00b6 class LanguageModelingDataCollator ( DataCollator ): | ... | def __call__ ( self , instances : List [ Instance ]) -> TensorDict process_tokens \u00b6 class LanguageModelingDataCollator ( DataCollator ): | ... | def process_tokens ( self , tensor_dicts : TensorDict ) -> TensorDict","title":"data_collator"},{"location":"api/data/data_loaders/data_collator/#allennlp_collate","text":"def allennlp_collate ( instances : List [ Instance ]) -> TensorDict This is the default function used to turn a list of Instance s into a TensorDict batch.","title":"allennlp_collate"},{"location":"api/data/data_loaders/data_collator/#datacollator","text":"class DataCollator ( Registrable ) This class is similar with DataCollator in Transformers Allow to do some dynamic operations for tensor in different batches Cause this method run before each epoch to convert List[Instance] to TensorDict","title":"DataCollator"},{"location":"api/data/data_loaders/data_collator/#default_implementation","text":"class DataCollator ( Registrable ): | ... | default_implementation = \"allennlp\"","title":"default_implementation"},{"location":"api/data/data_loaders/data_collator/#__call__","text":"class DataCollator ( Registrable ): | ... | def __call__ ( self , instances : List [ Instance ]) -> TensorDict","title":"__call__"},{"location":"api/data/data_loaders/data_collator/#defaultdatacollator","text":"@DataCollator . register ( \"allennlp\" ) class DefaultDataCollator ( DataCollator )","title":"DefaultDataCollator"},{"location":"api/data/data_loaders/data_collator/#__call___1","text":"class DefaultDataCollator ( DataCollator ): | ... | def __call__ ( self , instances : List [ Instance ]) -> TensorDict","title":"__call__"},{"location":"api/data/data_loaders/data_collator/#languagemodelingdatacollator","text":"@DataCollator . register ( \"language_model\" ) class LanguageModelingDataCollator ( DataCollator ): | def __init__ ( | self , | model_name : str , | mlm : bool = True , | mlm_probability : float = 0.15 , | filed_name : str = \"source\" , | namespace : str = \"tokens\" | ) Register as an DataCollator with name LanguageModelingDataCollator Used for language modeling.","title":"LanguageModelingDataCollator"},{"location":"api/data/data_loaders/data_collator/#__call___2","text":"class LanguageModelingDataCollator ( DataCollator ): | ... | def __call__ ( self , instances : List [ Instance ]) -> TensorDict","title":"__call__"},{"location":"api/data/data_loaders/data_collator/#process_tokens","text":"class LanguageModelingDataCollator ( DataCollator ): | ... | def process_tokens ( self , tensor_dicts : TensorDict ) -> TensorDict","title":"process_tokens"},{"location":"api/data/data_loaders/data_loader/","text":"allennlp .data .data_loaders .data_loader [SOURCE] TensorDict \u00b6 TensorDict = Dict [ str , Union [ torch . Tensor , Dict [ str , torch . Tensor ]]] TensorDict is the type we use for batches. DataLoader \u00b6 class DataLoader ( Registrable ) A DataLoader is responsible for generating batches of instances from a DatasetReader , or another source of data. This is purely an abstract base class. All concrete subclasses must provide implementations of the following methods: __iter__() that creates an iterable of TensorDict s, iter_instances() that creates an iterable of Instance s, index_with() that should index the data with a vocabulary, and set_target_device() , which updates the device that batch tensors should be put it when they are generated in __iter__() . Additionally, this class should also implement __len__() when possible. The default implementation is MultiProcessDataLoader . default_implementation \u00b6 class DataLoader ( Registrable ): | ... | default_implementation = \"multiprocess\" __iter__ \u00b6 class DataLoader ( Registrable ): | ... | def __iter__ ( self ) -> Iterator [ TensorDict ] iter_instances \u00b6 class DataLoader ( Registrable ): | ... | def iter_instances ( self ) -> Iterator [ Instance ] index_with \u00b6 class DataLoader ( Registrable ): | ... | def index_with ( self , vocab : Vocabulary ) -> None set_target_device \u00b6 class DataLoader ( Registrable ): | ... | def set_target_device ( self , device : torch . device ) -> None","title":"data_loader"},{"location":"api/data/data_loaders/data_loader/#tensordict","text":"TensorDict = Dict [ str , Union [ torch . Tensor , Dict [ str , torch . Tensor ]]] TensorDict is the type we use for batches.","title":"TensorDict"},{"location":"api/data/data_loaders/data_loader/#dataloader","text":"class DataLoader ( Registrable ) A DataLoader is responsible for generating batches of instances from a DatasetReader , or another source of data. This is purely an abstract base class. All concrete subclasses must provide implementations of the following methods: __iter__() that creates an iterable of TensorDict s, iter_instances() that creates an iterable of Instance s, index_with() that should index the data with a vocabulary, and set_target_device() , which updates the device that batch tensors should be put it when they are generated in __iter__() . Additionally, this class should also implement __len__() when possible. The default implementation is MultiProcessDataLoader .","title":"DataLoader"},{"location":"api/data/data_loaders/data_loader/#default_implementation","text":"class DataLoader ( Registrable ): | ... | default_implementation = \"multiprocess\"","title":"default_implementation"},{"location":"api/data/data_loaders/data_loader/#__iter__","text":"class DataLoader ( Registrable ): | ... | def __iter__ ( self ) -> Iterator [ TensorDict ]","title":"__iter__"},{"location":"api/data/data_loaders/data_loader/#iter_instances","text":"class DataLoader ( Registrable ): | ... | def iter_instances ( self ) -> Iterator [ Instance ]","title":"iter_instances"},{"location":"api/data/data_loaders/data_loader/#index_with","text":"class DataLoader ( Registrable ): | ... | def index_with ( self , vocab : Vocabulary ) -> None","title":"index_with"},{"location":"api/data/data_loaders/data_loader/#set_target_device","text":"class DataLoader ( Registrable ): | ... | def set_target_device ( self , device : torch . device ) -> None","title":"set_target_device"},{"location":"api/data/data_loaders/multiprocess_data_loader/","text":"allennlp .data .data_loaders .multiprocess_data_loader [SOURCE] MultiProcessDataLoader \u00b6 @DataLoader . register ( \"multiprocess\" ) class MultiProcessDataLoader ( DataLoader ): | def __init__ ( | self , | reader : DatasetReader , | data_path : DatasetReaderInput , | * , batch_size : int = None , | * , drop_last : bool = False , | * , shuffle : bool = False , | * , batch_sampler : BatchSampler = None , | * , batches_per_epoch : int = None , | * , num_workers : int = 0 , | * , max_instances_in_memory : int = None , | * , start_method : str = \"fork\" , | * , cuda_device : Optional [ Union [ int , str , torch . device ]] = None , | * , quiet : bool = False , | * , collate_fn : DataCollator = DefaultDataCollator () | ) -> None The MultiProcessDataLoader is a DataLoader that's optimized for AllenNLP experiments. See Using your reader with multi-process or distributed data loading for more information on how to optimize your DatasetReader for use with this DataLoader . Parameters \u00b6 reader : DatasetReader A DatasetReader used to load instances from the data_path . data_path : DatasetReaderInput Passed to DatasetReader.read() . Note In a typical AllenNLP configuration file, the reader and data_path parameters don't get an entry under the data_loader . The reader is constructed separately from the corresponding dataset_reader params, and the data_path is taken from the train_data_path , validation_data_path , or test_data_path . batch_size : int , optional (default = None ) When batch_sampler is unspecified, this option can be combined with drop_last and shuffle to control automatic batch sampling. drop_last : bool , optional (default = False ) When batch_sampler is unspecified, this option can be combined with batch_size and shuffle to control automatic batch sampling. If True , the last batch will be dropped if it doesn't contain a full batch_size number of Instance s. shuffle : bool , optional (default = False ) When batch_sampler is unspecified, this option can be combined with batch_size and drop_last to control automatic batch sampling. batch_sampler : BatchSampler , optional (default = None ) A BatchSampler to handle batching. This option is mutually exclusive with batch_size , drop_last , and shuffle . batches_per_epoch : int , optional (default = None ) If specified, exactly batches_per_epoch batches will be generated with each call to __iter__() . num_workers : int , optional (default = 0 ) The number of workers to use to read Instances in parallel. If num_workers = 0 , everything is done in the main process. Otherwise num_workers workers are forked or spawned (depending on the value of start_method ), each of which calls read() on their copy of the reader . This means that in order for multi-process loading to be efficient when num_workers > 1 , the reader needs to implement manual_multiprocess_sharding . Warning Multi-processing code in Python is complicated! We highly recommend you read the short Best practices and Common issues sections below before using this option. max_instances_in_memory : int , optional (default = None ) If not specified, all instances will be read and cached in memory for the duration of the data loader's life. This is generally ideal when your data can fit in memory during training. However, when your datasets are too big, using this option will turn on lazy loading, where only max_instances_in_memory instances are processed at a time. Note This setting will affect how a batch_sampler is applied. If max_instances_in_memory is None , the sampler will be applied to all Instances . Otherwise the sampler will be applied to only max_instances_in_memory Instances at a time. Therefore when using this option with a sampler, you should generally set it to a multiple of the sampler's batch_size (if it has one). start_method : str , optional (default = \"fork\" ) The start method used to spin up workers. On Linux or OS X, \"fork\" usually has the lowest overhead for starting workers but could potentially lead to dead-locks if you're using lower-level libraries that are not fork-safe. If you run into these issues, try using \"spawn\" instead. cuda_device : Optional[Union[int, str, torch.device]] , optional (default = None ) If given, batches will automatically be put on this device. Note This should typically not be set in an AllenNLP configuration file. The Trainer will automatically call set_target_device() before iterating over batches. quiet : bool , optional (default = False ) If True , tqdm progress bars will be disabled. collate_fn : DataCollator , optional (default = DefaultDataCollator ) Best practices \u00b6 Large datasets If your dataset is too big to fit into memory (a common problem), you'll need to load it lazily. This is done by simply setting the max_instances_in_memory parameter to a non-zero integer. The optimal value depends on your use case. If you're using a batch_sampler , you will generally get better samples by setting max_instances_in_memory to a higher number - such as 10 to 100 times your batch size - since this determines how many Instances your batch_sampler gets to sample from at a time. If you're not using a batch_sampler then this number is much less important. Setting it to 2 to 10 times your batch size is a reasonable value. Keep in mind that using max_instances_in_memory generally results in a slower training loop unless you load data in worker processes by setting the num_workers option to a non-zero integer (see below). That way data loading won't block the main process. Performance The quickest way to increase the performance of data loading is adjust the num_workers parameter. num_workers determines how many workers are used to read Instances from your DatasetReader . By default, this is set to 0 , which means everything is done in the main process. Before trying to set num_workers to a non-zero number, you should make sure your DatasetReader is optimized for use with multi-process data loading . Common issues \u00b6 Dead-locks Multiprocessing code in Python is complicated! Especially code that involves lower-level libraries which may be spawning their own threads. If you run into dead-locks while using num_workers > 0 , luckily there are two simple work-arounds which usually fix the issue. The first work-around is to disable parallelism for these low-level libraries. For example, setting the environment variables OMP_NUM_THREADS=1 and TOKENIZERS_PARALLELISM=0 will do so for PyTorch and Numpy (for CPU operations) and HuggingFace Tokenizers, respectively. Alternatively, changing the start_method to \"spawn\" (when available, depending on your OS) may fix your issues without disabling parallelism for other libraries. See issue #4848 for more info. Dead-locks could also be caused by running out of shared memory (see below). Shared memory restrictions Tensors are passed between processes using shared memory, and some systems impose strict limits on the allowed size of shared memory. Luckily this is simple to debug and simple to fix. First, to verify that this is your issue just watch your shared memory as your data loader runs. For example, run watch -n 0.3 'df -h | grep shm' . If you're seeing your shared memory blow up until it maxes-out, then you either need to decrease max_instances_in_memory or increase your system's ulimit . If you're using Docker, you can increase the shared memory available on a container by running it with the option --ipc=host or by setting --shm-size . See issue #4847 for more info. index_with \u00b6 class MultiProcessDataLoader ( DataLoader ): | ... | def index_with ( self , vocab : Vocabulary ) -> None __iter__ \u00b6 class MultiProcessDataLoader ( DataLoader ): | ... | def __iter__ ( self ) -> Iterator [ TensorDict ] iter_instances \u00b6 class MultiProcessDataLoader ( DataLoader ): | ... | def iter_instances ( self ) -> Iterator [ Instance ] set_target_device \u00b6 class MultiProcessDataLoader ( DataLoader ): | ... | def set_target_device ( self , device : torch . device ) -> None WorkerError \u00b6 class WorkerError ( Exception ): | def __init__ ( | self , | original_err_repr : str , | traceback : List [ str ] | ) -> None An error raised when a worker fails.","title":"multiprocess_data_loader"},{"location":"api/data/data_loaders/multiprocess_data_loader/#multiprocessdataloader","text":"@DataLoader . register ( \"multiprocess\" ) class MultiProcessDataLoader ( DataLoader ): | def __init__ ( | self , | reader : DatasetReader , | data_path : DatasetReaderInput , | * , batch_size : int = None , | * , drop_last : bool = False , | * , shuffle : bool = False , | * , batch_sampler : BatchSampler = None , | * , batches_per_epoch : int = None , | * , num_workers : int = 0 , | * , max_instances_in_memory : int = None , | * , start_method : str = \"fork\" , | * , cuda_device : Optional [ Union [ int , str , torch . device ]] = None , | * , quiet : bool = False , | * , collate_fn : DataCollator = DefaultDataCollator () | ) -> None The MultiProcessDataLoader is a DataLoader that's optimized for AllenNLP experiments. See Using your reader with multi-process or distributed data loading for more information on how to optimize your DatasetReader for use with this DataLoader .","title":"MultiProcessDataLoader"},{"location":"api/data/data_loaders/multiprocess_data_loader/#index_with","text":"class MultiProcessDataLoader ( DataLoader ): | ... | def index_with ( self , vocab : Vocabulary ) -> None","title":"index_with"},{"location":"api/data/data_loaders/multiprocess_data_loader/#__iter__","text":"class MultiProcessDataLoader ( DataLoader ): | ... | def __iter__ ( self ) -> Iterator [ TensorDict ]","title":"__iter__"},{"location":"api/data/data_loaders/multiprocess_data_loader/#iter_instances","text":"class MultiProcessDataLoader ( DataLoader ): | ... | def iter_instances ( self ) -> Iterator [ Instance ]","title":"iter_instances"},{"location":"api/data/data_loaders/multiprocess_data_loader/#set_target_device","text":"class MultiProcessDataLoader ( DataLoader ): | ... | def set_target_device ( self , device : torch . device ) -> None","title":"set_target_device"},{"location":"api/data/data_loaders/multiprocess_data_loader/#workererror","text":"class WorkerError ( Exception ): | def __init__ ( | self , | original_err_repr : str , | traceback : List [ str ] | ) -> None An error raised when a worker fails.","title":"WorkerError"},{"location":"api/data/data_loaders/multitask_data_loader/","text":"allennlp .data .data_loaders .multitask_data_loader [SOURCE] maybe_shuffle_instances \u00b6 def maybe_shuffle_instances ( loader : DataLoader , shuffle : bool ) -> Iterable [ Instance ] MultiTaskDataLoader \u00b6 @DataLoader . register ( \"multitask\" ) class MultiTaskDataLoader ( DataLoader ): | def __init__ ( | self , | reader : MultiTaskDatasetReader , | data_path : Dict [ str , str ], | scheduler : MultiTaskScheduler , | * , sampler : MultiTaskEpochSampler = None , | * , instances_per_epoch : int = None , | * , num_workers : Dict [ str , int ] = None , | * , max_instances_in_memory : Dict [ str , int ] = None , | * , start_method : Dict [ str , str ] = None , | * , instance_queue_size : Dict [ str , int ] = None , | * , instance_chunk_size : Dict [ str , int ] = None , | * , shuffle : bool = True , | * , cuda_device : Optional [ Union [ int , str , torch . device ]] = None | ) -> None A DataLoader intended for multi-task learning. The basic idea is that you use a MultiTaskDatasetReader , which takes a dictionary of DatasetReaders , keyed by some name. You use those same names for various parameters here, including the data paths that get passed to each reader. We will load each dataset and iterate over instances in them using a MultiTaskEpochSampler and a MultiTaskScheduler . The EpochSampler says how much to use from each dataset at each epoch, and the Scheduler orders the instances in the epoch however you want. Both of these are designed to be used in conjunction with trainer Callbacks , if desired, to have the sampling and/or scheduling behavior be dependent on the current state of training. While it is not necessarily required, this DatasetReader was designed to be used alongside a MultiTaskModel , which can handle instances coming from different datasets. If your datasets are similar enough (say, they are all reading comprehension datasets with the same format), or your model is flexible enough, then you could feasibly use this DataLoader with a normal, non-multitask Model . Registered as a DataLoader with name \"multitask\". Parameters \u00b6 reader : MultiTaskDatasetReader data_path : Dict[str, str] One file per underlying dataset reader in the MultiTaskDatasetReader , which will be passed to those readers to construct one DataLoader per dataset. scheduler : MultiTaskScheduler , optional (default = HomogeneousRoundRobinScheduler ) The scheduler determines how instances are ordered within an epoch. By default, we'll select one batch of instances from each dataset in turn, trying to ensure as uniform a mix of datasets as possible. Note that if your model can handle it, using a RoundRobinScheduler is likely better than a HomogeneousRoundRobinScheduler (because it does a better job mixing gradient signals from various datasets), so you may want to consider switching. We use the homogeneous version as default because it should work for any allennlp model, while the non-homogeneous one might not. sampler : MultiTaskEpochSampler , optional (default = None ) Only used if instances_per_epoch is not None . If we need to select a subset of the data for an epoch, this sampler will tell us with what proportion we should sample from each dataset. For instance, we might want to focus more on datasets that are underperforming in some way, by having those datasets contribute more instances this epoch than other datasets. instances_per_epoch : int , optional (default = None ) If not None , we will use this many instances per epoch of training, drawing from the underlying datasets according to the sampler . num_workers : Dict[str, int] , optional (default = None ) Used when creating one MultiProcessDataLoader per dataset. If you want non-default behavior for this parameter in the DataLoader for a particular dataset, pass the corresponding value here, keyed by the dataset name. max_instances_in_memory : Dict[str, int] , optional (default = None ) Used when creating one MultiProcessDataLoader per dataset. If you want non-default behavior for this parameter in the DataLoader for a particular dataset, pass the corresponding value here, keyed by the dataset name. start_method : Dict[str, str] , optional (default = None ) Used when creating one MultiProcessDataLoader per dataset. If you want non-default behavior for this parameter in the DataLoader for a particular dataset, pass the corresponding value here, keyed by the dataset name. instance_queue_size : Dict[str, int] , optional (default = None ) Used when creating one MultiProcessDataLoader per dataset. If you want non-default behavior for this parameter in the DataLoader for a particular dataset, pass the corresponding value here, keyed by the dataset name. instance_chunk_size : Dict[str, int] , optional (default = None ) Used when creating one MultiProcessDataLoader per dataset. If you want non-default behavior for this parameter in the DataLoader for a particular dataset, pass the corresponding value here, keyed by the dataset name. shuffle : bool , optional (default = True ) If False , we will not shuffle the instances that come from each underlying data loader. You almost certainly never want to use this except when debugging. cuda_device : Optional[Union[int, str, torch.device]] , optional (default = None ) If given, batches will automatically be put on this device. Note This should typically not be set in an AllenNLP configuration file. The Trainer will automatically call set_target_device() before iterating over batches. __iter__ \u00b6 class MultiTaskDataLoader ( DataLoader ): | ... | def __iter__ ( self ) -> Iterator [ TensorDict ] iter_instances \u00b6 class MultiTaskDataLoader ( DataLoader ): | ... | def iter_instances ( self ) -> Iterator [ Instance ] index_with \u00b6 class MultiTaskDataLoader ( DataLoader ): | ... | def index_with ( self , vocab : Vocabulary ) -> None set_target_device \u00b6 class MultiTaskDataLoader ( DataLoader ): | ... | def set_target_device ( self , device : torch . device ) -> None","title":"multitask_data_loader"},{"location":"api/data/data_loaders/multitask_data_loader/#maybe_shuffle_instances","text":"def maybe_shuffle_instances ( loader : DataLoader , shuffle : bool ) -> Iterable [ Instance ]","title":"maybe_shuffle_instances"},{"location":"api/data/data_loaders/multitask_data_loader/#multitaskdataloader","text":"@DataLoader . register ( \"multitask\" ) class MultiTaskDataLoader ( DataLoader ): | def __init__ ( | self , | reader : MultiTaskDatasetReader , | data_path : Dict [ str , str ], | scheduler : MultiTaskScheduler , | * , sampler : MultiTaskEpochSampler = None , | * , instances_per_epoch : int = None , | * , num_workers : Dict [ str , int ] = None , | * , max_instances_in_memory : Dict [ str , int ] = None , | * , start_method : Dict [ str , str ] = None , | * , instance_queue_size : Dict [ str , int ] = None , | * , instance_chunk_size : Dict [ str , int ] = None , | * , shuffle : bool = True , | * , cuda_device : Optional [ Union [ int , str , torch . device ]] = None | ) -> None A DataLoader intended for multi-task learning. The basic idea is that you use a MultiTaskDatasetReader , which takes a dictionary of DatasetReaders , keyed by some name. You use those same names for various parameters here, including the data paths that get passed to each reader. We will load each dataset and iterate over instances in them using a MultiTaskEpochSampler and a MultiTaskScheduler . The EpochSampler says how much to use from each dataset at each epoch, and the Scheduler orders the instances in the epoch however you want. Both of these are designed to be used in conjunction with trainer Callbacks , if desired, to have the sampling and/or scheduling behavior be dependent on the current state of training. While it is not necessarily required, this DatasetReader was designed to be used alongside a MultiTaskModel , which can handle instances coming from different datasets. If your datasets are similar enough (say, they are all reading comprehension datasets with the same format), or your model is flexible enough, then you could feasibly use this DataLoader with a normal, non-multitask Model . Registered as a DataLoader with name \"multitask\".","title":"MultiTaskDataLoader"},{"location":"api/data/data_loaders/multitask_data_loader/#__iter__","text":"class MultiTaskDataLoader ( DataLoader ): | ... | def __iter__ ( self ) -> Iterator [ TensorDict ]","title":"__iter__"},{"location":"api/data/data_loaders/multitask_data_loader/#iter_instances","text":"class MultiTaskDataLoader ( DataLoader ): | ... | def iter_instances ( self ) -> Iterator [ Instance ]","title":"iter_instances"},{"location":"api/data/data_loaders/multitask_data_loader/#index_with","text":"class MultiTaskDataLoader ( DataLoader ): | ... | def index_with ( self , vocab : Vocabulary ) -> None","title":"index_with"},{"location":"api/data/data_loaders/multitask_data_loader/#set_target_device","text":"class MultiTaskDataLoader ( DataLoader ): | ... | def set_target_device ( self , device : torch . device ) -> None","title":"set_target_device"},{"location":"api/data/data_loaders/multitask_epoch_sampler/","text":"allennlp .data .data_loaders .multitask_epoch_sampler [SOURCE] MultiTaskEpochSampler \u00b6 class MultiTaskEpochSampler ( Registrable ) A class that determines with what proportion each dataset should be sampled for a given epoch. This is used by the MultiTaskDataLoader . The main output of this class is the task proportion dictionary returned by get_task_proportions , which specifies what percentage of the instances for the current epoch should come from each dataset. To control this behavior as training progresses, there is an update_from_epoch_metrics method, which should be called from a Callback during training. get_task_proportions \u00b6 class MultiTaskEpochSampler ( Registrable ): | ... | def get_task_proportions ( | self , | data_loaders : Mapping [ str , DataLoader ] | ) -> Dict [ str , float ] Given a dictionary of DataLoaders for each dataset, returns what percentage of the instances for the current epoch of training should come from each dataset. The input dictionary could be used to determine how many datasets there are (e.g., for uniform sampling) or how big each dataset is (e.g., for sampling based on size), or it could be ignored entirely. update_from_epoch_metrics \u00b6 class MultiTaskEpochSampler ( Registrable ): | ... | def update_from_epoch_metrics ( | self , | epoch_metrics : Dict [ str , Any ] | ) -> None Some implementations of EpochSamplers change their behavior based on current epoch metrics. This method is meant to be called from a Callback , to let the sampler update its sampling proportions. If your sampling technique does not depend on epoch metrics, you do not need to implement this method. UniformSampler \u00b6 @MultiTaskEpochSampler . register ( \"uniform\" ) class UniformSampler ( MultiTaskEpochSampler ) Returns a uniform distribution over datasets at every epoch. Registered as a MultiTaskEpochSampler with name \"uniform\". get_task_proportions \u00b6 class UniformSampler ( MultiTaskEpochSampler ): | ... | def get_task_proportions ( | self , | data_loaders : Mapping [ str , DataLoader ] | ) -> Dict [ str , float ] WeightedSampler \u00b6 @MultiTaskEpochSampler . register ( \"weighted\" ) class WeightedSampler ( MultiTaskEpochSampler ): | def __init__ ( self , weights : Dict [ str , float ]) Returns a weighted distribution over datasets at every epoch, where every task has a weight. Registered as a MultiTaskEpochSampler with name \"weighted\". get_task_proportions \u00b6 class WeightedSampler ( MultiTaskEpochSampler ): | ... | def get_task_proportions ( | self , | data_loaders : Mapping [ str , DataLoader ] | ) -> Dict [ str , float ] ProportionalSampler \u00b6 @MultiTaskEpochSampler . register ( \"proportional\" ) class ProportionalSampler ( MultiTaskEpochSampler ) Samples from every dataset according to its size. This will have essentially the same effect as using all of the data at every epoch, but it lets you control for number of instances per epoch, if you want to do that. This requires that all data loaders have a __len__ (which means no lazy loading). If you need this functionality with lazy loading, implement your own sampler that takes dataset sizes as a constructor parameter. Registered as a MultiTaskEpochSampler with name \"proportional\". get_task_proportions \u00b6 class ProportionalSampler ( MultiTaskEpochSampler ): | ... | def get_task_proportions ( | self , | data_loaders : Mapping [ str , DataLoader ] | ) -> Dict [ str , float ]","title":"multitask_epoch_sampler"},{"location":"api/data/data_loaders/multitask_epoch_sampler/#multitaskepochsampler","text":"class MultiTaskEpochSampler ( Registrable ) A class that determines with what proportion each dataset should be sampled for a given epoch. This is used by the MultiTaskDataLoader . The main output of this class is the task proportion dictionary returned by get_task_proportions , which specifies what percentage of the instances for the current epoch should come from each dataset. To control this behavior as training progresses, there is an update_from_epoch_metrics method, which should be called from a Callback during training.","title":"MultiTaskEpochSampler"},{"location":"api/data/data_loaders/multitask_epoch_sampler/#get_task_proportions","text":"class MultiTaskEpochSampler ( Registrable ): | ... | def get_task_proportions ( | self , | data_loaders : Mapping [ str , DataLoader ] | ) -> Dict [ str , float ] Given a dictionary of DataLoaders for each dataset, returns what percentage of the instances for the current epoch of training should come from each dataset. The input dictionary could be used to determine how many datasets there are (e.g., for uniform sampling) or how big each dataset is (e.g., for sampling based on size), or it could be ignored entirely.","title":"get_task_proportions"},{"location":"api/data/data_loaders/multitask_epoch_sampler/#update_from_epoch_metrics","text":"class MultiTaskEpochSampler ( Registrable ): | ... | def update_from_epoch_metrics ( | self , | epoch_metrics : Dict [ str , Any ] | ) -> None Some implementations of EpochSamplers change their behavior based on current epoch metrics. This method is meant to be called from a Callback , to let the sampler update its sampling proportions. If your sampling technique does not depend on epoch metrics, you do not need to implement this method.","title":"update_from_epoch_metrics"},{"location":"api/data/data_loaders/multitask_epoch_sampler/#uniformsampler","text":"@MultiTaskEpochSampler . register ( \"uniform\" ) class UniformSampler ( MultiTaskEpochSampler ) Returns a uniform distribution over datasets at every epoch. Registered as a MultiTaskEpochSampler with name \"uniform\".","title":"UniformSampler"},{"location":"api/data/data_loaders/multitask_epoch_sampler/#get_task_proportions_1","text":"class UniformSampler ( MultiTaskEpochSampler ): | ... | def get_task_proportions ( | self , | data_loaders : Mapping [ str , DataLoader ] | ) -> Dict [ str , float ]","title":"get_task_proportions"},{"location":"api/data/data_loaders/multitask_epoch_sampler/#weightedsampler","text":"@MultiTaskEpochSampler . register ( \"weighted\" ) class WeightedSampler ( MultiTaskEpochSampler ): | def __init__ ( self , weights : Dict [ str , float ]) Returns a weighted distribution over datasets at every epoch, where every task has a weight. Registered as a MultiTaskEpochSampler with name \"weighted\".","title":"WeightedSampler"},{"location":"api/data/data_loaders/multitask_epoch_sampler/#get_task_proportions_2","text":"class WeightedSampler ( MultiTaskEpochSampler ): | ... | def get_task_proportions ( | self , | data_loaders : Mapping [ str , DataLoader ] | ) -> Dict [ str , float ]","title":"get_task_proportions"},{"location":"api/data/data_loaders/multitask_epoch_sampler/#proportionalsampler","text":"@MultiTaskEpochSampler . register ( \"proportional\" ) class ProportionalSampler ( MultiTaskEpochSampler ) Samples from every dataset according to its size. This will have essentially the same effect as using all of the data at every epoch, but it lets you control for number of instances per epoch, if you want to do that. This requires that all data loaders have a __len__ (which means no lazy loading). If you need this functionality with lazy loading, implement your own sampler that takes dataset sizes as a constructor parameter. Registered as a MultiTaskEpochSampler with name \"proportional\".","title":"ProportionalSampler"},{"location":"api/data/data_loaders/multitask_epoch_sampler/#get_task_proportions_3","text":"class ProportionalSampler ( MultiTaskEpochSampler ): | ... | def get_task_proportions ( | self , | data_loaders : Mapping [ str , DataLoader ] | ) -> Dict [ str , float ]","title":"get_task_proportions"},{"location":"api/data/data_loaders/multitask_scheduler/","text":"allennlp .data .data_loaders .multitask_scheduler [SOURCE] MultiTaskScheduler \u00b6 class MultiTaskScheduler ( Registrable ) A class that determines how to order instances within an epoch. This is used by the MultiTaskDataLoader . The main operation performed by this class is to take a dictionary of instance iterators, one for each dataset, and combine them into an iterator of batches, based on some scheduling algorithm (such as round robin, randomly choosing between available datasets, etc.). To control this behavior as training progresses, there is an update_from_epoch_metrics method available, which should be called from a Callback during training. Not all MultiTaskSchedulers will implement this method. batch_instances \u00b6 class MultiTaskScheduler ( Registrable ): | ... | def batch_instances ( | self , | epoch_instances : Dict [ str , Iterable [ Instance ]] | ) -> Iterable [ List [ Instance ]] Given a dictionary of Iterable[Instance] for each dataset, combines them into an Iterable of batches of instances. update_from_epoch_metrics \u00b6 class MultiTaskScheduler ( Registrable ): | ... | def update_from_epoch_metrics ( | self , | epoch_metrics : Dict [ str , Any ] | ) -> None In case you want to set the behavior of the scheduler based on current epoch metrics, you can do that by calling this method from a Callback . If your scheduling technique does not depend on epoch metrics, you do not need to implement this method. count_batches \u00b6 class MultiTaskScheduler ( Registrable ): | ... | def count_batches ( self , dataset_counts : Dict [ str , int ]) -> int Given the number of instances per dataset, this returns the total number of batches the scheduler will return. default_implementation \u00b6 class MultiTaskScheduler ( Registrable ): | ... | default_implementation = \"homogeneous_roundrobin\" RoundRobinScheduler \u00b6 @MultiTaskScheduler . register ( \"roundrobin\" ) class RoundRobinScheduler ( MultiTaskScheduler ): | def __init__ ( self , batch_size : int , drop_last : bool = False ) Orders instances in a round-robin fashion, where we take one instance from every dataset in turn. When one dataset runs out, we continue iterating round-robin through the rest. Registered as a MultiTaskScheduler with name \"roundrobin\". batch_instances \u00b6 class RoundRobinScheduler ( MultiTaskScheduler ): | ... | def batch_instances ( | self , | epoch_instances : Dict [ str , Iterable [ Instance ]] | ) -> Iterable [ List [ Instance ]] count_batches \u00b6 class RoundRobinScheduler ( MultiTaskScheduler ): | ... | def count_batches ( self , dataset_counts : Dict [ str , int ]) -> int HomogeneousRoundRobinScheduler \u00b6 @MultiTaskScheduler . register ( \"homogeneous_roundrobin\" ) class HomogeneousRoundRobinScheduler ( MultiTaskScheduler ): | def __init__ ( | self , | batch_size : Union [ int , Dict [ str , int ]], | drop_last : bool = False | ) Orders instances in a round-robin fashion, but grouped into batches composed entirely of instances from one dataset. We'll return one batch from one dataset, then another batch from a different dataset, etc. This is currently necessary in AllenNLP if your instances have different fields for different datasets, as we can't currently combine instances with different fields. When one dataset runs out, we continue iterating round-robin through the rest. If you want more fine-grained control over which datasets can be combined, it should be relatively straightforward to write your own scheduler, following this logic, which allows some datasets to be combined and others not. Registered as a MultiTaskScheduler with name \"homogeneous_roundrobin\". Parameters \u00b6 batch_size : Union[int, Dict[str, int]] Determines how many instances to group together in each dataset. If this is an int , the same value is used for all datasets; otherwise, the keys must correspond to the dataset names used elsewhere in the multi-task code. batch_instances \u00b6 class HomogeneousRoundRobinScheduler ( MultiTaskScheduler ): | ... | def batch_instances ( | self , | epoch_instances : Dict [ str , Iterable [ Instance ]] | ) -> Iterable [ List [ Instance ]] count_batches \u00b6 class HomogeneousRoundRobinScheduler ( MultiTaskScheduler ): | ... | def count_batches ( self , dataset_counts : Dict [ str , int ]) -> int","title":"multitask_scheduler"},{"location":"api/data/data_loaders/multitask_scheduler/#multitaskscheduler","text":"class MultiTaskScheduler ( Registrable ) A class that determines how to order instances within an epoch. This is used by the MultiTaskDataLoader . The main operation performed by this class is to take a dictionary of instance iterators, one for each dataset, and combine them into an iterator of batches, based on some scheduling algorithm (such as round robin, randomly choosing between available datasets, etc.). To control this behavior as training progresses, there is an update_from_epoch_metrics method available, which should be called from a Callback during training. Not all MultiTaskSchedulers will implement this method.","title":"MultiTaskScheduler"},{"location":"api/data/data_loaders/multitask_scheduler/#batch_instances","text":"class MultiTaskScheduler ( Registrable ): | ... | def batch_instances ( | self , | epoch_instances : Dict [ str , Iterable [ Instance ]] | ) -> Iterable [ List [ Instance ]] Given a dictionary of Iterable[Instance] for each dataset, combines them into an Iterable of batches of instances.","title":"batch_instances"},{"location":"api/data/data_loaders/multitask_scheduler/#update_from_epoch_metrics","text":"class MultiTaskScheduler ( Registrable ): | ... | def update_from_epoch_metrics ( | self , | epoch_metrics : Dict [ str , Any ] | ) -> None In case you want to set the behavior of the scheduler based on current epoch metrics, you can do that by calling this method from a Callback . If your scheduling technique does not depend on epoch metrics, you do not need to implement this method.","title":"update_from_epoch_metrics"},{"location":"api/data/data_loaders/multitask_scheduler/#count_batches","text":"class MultiTaskScheduler ( Registrable ): | ... | def count_batches ( self , dataset_counts : Dict [ str , int ]) -> int Given the number of instances per dataset, this returns the total number of batches the scheduler will return.","title":"count_batches"},{"location":"api/data/data_loaders/multitask_scheduler/#default_implementation","text":"class MultiTaskScheduler ( Registrable ): | ... | default_implementation = \"homogeneous_roundrobin\"","title":"default_implementation"},{"location":"api/data/data_loaders/multitask_scheduler/#roundrobinscheduler","text":"@MultiTaskScheduler . register ( \"roundrobin\" ) class RoundRobinScheduler ( MultiTaskScheduler ): | def __init__ ( self , batch_size : int , drop_last : bool = False ) Orders instances in a round-robin fashion, where we take one instance from every dataset in turn. When one dataset runs out, we continue iterating round-robin through the rest. Registered as a MultiTaskScheduler with name \"roundrobin\".","title":"RoundRobinScheduler"},{"location":"api/data/data_loaders/multitask_scheduler/#batch_instances_1","text":"class RoundRobinScheduler ( MultiTaskScheduler ): | ... | def batch_instances ( | self , | epoch_instances : Dict [ str , Iterable [ Instance ]] | ) -> Iterable [ List [ Instance ]]","title":"batch_instances"},{"location":"api/data/data_loaders/multitask_scheduler/#count_batches_1","text":"class RoundRobinScheduler ( MultiTaskScheduler ): | ... | def count_batches ( self , dataset_counts : Dict [ str , int ]) -> int","title":"count_batches"},{"location":"api/data/data_loaders/multitask_scheduler/#homogeneousroundrobinscheduler","text":"@MultiTaskScheduler . register ( \"homogeneous_roundrobin\" ) class HomogeneousRoundRobinScheduler ( MultiTaskScheduler ): | def __init__ ( | self , | batch_size : Union [ int , Dict [ str , int ]], | drop_last : bool = False | ) Orders instances in a round-robin fashion, but grouped into batches composed entirely of instances from one dataset. We'll return one batch from one dataset, then another batch from a different dataset, etc. This is currently necessary in AllenNLP if your instances have different fields for different datasets, as we can't currently combine instances with different fields. When one dataset runs out, we continue iterating round-robin through the rest. If you want more fine-grained control over which datasets can be combined, it should be relatively straightforward to write your own scheduler, following this logic, which allows some datasets to be combined and others not. Registered as a MultiTaskScheduler with name \"homogeneous_roundrobin\".","title":"HomogeneousRoundRobinScheduler"},{"location":"api/data/data_loaders/multitask_scheduler/#batch_instances_2","text":"class HomogeneousRoundRobinScheduler ( MultiTaskScheduler ): | ... | def batch_instances ( | self , | epoch_instances : Dict [ str , Iterable [ Instance ]] | ) -> Iterable [ List [ Instance ]]","title":"batch_instances"},{"location":"api/data/data_loaders/multitask_scheduler/#count_batches_2","text":"class HomogeneousRoundRobinScheduler ( MultiTaskScheduler ): | ... | def count_batches ( self , dataset_counts : Dict [ str , int ]) -> int","title":"count_batches"},{"location":"api/data/data_loaders/simple_data_loader/","text":"allennlp .data .data_loaders .simple_data_loader [SOURCE] SimpleDataLoader \u00b6 @DataLoader . register ( \"simple\" , constructor = \"from_dataset_reader\" ) class SimpleDataLoader ( DataLoader ): | def __init__ ( | self , | instances : List [ Instance ], | batch_size : int , | * , shuffle : bool = False , | * , batches_per_epoch : Optional [ int ] = None , | * , vocab : Optional [ Vocabulary ] = None | ) -> None A very simple DataLoader that is mostly used for testing. __iter__ \u00b6 class SimpleDataLoader ( DataLoader ): | ... | def __iter__ ( self ) -> Iterator [ TensorDict ] iter_instances \u00b6 class SimpleDataLoader ( DataLoader ): | ... | def iter_instances ( self ) -> Iterator [ Instance ] index_with \u00b6 class SimpleDataLoader ( DataLoader ): | ... | def index_with ( self , vocab : Vocabulary ) -> None set_target_device \u00b6 class SimpleDataLoader ( DataLoader ): | ... | def set_target_device ( self , device : torch . device ) -> None from_dataset_reader \u00b6 class SimpleDataLoader ( DataLoader ): | ... | @classmethod | def from_dataset_reader ( | cls , | reader : DatasetReader , | data_path : str , | batch_size : int , | shuffle : bool = False , | batches_per_epoch : Optional [ int ] = None , | quiet : bool = False | ) -> \"SimpleDataLoader\"","title":"simple_data_loader"},{"location":"api/data/data_loaders/simple_data_loader/#simpledataloader","text":"@DataLoader . register ( \"simple\" , constructor = \"from_dataset_reader\" ) class SimpleDataLoader ( DataLoader ): | def __init__ ( | self , | instances : List [ Instance ], | batch_size : int , | * , shuffle : bool = False , | * , batches_per_epoch : Optional [ int ] = None , | * , vocab : Optional [ Vocabulary ] = None | ) -> None A very simple DataLoader that is mostly used for testing.","title":"SimpleDataLoader"},{"location":"api/data/data_loaders/simple_data_loader/#__iter__","text":"class SimpleDataLoader ( DataLoader ): | ... | def __iter__ ( self ) -> Iterator [ TensorDict ]","title":"__iter__"},{"location":"api/data/data_loaders/simple_data_loader/#iter_instances","text":"class SimpleDataLoader ( DataLoader ): | ... | def iter_instances ( self ) -> Iterator [ Instance ]","title":"iter_instances"},{"location":"api/data/data_loaders/simple_data_loader/#index_with","text":"class SimpleDataLoader ( DataLoader ): | ... | def index_with ( self , vocab : Vocabulary ) -> None","title":"index_with"},{"location":"api/data/data_loaders/simple_data_loader/#set_target_device","text":"class SimpleDataLoader ( DataLoader ): | ... | def set_target_device ( self , device : torch . device ) -> None","title":"set_target_device"},{"location":"api/data/data_loaders/simple_data_loader/#from_dataset_reader","text":"class SimpleDataLoader ( DataLoader ): | ... | @classmethod | def from_dataset_reader ( | cls , | reader : DatasetReader , | data_path : str , | batch_size : int , | shuffle : bool = False , | batches_per_epoch : Optional [ int ] = None , | quiet : bool = False | ) -> \"SimpleDataLoader\"","title":"from_dataset_reader"},{"location":"api/data/dataset_readers/babi/","text":"allennlp .data .dataset_readers .babi [SOURCE] BabiReader \u00b6 @DatasetReader . register ( \"babi\" ) class BabiReader ( DatasetReader ): | def __init__ ( | self , | keep_sentences : bool = False , | token_indexers : Dict [ str , TokenIndexer ] = None , | ** kwargs | ) -> None Reads one single task in the bAbI tasks format as formulated in Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks (https://arxiv.org/abs/1502.05698). Since this class handle a single file, if one wants to load multiple tasks together it has to merge them into a single file and use this reader. Registered as a DatasetReader with name \"babi\". Parameters \u00b6 keep_sentences : bool , optional (default = False ) Whether to keep each sentence in the context or to concatenate them. Default is False that corresponds to concatenation. token_indexers : Dict[str, TokenIndexer] , optional (default = {\"tokens\": SingleIdTokenIndexer()} ) We use this to define the input representation for the text. See TokenIndexer . text_to_instance \u00b6 class BabiReader ( DatasetReader ): | ... | def text_to_instance ( | self , | context : List [ List [ str ]], | question : List [ str ], | answer : str , | supports : List [ int ] | ) -> Instance apply_token_indexers \u00b6 class BabiReader ( DatasetReader ): | ... | def apply_token_indexers ( self , instance : Instance ) -> None","title":"babi"},{"location":"api/data/dataset_readers/babi/#babireader","text":"@DatasetReader . register ( \"babi\" ) class BabiReader ( DatasetReader ): | def __init__ ( | self , | keep_sentences : bool = False , | token_indexers : Dict [ str , TokenIndexer ] = None , | ** kwargs | ) -> None Reads one single task in the bAbI tasks format as formulated in Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks (https://arxiv.org/abs/1502.05698). Since this class handle a single file, if one wants to load multiple tasks together it has to merge them into a single file and use this reader. Registered as a DatasetReader with name \"babi\".","title":"BabiReader"},{"location":"api/data/dataset_readers/babi/#text_to_instance","text":"class BabiReader ( DatasetReader ): | ... | def text_to_instance ( | self , | context : List [ List [ str ]], | question : List [ str ], | answer : str , | supports : List [ int ] | ) -> Instance","title":"text_to_instance"},{"location":"api/data/dataset_readers/babi/#apply_token_indexers","text":"class BabiReader ( DatasetReader ): | ... | def apply_token_indexers ( self , instance : Instance ) -> None","title":"apply_token_indexers"},{"location":"api/data/dataset_readers/conll2003/","text":"allennlp .data .dataset_readers .conll2003 [SOURCE] Conll2003DatasetReader \u00b6 @DatasetReader . register ( \"conll2003\" ) class Conll2003DatasetReader ( DatasetReader ): | def __init__ ( | self , | token_indexers : Dict [ str , TokenIndexer ] = None , | tag_label : str = \"ner\" , | feature_labels : Sequence [ str ] = (), | convert_to_coding_scheme : Optional [ str ] = None , | label_namespace : str = \"labels\" , | ** kwargs | ) -> None Reads instances from a pretokenised file where each line is in the following format: WORD POS-TAG CHUNK-TAG NER-TAG with a blank line indicating the end of each sentence and -DOCSTART- -X- -X- O indicating the end of each article, and converts it into a Dataset suitable for sequence tagging. Each Instance contains the words in the \"tokens\" TextField . The values corresponding to the tag_label values will get loaded into the \"tags\" SequenceLabelField . And if you specify any feature_labels (you probably shouldn't), the corresponding values will get loaded into their own SequenceLabelField s. This dataset reader ignores the \"article\" divisions and simply treats each sentence as an independent Instance . (Technically the reader splits sentences on any combination of blank lines and \"DOCSTART\" tags; in particular, it does the right thing on well formed inputs.) Registered as a DatasetReader with name \"conll2003\". Parameters \u00b6 token_indexers : Dict[str, TokenIndexer] , optional (default = {\"tokens\": SingleIdTokenIndexer()} ) We use this to define the input representation for the text. See TokenIndexer . tag_label : str , optional (default = ner ) Specify ner , pos , or chunk to have that tag loaded into the instance field tag . feature_labels : Sequence[str] , optional (default = () ) These labels will be loaded as features into the corresponding instance fields: pos -> pos_tags , chunk -> chunk_tags , ner -> ner_tags Each will have its own namespace : pos_tags , chunk_tags , ner_tags . If you want to use one of the tags as a feature in your model, it should be specified here. convert_to_coding_scheme : str , optional (default = None ) Specifies the coding scheme for ner_labels and chunk_labels . Conll2003DatasetReader assumes a coding scheme of input data is IOB1 . Valid options are None and BIOUL . The None default maintains the original IOB1 scheme in the CoNLL 2003 NER data. In the IOB1 scheme, I is a token inside a span, O is a token outside a span and B is the beginning of span immediately following another span of the same type. coding_scheme : str , optional (default = IOB1 ) This parameter is deprecated. If you specify coding_scheme to IOB1 , consider simply removing it or specifying convert_to_coding_scheme to None . If you want to specify BIOUL for coding_scheme , replace it with convert_to_coding_scheme . label_namespace : str , optional (default = labels ) Specifies the namespace for the chosen tag_label . text_to_instance \u00b6 class Conll2003DatasetReader ( DatasetReader ): | ... | def text_to_instance ( | self , | tokens : List [ Token ], | pos_tags : List [ str ] = None , | chunk_tags : List [ str ] = None , | ner_tags : List [ str ] = None | ) -> Instance We take pre-tokenized input here, because we don't have a tokenizer in this class. apply_token_indexers \u00b6 class Conll2003DatasetReader ( DatasetReader ): | ... | def apply_token_indexers ( self , instance : Instance ) -> None","title":"conll2003"},{"location":"api/data/dataset_readers/conll2003/#conll2003datasetreader","text":"@DatasetReader . register ( \"conll2003\" ) class Conll2003DatasetReader ( DatasetReader ): | def __init__ ( | self , | token_indexers : Dict [ str , TokenIndexer ] = None , | tag_label : str = \"ner\" , | feature_labels : Sequence [ str ] = (), | convert_to_coding_scheme : Optional [ str ] = None , | label_namespace : str = \"labels\" , | ** kwargs | ) -> None Reads instances from a pretokenised file where each line is in the following format: WORD POS-TAG CHUNK-TAG NER-TAG with a blank line indicating the end of each sentence and -DOCSTART- -X- -X- O indicating the end of each article, and converts it into a Dataset suitable for sequence tagging. Each Instance contains the words in the \"tokens\" TextField . The values corresponding to the tag_label values will get loaded into the \"tags\" SequenceLabelField . And if you specify any feature_labels (you probably shouldn't), the corresponding values will get loaded into their own SequenceLabelField s. This dataset reader ignores the \"article\" divisions and simply treats each sentence as an independent Instance . (Technically the reader splits sentences on any combination of blank lines and \"DOCSTART\" tags; in particular, it does the right thing on well formed inputs.) Registered as a DatasetReader with name \"conll2003\".","title":"Conll2003DatasetReader"},{"location":"api/data/dataset_readers/conll2003/#text_to_instance","text":"class Conll2003DatasetReader ( DatasetReader ): | ... | def text_to_instance ( | self , | tokens : List [ Token ], | pos_tags : List [ str ] = None , | chunk_tags : List [ str ] = None , | ner_tags : List [ str ] = None | ) -> Instance We take pre-tokenized input here, because we don't have a tokenizer in this class.","title":"text_to_instance"},{"location":"api/data/dataset_readers/conll2003/#apply_token_indexers","text":"class Conll2003DatasetReader ( DatasetReader ): | ... | def apply_token_indexers ( self , instance : Instance ) -> None","title":"apply_token_indexers"},{"location":"api/data/dataset_readers/dataset_reader/","text":"allennlp .data .dataset_readers .dataset_reader [SOURCE] WorkerInfo \u00b6 @dataclass class WorkerInfo Contains information about the worker context when a DatasetReader is being used within a multi-process DataLoader . From a DatasetReader this can accessed with the get_worker_info() method. num_workers \u00b6 class WorkerInfo : | ... | num_workers : int = None The total number of workers. id \u00b6 class WorkerInfo : | ... | id : int = None The 0-indexed ID of the current worker. DistributedInfo \u00b6 @dataclass class DistributedInfo Contains information about the global process rank and total world size when the reader is being used within distributed training. From a DatasetReader this can be accessed with the get_distributed_info() method. world_size \u00b6 class DistributedInfo : | ... | world_size : int = None The total number of processes in the distributed group. global_rank \u00b6 class DistributedInfo : | ... | global_rank : int = None The 0-indexed ID of the current process within the distributed group. This will be between 0 and world_size - 1 , inclusive. PathOrStr \u00b6 PathOrStr = Union [ PathLike , str ] DatasetReaderInput \u00b6 DatasetReaderInput = Union [ PathOrStr , List [ PathOrStr ], Dict [ str , PathOrStr ]] DatasetReader \u00b6 class DatasetReader ( Registrable ): | def __init__ ( | self , | max_instances : Optional [ int ] = None , | manual_distributed_sharding : bool = False , | manual_multiprocess_sharding : bool = False , | serialization_dir : Optional [ str ] = None | ) -> None A DatasetReader knows how to turn a file containing a dataset into a collection of Instance s. To implement your own, just override the _read(file_path) method to return an Iterable of the instances. Ideally this should be a lazy generator that yields them one at a time. All parameters necessary to _read the data apart from the filepath should be passed to the constructor of the DatasetReader . You should also implement text_to_instance(*inputs) , which should be used to turn raw data into Instance s. This method is required in order to use a Predictor with your reader. Usually the _read() method is implemented to call text_to_instance() . Parameters \u00b6 max_instances : int , optional (default = None ) If given, will stop reading after this many instances. This is a useful setting for debugging. Setting this disables caching. manual_distributed_sharding : bool , optional (default = False ) By default, when used in a distributed setting, DatasetReader makes sure that each trainer process only receives a subset of the data. It does this by reading the whole dataset in each worker, but filtering out the instances that are not needed. While this ensures that each worker will recieve unique instances, it's not a very efficient way to do so since each worker still needs to process every single instance. A better way to handle this is to manually handle the filtering within your _read() method, in which case you should set manual_distributed_sharding to True so that the base class knows that you handling the filtering. See the section below about how to do this. manual_multiprocess_sharding : bool , optional (default = False ) This is similar to the manual_distributed_sharding parameter, but applies to multi-process data loading. By default, when this reader is used by a multi-process data loader (i.e. a DataLoader with num_workers > 1 ), each worker will filter out all but a subset of the instances that are needed so that you don't end up with duplicates. However, there is really no benefit to using multiple workers in your DataLoader unless you implement the sharding within your _read() method, in which case you should set manual_multiprocess_sharding to True , just as with manual_distributed_sharding . See the section below about how to do this. serialization_dir : str , optional (default = None ) The directory in which the training output is saved to, or the directory the model is loaded from. Note This is typically not given an entry in a configuration file. It will be set automatically when using the built-in allennp commands. Using your reader with multi-process or distributed data loading \u00b6 There are two things you may need to update in your DatasetReader in order for it to be efficient in the multi-process or distributed data loading context. The _read() method should handle filtering out all but the instances that each particular worker should generate. This is important because the default mechanism for filtering out Instance s in the distributed or multi-process DataLoader setting is not very efficient, since every worker would still need to process every single Instance in your dataset. But by manually handling the filtering / sharding within your _read() method, each worker only needs to perform a subset of the work required to create instances. For example, if you were training using 2 GPUs and your _read() method reads a file line-by-line, creating one Instance for each line, you could just check the node rank within _read() and then throw away every other line starting at the line number corresponding to the node rank. The helper method shard_iterable() is there to make this easy for you. You can wrap this around any iterable object in your _read() method, and it will return an iterator that skips the right items based on the distributed training or multi-process loading context. This method can always be called regardless of whether or not you're actually using distributed training or multi-process loading. Remember though that when you handle the sharding manually within _read() , you need to let the DatasetReader know about this so that it doesn't do any additional filtering. Therefore you need to ensure that both self.manual_distributed_sharding and self.manual_multiprocess_sharding are set to True . If you call the helper method shard_iterable() without setting these to True , you'll get an exception. If the instances generated by _read() contain TextField s, those TextField s should not have any token indexers assigned. The token indexers need to be applied in the apply_token_indexers() method instead. This is highly recommended because if the instances generated by your _read() method have token indexers attached, those indexers will be duplicated when they are sent across processes. If your token indexers contain large objects (such as PretrainedTransformerTokenIndexer s) this could take up a massive amount of memory. read \u00b6 class DatasetReader ( Registrable ): | ... | def read ( self , file_path : DatasetReaderInput ) -> Iterator [ Instance ] Returns an iterator of instances that can be read from the file path. _read \u00b6 class DatasetReader ( Registrable ): | ... | def _read ( self , file_path ) -> Iterable [ Instance ] Reads the instances from the given file_path and returns them as an Iterable . You are strongly encouraged to use a generator so that users can read a dataset in a lazy way, if they so choose. text_to_instance \u00b6 class DatasetReader ( Registrable ): | ... | def text_to_instance ( self , * inputs ) -> Instance Does whatever tokenization or processing is necessary to go from textual input to an Instance . The primary intended use for this is with a Predictor , which gets text input as a JSON object and needs to process it to be input to a model. The intent here is to share code between _read and what happens at model serving time, or any other time you want to make a prediction from new data. We need to process the data in the same way it was done at training time. Allowing the DatasetReader to process new text lets us accomplish this, as we can just call DatasetReader.text_to_instance when serving predictions. The input type here is rather vaguely specified, unfortunately. The Predictor will have to make some assumptions about the kind of DatasetReader that it's using, in order to pass it the right information. apply_token_indexers \u00b6 class DatasetReader ( Registrable ): | ... | def apply_token_indexers ( self , instance : Instance ) -> None If Instance s created by this reader contain TextField s without token_indexers , this method can be overriden to set the token_indexers of those fields. E.g. if you have you have \"source\" TextField , you could implement this method like this: def apply_token_indexers ( self , instance : Instance ) -> None : instance [ \"source\" ] . token_indexers = self . _token_indexers If your TextField s are wrapped in a ListField , you can access them via field_list . E.g. if you had a \"source\" field of ListField[TextField] objects, you could: for text_field in instance [ \"source\" ] . field_list : text_field . token_indexers = self . _token_indexers get_worker_info \u00b6 class DatasetReader ( Registrable ): | ... | def get_worker_info ( self ) -> Optional [ WorkerInfo ] Provides a WorkerInfo object when the reader is being used within a worker of a multi-process DataLoader . If the reader is in the main process, this is just None . Note This is different than distributed training. If the DatasetReader is being used within distributed training, get_worker_info() will only provide information on the DataLoader worker within its node. Use get_distributed_info to get information on distributed training context. get_distributed_info \u00b6 class DatasetReader ( Registrable ): | ... | def get_distributed_info ( self ) -> Optional [ DistributedInfo ] Provides a DistributedInfo object when the reader is being used within distributed training. If not in distributed training, this is just None . shard_iterable \u00b6 class DatasetReader ( Registrable ): | ... | def shard_iterable ( self , iterable : Iterable [ _T ]) -> Iterator [ _T ] Helper method that determines which items in an iterable object to skip based on the current node rank (for distributed training) and worker ID (for multi-process data loading).","title":"dataset_reader"},{"location":"api/data/dataset_readers/dataset_reader/#workerinfo","text":"@dataclass class WorkerInfo Contains information about the worker context when a DatasetReader is being used within a multi-process DataLoader . From a DatasetReader this can accessed with the get_worker_info() method.","title":"WorkerInfo"},{"location":"api/data/dataset_readers/dataset_reader/#num_workers","text":"class WorkerInfo : | ... | num_workers : int = None The total number of workers.","title":"num_workers"},{"location":"api/data/dataset_readers/dataset_reader/#id","text":"class WorkerInfo : | ... | id : int = None The 0-indexed ID of the current worker.","title":"id"},{"location":"api/data/dataset_readers/dataset_reader/#distributedinfo","text":"@dataclass class DistributedInfo Contains information about the global process rank and total world size when the reader is being used within distributed training. From a DatasetReader this can be accessed with the get_distributed_info() method.","title":"DistributedInfo"},{"location":"api/data/dataset_readers/dataset_reader/#world_size","text":"class DistributedInfo : | ... | world_size : int = None The total number of processes in the distributed group.","title":"world_size"},{"location":"api/data/dataset_readers/dataset_reader/#global_rank","text":"class DistributedInfo : | ... | global_rank : int = None The 0-indexed ID of the current process within the distributed group. This will be between 0 and world_size - 1 , inclusive.","title":"global_rank"},{"location":"api/data/dataset_readers/dataset_reader/#pathorstr","text":"PathOrStr = Union [ PathLike , str ]","title":"PathOrStr"},{"location":"api/data/dataset_readers/dataset_reader/#datasetreaderinput","text":"DatasetReaderInput = Union [ PathOrStr , List [ PathOrStr ], Dict [ str , PathOrStr ]]","title":"DatasetReaderInput"},{"location":"api/data/dataset_readers/dataset_reader/#datasetreader","text":"class DatasetReader ( Registrable ): | def __init__ ( | self , | max_instances : Optional [ int ] = None , | manual_distributed_sharding : bool = False , | manual_multiprocess_sharding : bool = False , | serialization_dir : Optional [ str ] = None | ) -> None A DatasetReader knows how to turn a file containing a dataset into a collection of Instance s. To implement your own, just override the _read(file_path) method to return an Iterable of the instances. Ideally this should be a lazy generator that yields them one at a time. All parameters necessary to _read the data apart from the filepath should be passed to the constructor of the DatasetReader . You should also implement text_to_instance(*inputs) , which should be used to turn raw data into Instance s. This method is required in order to use a Predictor with your reader. Usually the _read() method is implemented to call text_to_instance() .","title":"DatasetReader"},{"location":"api/data/dataset_readers/dataset_reader/#read","text":"class DatasetReader ( Registrable ): | ... | def read ( self , file_path : DatasetReaderInput ) -> Iterator [ Instance ] Returns an iterator of instances that can be read from the file path.","title":"read"},{"location":"api/data/dataset_readers/dataset_reader/#_read","text":"class DatasetReader ( Registrable ): | ... | def _read ( self , file_path ) -> Iterable [ Instance ] Reads the instances from the given file_path and returns them as an Iterable . You are strongly encouraged to use a generator so that users can read a dataset in a lazy way, if they so choose.","title":"_read"},{"location":"api/data/dataset_readers/dataset_reader/#text_to_instance","text":"class DatasetReader ( Registrable ): | ... | def text_to_instance ( self , * inputs ) -> Instance Does whatever tokenization or processing is necessary to go from textual input to an Instance . The primary intended use for this is with a Predictor , which gets text input as a JSON object and needs to process it to be input to a model. The intent here is to share code between _read and what happens at model serving time, or any other time you want to make a prediction from new data. We need to process the data in the same way it was done at training time. Allowing the DatasetReader to process new text lets us accomplish this, as we can just call DatasetReader.text_to_instance when serving predictions. The input type here is rather vaguely specified, unfortunately. The Predictor will have to make some assumptions about the kind of DatasetReader that it's using, in order to pass it the right information.","title":"text_to_instance"},{"location":"api/data/dataset_readers/dataset_reader/#apply_token_indexers","text":"class DatasetReader ( Registrable ): | ... | def apply_token_indexers ( self , instance : Instance ) -> None If Instance s created by this reader contain TextField s without token_indexers , this method can be overriden to set the token_indexers of those fields. E.g. if you have you have \"source\" TextField , you could implement this method like this: def apply_token_indexers ( self , instance : Instance ) -> None : instance [ \"source\" ] . token_indexers = self . _token_indexers If your TextField s are wrapped in a ListField , you can access them via field_list . E.g. if you had a \"source\" field of ListField[TextField] objects, you could: for text_field in instance [ \"source\" ] . field_list : text_field . token_indexers = self . _token_indexers","title":"apply_token_indexers"},{"location":"api/data/dataset_readers/dataset_reader/#get_worker_info","text":"class DatasetReader ( Registrable ): | ... | def get_worker_info ( self ) -> Optional [ WorkerInfo ] Provides a WorkerInfo object when the reader is being used within a worker of a multi-process DataLoader . If the reader is in the main process, this is just None . Note This is different than distributed training. If the DatasetReader is being used within distributed training, get_worker_info() will only provide information on the DataLoader worker within its node. Use get_distributed_info to get information on distributed training context.","title":"get_worker_info"},{"location":"api/data/dataset_readers/dataset_reader/#get_distributed_info","text":"class DatasetReader ( Registrable ): | ... | def get_distributed_info ( self ) -> Optional [ DistributedInfo ] Provides a DistributedInfo object when the reader is being used within distributed training. If not in distributed training, this is just None .","title":"get_distributed_info"},{"location":"api/data/dataset_readers/dataset_reader/#shard_iterable","text":"class DatasetReader ( Registrable ): | ... | def shard_iterable ( self , iterable : Iterable [ _T ]) -> Iterator [ _T ] Helper method that determines which items in an iterable object to skip based on the current node rank (for distributed training) and worker ID (for multi-process data loading).","title":"shard_iterable"},{"location":"api/data/dataset_readers/interleaving_dataset_reader/","text":"allennlp .data .dataset_readers .interleaving_dataset_reader [SOURCE] InterleavingDatasetReader \u00b6 @DatasetReader . register ( \"interleaving\" ) class InterleavingDatasetReader ( DatasetReader ): | def __init__ ( | self , | readers : Dict [ str , DatasetReader ], | dataset_field_name : str = \"dataset\" , | scheme : str = \"round_robin\" , | ** kwargs | ) -> None A DatasetReader that wraps multiple other dataset readers, and interleaves their instances, adding a MetadataField to indicate the provenance of each instance. Unlike most of our other dataset readers, here the file_path passed into read() should be a JSON-serialized dictionary with one file_path per wrapped dataset reader (and with corresponding keys). Registered as a DatasetReader with name \"interleaving\". Parameters \u00b6 readers : Dict[str, DatasetReader] The dataset readers to wrap. The keys of this dictionary will be used as the values in the MetadataField indicating provenance. dataset_field_name : str , optional (default = \"dataset\" ) The name of the MetadataField indicating which dataset an instance came from. scheme : str , optional (default = \"round_robin\" ) Indicates how to interleave instances. Currently the two options are \"round_robin\", which repeatedly cycles through the datasets grabbing one instance from each; and \"all_at_once\", which yields all the instances from the first dataset, then all the instances from the second dataset, and so on. You could imagine also implementing some sort of over- or under-sampling, although hasn't been done. text_to_instance \u00b6 class InterleavingDatasetReader ( DatasetReader ): | ... | def text_to_instance ( | self , | dataset_key : str , | * args , | ** kwargs | ) -> Instance apply_token_indexers \u00b6 class InterleavingDatasetReader ( DatasetReader ): | ... | def apply_token_indexers ( self , instance : Instance ) -> None","title":"interleaving_dataset_reader"},{"location":"api/data/dataset_readers/interleaving_dataset_reader/#interleavingdatasetreader","text":"@DatasetReader . register ( \"interleaving\" ) class InterleavingDatasetReader ( DatasetReader ): | def __init__ ( | self , | readers : Dict [ str , DatasetReader ], | dataset_field_name : str = \"dataset\" , | scheme : str = \"round_robin\" , | ** kwargs | ) -> None A DatasetReader that wraps multiple other dataset readers, and interleaves their instances, adding a MetadataField to indicate the provenance of each instance. Unlike most of our other dataset readers, here the file_path passed into read() should be a JSON-serialized dictionary with one file_path per wrapped dataset reader (and with corresponding keys). Registered as a DatasetReader with name \"interleaving\".","title":"InterleavingDatasetReader"},{"location":"api/data/dataset_readers/interleaving_dataset_reader/#text_to_instance","text":"class InterleavingDatasetReader ( DatasetReader ): | ... | def text_to_instance ( | self , | dataset_key : str , | * args , | ** kwargs | ) -> Instance","title":"text_to_instance"},{"location":"api/data/dataset_readers/interleaving_dataset_reader/#apply_token_indexers","text":"class InterleavingDatasetReader ( DatasetReader ): | ... | def apply_token_indexers ( self , instance : Instance ) -> None","title":"apply_token_indexers"},{"location":"api/data/dataset_readers/multitask/","text":"allennlp .data .dataset_readers .multitask [SOURCE] MultiTaskDatasetReader \u00b6 @DatasetReader . register ( \"multitask\" ) class MultiTaskDatasetReader ( DatasetReader ): | def __init__ ( self , readers : Dict [ str , DatasetReader ]) -> None This DatasetReader simply collects a dictionary of other DatasetReaders . It is designed for a different class (the MultiTaskDataLoader ) to actually read from each of the underlying dataset readers, and so this really is just a glorified dictionary that we can construct as a DatasetReader . We throw an error if you try to actually call read() , because you should be doing that differently. Registered as a DatasetReader with name \"multitask\". Parameters \u00b6 readers : Dict[str, DatasetReader] A mapping from dataset name to DatasetReader objects for reading that dataset. You can use whatever names you want for the datasets, but they have to match the keys you use for data files and in other places in the MultiTaskDataLoader and MultiTaskScheduler . read \u00b6 class MultiTaskDatasetReader ( DatasetReader ): | ... | def read ( | self , | file_paths : Union [ PathLike , str , Dict [ str , Union [ PathLike , str ]]], | * , force_task : Optional [ str ] = None | ) -> Union [ Iterator [ Instance ], Dict [ str , Iterator [ Instance ]]]","title":"multitask"},{"location":"api/data/dataset_readers/multitask/#multitaskdatasetreader","text":"@DatasetReader . register ( \"multitask\" ) class MultiTaskDatasetReader ( DatasetReader ): | def __init__ ( self , readers : Dict [ str , DatasetReader ]) -> None This DatasetReader simply collects a dictionary of other DatasetReaders . It is designed for a different class (the MultiTaskDataLoader ) to actually read from each of the underlying dataset readers, and so this really is just a glorified dictionary that we can construct as a DatasetReader . We throw an error if you try to actually call read() , because you should be doing that differently. Registered as a DatasetReader with name \"multitask\".","title":"MultiTaskDatasetReader"},{"location":"api/data/dataset_readers/multitask/#read","text":"class MultiTaskDatasetReader ( DatasetReader ): | ... | def read ( | self , | file_paths : Union [ PathLike , str , Dict [ str , Union [ PathLike , str ]]], | * , force_task : Optional [ str ] = None | ) -> Union [ Iterator [ Instance ], Dict [ str , Iterator [ Instance ]]]","title":"read"},{"location":"api/data/dataset_readers/sequence_tagging/","text":"allennlp .data .dataset_readers .sequence_tagging [SOURCE] DEFAULT_WORD_TAG_DELIMITER \u00b6 DEFAULT_WORD_TAG_DELIMITER = \"###\" SequenceTaggingDatasetReader \u00b6 @DatasetReader . register ( \"sequence_tagging\" ) class SequenceTaggingDatasetReader ( DatasetReader ): | def __init__ ( | self , | word_tag_delimiter : str = DEFAULT_WORD_TAG_DELIMITER , | token_delimiter : str = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | ** kwargs | ) -> None Reads instances from a pretokenised file where each line is in the following format: WORD###TAG [TAB] WORD###TAG [TAB] ..... \\n and converts it into a Dataset suitable for sequence tagging. You can also specify alternative delimiters in the constructor. Registered as a DatasetReader with name \"sequence_tagging\". Parameters \u00b6 word_tag_delimiter : str , optional (default = \"###\" ) The text that separates each WORD from its TAG. token_delimiter : str , optional (default = None ) The text that separates each WORD-TAG pair from the next pair. If None then the line will just be split on whitespace. token_indexers : Dict[str, TokenIndexer] , optional (default = {\"tokens\": SingleIdTokenIndexer()} ) We use this to define the input representation for the text. See TokenIndexer . Note that the output tags will always correspond to single token IDs based on how they are pre-tokenised in the data file. text_to_instance \u00b6 class SequenceTaggingDatasetReader ( DatasetReader ): | ... | def text_to_instance ( | self , | tokens : List [ Token ], | tags : List [ str ] = None | ) -> Instance We take pre-tokenized input here, because we don't have a tokenizer in this class. apply_token_indexers \u00b6 class SequenceTaggingDatasetReader ( DatasetReader ): | ... | def apply_token_indexers ( self , instance : Instance ) -> None","title":"sequence_tagging"},{"location":"api/data/dataset_readers/sequence_tagging/#default_word_tag_delimiter","text":"DEFAULT_WORD_TAG_DELIMITER = \"###\"","title":"DEFAULT_WORD_TAG_DELIMITER"},{"location":"api/data/dataset_readers/sequence_tagging/#sequencetaggingdatasetreader","text":"@DatasetReader . register ( \"sequence_tagging\" ) class SequenceTaggingDatasetReader ( DatasetReader ): | def __init__ ( | self , | word_tag_delimiter : str = DEFAULT_WORD_TAG_DELIMITER , | token_delimiter : str = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | ** kwargs | ) -> None Reads instances from a pretokenised file where each line is in the following format: WORD###TAG [TAB] WORD###TAG [TAB] ..... \\n and converts it into a Dataset suitable for sequence tagging. You can also specify alternative delimiters in the constructor. Registered as a DatasetReader with name \"sequence_tagging\".","title":"SequenceTaggingDatasetReader"},{"location":"api/data/dataset_readers/sequence_tagging/#text_to_instance","text":"class SequenceTaggingDatasetReader ( DatasetReader ): | ... | def text_to_instance ( | self , | tokens : List [ Token ], | tags : List [ str ] = None | ) -> Instance We take pre-tokenized input here, because we don't have a tokenizer in this class.","title":"text_to_instance"},{"location":"api/data/dataset_readers/sequence_tagging/#apply_token_indexers","text":"class SequenceTaggingDatasetReader ( DatasetReader ): | ... | def apply_token_indexers ( self , instance : Instance ) -> None","title":"apply_token_indexers"},{"location":"api/data/dataset_readers/sharded_dataset_reader/","text":"allennlp .data .dataset_readers .sharded_dataset_reader [SOURCE] ShardedDatasetReader \u00b6 @DatasetReader . register ( \"sharded\" ) class ShardedDatasetReader ( DatasetReader ): | def __init__ ( self , base_reader : DatasetReader , ** kwargs ) -> None Wraps another dataset reader and uses it to read from multiple input files. Note that in this case the file_path passed to read() should either be a glob path or a path or URL to an archive file ('.zip' or '.tar.gz'). The dataset reader will return instances from all files matching the glob, or all files within the archive. The order the files are processed in is deterministic to enable the instances to be filtered according to worker rank in the distributed training or multi-process data loading scenarios. In either case, the number of file shards should ideally be a multiple of the number of workers, and each file should produce roughly the same number of instances. Registered as a DatasetReader with name \"sharded\". Parameters \u00b6 base_reader : DatasetReader Reader with a read method that accepts a single file. text_to_instance \u00b6 class ShardedDatasetReader ( DatasetReader ): | ... | def text_to_instance ( self , * args , ** kwargs ) -> Instance Just delegate to the base reader text_to_instance. apply_token_indexers \u00b6 class ShardedDatasetReader ( DatasetReader ): | ... | def apply_token_indexers ( self , instance : Instance ) -> None","title":"sharded_dataset_reader"},{"location":"api/data/dataset_readers/sharded_dataset_reader/#shardeddatasetreader","text":"@DatasetReader . register ( \"sharded\" ) class ShardedDatasetReader ( DatasetReader ): | def __init__ ( self , base_reader : DatasetReader , ** kwargs ) -> None Wraps another dataset reader and uses it to read from multiple input files. Note that in this case the file_path passed to read() should either be a glob path or a path or URL to an archive file ('.zip' or '.tar.gz'). The dataset reader will return instances from all files matching the glob, or all files within the archive. The order the files are processed in is deterministic to enable the instances to be filtered according to worker rank in the distributed training or multi-process data loading scenarios. In either case, the number of file shards should ideally be a multiple of the number of workers, and each file should produce roughly the same number of instances. Registered as a DatasetReader with name \"sharded\".","title":"ShardedDatasetReader"},{"location":"api/data/dataset_readers/sharded_dataset_reader/#text_to_instance","text":"class ShardedDatasetReader ( DatasetReader ): | ... | def text_to_instance ( self , * args , ** kwargs ) -> Instance Just delegate to the base reader text_to_instance.","title":"text_to_instance"},{"location":"api/data/dataset_readers/sharded_dataset_reader/#apply_token_indexers","text":"class ShardedDatasetReader ( DatasetReader ): | ... | def apply_token_indexers ( self , instance : Instance ) -> None","title":"apply_token_indexers"},{"location":"api/data/dataset_readers/text_classification_json/","text":"allennlp .data .dataset_readers .text_classification_json [SOURCE] TextClassificationJsonReader \u00b6 @DatasetReader . register ( \"text_classification_json\" ) class TextClassificationJsonReader ( DatasetReader ): | def __init__ ( | self , | token_indexers : Dict [ str , TokenIndexer ] = None , | tokenizer : Tokenizer = None , | segment_sentences : bool = False , | max_sequence_length : int = None , | skip_label_indexing : bool = False , | text_key : str = \"text\" , | label_key : str = \"label\" , | ** kwargs | ) -> None Reads tokens and their labels from a labeled text classification dataset. The output of read is a list of Instance s with the fields: tokens : TextField and label : LabelField Registered as a DatasetReader with name \"text_classification_json\". Parameters \u00b6 token_indexers : Dict[str, TokenIndexer] , optional optional (default= {\"tokens\": SingleIdTokenIndexer()} ) We use this to define the input representation for the text. See TokenIndexer . tokenizer : Tokenizer , optional (default = {\"tokens\": SpacyTokenizer()} ) Tokenizer to use to split the input text into words or other kinds of tokens. segment_sentences : bool , optional (default = False ) If True, we will first segment the text into sentences using SpaCy and then tokenize words. Necessary for some models that require pre-segmentation of sentences, like the Hierarchical Attention Network . max_sequence_length : int , optional (default = None ) If specified, will truncate tokens to specified maximum length. skip_label_indexing : bool , optional (default = False ) Whether or not to skip label indexing. You might want to skip label indexing if your labels are numbers, so the dataset reader doesn't re-number them starting from 0. text_key : str , optional (default = \"text\" ) The key name of the source field in the JSON data file. label_key : str , optional (default = \"label\" ) The key name of the target field in the JSON data file. text_to_instance \u00b6 class TextClassificationJsonReader ( DatasetReader ): | ... | def text_to_instance ( | self , | text : str , | label : Union [ str , int ] = None | ) -> Instance Parameters \u00b6 text : str The text to classify label : str , optional (default = None ) The label for this text. Returns \u00b6 An Instance containing the following fields: tokens ( TextField ) : The tokens in the sentence or phrase. label ( LabelField ) : The label label of the sentence or phrase. apply_token_indexers \u00b6 class TextClassificationJsonReader ( DatasetReader ): | ... | def apply_token_indexers ( self , instance : Instance ) -> None","title":"text_classification_json"},{"location":"api/data/dataset_readers/text_classification_json/#textclassificationjsonreader","text":"@DatasetReader . register ( \"text_classification_json\" ) class TextClassificationJsonReader ( DatasetReader ): | def __init__ ( | self , | token_indexers : Dict [ str , TokenIndexer ] = None , | tokenizer : Tokenizer = None , | segment_sentences : bool = False , | max_sequence_length : int = None , | skip_label_indexing : bool = False , | text_key : str = \"text\" , | label_key : str = \"label\" , | ** kwargs | ) -> None Reads tokens and their labels from a labeled text classification dataset. The output of read is a list of Instance s with the fields: tokens : TextField and label : LabelField Registered as a DatasetReader with name \"text_classification_json\".","title":"TextClassificationJsonReader"},{"location":"api/data/dataset_readers/text_classification_json/#text_to_instance","text":"class TextClassificationJsonReader ( DatasetReader ): | ... | def text_to_instance ( | self , | text : str , | label : Union [ str , int ] = None | ) -> Instance","title":"text_to_instance"},{"location":"api/data/dataset_readers/text_classification_json/#apply_token_indexers","text":"class TextClassificationJsonReader ( DatasetReader ): | ... | def apply_token_indexers ( self , instance : Instance ) -> None","title":"apply_token_indexers"},{"location":"api/data/dataset_readers/dataset_utils/span_utils/","text":"allennlp .data .dataset_readers .dataset_utils .span_utils [SOURCE] TypedSpan \u00b6 TypedSpan = Tuple [ int , Tuple [ int , int ]] TypedStringSpan \u00b6 TypedStringSpan = Tuple [ str , Tuple [ int , int ]] InvalidTagSequence \u00b6 class InvalidTagSequence ( Exception ): | def __init__ ( self , tag_sequence = None ) T \u00b6 T = TypeVar ( \"T\" , str , Token ) enumerate_spans \u00b6 def enumerate_spans ( sentence : List [ T ], offset : int = 0 , max_span_width : int = None , min_span_width : int = 1 , filter_function : Callable [[ List [ T ]], bool ] = None ) -> List [ Tuple [ int , int ]] Given a sentence, return all token spans within the sentence. Spans are inclusive . Additionally, you can provide a maximum and minimum span width, which will be used to exclude spans outside of this range. Finally, you can provide a function mapping List[T] -> bool , which will be applied to every span to decide whether that span should be included. This allows filtering by length, regex matches, pos tags or any Spacy Token attributes, for example. Parameters \u00b6 sentence : List[T] The sentence to generate spans for. The type is generic, as this function can be used with strings, or Spacy Tokens or other sequences. offset : int , optional (default = 0 ) A numeric offset to add to all span start and end indices. This is helpful if the sentence is part of a larger structure, such as a document, which the indices need to respect. max_span_width : int , optional (default = None ) The maximum length of spans which should be included. Defaults to len(sentence). min_span_width : int , optional (default = 1 ) The minimum length of spans which should be included. Defaults to 1. filter_function : Callable[[List[T]], bool] , optional (default = None ) A function mapping sequences of the passed type T to a boolean value. If True , the span is included in the returned spans from the sentence, otherwise it is excluded.. bio_tags_to_spans \u00b6 def bio_tags_to_spans ( tag_sequence : List [ str ], classes_to_ignore : List [ str ] = None ) -> List [ TypedStringSpan ] Given a sequence corresponding to BIO tags, extracts spans. Spans are inclusive and can be of zero length, representing a single word span. Ill-formed spans are also included (i.e those which do not start with a \"B-LABEL\"), as otherwise it is possible to get a perfect precision score whilst still predicting ill-formed spans in addition to the correct spans. This function works properly when the spans are unlabeled (i.e., your labels are simply \"B\", \"I\", and \"O\"). Parameters \u00b6 tag_sequence : List[str] The integer class labels for a sequence. classes_to_ignore : List[str] , optional (default = None ) A list of string class labels excluding the bio tag which should be ignored when extracting spans. Returns \u00b6 spans : List[TypedStringSpan] The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)). Note that the label does not contain any BIO tag prefixes. iob1_tags_to_spans \u00b6 def iob1_tags_to_spans ( tag_sequence : List [ str ], classes_to_ignore : List [ str ] = None ) -> List [ TypedStringSpan ] Given a sequence corresponding to IOB1 tags, extracts spans. Spans are inclusive and can be of zero length, representing a single word span. Ill-formed spans are also included (i.e., those where \"B-LABEL\" is not preceded by \"I-LABEL\" or \"B-LABEL\"). Parameters \u00b6 tag_sequence : List[str] The integer class labels for a sequence. classes_to_ignore : List[str] , optional (default = None ) A list of string class labels excluding the bio tag which should be ignored when extracting spans. Returns \u00b6 spans : List[TypedStringSpan] The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)). Note that the label does not contain any BIO tag prefixes. bioul_tags_to_spans \u00b6 def bioul_tags_to_spans ( tag_sequence : List [ str ], classes_to_ignore : List [ str ] = None ) -> List [ TypedStringSpan ] Given a sequence corresponding to BIOUL tags, extracts spans. Spans are inclusive and can be of zero length, representing a single word span. Ill-formed spans are not allowed and will raise InvalidTagSequence . This function works properly when the spans are unlabeled (i.e., your labels are simply \"B\", \"I\", \"O\", \"U\", and \"L\"). Parameters \u00b6 tag_sequence : List[str] The tag sequence encoded in BIOUL, e.g. [\"B-PER\", \"L-PER\", \"O\"]. classes_to_ignore : List[str] , optional (default = None ) A list of string class labels excluding the bio tag which should be ignored when extracting spans. Returns \u00b6 spans : List[TypedStringSpan] The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)). iob1_to_bioul \u00b6 def iob1_to_bioul ( tag_sequence : List [ str ]) -> List [ str ] to_bioul \u00b6 def to_bioul ( tag_sequence : List [ str ], encoding : str = \"IOB1\" ) -> List [ str ] Given a tag sequence encoded with IOB1 labels, recode to BIOUL. In the IOB1 scheme, I is a token inside a span, O is a token outside a span and B is the beginning of span immediately following another span of the same type. In the BIO scheme, I is a token inside a span, O is a token outside a span and B is the beginning of a span. Parameters \u00b6 tag_sequence : List[str] The tag sequence encoded in IOB1, e.g. [\"I-PER\", \"I-PER\", \"O\"]. encoding : str , optional (default = \"IOB1\" ) The encoding type to convert from. Must be either \"IOB1\" or \"BIO\". Returns \u00b6 bioul_sequence : List[str] The tag sequence encoded in IOB1, e.g. [\"B-PER\", \"L-PER\", \"O\"]. bmes_tags_to_spans \u00b6 def bmes_tags_to_spans ( tag_sequence : List [ str ], classes_to_ignore : List [ str ] = None ) -> List [ TypedStringSpan ] Given a sequence corresponding to BMES tags, extracts spans. Spans are inclusive and can be of zero length, representing a single word span. Ill-formed spans are also included (i.e those which do not start with a \"B-LABEL\"), as otherwise it is possible to get a perfect precision score whilst still predicting ill-formed spans in addition to the correct spans. This function works properly when the spans are unlabeled (i.e., your labels are simply \"B\", \"M\", \"E\" and \"S\"). Parameters \u00b6 tag_sequence : List[str] The integer class labels for a sequence. classes_to_ignore : List[str] , optional (default = None ) A list of string class labels excluding the bio tag which should be ignored when extracting spans. Returns \u00b6 spans : List[TypedStringSpan] The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)). Note that the label does not contain any BIO tag prefixes.","title":"span_utils"},{"location":"api/data/dataset_readers/dataset_utils/span_utils/#typedspan","text":"TypedSpan = Tuple [ int , Tuple [ int , int ]]","title":"TypedSpan"},{"location":"api/data/dataset_readers/dataset_utils/span_utils/#typedstringspan","text":"TypedStringSpan = Tuple [ str , Tuple [ int , int ]]","title":"TypedStringSpan"},{"location":"api/data/dataset_readers/dataset_utils/span_utils/#invalidtagsequence","text":"class InvalidTagSequence ( Exception ): | def __init__ ( self , tag_sequence = None )","title":"InvalidTagSequence"},{"location":"api/data/dataset_readers/dataset_utils/span_utils/#t","text":"T = TypeVar ( \"T\" , str , Token )","title":"T"},{"location":"api/data/dataset_readers/dataset_utils/span_utils/#enumerate_spans","text":"def enumerate_spans ( sentence : List [ T ], offset : int = 0 , max_span_width : int = None , min_span_width : int = 1 , filter_function : Callable [[ List [ T ]], bool ] = None ) -> List [ Tuple [ int , int ]] Given a sentence, return all token spans within the sentence. Spans are inclusive . Additionally, you can provide a maximum and minimum span width, which will be used to exclude spans outside of this range. Finally, you can provide a function mapping List[T] -> bool , which will be applied to every span to decide whether that span should be included. This allows filtering by length, regex matches, pos tags or any Spacy Token attributes, for example.","title":"enumerate_spans"},{"location":"api/data/dataset_readers/dataset_utils/span_utils/#bio_tags_to_spans","text":"def bio_tags_to_spans ( tag_sequence : List [ str ], classes_to_ignore : List [ str ] = None ) -> List [ TypedStringSpan ] Given a sequence corresponding to BIO tags, extracts spans. Spans are inclusive and can be of zero length, representing a single word span. Ill-formed spans are also included (i.e those which do not start with a \"B-LABEL\"), as otherwise it is possible to get a perfect precision score whilst still predicting ill-formed spans in addition to the correct spans. This function works properly when the spans are unlabeled (i.e., your labels are simply \"B\", \"I\", and \"O\").","title":"bio_tags_to_spans"},{"location":"api/data/dataset_readers/dataset_utils/span_utils/#iob1_tags_to_spans","text":"def iob1_tags_to_spans ( tag_sequence : List [ str ], classes_to_ignore : List [ str ] = None ) -> List [ TypedStringSpan ] Given a sequence corresponding to IOB1 tags, extracts spans. Spans are inclusive and can be of zero length, representing a single word span. Ill-formed spans are also included (i.e., those where \"B-LABEL\" is not preceded by \"I-LABEL\" or \"B-LABEL\").","title":"iob1_tags_to_spans"},{"location":"api/data/dataset_readers/dataset_utils/span_utils/#bioul_tags_to_spans","text":"def bioul_tags_to_spans ( tag_sequence : List [ str ], classes_to_ignore : List [ str ] = None ) -> List [ TypedStringSpan ] Given a sequence corresponding to BIOUL tags, extracts spans. Spans are inclusive and can be of zero length, representing a single word span. Ill-formed spans are not allowed and will raise InvalidTagSequence . This function works properly when the spans are unlabeled (i.e., your labels are simply \"B\", \"I\", \"O\", \"U\", and \"L\").","title":"bioul_tags_to_spans"},{"location":"api/data/dataset_readers/dataset_utils/span_utils/#iob1_to_bioul","text":"def iob1_to_bioul ( tag_sequence : List [ str ]) -> List [ str ]","title":"iob1_to_bioul"},{"location":"api/data/dataset_readers/dataset_utils/span_utils/#to_bioul","text":"def to_bioul ( tag_sequence : List [ str ], encoding : str = \"IOB1\" ) -> List [ str ] Given a tag sequence encoded with IOB1 labels, recode to BIOUL. In the IOB1 scheme, I is a token inside a span, O is a token outside a span and B is the beginning of span immediately following another span of the same type. In the BIO scheme, I is a token inside a span, O is a token outside a span and B is the beginning of a span.","title":"to_bioul"},{"location":"api/data/dataset_readers/dataset_utils/span_utils/#bmes_tags_to_spans","text":"def bmes_tags_to_spans ( tag_sequence : List [ str ], classes_to_ignore : List [ str ] = None ) -> List [ TypedStringSpan ] Given a sequence corresponding to BMES tags, extracts spans. Spans are inclusive and can be of zero length, representing a single word span. Ill-formed spans are also included (i.e those which do not start with a \"B-LABEL\"), as otherwise it is possible to get a perfect precision score whilst still predicting ill-formed spans in addition to the correct spans. This function works properly when the spans are unlabeled (i.e., your labels are simply \"B\", \"M\", \"E\" and \"S\").","title":"bmes_tags_to_spans"},{"location":"api/data/fields/adjacency_field/","text":"allennlp .data .fields .adjacency_field [SOURCE] AdjacencyField \u00b6 class AdjacencyField ( Field [ torch . Tensor ]): | def __init__ ( | self , | indices : List [ Tuple [ int , int ]], | sequence_field : SequenceField , | labels : List [ str ] = None , | label_namespace : str = \"labels\" , | padding_value : int = - 1 | ) -> None A AdjacencyField defines directed adjacency relations between elements in a SequenceField . Because it's a labeling of some other field, we take that field as input here and use it to determine our padding and other things. This field will get converted into an array of shape (sequence_field_length, sequence_field_length), where the (i, j)th array element is either a binary flag indicating there is an edge from i to j, or an integer label k, indicating there is a label from i to j of type k. Parameters \u00b6 indices : List[Tuple[int, int]] sequence_field : SequenceField A field containing the sequence that this AdjacencyField is labeling. Most often, this is a TextField , for tagging edge relations between tokens in a sentence. labels : List[str] , optional (default = None ) Optional labels for the edges of the adjacency matrix. label_namespace : str , optional (default = 'labels' ) The namespace to use for converting tag strings into integers. We convert tag strings to integers for you, and this parameter tells the Vocabulary object which mapping from strings to integers to use (so that \"O\" as a tag doesn't get the same id as \"O\" as a word). padding_value : int , optional (default = -1 ) The value to use as padding. count_vocab_items \u00b6 class AdjacencyField ( Field [ torch . Tensor ]): | ... | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]]) index \u00b6 class AdjacencyField ( Field [ torch . Tensor ]): | ... | def index ( self , vocab : Vocabulary ) get_padding_lengths \u00b6 class AdjacencyField ( Field [ torch . Tensor ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ] as_tensor \u00b6 class AdjacencyField ( Field [ torch . Tensor ]): | ... | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor empty_field \u00b6 class AdjacencyField ( Field [ torch . Tensor ]): | ... | def empty_field ( self ) -> \"AdjacencyField\" human_readable_repr \u00b6 class AdjacencyField ( Field [ torch . Tensor ]): | ... | def human_readable_repr ( self )","title":"adjacency_field"},{"location":"api/data/fields/adjacency_field/#adjacencyfield","text":"class AdjacencyField ( Field [ torch . Tensor ]): | def __init__ ( | self , | indices : List [ Tuple [ int , int ]], | sequence_field : SequenceField , | labels : List [ str ] = None , | label_namespace : str = \"labels\" , | padding_value : int = - 1 | ) -> None A AdjacencyField defines directed adjacency relations between elements in a SequenceField . Because it's a labeling of some other field, we take that field as input here and use it to determine our padding and other things. This field will get converted into an array of shape (sequence_field_length, sequence_field_length), where the (i, j)th array element is either a binary flag indicating there is an edge from i to j, or an integer label k, indicating there is a label from i to j of type k.","title":"AdjacencyField"},{"location":"api/data/fields/adjacency_field/#count_vocab_items","text":"class AdjacencyField ( Field [ torch . Tensor ]): | ... | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]])","title":"count_vocab_items"},{"location":"api/data/fields/adjacency_field/#index","text":"class AdjacencyField ( Field [ torch . Tensor ]): | ... | def index ( self , vocab : Vocabulary )","title":"index"},{"location":"api/data/fields/adjacency_field/#get_padding_lengths","text":"class AdjacencyField ( Field [ torch . Tensor ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ]","title":"get_padding_lengths"},{"location":"api/data/fields/adjacency_field/#as_tensor","text":"class AdjacencyField ( Field [ torch . Tensor ]): | ... | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor","title":"as_tensor"},{"location":"api/data/fields/adjacency_field/#empty_field","text":"class AdjacencyField ( Field [ torch . Tensor ]): | ... | def empty_field ( self ) -> \"AdjacencyField\"","title":"empty_field"},{"location":"api/data/fields/adjacency_field/#human_readable_repr","text":"class AdjacencyField ( Field [ torch . Tensor ]): | ... | def human_readable_repr ( self )","title":"human_readable_repr"},{"location":"api/data/fields/array_field/","text":"allennlp .data .fields .array_field [SOURCE] ArrayField \u00b6 ArrayField = TensorField For backwards compatibility, we keep the name ArrayField .","title":"array_field"},{"location":"api/data/fields/array_field/#arrayfield","text":"ArrayField = TensorField For backwards compatibility, we keep the name ArrayField .","title":"ArrayField"},{"location":"api/data/fields/field/","text":"allennlp .data .fields .field [SOURCE] DataArray \u00b6 DataArray = TypeVar ( \"DataArray\" , torch . Tensor , Dict [ str , torch . Tensor ], Dict [ str , Dict [ str , torch . Tensor ]] ... Field \u00b6 class Field ( Generic [ DataArray ]) A Field is some piece of a data instance that ends up as an tensor in a model (either as an input or an output). Data instances are just collections of fields. Fields go through up to two steps of processing: (1) tokenized fields are converted into token ids, (2) fields containing token ids (or any other numeric data) are padded (if necessary) and converted into tensors. The Field API has methods around both of these steps, though they may not be needed for some concrete Field classes - if your field doesn't have any strings that need indexing, you don't need to implement count_vocab_items or index . These methods pass by default. Once a vocabulary is computed and all fields are indexed, we will determine padding lengths, then intelligently batch together instances and pad them into actual tensors. count_vocab_items \u00b6 class Field ( Generic [ DataArray ]): | ... | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]]) If there are strings in this field that need to be converted into integers through a Vocabulary , here is where we count them, to determine which tokens are in or out of the vocabulary. If your Field does not have any strings that need to be converted into indices, you do not need to implement this method. A note on this counter : because Fields can represent conceptually different things, we separate the vocabulary items by namespaces . This way, we can use a single shared mechanism to handle all mappings from strings to integers in all fields, while keeping words in a TextField from sharing the same ids with labels in a LabelField (e.g., \"entailment\" or \"contradiction\" are labels in an entailment task) Additionally, a single Field might want to use multiple namespaces - TextFields can be represented as a combination of word ids and character ids, and you don't want words and characters to share the same vocabulary - \"a\" as a word should get a different id from \"a\" as a character, and the vocabulary sizes of words and characters are very different. Because of this, the first key in the counter object is a namespace , like \"tokens\", \"token_characters\", \"tags\", or \"labels\", and the second key is the actual vocabulary item. human_readable_repr \u00b6 class Field ( Generic [ DataArray ]): | ... | def human_readable_repr ( self ) -> Any This method should be implemented by subclasses to return a structured, yet human-readable representation of the field. Note human_readable_repr() is not meant to be used as a method to serialize a Field since the return value does not necessarily contain all of the attributes of the Field instance. But the object returned should be JSON-serializable. index \u00b6 class Field ( Generic [ DataArray ]): | ... | def index ( self , vocab : Vocabulary ) Given a Vocabulary , converts all strings in this field into (typically) integers. This modifies the Field object, it does not return anything. If your Field does not have any strings that need to be converted into indices, you do not need to implement this method. get_padding_lengths \u00b6 class Field ( Generic [ DataArray ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ] If there are things in this field that need padding, note them here. In order to pad a batch of instance, we get all of the lengths from the batch, take the max, and pad everything to that length (or use a pre-specified maximum length). The return value is a dictionary mapping keys to lengths, like {'num_tokens': 13} . This is always called after index . as_tensor \u00b6 class Field ( Generic [ DataArray ]): | ... | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> DataArray Given a set of specified padding lengths, actually pad the data in this field and return a torch Tensor (or a more complex data structure) of the correct shape. We also take a couple of parameters that are important when constructing torch Tensors. Parameters \u00b6 padding_lengths : Dict[str, int] This dictionary will have the same keys that were produced in get_padding_lengths . The values specify the lengths to use when padding each relevant dimension, aggregated across all instances in a batch. empty_field \u00b6 class Field ( Generic [ DataArray ]): | ... | def empty_field ( self ) -> \"Field\" So that ListField can pad the number of fields in a list (e.g., the number of answer option TextFields ), we need a representation of an empty field of each type. This returns that. This will only ever be called when we're to the point of calling as_tensor , so you don't need to worry about get_padding_lengths , count_vocab_items , etc., being called on this empty field. We make this an instance method instead of a static method so that if there is any state in the Field, we can copy it over (e.g., the token indexers in TextField ). batch_tensors \u00b6 class Field ( Generic [ DataArray ]): | ... | def batch_tensors ( self , tensor_list : List [ DataArray ]) -> DataArray Takes the output of Field.as_tensor() from a list of Instances and merges it into one batched tensor for this Field . The default implementation here in the base class handles cases where as_tensor returns a single torch tensor per instance. If your subclass returns something other than this, you need to override this method. This operation does not modify self , but in some cases we need the information contained in self in order to perform the batching, so this is an instance method, not a class method. duplicate \u00b6 class Field ( Generic [ DataArray ]): | ... | def duplicate ( self )","title":"field"},{"location":"api/data/fields/field/#dataarray","text":"DataArray = TypeVar ( \"DataArray\" , torch . Tensor , Dict [ str , torch . Tensor ], Dict [ str , Dict [ str , torch . Tensor ]] ...","title":"DataArray"},{"location":"api/data/fields/field/#field","text":"class Field ( Generic [ DataArray ]) A Field is some piece of a data instance that ends up as an tensor in a model (either as an input or an output). Data instances are just collections of fields. Fields go through up to two steps of processing: (1) tokenized fields are converted into token ids, (2) fields containing token ids (or any other numeric data) are padded (if necessary) and converted into tensors. The Field API has methods around both of these steps, though they may not be needed for some concrete Field classes - if your field doesn't have any strings that need indexing, you don't need to implement count_vocab_items or index . These methods pass by default. Once a vocabulary is computed and all fields are indexed, we will determine padding lengths, then intelligently batch together instances and pad them into actual tensors.","title":"Field"},{"location":"api/data/fields/field/#count_vocab_items","text":"class Field ( Generic [ DataArray ]): | ... | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]]) If there are strings in this field that need to be converted into integers through a Vocabulary , here is where we count them, to determine which tokens are in or out of the vocabulary. If your Field does not have any strings that need to be converted into indices, you do not need to implement this method. A note on this counter : because Fields can represent conceptually different things, we separate the vocabulary items by namespaces . This way, we can use a single shared mechanism to handle all mappings from strings to integers in all fields, while keeping words in a TextField from sharing the same ids with labels in a LabelField (e.g., \"entailment\" or \"contradiction\" are labels in an entailment task) Additionally, a single Field might want to use multiple namespaces - TextFields can be represented as a combination of word ids and character ids, and you don't want words and characters to share the same vocabulary - \"a\" as a word should get a different id from \"a\" as a character, and the vocabulary sizes of words and characters are very different. Because of this, the first key in the counter object is a namespace , like \"tokens\", \"token_characters\", \"tags\", or \"labels\", and the second key is the actual vocabulary item.","title":"count_vocab_items"},{"location":"api/data/fields/field/#human_readable_repr","text":"class Field ( Generic [ DataArray ]): | ... | def human_readable_repr ( self ) -> Any This method should be implemented by subclasses to return a structured, yet human-readable representation of the field. Note human_readable_repr() is not meant to be used as a method to serialize a Field since the return value does not necessarily contain all of the attributes of the Field instance. But the object returned should be JSON-serializable.","title":"human_readable_repr"},{"location":"api/data/fields/field/#index","text":"class Field ( Generic [ DataArray ]): | ... | def index ( self , vocab : Vocabulary ) Given a Vocabulary , converts all strings in this field into (typically) integers. This modifies the Field object, it does not return anything. If your Field does not have any strings that need to be converted into indices, you do not need to implement this method.","title":"index"},{"location":"api/data/fields/field/#get_padding_lengths","text":"class Field ( Generic [ DataArray ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ] If there are things in this field that need padding, note them here. In order to pad a batch of instance, we get all of the lengths from the batch, take the max, and pad everything to that length (or use a pre-specified maximum length). The return value is a dictionary mapping keys to lengths, like {'num_tokens': 13} . This is always called after index .","title":"get_padding_lengths"},{"location":"api/data/fields/field/#as_tensor","text":"class Field ( Generic [ DataArray ]): | ... | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> DataArray Given a set of specified padding lengths, actually pad the data in this field and return a torch Tensor (or a more complex data structure) of the correct shape. We also take a couple of parameters that are important when constructing torch Tensors.","title":"as_tensor"},{"location":"api/data/fields/field/#empty_field","text":"class Field ( Generic [ DataArray ]): | ... | def empty_field ( self ) -> \"Field\" So that ListField can pad the number of fields in a list (e.g., the number of answer option TextFields ), we need a representation of an empty field of each type. This returns that. This will only ever be called when we're to the point of calling as_tensor , so you don't need to worry about get_padding_lengths , count_vocab_items , etc., being called on this empty field. We make this an instance method instead of a static method so that if there is any state in the Field, we can copy it over (e.g., the token indexers in TextField ).","title":"empty_field"},{"location":"api/data/fields/field/#batch_tensors","text":"class Field ( Generic [ DataArray ]): | ... | def batch_tensors ( self , tensor_list : List [ DataArray ]) -> DataArray Takes the output of Field.as_tensor() from a list of Instances and merges it into one batched tensor for this Field . The default implementation here in the base class handles cases where as_tensor returns a single torch tensor per instance. If your subclass returns something other than this, you need to override this method. This operation does not modify self , but in some cases we need the information contained in self in order to perform the batching, so this is an instance method, not a class method.","title":"batch_tensors"},{"location":"api/data/fields/field/#duplicate","text":"class Field ( Generic [ DataArray ]): | ... | def duplicate ( self )","title":"duplicate"},{"location":"api/data/fields/flag_field/","text":"allennlp .data .fields .flag_field [SOURCE] FlagField \u00b6 class FlagField ( Field [ Any ]): | def __init__ ( self , flag_value : Any ) -> None A class representing a flag, which must be constant across all instances in a batch. This will be passed to a forward method as a single value of whatever type you pass in. get_padding_lengths \u00b6 class FlagField ( Field [ Any ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ] as_tensor \u00b6 class FlagField ( Field [ Any ]): | ... | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> Any empty_field \u00b6 class FlagField ( Field [ Any ]): | ... | def empty_field ( self ) batch_tensors \u00b6 class FlagField ( Field [ Any ]): | ... | def batch_tensors ( self , tensor_list : List [ Any ]) -> Any human_readable_repr \u00b6 class FlagField ( Field [ Any ]): | ... | def human_readable_repr ( self ) -> Any","title":"flag_field"},{"location":"api/data/fields/flag_field/#flagfield","text":"class FlagField ( Field [ Any ]): | def __init__ ( self , flag_value : Any ) -> None A class representing a flag, which must be constant across all instances in a batch. This will be passed to a forward method as a single value of whatever type you pass in.","title":"FlagField"},{"location":"api/data/fields/flag_field/#get_padding_lengths","text":"class FlagField ( Field [ Any ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ]","title":"get_padding_lengths"},{"location":"api/data/fields/flag_field/#as_tensor","text":"class FlagField ( Field [ Any ]): | ... | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> Any","title":"as_tensor"},{"location":"api/data/fields/flag_field/#empty_field","text":"class FlagField ( Field [ Any ]): | ... | def empty_field ( self )","title":"empty_field"},{"location":"api/data/fields/flag_field/#batch_tensors","text":"class FlagField ( Field [ Any ]): | ... | def batch_tensors ( self , tensor_list : List [ Any ]) -> Any","title":"batch_tensors"},{"location":"api/data/fields/flag_field/#human_readable_repr","text":"class FlagField ( Field [ Any ]): | ... | def human_readable_repr ( self ) -> Any","title":"human_readable_repr"},{"location":"api/data/fields/index_field/","text":"allennlp .data .fields .index_field [SOURCE] IndexField \u00b6 class IndexField ( Field [ torch . Tensor ]): | def __init__ ( self , index : int , sequence_field : SequenceField ) -> None An IndexField is an index into a SequenceField , as might be used for representing a correct answer option in a list, or a span begin and span end position in a passage, for example. Because it's an index into a SequenceField , we take one of those as input and use it to compute padding lengths. Parameters \u00b6 index : int The index of the answer in the SequenceField . This is typically the \"correct answer\" in some classification decision over the sequence, like where an answer span starts in SQuAD, or which answer option is correct in a multiple choice question. A value of -1 means there is no label, which can be used for padding or other purposes. sequence_field : SequenceField A field containing the sequence that this IndexField is a pointer into. get_padding_lengths \u00b6 class IndexField ( Field [ torch . Tensor ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ] as_tensor \u00b6 class IndexField ( Field [ torch . Tensor ]): | ... | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor empty_field \u00b6 class IndexField ( Field [ torch . Tensor ]): | ... | def empty_field ( self ) human_readable_repr \u00b6 class IndexField ( Field [ torch . Tensor ]): | ... | def human_readable_repr ( self )","title":"index_field"},{"location":"api/data/fields/index_field/#indexfield","text":"class IndexField ( Field [ torch . Tensor ]): | def __init__ ( self , index : int , sequence_field : SequenceField ) -> None An IndexField is an index into a SequenceField , as might be used for representing a correct answer option in a list, or a span begin and span end position in a passage, for example. Because it's an index into a SequenceField , we take one of those as input and use it to compute padding lengths.","title":"IndexField"},{"location":"api/data/fields/index_field/#get_padding_lengths","text":"class IndexField ( Field [ torch . Tensor ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ]","title":"get_padding_lengths"},{"location":"api/data/fields/index_field/#as_tensor","text":"class IndexField ( Field [ torch . Tensor ]): | ... | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor","title":"as_tensor"},{"location":"api/data/fields/index_field/#empty_field","text":"class IndexField ( Field [ torch . Tensor ]): | ... | def empty_field ( self )","title":"empty_field"},{"location":"api/data/fields/index_field/#human_readable_repr","text":"class IndexField ( Field [ torch . Tensor ]): | ... | def human_readable_repr ( self )","title":"human_readable_repr"},{"location":"api/data/fields/label_field/","text":"allennlp .data .fields .label_field [SOURCE] LabelField \u00b6 class LabelField ( Field [ torch . Tensor ]): | def __init__ ( | self , | label : Union [ str , int ], | label_namespace : str = \"labels\" , | skip_indexing : bool = False | ) -> None A LabelField is a categorical label of some kind, where the labels are either strings of text or 0-indexed integers (if you wish to skip indexing by passing skip_indexing=True). If the labels need indexing, we will use a Vocabulary to convert the string labels into integers. This field will get converted into an integer index representing the class label. Parameters \u00b6 label : Union[str, int] label_namespace : str , optional (default = \"labels\" ) The namespace to use for converting label strings into integers. We map label strings to integers for you (e.g., \"entailment\" and \"contradiction\" get converted to 0, 1, ...), and this namespace tells the Vocabulary object which mapping from strings to integers to use (so \"entailment\" as a label doesn't get the same integer id as \"entailment\" as a word). If you have multiple different label fields in your data, you should make sure you use different namespaces for each one, always using the suffix \"labels\" (e.g., \"passage_labels\" and \"question_labels\"). skip_indexing : bool , optional (default = False ) If your labels are 0-indexed integers, you can pass in this flag, and we'll skip the indexing step. If this is False and your labels are not strings, this throws a ConfigurationError . count_vocab_items \u00b6 class LabelField ( Field [ torch . Tensor ]): | ... | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]]) index \u00b6 class LabelField ( Field [ torch . Tensor ]): | ... | def index ( self , vocab : Vocabulary ) get_padding_lengths \u00b6 class LabelField ( Field [ torch . Tensor ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ] as_tensor \u00b6 class LabelField ( Field [ torch . Tensor ]): | ... | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor empty_field \u00b6 class LabelField ( Field [ torch . Tensor ]): | ... | def empty_field ( self ) human_readable_repr \u00b6 class LabelField ( Field [ torch . Tensor ]): | ... | def human_readable_repr ( self ) -> Union [ str , int ]","title":"label_field"},{"location":"api/data/fields/label_field/#labelfield","text":"class LabelField ( Field [ torch . Tensor ]): | def __init__ ( | self , | label : Union [ str , int ], | label_namespace : str = \"labels\" , | skip_indexing : bool = False | ) -> None A LabelField is a categorical label of some kind, where the labels are either strings of text or 0-indexed integers (if you wish to skip indexing by passing skip_indexing=True). If the labels need indexing, we will use a Vocabulary to convert the string labels into integers. This field will get converted into an integer index representing the class label.","title":"LabelField"},{"location":"api/data/fields/label_field/#count_vocab_items","text":"class LabelField ( Field [ torch . Tensor ]): | ... | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]])","title":"count_vocab_items"},{"location":"api/data/fields/label_field/#index","text":"class LabelField ( Field [ torch . Tensor ]): | ... | def index ( self , vocab : Vocabulary )","title":"index"},{"location":"api/data/fields/label_field/#get_padding_lengths","text":"class LabelField ( Field [ torch . Tensor ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ]","title":"get_padding_lengths"},{"location":"api/data/fields/label_field/#as_tensor","text":"class LabelField ( Field [ torch . Tensor ]): | ... | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor","title":"as_tensor"},{"location":"api/data/fields/label_field/#empty_field","text":"class LabelField ( Field [ torch . Tensor ]): | ... | def empty_field ( self )","title":"empty_field"},{"location":"api/data/fields/label_field/#human_readable_repr","text":"class LabelField ( Field [ torch . Tensor ]): | ... | def human_readable_repr ( self ) -> Union [ str , int ]","title":"human_readable_repr"},{"location":"api/data/fields/list_field/","text":"allennlp .data .fields .list_field [SOURCE] ListField \u00b6 class ListField ( SequenceField [ DataArray ]): | def __init__ ( self , field_list : Sequence [ Field ]) -> None A ListField is a list of other fields. You would use this to represent, e.g., a list of answer options that are themselves TextFields . This field will get converted into a tensor that has one more mode than the items in the list. If this is a list of TextFields that have shape (num_words, num_characters), this ListField will output a tensor of shape (num_sentences, num_words, num_characters). Parameters \u00b6 field_list : List[Field] A list of Field objects to be concatenated into a single input tensor. All of the contained Field objects must be of the same type. __iter__ \u00b6 class ListField ( SequenceField [ DataArray ]): | ... | def __iter__ ( self ) -> Iterator [ Field ] count_vocab_items \u00b6 class ListField ( SequenceField [ DataArray ]): | ... | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]]) index \u00b6 class ListField ( SequenceField [ DataArray ]): | ... | def index ( self , vocab : Vocabulary ) get_padding_lengths \u00b6 class ListField ( SequenceField [ DataArray ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ] sequence_length \u00b6 class ListField ( SequenceField [ DataArray ]): | ... | def sequence_length ( self ) -> int as_tensor \u00b6 class ListField ( SequenceField [ DataArray ]): | ... | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> DataArray empty_field \u00b6 class ListField ( SequenceField [ DataArray ]): | ... | def empty_field ( self ) batch_tensors \u00b6 class ListField ( SequenceField [ DataArray ]): | ... | def batch_tensors ( self , tensor_list : List [ DataArray ]) -> DataArray human_readable_repr \u00b6 class ListField ( SequenceField [ DataArray ]): | ... | def human_readable_repr ( self ) -> List [ Any ]","title":"list_field"},{"location":"api/data/fields/list_field/#listfield","text":"class ListField ( SequenceField [ DataArray ]): | def __init__ ( self , field_list : Sequence [ Field ]) -> None A ListField is a list of other fields. You would use this to represent, e.g., a list of answer options that are themselves TextFields . This field will get converted into a tensor that has one more mode than the items in the list. If this is a list of TextFields that have shape (num_words, num_characters), this ListField will output a tensor of shape (num_sentences, num_words, num_characters).","title":"ListField"},{"location":"api/data/fields/list_field/#__iter__","text":"class ListField ( SequenceField [ DataArray ]): | ... | def __iter__ ( self ) -> Iterator [ Field ]","title":"__iter__"},{"location":"api/data/fields/list_field/#count_vocab_items","text":"class ListField ( SequenceField [ DataArray ]): | ... | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]])","title":"count_vocab_items"},{"location":"api/data/fields/list_field/#index","text":"class ListField ( SequenceField [ DataArray ]): | ... | def index ( self , vocab : Vocabulary )","title":"index"},{"location":"api/data/fields/list_field/#get_padding_lengths","text":"class ListField ( SequenceField [ DataArray ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ]","title":"get_padding_lengths"},{"location":"api/data/fields/list_field/#sequence_length","text":"class ListField ( SequenceField [ DataArray ]): | ... | def sequence_length ( self ) -> int","title":"sequence_length"},{"location":"api/data/fields/list_field/#as_tensor","text":"class ListField ( SequenceField [ DataArray ]): | ... | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> DataArray","title":"as_tensor"},{"location":"api/data/fields/list_field/#empty_field","text":"class ListField ( SequenceField [ DataArray ]): | ... | def empty_field ( self )","title":"empty_field"},{"location":"api/data/fields/list_field/#batch_tensors","text":"class ListField ( SequenceField [ DataArray ]): | ... | def batch_tensors ( self , tensor_list : List [ DataArray ]) -> DataArray","title":"batch_tensors"},{"location":"api/data/fields/list_field/#human_readable_repr","text":"class ListField ( SequenceField [ DataArray ]): | ... | def human_readable_repr ( self ) -> List [ Any ]","title":"human_readable_repr"},{"location":"api/data/fields/metadata_field/","text":"allennlp .data .fields .metadata_field [SOURCE] MetadataField \u00b6 class MetadataField ( Field [ DataArray ], Mapping [ str , Any ]): | def __init__ ( self , metadata : Any ) -> None A MetadataField is a Field that does not get converted into tensors. It just carries side information that might be needed later on, for computing some third-party metric, or outputting debugging information, or whatever else you need. We use this in the BiDAF model, for instance, to keep track of question IDs and passage token offsets, so we can more easily use the official evaluation script to compute metrics. We don't try to do any kind of smart combination of this field for batched input - when you use this Field in a model, you'll get a list of metadata objects, one for each instance in the batch. Parameters \u00b6 metadata : Any Some object containing the metadata that you want to store. It's likely that you'll want this to be a dictionary, but it could be anything you want. __iter__ \u00b6 class MetadataField ( Field [ DataArray ], Mapping [ str , Any ]): | ... | def __iter__ ( self ) get_padding_lengths \u00b6 class MetadataField ( Field [ DataArray ], Mapping [ str , Any ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ] as_tensor \u00b6 class MetadataField ( Field [ DataArray ], Mapping [ str , Any ]): | ... | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> DataArray empty_field \u00b6 class MetadataField ( Field [ DataArray ], Mapping [ str , Any ]): | ... | def empty_field ( self ) -> \"MetadataField\" batch_tensors \u00b6 class MetadataField ( Field [ DataArray ], Mapping [ str , Any ]): | ... | def batch_tensors ( | self , | tensor_list : List [ DataArray ] | ) -> List [ DataArray ] human_readable_repr \u00b6 class MetadataField ( Field [ DataArray ], Mapping [ str , Any ]): | ... | def human_readable_repr ( self )","title":"metadata_field"},{"location":"api/data/fields/metadata_field/#metadatafield","text":"class MetadataField ( Field [ DataArray ], Mapping [ str , Any ]): | def __init__ ( self , metadata : Any ) -> None A MetadataField is a Field that does not get converted into tensors. It just carries side information that might be needed later on, for computing some third-party metric, or outputting debugging information, or whatever else you need. We use this in the BiDAF model, for instance, to keep track of question IDs and passage token offsets, so we can more easily use the official evaluation script to compute metrics. We don't try to do any kind of smart combination of this field for batched input - when you use this Field in a model, you'll get a list of metadata objects, one for each instance in the batch.","title":"MetadataField"},{"location":"api/data/fields/metadata_field/#__iter__","text":"class MetadataField ( Field [ DataArray ], Mapping [ str , Any ]): | ... | def __iter__ ( self )","title":"__iter__"},{"location":"api/data/fields/metadata_field/#get_padding_lengths","text":"class MetadataField ( Field [ DataArray ], Mapping [ str , Any ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ]","title":"get_padding_lengths"},{"location":"api/data/fields/metadata_field/#as_tensor","text":"class MetadataField ( Field [ DataArray ], Mapping [ str , Any ]): | ... | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> DataArray","title":"as_tensor"},{"location":"api/data/fields/metadata_field/#empty_field","text":"class MetadataField ( Field [ DataArray ], Mapping [ str , Any ]): | ... | def empty_field ( self ) -> \"MetadataField\"","title":"empty_field"},{"location":"api/data/fields/metadata_field/#batch_tensors","text":"class MetadataField ( Field [ DataArray ], Mapping [ str , Any ]): | ... | def batch_tensors ( | self , | tensor_list : List [ DataArray ] | ) -> List [ DataArray ]","title":"batch_tensors"},{"location":"api/data/fields/metadata_field/#human_readable_repr","text":"class MetadataField ( Field [ DataArray ], Mapping [ str , Any ]): | ... | def human_readable_repr ( self )","title":"human_readable_repr"},{"location":"api/data/fields/multilabel_field/","text":"allennlp .data .fields .multilabel_field [SOURCE] MultiLabelField \u00b6 class MultiLabelField ( Field [ torch . Tensor ]): | def __init__ ( | self , | labels : Sequence [ Union [ str , int ]], | label_namespace : str = \"labels\" , | skip_indexing : bool = False , | num_labels : Optional [ int ] = None | ) -> None A MultiLabelField is an extension of the LabelField that allows for multiple labels. It is particularly useful in multi-label classification where more than one label can be correct. As with the LabelField , labels are either strings of text or 0-indexed integers (if you wish to skip indexing by passing skip_indexing=True). If the labels need indexing, we will use a Vocabulary to convert the string labels into integers. This field will get converted into a vector of length equal to the vocabulary size with one hot encoding for the labels (all zeros, and ones for the labels). Parameters \u00b6 labels : Sequence[Union[str, int]] label_namespace : str , optional (default = \"labels\" ) The namespace to use for converting label strings into integers. We map label strings to integers for you (e.g., \"entailment\" and \"contradiction\" get converted to 0, 1, ...), and this namespace tells the Vocabulary object which mapping from strings to integers to use (so \"entailment\" as a label doesn't get the same integer id as \"entailment\" as a word). If you have multiple different label fields in your data, you should make sure you use different namespaces for each one, always using the suffix \"labels\" (e.g., \"passage_labels\" and \"question_labels\"). skip_indexing : bool , optional (default = False ) If your labels are 0-indexed integers, you can pass in this flag, and we'll skip the indexing step. If this is False and your labels are not strings, this throws a ConfigurationError . num_labels : int , optional (default = None ) If skip_indexing=True , the total number of possible labels should be provided, which is required to decide the size of the output tensor. num_labels should equal largest label id + 1. If skip_indexing=False , num_labels is not required. count_vocab_items \u00b6 class MultiLabelField ( Field [ torch . Tensor ]): | ... | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]]) index \u00b6 class MultiLabelField ( Field [ torch . Tensor ]): | ... | def index ( self , vocab : Vocabulary ) get_padding_lengths \u00b6 class MultiLabelField ( Field [ torch . Tensor ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ] as_tensor \u00b6 class MultiLabelField ( Field [ torch . Tensor ]): | ... | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor empty_field \u00b6 class MultiLabelField ( Field [ torch . Tensor ]): | ... | def empty_field ( self ) human_readable_repr \u00b6 class MultiLabelField ( Field [ torch . Tensor ]): | ... | def human_readable_repr ( self ) -> Sequence [ Union [ str , int ]]","title":"multilabel_field"},{"location":"api/data/fields/multilabel_field/#multilabelfield","text":"class MultiLabelField ( Field [ torch . Tensor ]): | def __init__ ( | self , | labels : Sequence [ Union [ str , int ]], | label_namespace : str = \"labels\" , | skip_indexing : bool = False , | num_labels : Optional [ int ] = None | ) -> None A MultiLabelField is an extension of the LabelField that allows for multiple labels. It is particularly useful in multi-label classification where more than one label can be correct. As with the LabelField , labels are either strings of text or 0-indexed integers (if you wish to skip indexing by passing skip_indexing=True). If the labels need indexing, we will use a Vocabulary to convert the string labels into integers. This field will get converted into a vector of length equal to the vocabulary size with one hot encoding for the labels (all zeros, and ones for the labels).","title":"MultiLabelField"},{"location":"api/data/fields/multilabel_field/#count_vocab_items","text":"class MultiLabelField ( Field [ torch . Tensor ]): | ... | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]])","title":"count_vocab_items"},{"location":"api/data/fields/multilabel_field/#index","text":"class MultiLabelField ( Field [ torch . Tensor ]): | ... | def index ( self , vocab : Vocabulary )","title":"index"},{"location":"api/data/fields/multilabel_field/#get_padding_lengths","text":"class MultiLabelField ( Field [ torch . Tensor ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ]","title":"get_padding_lengths"},{"location":"api/data/fields/multilabel_field/#as_tensor","text":"class MultiLabelField ( Field [ torch . Tensor ]): | ... | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor","title":"as_tensor"},{"location":"api/data/fields/multilabel_field/#empty_field","text":"class MultiLabelField ( Field [ torch . Tensor ]): | ... | def empty_field ( self )","title":"empty_field"},{"location":"api/data/fields/multilabel_field/#human_readable_repr","text":"class MultiLabelField ( Field [ torch . Tensor ]): | ... | def human_readable_repr ( self ) -> Sequence [ Union [ str , int ]]","title":"human_readable_repr"},{"location":"api/data/fields/namespace_swapping_field/","text":"allennlp .data .fields .namespace_swapping_field [SOURCE] NamespaceSwappingField \u00b6 class NamespaceSwappingField ( Field [ torch . Tensor ]): | def __init__ ( | self , | source_tokens : List [ Token ], | target_namespace : str | ) -> None A NamespaceSwappingField is used to map tokens in one namespace to tokens in another namespace. It is used by seq2seq models with a copy mechanism that copies tokens from the source sentence into the target sentence. Parameters \u00b6 source_tokens : List[Token] The tokens from the source sentence. target_namespace : str The namespace that the tokens from the source sentence will be mapped to. index \u00b6 class NamespaceSwappingField ( Field [ torch . Tensor ]): | ... | def index ( self , vocab : Vocabulary ) get_padding_lengths \u00b6 class NamespaceSwappingField ( Field [ torch . Tensor ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ] as_tensor \u00b6 class NamespaceSwappingField ( Field [ torch . Tensor ]): | ... | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor empty_field \u00b6 class NamespaceSwappingField ( Field [ torch . Tensor ]): | ... | def empty_field ( self ) -> \"NamespaceSwappingField\" human_readable_repr \u00b6 class NamespaceSwappingField ( Field [ torch . Tensor ]): | ... | def human_readable_repr ( self ) -> Dict [ str , Any ]","title":"namespace_swapping_field"},{"location":"api/data/fields/namespace_swapping_field/#namespaceswappingfield","text":"class NamespaceSwappingField ( Field [ torch . Tensor ]): | def __init__ ( | self , | source_tokens : List [ Token ], | target_namespace : str | ) -> None A NamespaceSwappingField is used to map tokens in one namespace to tokens in another namespace. It is used by seq2seq models with a copy mechanism that copies tokens from the source sentence into the target sentence.","title":"NamespaceSwappingField"},{"location":"api/data/fields/namespace_swapping_field/#index","text":"class NamespaceSwappingField ( Field [ torch . Tensor ]): | ... | def index ( self , vocab : Vocabulary )","title":"index"},{"location":"api/data/fields/namespace_swapping_field/#get_padding_lengths","text":"class NamespaceSwappingField ( Field [ torch . Tensor ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ]","title":"get_padding_lengths"},{"location":"api/data/fields/namespace_swapping_field/#as_tensor","text":"class NamespaceSwappingField ( Field [ torch . Tensor ]): | ... | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor","title":"as_tensor"},{"location":"api/data/fields/namespace_swapping_field/#empty_field","text":"class NamespaceSwappingField ( Field [ torch . Tensor ]): | ... | def empty_field ( self ) -> \"NamespaceSwappingField\"","title":"empty_field"},{"location":"api/data/fields/namespace_swapping_field/#human_readable_repr","text":"class NamespaceSwappingField ( Field [ torch . Tensor ]): | ... | def human_readable_repr ( self ) -> Dict [ str , Any ]","title":"human_readable_repr"},{"location":"api/data/fields/sequence_field/","text":"allennlp .data .fields .sequence_field [SOURCE] SequenceField \u00b6 class SequenceField ( Field [ DataArray ]) A SequenceField represents a sequence of things. This class just adds a method onto Field : sequence_length . It exists so that SequenceLabelField , IndexField and other similar Fields can have a single type to require, with a consistent API, whether they are pointing to words in a TextField , items in a ListField , or something else. sequence_length \u00b6 class SequenceField ( Field [ DataArray ]): | ... | def sequence_length ( self ) -> int How many elements are there in this sequence? empty_field \u00b6 class SequenceField ( Field [ DataArray ]): | ... | def empty_field ( self ) -> \"SequenceField\"","title":"sequence_field"},{"location":"api/data/fields/sequence_field/#sequencefield","text":"class SequenceField ( Field [ DataArray ]) A SequenceField represents a sequence of things. This class just adds a method onto Field : sequence_length . It exists so that SequenceLabelField , IndexField and other similar Fields can have a single type to require, with a consistent API, whether they are pointing to words in a TextField , items in a ListField , or something else.","title":"SequenceField"},{"location":"api/data/fields/sequence_field/#sequence_length","text":"class SequenceField ( Field [ DataArray ]): | ... | def sequence_length ( self ) -> int How many elements are there in this sequence?","title":"sequence_length"},{"location":"api/data/fields/sequence_field/#empty_field","text":"class SequenceField ( Field [ DataArray ]): | ... | def empty_field ( self ) -> \"SequenceField\"","title":"empty_field"},{"location":"api/data/fields/sequence_label_field/","text":"allennlp .data .fields .sequence_label_field [SOURCE] SequenceLabelField \u00b6 class SequenceLabelField ( Field [ torch . Tensor ]): | def __init__ ( | self , | labels : Union [ List [ str ], List [ int ]], | sequence_field : SequenceField , | label_namespace : str = \"labels\" | ) -> None A SequenceLabelField assigns a categorical label to each element in a SequenceField . Because it's a labeling of some other field, we take that field as input here, and we use it to determine our padding and other things. This field will get converted into a list of integer class ids, representing the correct class for each element in the sequence. Parameters \u00b6 labels : Union[List[str], List[int]] A sequence of categorical labels, encoded as strings or integers. These could be POS tags like [NN, JJ, ...], BIO tags like [B-PERS, I-PERS, O, O, ...], or any other categorical tag sequence. If the labels are encoded as integers, they will not be indexed using a vocab. sequence_field : SequenceField A field containing the sequence that this SequenceLabelField is labeling. Most often, this is a TextField , for tagging individual tokens in a sentence. label_namespace : str , optional (default = 'labels' ) The namespace to use for converting tag strings into integers. We convert tag strings to integers for you, and this parameter tells the Vocabulary object which mapping from strings to integers to use (so that \"O\" as a tag doesn't get the same id as \"O\" as a word). __iter__ \u00b6 class SequenceLabelField ( Field [ torch . Tensor ]): | ... | def __iter__ ( self ) -> Iterator [ Union [ str , int ]] count_vocab_items \u00b6 class SequenceLabelField ( Field [ torch . Tensor ]): | ... | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]]) index \u00b6 class SequenceLabelField ( Field [ torch . Tensor ]): | ... | def index ( self , vocab : Vocabulary ) get_padding_lengths \u00b6 class SequenceLabelField ( Field [ torch . Tensor ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ] as_tensor \u00b6 class SequenceLabelField ( Field [ torch . Tensor ]): | ... | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor empty_field \u00b6 class SequenceLabelField ( Field [ torch . Tensor ]): | ... | def empty_field ( self ) -> \"SequenceLabelField\" human_readable_repr \u00b6 class SequenceLabelField ( Field [ torch . Tensor ]): | ... | def human_readable_repr ( self ) -> Union [ List [ str ], List [ int ]]","title":"sequence_label_field"},{"location":"api/data/fields/sequence_label_field/#sequencelabelfield","text":"class SequenceLabelField ( Field [ torch . Tensor ]): | def __init__ ( | self , | labels : Union [ List [ str ], List [ int ]], | sequence_field : SequenceField , | label_namespace : str = \"labels\" | ) -> None A SequenceLabelField assigns a categorical label to each element in a SequenceField . Because it's a labeling of some other field, we take that field as input here, and we use it to determine our padding and other things. This field will get converted into a list of integer class ids, representing the correct class for each element in the sequence.","title":"SequenceLabelField"},{"location":"api/data/fields/sequence_label_field/#__iter__","text":"class SequenceLabelField ( Field [ torch . Tensor ]): | ... | def __iter__ ( self ) -> Iterator [ Union [ str , int ]]","title":"__iter__"},{"location":"api/data/fields/sequence_label_field/#count_vocab_items","text":"class SequenceLabelField ( Field [ torch . Tensor ]): | ... | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]])","title":"count_vocab_items"},{"location":"api/data/fields/sequence_label_field/#index","text":"class SequenceLabelField ( Field [ torch . Tensor ]): | ... | def index ( self , vocab : Vocabulary )","title":"index"},{"location":"api/data/fields/sequence_label_field/#get_padding_lengths","text":"class SequenceLabelField ( Field [ torch . Tensor ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ]","title":"get_padding_lengths"},{"location":"api/data/fields/sequence_label_field/#as_tensor","text":"class SequenceLabelField ( Field [ torch . Tensor ]): | ... | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor","title":"as_tensor"},{"location":"api/data/fields/sequence_label_field/#empty_field","text":"class SequenceLabelField ( Field [ torch . Tensor ]): | ... | def empty_field ( self ) -> \"SequenceLabelField\"","title":"empty_field"},{"location":"api/data/fields/sequence_label_field/#human_readable_repr","text":"class SequenceLabelField ( Field [ torch . Tensor ]): | ... | def human_readable_repr ( self ) -> Union [ List [ str ], List [ int ]]","title":"human_readable_repr"},{"location":"api/data/fields/span_field/","text":"allennlp .data .fields .span_field [SOURCE] SpanField \u00b6 class SpanField ( Field [ torch . Tensor ]): | def __init__ ( | self , | span_start : int , | span_end : int , | sequence_field : SequenceField | ) -> None A SpanField is a pair of inclusive, zero-indexed (start, end) indices into a SequenceField , used to represent a span of text. Because it's a pair of indices into a SequenceField , we take one of those as input to make the span's dependence explicit and to validate that the span is well defined. Parameters \u00b6 span_start : int The index of the start of the span in the SequenceField . span_end : int The inclusive index of the end of the span in the SequenceField . sequence_field : SequenceField A field containing the sequence that this SpanField is a span inside. get_padding_lengths \u00b6 class SpanField ( Field [ torch . Tensor ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ] as_tensor \u00b6 class SpanField ( Field [ torch . Tensor ]): | ... | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor empty_field \u00b6 class SpanField ( Field [ torch . Tensor ]): | ... | def empty_field ( self ) human_readable_repr \u00b6 class SpanField ( Field [ torch . Tensor ]): | ... | def human_readable_repr ( self ) -> Tuple [ int , int ]","title":"span_field"},{"location":"api/data/fields/span_field/#spanfield","text":"class SpanField ( Field [ torch . Tensor ]): | def __init__ ( | self , | span_start : int , | span_end : int , | sequence_field : SequenceField | ) -> None A SpanField is a pair of inclusive, zero-indexed (start, end) indices into a SequenceField , used to represent a span of text. Because it's a pair of indices into a SequenceField , we take one of those as input to make the span's dependence explicit and to validate that the span is well defined.","title":"SpanField"},{"location":"api/data/fields/span_field/#get_padding_lengths","text":"class SpanField ( Field [ torch . Tensor ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ]","title":"get_padding_lengths"},{"location":"api/data/fields/span_field/#as_tensor","text":"class SpanField ( Field [ torch . Tensor ]): | ... | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor","title":"as_tensor"},{"location":"api/data/fields/span_field/#empty_field","text":"class SpanField ( Field [ torch . Tensor ]): | ... | def empty_field ( self )","title":"empty_field"},{"location":"api/data/fields/span_field/#human_readable_repr","text":"class SpanField ( Field [ torch . Tensor ]): | ... | def human_readable_repr ( self ) -> Tuple [ int , int ]","title":"human_readable_repr"},{"location":"api/data/fields/tensor_field/","text":"allennlp .data .fields .tensor_field [SOURCE] TensorField \u00b6 class TensorField ( Field [ torch . Tensor ]): | def __init__ ( | self , | tensor : Union [ torch . Tensor , np . ndarray ], | padding_value : Any = 0.0 , | dtype : Optional [ Union [ np . dtype , torch . dtype ]] = None | ) -> None A class representing a tensor, which could have arbitrary dimensions. A batch of these tensors are padded to the max dimension length in the batch for each dimension. get_padding_lengths \u00b6 class TensorField ( Field [ torch . Tensor ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ] as_tensor \u00b6 class TensorField ( Field [ torch . Tensor ]): | ... | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor empty_field \u00b6 class TensorField ( Field [ torch . Tensor ]): | ... | def empty_field ( self ) array \u00b6 class TensorField ( Field [ torch . Tensor ]): | ... | @property | def array ( self ) This is a compatibility method that returns the underlying tensor as a numpy array. human_readable_repr \u00b6 class TensorField ( Field [ torch . Tensor ]): | ... | def human_readable_repr ( self ) -> JsonDict","title":"tensor_field"},{"location":"api/data/fields/tensor_field/#tensorfield","text":"class TensorField ( Field [ torch . Tensor ]): | def __init__ ( | self , | tensor : Union [ torch . Tensor , np . ndarray ], | padding_value : Any = 0.0 , | dtype : Optional [ Union [ np . dtype , torch . dtype ]] = None | ) -> None A class representing a tensor, which could have arbitrary dimensions. A batch of these tensors are padded to the max dimension length in the batch for each dimension.","title":"TensorField"},{"location":"api/data/fields/tensor_field/#get_padding_lengths","text":"class TensorField ( Field [ torch . Tensor ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ]","title":"get_padding_lengths"},{"location":"api/data/fields/tensor_field/#as_tensor","text":"class TensorField ( Field [ torch . Tensor ]): | ... | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor","title":"as_tensor"},{"location":"api/data/fields/tensor_field/#empty_field","text":"class TensorField ( Field [ torch . Tensor ]): | ... | def empty_field ( self )","title":"empty_field"},{"location":"api/data/fields/tensor_field/#array","text":"class TensorField ( Field [ torch . Tensor ]): | ... | @property | def array ( self ) This is a compatibility method that returns the underlying tensor as a numpy array.","title":"array"},{"location":"api/data/fields/tensor_field/#human_readable_repr","text":"class TensorField ( Field [ torch . Tensor ]): | ... | def human_readable_repr ( self ) -> JsonDict","title":"human_readable_repr"},{"location":"api/data/fields/text_field/","text":"allennlp .data .fields .text_field [SOURCE] A TextField represents a string of text, the kind that you might want to represent with standard word vectors, or pass through an LSTM. TextFieldTensors \u00b6 TextFieldTensors = Dict [ str , Dict [ str , torch . Tensor ]] TextField \u00b6 class TextField ( SequenceField [ TextFieldTensors ]): | def __init__ ( | self , | tokens : List [ Token ], | token_indexers : Optional [ Dict [ str , TokenIndexer ]] = None | ) -> None This Field represents a list of string tokens. Before constructing this object, you need to tokenize raw strings using a Tokenizer . Because string tokens can be represented as indexed arrays in a number of ways, we also take a dictionary of TokenIndexer objects that will be used to convert the tokens into indices. Each TokenIndexer could represent each token as a single ID, or a list of character IDs, or something else. This field will get converted into a dictionary of arrays, one for each TokenIndexer . A SingleIdTokenIndexer produces an array of shape (num_tokens,), while a TokenCharactersIndexer produces an array of shape (num_tokens, num_characters). token_indexers \u00b6 class TextField ( SequenceField [ TextFieldTensors ]): | ... | @property | def token_indexers ( self ) -> Dict [ str , TokenIndexer ] token_indexers \u00b6 class TextField ( SequenceField [ TextFieldTensors ]): | ... | @token_indexers . setter | def token_indexers ( | self , | token_indexers : Dict [ str , TokenIndexer ] | ) -> None count_vocab_items \u00b6 class TextField ( SequenceField [ TextFieldTensors ]): | ... | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]]) index \u00b6 class TextField ( SequenceField [ TextFieldTensors ]): | ... | def index ( self , vocab : Vocabulary ) get_padding_lengths \u00b6 class TextField ( SequenceField [ TextFieldTensors ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ] The TextField has a list of Tokens , and each Token gets converted into arrays by (potentially) several TokenIndexers . This method gets the max length (over tokens) associated with each of these arrays. sequence_length \u00b6 class TextField ( SequenceField [ TextFieldTensors ]): | ... | def sequence_length ( self ) -> int as_tensor \u00b6 class TextField ( SequenceField [ TextFieldTensors ]): | ... | def as_tensor ( | self , | padding_lengths : Dict [ str , int ] | ) -> TextFieldTensors empty_field \u00b6 class TextField ( SequenceField [ TextFieldTensors ]): | ... | def empty_field ( self ) batch_tensors \u00b6 class TextField ( SequenceField [ TextFieldTensors ]): | ... | def batch_tensors ( | self , | tensor_list : List [ TextFieldTensors ] | ) -> TextFieldTensors __iter__ \u00b6 class TextField ( SequenceField [ TextFieldTensors ]): | ... | def __iter__ ( self ) -> Iterator [ Token ] duplicate \u00b6 class TextField ( SequenceField [ TextFieldTensors ]): | ... | def duplicate ( self ) Overrides the behavior of duplicate so that self._token_indexers won't actually be deep-copied. Not only would it be extremely inefficient to deep-copy the token indexers, but it also fails in many cases since some tokenizers (like those used in the 'transformers' lib) cannot actually be deep-copied. human_readable_repr \u00b6 class TextField ( SequenceField [ TextFieldTensors ]): | ... | def human_readable_repr ( self ) -> List [ str ]","title":"text_field"},{"location":"api/data/fields/text_field/#textfieldtensors","text":"TextFieldTensors = Dict [ str , Dict [ str , torch . Tensor ]]","title":"TextFieldTensors"},{"location":"api/data/fields/text_field/#textfield","text":"class TextField ( SequenceField [ TextFieldTensors ]): | def __init__ ( | self , | tokens : List [ Token ], | token_indexers : Optional [ Dict [ str , TokenIndexer ]] = None | ) -> None This Field represents a list of string tokens. Before constructing this object, you need to tokenize raw strings using a Tokenizer . Because string tokens can be represented as indexed arrays in a number of ways, we also take a dictionary of TokenIndexer objects that will be used to convert the tokens into indices. Each TokenIndexer could represent each token as a single ID, or a list of character IDs, or something else. This field will get converted into a dictionary of arrays, one for each TokenIndexer . A SingleIdTokenIndexer produces an array of shape (num_tokens,), while a TokenCharactersIndexer produces an array of shape (num_tokens, num_characters).","title":"TextField"},{"location":"api/data/fields/text_field/#token_indexers","text":"class TextField ( SequenceField [ TextFieldTensors ]): | ... | @property | def token_indexers ( self ) -> Dict [ str , TokenIndexer ]","title":"token_indexers"},{"location":"api/data/fields/text_field/#token_indexers_1","text":"class TextField ( SequenceField [ TextFieldTensors ]): | ... | @token_indexers . setter | def token_indexers ( | self , | token_indexers : Dict [ str , TokenIndexer ] | ) -> None","title":"token_indexers"},{"location":"api/data/fields/text_field/#count_vocab_items","text":"class TextField ( SequenceField [ TextFieldTensors ]): | ... | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]])","title":"count_vocab_items"},{"location":"api/data/fields/text_field/#index","text":"class TextField ( SequenceField [ TextFieldTensors ]): | ... | def index ( self , vocab : Vocabulary )","title":"index"},{"location":"api/data/fields/text_field/#get_padding_lengths","text":"class TextField ( SequenceField [ TextFieldTensors ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ] The TextField has a list of Tokens , and each Token gets converted into arrays by (potentially) several TokenIndexers . This method gets the max length (over tokens) associated with each of these arrays.","title":"get_padding_lengths"},{"location":"api/data/fields/text_field/#sequence_length","text":"class TextField ( SequenceField [ TextFieldTensors ]): | ... | def sequence_length ( self ) -> int","title":"sequence_length"},{"location":"api/data/fields/text_field/#as_tensor","text":"class TextField ( SequenceField [ TextFieldTensors ]): | ... | def as_tensor ( | self , | padding_lengths : Dict [ str , int ] | ) -> TextFieldTensors","title":"as_tensor"},{"location":"api/data/fields/text_field/#empty_field","text":"class TextField ( SequenceField [ TextFieldTensors ]): | ... | def empty_field ( self )","title":"empty_field"},{"location":"api/data/fields/text_field/#batch_tensors","text":"class TextField ( SequenceField [ TextFieldTensors ]): | ... | def batch_tensors ( | self , | tensor_list : List [ TextFieldTensors ] | ) -> TextFieldTensors","title":"batch_tensors"},{"location":"api/data/fields/text_field/#__iter__","text":"class TextField ( SequenceField [ TextFieldTensors ]): | ... | def __iter__ ( self ) -> Iterator [ Token ]","title":"__iter__"},{"location":"api/data/fields/text_field/#duplicate","text":"class TextField ( SequenceField [ TextFieldTensors ]): | ... | def duplicate ( self ) Overrides the behavior of duplicate so that self._token_indexers won't actually be deep-copied. Not only would it be extremely inefficient to deep-copy the token indexers, but it also fails in many cases since some tokenizers (like those used in the 'transformers' lib) cannot actually be deep-copied.","title":"duplicate"},{"location":"api/data/fields/text_field/#human_readable_repr","text":"class TextField ( SequenceField [ TextFieldTensors ]): | ... | def human_readable_repr ( self ) -> List [ str ]","title":"human_readable_repr"},{"location":"api/data/fields/transformer_text_field/","text":"allennlp .data .fields .transformer_text_field [SOURCE] TransformerTextField \u00b6 class TransformerTextField ( Field [ torch . Tensor ]): | def __init__ ( | self , | input_ids : Union [ torch . Tensor , List [ int ]], | token_type_ids : Optional [ Union [ torch . Tensor , List [ int ]]] = None , | attention_mask : Optional [ Union [ torch . Tensor , List [ int ]]] = None , | special_tokens_mask : Optional [ Union [ torch . Tensor , List [ int ]]] = None , | offsets_mapping : Optional [ Union [ torch . Tensor , List [ int ]]] = None , | padding_token_id : int = 0 | ) -> None A TransformerTextField is a collection of several tensors that are are a representation of text, tokenized and ready to become input to a transformer. The naming pattern of the tensors follows the pattern that's produced by the huggingface tokenizers, and expected by the huggingface transformers. get_padding_lengths \u00b6 class TransformerTextField ( Field [ torch . Tensor ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ] as_tensor \u00b6 class TransformerTextField ( Field [ torch . Tensor ]): | ... | def as_tensor ( | self , | padding_lengths : Dict [ str , int ] | ) -> Dict [ str , torch . Tensor ] empty_field \u00b6 class TransformerTextField ( Field [ torch . Tensor ]): | ... | def empty_field ( self ) batch_tensors \u00b6 class TransformerTextField ( Field [ torch . Tensor ]): | ... | def batch_tensors ( | self , | tensor_list : List [ Dict [ str , torch . Tensor ]] | ) -> Dict [ str , torch . Tensor ] human_readable_repr \u00b6 class TransformerTextField ( Field [ torch . Tensor ]): | ... | def human_readable_repr ( self ) -> Dict [ str , Any ]","title":"transformer_text_field"},{"location":"api/data/fields/transformer_text_field/#transformertextfield","text":"class TransformerTextField ( Field [ torch . Tensor ]): | def __init__ ( | self , | input_ids : Union [ torch . Tensor , List [ int ]], | token_type_ids : Optional [ Union [ torch . Tensor , List [ int ]]] = None , | attention_mask : Optional [ Union [ torch . Tensor , List [ int ]]] = None , | special_tokens_mask : Optional [ Union [ torch . Tensor , List [ int ]]] = None , | offsets_mapping : Optional [ Union [ torch . Tensor , List [ int ]]] = None , | padding_token_id : int = 0 | ) -> None A TransformerTextField is a collection of several tensors that are are a representation of text, tokenized and ready to become input to a transformer. The naming pattern of the tensors follows the pattern that's produced by the huggingface tokenizers, and expected by the huggingface transformers.","title":"TransformerTextField"},{"location":"api/data/fields/transformer_text_field/#get_padding_lengths","text":"class TransformerTextField ( Field [ torch . Tensor ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ]","title":"get_padding_lengths"},{"location":"api/data/fields/transformer_text_field/#as_tensor","text":"class TransformerTextField ( Field [ torch . Tensor ]): | ... | def as_tensor ( | self , | padding_lengths : Dict [ str , int ] | ) -> Dict [ str , torch . Tensor ]","title":"as_tensor"},{"location":"api/data/fields/transformer_text_field/#empty_field","text":"class TransformerTextField ( Field [ torch . Tensor ]): | ... | def empty_field ( self )","title":"empty_field"},{"location":"api/data/fields/transformer_text_field/#batch_tensors","text":"class TransformerTextField ( Field [ torch . Tensor ]): | ... | def batch_tensors ( | self , | tensor_list : List [ Dict [ str , torch . Tensor ]] | ) -> Dict [ str , torch . Tensor ]","title":"batch_tensors"},{"location":"api/data/fields/transformer_text_field/#human_readable_repr","text":"class TransformerTextField ( Field [ torch . Tensor ]): | ... | def human_readable_repr ( self ) -> Dict [ str , Any ]","title":"human_readable_repr"},{"location":"api/data/samplers/batch_sampler/","text":"allennlp .data .samplers .batch_sampler [SOURCE] BatchSampler \u00b6 class BatchSampler ( Registrable ) get_batch_indices \u00b6 class BatchSampler ( Registrable ): | ... | def get_batch_indices ( | self , | instances : Sequence [ Instance ] | ) -> Iterable [ List [ int ]] get_num_batches \u00b6 class BatchSampler ( Registrable ): | ... | def get_num_batches ( self , instances : Sequence [ Instance ]) -> int get_batch_size \u00b6 class BatchSampler ( Registrable ): | ... | def get_batch_size ( self ) -> Optional [ int ] Not all BatchSamplers define a consistent batch_size , but those that do should override this method.","title":"batch_sampler"},{"location":"api/data/samplers/batch_sampler/#batchsampler","text":"class BatchSampler ( Registrable )","title":"BatchSampler"},{"location":"api/data/samplers/batch_sampler/#get_batch_indices","text":"class BatchSampler ( Registrable ): | ... | def get_batch_indices ( | self , | instances : Sequence [ Instance ] | ) -> Iterable [ List [ int ]]","title":"get_batch_indices"},{"location":"api/data/samplers/batch_sampler/#get_num_batches","text":"class BatchSampler ( Registrable ): | ... | def get_num_batches ( self , instances : Sequence [ Instance ]) -> int","title":"get_num_batches"},{"location":"api/data/samplers/batch_sampler/#get_batch_size","text":"class BatchSampler ( Registrable ): | ... | def get_batch_size ( self ) -> Optional [ int ] Not all BatchSamplers define a consistent batch_size , but those that do should override this method.","title":"get_batch_size"},{"location":"api/data/samplers/bucket_batch_sampler/","text":"allennlp .data .samplers .bucket_batch_sampler [SOURCE] add_noise_to_value \u00b6 def add_noise_to_value ( value : int , noise_param : float ) BucketBatchSampler \u00b6 @BatchSampler . register ( \"bucket\" ) class BucketBatchSampler ( BatchSampler ): | def __init__ ( | self , | batch_size : int , | sorting_keys : List [ str ] = None , | padding_noise : float = 0.1 , | drop_last : bool = False , | shuffle : bool = True | ) An sampler which by default, argsorts batches with respect to the maximum input lengths per batch . You can provide a list of field names and padding keys (or pass none, in which case they will be inferred) which the dataset will be sorted by before doing this batching, causing inputs with similar length to be batched together, making computation more efficient (as less time is wasted on padded elements of the batch). Parameters \u00b6 batch_size : int The size of each batch of instances yielded when calling the data_loader. sorting_keys : List[str] , optional To bucket inputs into batches, we want to group the instances by padding length, so that we minimize the amount of padding necessary per batch. In order to do this, we need to know which fields need what type of padding, and in what order. Specifying the right keys for this is a bit cryptic, so if this is not given we try to auto-detect the right keys by iterating through a few instances upfront, reading all of the padding keys and seeing which one has the longest length. We use that one for padding. This should give reasonable results in most cases. Some cases where it might not be the right thing to do are when you have a ListField[TextField] , or when you have a really long, constant length TensorField . When you need to specify this yourself, you can create an instance from your dataset and call Instance.get_padding_lengths() to see a list of all keys used in your data. You should give one or more of those as the sorting keys here. padding_noise : float , optional (default = .1 ) When sorting by padding length, we add a bit of noise to the lengths, so that the sorting isn't deterministic. This parameter determines how much noise we add, as a percentage of the actual padding value for each instance. drop_last : bool , optional (default = False ) If True , the sampler will drop the last batch if its size would be less than batch_size`. shuffle : bool , optional (default = True ) If False , the sampler won't shuffle the batches. padding_noise will be ignored and set to 0.0 . get_batch_indices \u00b6 class BucketBatchSampler ( BatchSampler ): | ... | def get_batch_indices ( | self , | instances : Sequence [ Instance ] | ) -> Iterable [ List [ int ]] get_num_batches \u00b6 class BucketBatchSampler ( BatchSampler ): | ... | def get_num_batches ( self , instances : Sequence [ Instance ]) -> int get_batch_size \u00b6 class BucketBatchSampler ( BatchSampler ): | ... | def get_batch_size ( self ) -> Optional [ int ]","title":"bucket_batch_sampler"},{"location":"api/data/samplers/bucket_batch_sampler/#add_noise_to_value","text":"def add_noise_to_value ( value : int , noise_param : float )","title":"add_noise_to_value"},{"location":"api/data/samplers/bucket_batch_sampler/#bucketbatchsampler","text":"@BatchSampler . register ( \"bucket\" ) class BucketBatchSampler ( BatchSampler ): | def __init__ ( | self , | batch_size : int , | sorting_keys : List [ str ] = None , | padding_noise : float = 0.1 , | drop_last : bool = False , | shuffle : bool = True | ) An sampler which by default, argsorts batches with respect to the maximum input lengths per batch . You can provide a list of field names and padding keys (or pass none, in which case they will be inferred) which the dataset will be sorted by before doing this batching, causing inputs with similar length to be batched together, making computation more efficient (as less time is wasted on padded elements of the batch).","title":"BucketBatchSampler"},{"location":"api/data/samplers/bucket_batch_sampler/#get_batch_indices","text":"class BucketBatchSampler ( BatchSampler ): | ... | def get_batch_indices ( | self , | instances : Sequence [ Instance ] | ) -> Iterable [ List [ int ]]","title":"get_batch_indices"},{"location":"api/data/samplers/bucket_batch_sampler/#get_num_batches","text":"class BucketBatchSampler ( BatchSampler ): | ... | def get_num_batches ( self , instances : Sequence [ Instance ]) -> int","title":"get_num_batches"},{"location":"api/data/samplers/bucket_batch_sampler/#get_batch_size","text":"class BucketBatchSampler ( BatchSampler ): | ... | def get_batch_size ( self ) -> Optional [ int ]","title":"get_batch_size"},{"location":"api/data/samplers/max_tokens_batch_sampler/","text":"allennlp .data .samplers .max_tokens_batch_sampler [SOURCE] A \u00b6 A = TypeVar ( \"A\" ) MaxTokensBatchSampler \u00b6 @BatchSampler . register ( \"max_tokens_sampler\" ) class MaxTokensBatchSampler ( BucketBatchSampler ): | def __init__ ( | self , | max_tokens : int , | sorting_keys : List [ str ] = None , | padding_noise : float = 0.1 | ) An sampler which by default, argsorts batches with respect to the maximum input lengths per batch . Batches are then created such that the number of tokens in a batch does not exceed the given maximum number of tokens. You can provide a list of field names and padding keys (or pass none, in which case they will be inferred) which the dataset will be sorted by before doing this batching, causing inputs with similar length to be batched together, making computation more efficient (as less time is wasted on padded elements of the batch). Parameters \u00b6 max_tokens : int The maximum number of tokens to include in a batch. sorting_keys : List[str] , optional To bucket inputs into batches, we want to group the instances by padding length, so that we minimize the amount of padding necessary per batch. In order to do this, we need to know which fields need what type of padding, and in what order. Specifying the right keys for this is a bit cryptic, so if this is not given we try to auto-detect the right keys by iterating through a few instances upfront, reading all of the padding keys and seeing which one has the longest length. We use that one for padding. This should give reasonable results in most cases. Some cases where it might not be the right thing to do are when you have a ListField[TextField] , or when you have a really long, constant length TensorField . When you need to specify this yourself, you can create an instance from your dataset and call Instance.get_padding_lengths() to see a list of all keys used in your data. You should give one or more of those as the sorting keys here. padding_noise : float , optional (default = 0.1 ) When sorting by padding length, we add a bit of noise to the lengths, so that the sorting isn't deterministic. This parameter determines how much noise we add, as a percentage of the actual padding value for each instance. get_batch_indices \u00b6 class MaxTokensBatchSampler ( BucketBatchSampler ): | ... | def get_batch_indices ( | self , | instances : Sequence [ Instance ] | ) -> Iterable [ List [ int ]] get_num_batches \u00b6 class MaxTokensBatchSampler ( BucketBatchSampler ): | ... | def get_num_batches ( self , instances : Sequence [ Instance ]) -> int","title":"max_tokens_batch_sampler"},{"location":"api/data/samplers/max_tokens_batch_sampler/#a","text":"A = TypeVar ( \"A\" )","title":"A"},{"location":"api/data/samplers/max_tokens_batch_sampler/#maxtokensbatchsampler","text":"@BatchSampler . register ( \"max_tokens_sampler\" ) class MaxTokensBatchSampler ( BucketBatchSampler ): | def __init__ ( | self , | max_tokens : int , | sorting_keys : List [ str ] = None , | padding_noise : float = 0.1 | ) An sampler which by default, argsorts batches with respect to the maximum input lengths per batch . Batches are then created such that the number of tokens in a batch does not exceed the given maximum number of tokens. You can provide a list of field names and padding keys (or pass none, in which case they will be inferred) which the dataset will be sorted by before doing this batching, causing inputs with similar length to be batched together, making computation more efficient (as less time is wasted on padded elements of the batch).","title":"MaxTokensBatchSampler"},{"location":"api/data/samplers/max_tokens_batch_sampler/#get_batch_indices","text":"class MaxTokensBatchSampler ( BucketBatchSampler ): | ... | def get_batch_indices ( | self , | instances : Sequence [ Instance ] | ) -> Iterable [ List [ int ]]","title":"get_batch_indices"},{"location":"api/data/samplers/max_tokens_batch_sampler/#get_num_batches","text":"class MaxTokensBatchSampler ( BucketBatchSampler ): | ... | def get_num_batches ( self , instances : Sequence [ Instance ]) -> int","title":"get_num_batches"},{"location":"api/data/token_indexers/elmo_indexer/","text":"allennlp .data .token_indexers .elmo_indexer [SOURCE] ELMoCharacterMapper \u00b6 class ELMoCharacterMapper : | def __init__ ( self , tokens_to_add : Dict [ str , int ] = None ) -> None Maps individual tokens to sequences of character ids, compatible with ELMo. To be consistent with previously trained models, we include it here as special of existing character indexers. We allow to add optional additional special tokens with designated character ids with tokens_to_add . max_word_length \u00b6 class ELMoCharacterMapper : | ... | max_word_length = 50 beginning_of_sentence_character \u00b6 class ELMoCharacterMapper : | ... | beginning_of_sentence_character = 256 end_of_sentence_character \u00b6 class ELMoCharacterMapper : | ... | end_of_sentence_character = 257 beginning_of_word_character \u00b6 class ELMoCharacterMapper : | ... | beginning_of_word_character = 258 end_of_word_character \u00b6 class ELMoCharacterMapper : | ... | end_of_word_character = 259 padding_character \u00b6 class ELMoCharacterMapper : | ... | padding_character = 260 beginning_of_sentence_characters \u00b6 class ELMoCharacterMapper : | ... | beginning_of_sentence_characters = _make_bos_eos ( beginning_of_sentence_character , padding_character , beginning ... end_of_sentence_characters \u00b6 class ELMoCharacterMapper : | ... | end_of_sentence_characters = _make_bos_eos ( end_of_sentence_character , padding_character , beginning_of_wo ... bos_token \u00b6 class ELMoCharacterMapper : | ... | bos_token = \"<S>\" eos_token \u00b6 class ELMoCharacterMapper : | ... | eos_token = \"</S>\" convert_word_to_char_ids \u00b6 class ELMoCharacterMapper : | ... | def convert_word_to_char_ids ( self , word : str ) -> List [ int ] ELMoTokenCharactersIndexer \u00b6 @TokenIndexer . register ( \"elmo_characters\" ) class ELMoTokenCharactersIndexer ( TokenIndexer ): | def __init__ ( | self , | namespace : str = \"elmo_characters\" , | tokens_to_add : Dict [ str , int ] = None , | token_min_padding_length : int = 0 | ) -> None Convert a token to an array of character ids to compute ELMo representations. Registered as a TokenIndexer with name \"elmo_characters\". Parameters \u00b6 namespace : str , optional (default = elmo_characters ) tokens_to_add : Dict[str, int] , optional (default = None ) If not None, then provides a mapping of special tokens to character ids. When using pre-trained models, then the character id must be less then 261, and we recommend using un-used ids (e.g. 1-32). token_min_padding_length : int , optional (default = 0 ) See TokenIndexer . count_vocab_items \u00b6 class ELMoTokenCharactersIndexer ( TokenIndexer ): | ... | def count_vocab_items ( | self , | token : Token , | counter : Dict [ str , Dict [ str , int ]] | ) get_empty_token_list \u00b6 class ELMoTokenCharactersIndexer ( TokenIndexer ): | ... | def get_empty_token_list ( self ) -> IndexedTokenList tokens_to_indices \u00b6 class ELMoTokenCharactersIndexer ( TokenIndexer ): | ... | def tokens_to_indices ( | self , | tokens : List [ Token ], | vocabulary : Vocabulary | ) -> Dict [ str , List [ List [ int ]]] as_padded_tensor_dict \u00b6 class ELMoTokenCharactersIndexer ( TokenIndexer ): | ... | def as_padded_tensor_dict ( | self , | tokens : IndexedTokenList , | padding_lengths : Dict [ str , int ] | ) -> Dict [ str , torch . Tensor ]","title":"elmo_indexer"},{"location":"api/data/token_indexers/elmo_indexer/#elmocharactermapper","text":"class ELMoCharacterMapper : | def __init__ ( self , tokens_to_add : Dict [ str , int ] = None ) -> None Maps individual tokens to sequences of character ids, compatible with ELMo. To be consistent with previously trained models, we include it here as special of existing character indexers. We allow to add optional additional special tokens with designated character ids with tokens_to_add .","title":"ELMoCharacterMapper"},{"location":"api/data/token_indexers/elmo_indexer/#max_word_length","text":"class ELMoCharacterMapper : | ... | max_word_length = 50","title":"max_word_length"},{"location":"api/data/token_indexers/elmo_indexer/#beginning_of_sentence_character","text":"class ELMoCharacterMapper : | ... | beginning_of_sentence_character = 256","title":"beginning_of_sentence_character"},{"location":"api/data/token_indexers/elmo_indexer/#end_of_sentence_character","text":"class ELMoCharacterMapper : | ... | end_of_sentence_character = 257","title":"end_of_sentence_character"},{"location":"api/data/token_indexers/elmo_indexer/#beginning_of_word_character","text":"class ELMoCharacterMapper : | ... | beginning_of_word_character = 258","title":"beginning_of_word_character"},{"location":"api/data/token_indexers/elmo_indexer/#end_of_word_character","text":"class ELMoCharacterMapper : | ... | end_of_word_character = 259","title":"end_of_word_character"},{"location":"api/data/token_indexers/elmo_indexer/#padding_character","text":"class ELMoCharacterMapper : | ... | padding_character = 260","title":"padding_character"},{"location":"api/data/token_indexers/elmo_indexer/#beginning_of_sentence_characters","text":"class ELMoCharacterMapper : | ... | beginning_of_sentence_characters = _make_bos_eos ( beginning_of_sentence_character , padding_character , beginning ...","title":"beginning_of_sentence_characters"},{"location":"api/data/token_indexers/elmo_indexer/#end_of_sentence_characters","text":"class ELMoCharacterMapper : | ... | end_of_sentence_characters = _make_bos_eos ( end_of_sentence_character , padding_character , beginning_of_wo ...","title":"end_of_sentence_characters"},{"location":"api/data/token_indexers/elmo_indexer/#bos_token","text":"class ELMoCharacterMapper : | ... | bos_token = \"<S>\"","title":"bos_token"},{"location":"api/data/token_indexers/elmo_indexer/#eos_token","text":"class ELMoCharacterMapper : | ... | eos_token = \"</S>\"","title":"eos_token"},{"location":"api/data/token_indexers/elmo_indexer/#convert_word_to_char_ids","text":"class ELMoCharacterMapper : | ... | def convert_word_to_char_ids ( self , word : str ) -> List [ int ]","title":"convert_word_to_char_ids"},{"location":"api/data/token_indexers/elmo_indexer/#elmotokencharactersindexer","text":"@TokenIndexer . register ( \"elmo_characters\" ) class ELMoTokenCharactersIndexer ( TokenIndexer ): | def __init__ ( | self , | namespace : str = \"elmo_characters\" , | tokens_to_add : Dict [ str , int ] = None , | token_min_padding_length : int = 0 | ) -> None Convert a token to an array of character ids to compute ELMo representations. Registered as a TokenIndexer with name \"elmo_characters\".","title":"ELMoTokenCharactersIndexer"},{"location":"api/data/token_indexers/elmo_indexer/#count_vocab_items","text":"class ELMoTokenCharactersIndexer ( TokenIndexer ): | ... | def count_vocab_items ( | self , | token : Token , | counter : Dict [ str , Dict [ str , int ]] | )","title":"count_vocab_items"},{"location":"api/data/token_indexers/elmo_indexer/#get_empty_token_list","text":"class ELMoTokenCharactersIndexer ( TokenIndexer ): | ... | def get_empty_token_list ( self ) -> IndexedTokenList","title":"get_empty_token_list"},{"location":"api/data/token_indexers/elmo_indexer/#tokens_to_indices","text":"class ELMoTokenCharactersIndexer ( TokenIndexer ): | ... | def tokens_to_indices ( | self , | tokens : List [ Token ], | vocabulary : Vocabulary | ) -> Dict [ str , List [ List [ int ]]]","title":"tokens_to_indices"},{"location":"api/data/token_indexers/elmo_indexer/#as_padded_tensor_dict","text":"class ELMoTokenCharactersIndexer ( TokenIndexer ): | ... | def as_padded_tensor_dict ( | self , | tokens : IndexedTokenList , | padding_lengths : Dict [ str , int ] | ) -> Dict [ str , torch . Tensor ]","title":"as_padded_tensor_dict"},{"location":"api/data/token_indexers/pretrained_transformer_indexer/","text":"allennlp .data .token_indexers .pretrained_transformer_indexer [SOURCE] PretrainedTransformerIndexer \u00b6 @TokenIndexer . register ( \"pretrained_transformer\" ) class PretrainedTransformerIndexer ( TokenIndexer ): | def __init__ ( | self , | model_name : str , | namespace : str = \"tags\" , | max_length : int = None , | tokenizer_kwargs : Optional [ Dict [ str , Any ]] = None , | ** kwargs | ) -> None This TokenIndexer assumes that Tokens already have their indexes in them (see text_id field). We still require model_name because we want to form allennlp vocabulary from pretrained one. This Indexer is only really appropriate to use if you've also used a corresponding PretrainedTransformerTokenizer to tokenize your input. Otherwise you'll have a mismatch between your tokens and your vocabulary, and you'll get a lot of UNK tokens. Registered as a TokenIndexer with name \"pretrained_transformer\". Parameters \u00b6 model_name : str The name of the transformers model to use. namespace : str , optional (default = tags ) We will add the tokens in the pytorch_transformer vocabulary to this vocabulary namespace. We use a somewhat confusing default value of tags so that we do not add padding or UNK tokens to this namespace, which would break on loading because we wouldn't find our default OOV token. max_length : int , optional (default = None ) If not None, split the document into segments of this many tokens (including special tokens) before feeding into the embedder. The embedder embeds these segments independently and concatenate the results to get the original document representation. Should be set to the same value as the max_length option on the PretrainedTransformerEmbedder . tokenizer_kwargs : Dict[str, Any] , optional (default = None ) Dictionary with additional arguments for AutoTokenizer.from_pretrained . count_vocab_items \u00b6 class PretrainedTransformerIndexer ( TokenIndexer ): | ... | def count_vocab_items ( | self , | token : Token , | counter : Dict [ str , Dict [ str , int ]] | ) tokens_to_indices \u00b6 class PretrainedTransformerIndexer ( TokenIndexer ): | ... | def tokens_to_indices ( | self , | tokens : List [ Token ], | vocabulary : Vocabulary | ) -> IndexedTokenList indices_to_tokens \u00b6 class PretrainedTransformerIndexer ( TokenIndexer ): | ... | def indices_to_tokens ( | self , | indexed_tokens : IndexedTokenList , | vocabulary : Vocabulary | ) -> List [ Token ] get_empty_token_list \u00b6 class PretrainedTransformerIndexer ( TokenIndexer ): | ... | def get_empty_token_list ( self ) -> IndexedTokenList as_padded_tensor_dict \u00b6 class PretrainedTransformerIndexer ( TokenIndexer ): | ... | def as_padded_tensor_dict ( | self , | tokens : IndexedTokenList , | padding_lengths : Dict [ str , int ] | ) -> Dict [ str , torch . Tensor ]","title":"pretrained_transformer_indexer"},{"location":"api/data/token_indexers/pretrained_transformer_indexer/#pretrainedtransformerindexer","text":"@TokenIndexer . register ( \"pretrained_transformer\" ) class PretrainedTransformerIndexer ( TokenIndexer ): | def __init__ ( | self , | model_name : str , | namespace : str = \"tags\" , | max_length : int = None , | tokenizer_kwargs : Optional [ Dict [ str , Any ]] = None , | ** kwargs | ) -> None This TokenIndexer assumes that Tokens already have their indexes in them (see text_id field). We still require model_name because we want to form allennlp vocabulary from pretrained one. This Indexer is only really appropriate to use if you've also used a corresponding PretrainedTransformerTokenizer to tokenize your input. Otherwise you'll have a mismatch between your tokens and your vocabulary, and you'll get a lot of UNK tokens. Registered as a TokenIndexer with name \"pretrained_transformer\".","title":"PretrainedTransformerIndexer"},{"location":"api/data/token_indexers/pretrained_transformer_indexer/#count_vocab_items","text":"class PretrainedTransformerIndexer ( TokenIndexer ): | ... | def count_vocab_items ( | self , | token : Token , | counter : Dict [ str , Dict [ str , int ]] | )","title":"count_vocab_items"},{"location":"api/data/token_indexers/pretrained_transformer_indexer/#tokens_to_indices","text":"class PretrainedTransformerIndexer ( TokenIndexer ): | ... | def tokens_to_indices ( | self , | tokens : List [ Token ], | vocabulary : Vocabulary | ) -> IndexedTokenList","title":"tokens_to_indices"},{"location":"api/data/token_indexers/pretrained_transformer_indexer/#indices_to_tokens","text":"class PretrainedTransformerIndexer ( TokenIndexer ): | ... | def indices_to_tokens ( | self , | indexed_tokens : IndexedTokenList , | vocabulary : Vocabulary | ) -> List [ Token ]","title":"indices_to_tokens"},{"location":"api/data/token_indexers/pretrained_transformer_indexer/#get_empty_token_list","text":"class PretrainedTransformerIndexer ( TokenIndexer ): | ... | def get_empty_token_list ( self ) -> IndexedTokenList","title":"get_empty_token_list"},{"location":"api/data/token_indexers/pretrained_transformer_indexer/#as_padded_tensor_dict","text":"class PretrainedTransformerIndexer ( TokenIndexer ): | ... | def as_padded_tensor_dict ( | self , | tokens : IndexedTokenList , | padding_lengths : Dict [ str , int ] | ) -> Dict [ str , torch . Tensor ]","title":"as_padded_tensor_dict"},{"location":"api/data/token_indexers/pretrained_transformer_mismatched_indexer/","text":"allennlp .data .token_indexers .pretrained_transformer_mismatched_indexer [SOURCE] PretrainedTransformerMismatchedIndexer \u00b6 @TokenIndexer . register ( \"pretrained_transformer_mismatched\" ) class PretrainedTransformerMismatchedIndexer ( TokenIndexer ): | def __init__ ( | self , | model_name : str , | namespace : str = \"tags\" , | max_length : int = None , | tokenizer_kwargs : Optional [ Dict [ str , Any ]] = None , | ** kwargs | ) -> None Use this indexer when (for whatever reason) you are not using a corresponding PretrainedTransformerTokenizer on your input. We assume that you used a tokenizer that splits strings into words, while the transformer expects wordpieces as input. This indexer splits the words into wordpieces and flattens them out. You should use the corresponding PretrainedTransformerMismatchedEmbedder to embed these wordpieces and then pull out a single vector for each original word. Registered as a TokenIndexer with name \"pretrained_transformer_mismatched\". Parameters \u00b6 model_name : str The name of the transformers model to use. namespace : str , optional (default = tags ) We will add the tokens in the pytorch_transformer vocabulary to this vocabulary namespace. We use a somewhat confusing default value of tags so that we do not add padding or UNK tokens to this namespace, which would break on loading because we wouldn't find our default OOV token. max_length : int , optional (default = None ) If positive, split the document into segments of this many tokens (including special tokens) before feeding into the embedder. The embedder embeds these segments independently and concatenate the results to get the original document representation. Should be set to the same value as the max_length option on the PretrainedTransformerMismatchedEmbedder . tokenizer_kwargs : Dict[str, Any] , optional (default = None ) Dictionary with additional arguments for AutoTokenizer.from_pretrained . count_vocab_items \u00b6 class PretrainedTransformerMismatchedIndexer ( TokenIndexer ): | ... | def count_vocab_items ( | self , | token : Token , | counter : Dict [ str , Dict [ str , int ]] | ) tokens_to_indices \u00b6 class PretrainedTransformerMismatchedIndexer ( TokenIndexer ): | ... | def tokens_to_indices ( | self , | tokens : List [ Token ], | vocabulary : Vocabulary | ) -> IndexedTokenList get_empty_token_list \u00b6 class PretrainedTransformerMismatchedIndexer ( TokenIndexer ): | ... | def get_empty_token_list ( self ) -> IndexedTokenList as_padded_tensor_dict \u00b6 class PretrainedTransformerMismatchedIndexer ( TokenIndexer ): | ... | def as_padded_tensor_dict ( | self , | tokens : IndexedTokenList , | padding_lengths : Dict [ str , int ] | ) -> Dict [ str , torch . Tensor ]","title":"pretrained_transformer_mismatched_indexer"},{"location":"api/data/token_indexers/pretrained_transformer_mismatched_indexer/#pretrainedtransformermismatchedindexer","text":"@TokenIndexer . register ( \"pretrained_transformer_mismatched\" ) class PretrainedTransformerMismatchedIndexer ( TokenIndexer ): | def __init__ ( | self , | model_name : str , | namespace : str = \"tags\" , | max_length : int = None , | tokenizer_kwargs : Optional [ Dict [ str , Any ]] = None , | ** kwargs | ) -> None Use this indexer when (for whatever reason) you are not using a corresponding PretrainedTransformerTokenizer on your input. We assume that you used a tokenizer that splits strings into words, while the transformer expects wordpieces as input. This indexer splits the words into wordpieces and flattens them out. You should use the corresponding PretrainedTransformerMismatchedEmbedder to embed these wordpieces and then pull out a single vector for each original word. Registered as a TokenIndexer with name \"pretrained_transformer_mismatched\".","title":"PretrainedTransformerMismatchedIndexer"},{"location":"api/data/token_indexers/pretrained_transformer_mismatched_indexer/#count_vocab_items","text":"class PretrainedTransformerMismatchedIndexer ( TokenIndexer ): | ... | def count_vocab_items ( | self , | token : Token , | counter : Dict [ str , Dict [ str , int ]] | )","title":"count_vocab_items"},{"location":"api/data/token_indexers/pretrained_transformer_mismatched_indexer/#tokens_to_indices","text":"class PretrainedTransformerMismatchedIndexer ( TokenIndexer ): | ... | def tokens_to_indices ( | self , | tokens : List [ Token ], | vocabulary : Vocabulary | ) -> IndexedTokenList","title":"tokens_to_indices"},{"location":"api/data/token_indexers/pretrained_transformer_mismatched_indexer/#get_empty_token_list","text":"class PretrainedTransformerMismatchedIndexer ( TokenIndexer ): | ... | def get_empty_token_list ( self ) -> IndexedTokenList","title":"get_empty_token_list"},{"location":"api/data/token_indexers/pretrained_transformer_mismatched_indexer/#as_padded_tensor_dict","text":"class PretrainedTransformerMismatchedIndexer ( TokenIndexer ): | ... | def as_padded_tensor_dict ( | self , | tokens : IndexedTokenList , | padding_lengths : Dict [ str , int ] | ) -> Dict [ str , torch . Tensor ]","title":"as_padded_tensor_dict"},{"location":"api/data/token_indexers/single_id_token_indexer/","text":"allennlp .data .token_indexers .single_id_token_indexer [SOURCE] SingleIdTokenIndexer \u00b6 @TokenIndexer . register ( \"single_id\" ) class SingleIdTokenIndexer ( TokenIndexer ): | def __init__ ( | self , | namespace : Optional [ str ] = \"tokens\" , | lowercase_tokens : bool = False , | start_tokens : List [ str ] = None , | end_tokens : List [ str ] = None , | feature_name : str = \"text\" , | default_value : str = _DEFAULT_VALUE , | token_min_padding_length : int = 0 | ) -> None This TokenIndexer represents tokens as single integers. Registered as a TokenIndexer with name \"single_id\". Parameters \u00b6 namespace : Optional[str] , optional (default = \"tokens\" ) We will use this namespace in the Vocabulary to map strings to indices. If you explicitly pass in None here, we will skip indexing and vocabulary lookups. This means that the feature_name you use must correspond to an integer value (like text_id , for instance, which gets set by some tokenizers, such as when using byte encoding). lowercase_tokens : bool , optional (default = False ) If True , we will call token.lower() before getting an index for the token from the vocabulary. start_tokens : List[str] , optional (default = None ) These are prepended to the tokens provided to tokens_to_indices . end_tokens : List[str] , optional (default = None ) These are appended to the tokens provided to tokens_to_indices . feature_name : str , optional (default = \"text\" ) We will use the Token attribute with this name as input. This is potentially useful, e.g., for using NER tags instead of (or in addition to) surface forms as your inputs (passing ent_type_ here would do that). If you use a non-default value here, you almost certainly want to also change the namespace parameter, and you might want to give a default_value . default_value : str , optional When you want to use a non-default feature_name , you sometimes want to have a default value to go with it, e.g., in case you don't have an NER tag for a particular token, for some reason. This value will get used if we don't find a value in feature_name . If this is not given, we will crash if a token doesn't have a value for the given feature_name , so that you don't get weird, silent errors by default. token_min_padding_length : int , optional (default = 0 ) See TokenIndexer . count_vocab_items \u00b6 class SingleIdTokenIndexer ( TokenIndexer ): | ... | def count_vocab_items ( | self , | token : Token , | counter : Dict [ str , Dict [ str , int ]] | ) tokens_to_indices \u00b6 class SingleIdTokenIndexer ( TokenIndexer ): | ... | def tokens_to_indices ( | self , | tokens : List [ Token ], | vocabulary : Vocabulary | ) -> Dict [ str , List [ int ]] get_empty_token_list \u00b6 class SingleIdTokenIndexer ( TokenIndexer ): | ... | def get_empty_token_list ( self ) -> IndexedTokenList","title":"single_id_token_indexer"},{"location":"api/data/token_indexers/single_id_token_indexer/#singleidtokenindexer","text":"@TokenIndexer . register ( \"single_id\" ) class SingleIdTokenIndexer ( TokenIndexer ): | def __init__ ( | self , | namespace : Optional [ str ] = \"tokens\" , | lowercase_tokens : bool = False , | start_tokens : List [ str ] = None , | end_tokens : List [ str ] = None , | feature_name : str = \"text\" , | default_value : str = _DEFAULT_VALUE , | token_min_padding_length : int = 0 | ) -> None This TokenIndexer represents tokens as single integers. Registered as a TokenIndexer with name \"single_id\".","title":"SingleIdTokenIndexer"},{"location":"api/data/token_indexers/single_id_token_indexer/#count_vocab_items","text":"class SingleIdTokenIndexer ( TokenIndexer ): | ... | def count_vocab_items ( | self , | token : Token , | counter : Dict [ str , Dict [ str , int ]] | )","title":"count_vocab_items"},{"location":"api/data/token_indexers/single_id_token_indexer/#tokens_to_indices","text":"class SingleIdTokenIndexer ( TokenIndexer ): | ... | def tokens_to_indices ( | self , | tokens : List [ Token ], | vocabulary : Vocabulary | ) -> Dict [ str , List [ int ]]","title":"tokens_to_indices"},{"location":"api/data/token_indexers/single_id_token_indexer/#get_empty_token_list","text":"class SingleIdTokenIndexer ( TokenIndexer ): | ... | def get_empty_token_list ( self ) -> IndexedTokenList","title":"get_empty_token_list"},{"location":"api/data/token_indexers/spacy_indexer/","text":"allennlp .data .token_indexers .spacy_indexer [SOURCE] SpacyTokenIndexer \u00b6 @TokenIndexer . register ( \"spacy\" ) class SpacyTokenIndexer ( TokenIndexer ): | def __init__ ( | self , | hidden_dim : int = 96 , | token_min_padding_length : int = 0 | ) -> None This SpacyTokenIndexer represents tokens as word vectors from a spacy model. You might want to do this for two main reasons; easier integration with a spacy pipeline and no out of vocabulary tokens. Registered as a TokenIndexer with name \"spacy\". Parameters \u00b6 hidden_dim : int , optional (default = 96 ) The dimension of the vectors that spacy generates for representing words. token_min_padding_length : int , optional (default = 0 ) See TokenIndexer . count_vocab_items \u00b6 class SpacyTokenIndexer ( TokenIndexer ): | ... | def count_vocab_items ( | self , | token : Token , | counter : Dict [ str , Dict [ str , int ]] | ) tokens_to_indices \u00b6 class SpacyTokenIndexer ( TokenIndexer ): | ... | def tokens_to_indices ( | self , | tokens : List [ SpacyToken ], | vocabulary : Vocabulary | ) -> Dict [ str , List [ numpy . ndarray ]] as_padded_tensor_dict \u00b6 class SpacyTokenIndexer ( TokenIndexer ): | ... | def as_padded_tensor_dict ( | self , | tokens : IndexedTokenList , | padding_lengths : Dict [ str , int ] | ) -> Dict [ str , torch . Tensor ]","title":"spacy_indexer"},{"location":"api/data/token_indexers/spacy_indexer/#spacytokenindexer","text":"@TokenIndexer . register ( \"spacy\" ) class SpacyTokenIndexer ( TokenIndexer ): | def __init__ ( | self , | hidden_dim : int = 96 , | token_min_padding_length : int = 0 | ) -> None This SpacyTokenIndexer represents tokens as word vectors from a spacy model. You might want to do this for two main reasons; easier integration with a spacy pipeline and no out of vocabulary tokens. Registered as a TokenIndexer with name \"spacy\".","title":"SpacyTokenIndexer"},{"location":"api/data/token_indexers/spacy_indexer/#count_vocab_items","text":"class SpacyTokenIndexer ( TokenIndexer ): | ... | def count_vocab_items ( | self , | token : Token , | counter : Dict [ str , Dict [ str , int ]] | )","title":"count_vocab_items"},{"location":"api/data/token_indexers/spacy_indexer/#tokens_to_indices","text":"class SpacyTokenIndexer ( TokenIndexer ): | ... | def tokens_to_indices ( | self , | tokens : List [ SpacyToken ], | vocabulary : Vocabulary | ) -> Dict [ str , List [ numpy . ndarray ]]","title":"tokens_to_indices"},{"location":"api/data/token_indexers/spacy_indexer/#as_padded_tensor_dict","text":"class SpacyTokenIndexer ( TokenIndexer ): | ... | def as_padded_tensor_dict ( | self , | tokens : IndexedTokenList , | padding_lengths : Dict [ str , int ] | ) -> Dict [ str , torch . Tensor ]","title":"as_padded_tensor_dict"},{"location":"api/data/token_indexers/token_characters_indexer/","text":"allennlp .data .token_indexers .token_characters_indexer [SOURCE] TokenCharactersIndexer \u00b6 @TokenIndexer . register ( \"characters\" ) class TokenCharactersIndexer ( TokenIndexer ): | def __init__ ( | self , | namespace : str = \"token_characters\" , | character_tokenizer : CharacterTokenizer = CharacterTokenizer (), | start_tokens : List [ str ] = None , | end_tokens : List [ str ] = None , | min_padding_length : int = 0 , | token_min_padding_length : int = 0 | ) -> None This TokenIndexer represents tokens as lists of character indices. Registered as a TokenIndexer with name \"characters\". Parameters \u00b6 namespace : str , optional (default = token_characters ) We will use this namespace in the Vocabulary to map the characters in each token to indices. character_tokenizer : CharacterTokenizer , optional (default = CharacterTokenizer() ) We use a CharacterTokenizer to handle splitting tokens into characters, as it has options for byte encoding and other things. The default here is to instantiate a CharacterTokenizer with its default parameters, which uses unicode characters and retains casing. start_tokens : List[str] , optional (default = None ) These are prepended to the tokens provided to tokens_to_indices . end_tokens : List[str] , optional (default = None ) These are appended to the tokens provided to tokens_to_indices . min_padding_length : int , optional (default = 0 ) We use this value as the minimum length of padding. Usually used with CnnEncoder , its value should be set to the maximum value of ngram_filter_sizes correspondingly. token_min_padding_length : int , optional (default = 0 ) See TokenIndexer . count_vocab_items \u00b6 class TokenCharactersIndexer ( TokenIndexer ): | ... | def count_vocab_items ( | self , | token : Token , | counter : Dict [ str , Dict [ str , int ]] | ) tokens_to_indices \u00b6 class TokenCharactersIndexer ( TokenIndexer ): | ... | def tokens_to_indices ( | self , | tokens : List [ Token ], | vocabulary : Vocabulary | ) -> Dict [ str , List [ List [ int ]]] get_padding_lengths \u00b6 class TokenCharactersIndexer ( TokenIndexer ): | ... | def get_padding_lengths ( | self , | indexed_tokens : IndexedTokenList | ) -> Dict [ str , int ] as_padded_tensor_dict \u00b6 class TokenCharactersIndexer ( TokenIndexer ): | ... | def as_padded_tensor_dict ( | self , | tokens : IndexedTokenList , | padding_lengths : Dict [ str , int ] | ) -> Dict [ str , torch . Tensor ] get_empty_token_list \u00b6 class TokenCharactersIndexer ( TokenIndexer ): | ... | def get_empty_token_list ( self ) -> IndexedTokenList","title":"token_characters_indexer"},{"location":"api/data/token_indexers/token_characters_indexer/#tokencharactersindexer","text":"@TokenIndexer . register ( \"characters\" ) class TokenCharactersIndexer ( TokenIndexer ): | def __init__ ( | self , | namespace : str = \"token_characters\" , | character_tokenizer : CharacterTokenizer = CharacterTokenizer (), | start_tokens : List [ str ] = None , | end_tokens : List [ str ] = None , | min_padding_length : int = 0 , | token_min_padding_length : int = 0 | ) -> None This TokenIndexer represents tokens as lists of character indices. Registered as a TokenIndexer with name \"characters\".","title":"TokenCharactersIndexer"},{"location":"api/data/token_indexers/token_characters_indexer/#count_vocab_items","text":"class TokenCharactersIndexer ( TokenIndexer ): | ... | def count_vocab_items ( | self , | token : Token , | counter : Dict [ str , Dict [ str , int ]] | )","title":"count_vocab_items"},{"location":"api/data/token_indexers/token_characters_indexer/#tokens_to_indices","text":"class TokenCharactersIndexer ( TokenIndexer ): | ... | def tokens_to_indices ( | self , | tokens : List [ Token ], | vocabulary : Vocabulary | ) -> Dict [ str , List [ List [ int ]]]","title":"tokens_to_indices"},{"location":"api/data/token_indexers/token_characters_indexer/#get_padding_lengths","text":"class TokenCharactersIndexer ( TokenIndexer ): | ... | def get_padding_lengths ( | self , | indexed_tokens : IndexedTokenList | ) -> Dict [ str , int ]","title":"get_padding_lengths"},{"location":"api/data/token_indexers/token_characters_indexer/#as_padded_tensor_dict","text":"class TokenCharactersIndexer ( TokenIndexer ): | ... | def as_padded_tensor_dict ( | self , | tokens : IndexedTokenList , | padding_lengths : Dict [ str , int ] | ) -> Dict [ str , torch . Tensor ]","title":"as_padded_tensor_dict"},{"location":"api/data/token_indexers/token_characters_indexer/#get_empty_token_list","text":"class TokenCharactersIndexer ( TokenIndexer ): | ... | def get_empty_token_list ( self ) -> IndexedTokenList","title":"get_empty_token_list"},{"location":"api/data/token_indexers/token_indexer/","text":"allennlp .data .token_indexers .token_indexer [SOURCE] IndexedTokenList \u00b6 IndexedTokenList = Dict [ str , List [ Any ]] TokenIndexer \u00b6 class TokenIndexer ( Registrable ): | def __init__ ( self , token_min_padding_length : int = 0 ) -> None A TokenIndexer determines how string tokens get represented as arrays of indices in a model. This class both converts strings into numerical values, with the help of a Vocabulary , and it produces actual arrays. Tokens can be represented as single IDs (e.g., the word \"cat\" gets represented by the number 34), or as lists of character IDs (e.g., \"cat\" gets represented by the numbers [23, 10, 18]), or in some other way that you can come up with (e.g., if you have some structured input you want to represent in a special way in your data arrays, you can do that here). Parameters \u00b6 token_min_padding_length : int , optional (default = 0 ) The minimum padding length required for the TokenIndexer . For example, the minimum padding length of SingleIdTokenIndexer is the largest size of filter when using CnnEncoder . Note that if you set this for one TokenIndexer, you likely have to set it for all TokenIndexer for the same field, otherwise you'll get mismatched tensor sizes. default_implementation \u00b6 class TokenIndexer ( Registrable ): | ... | default_implementation = \"single_id\" has_warned_for_as_padded_tensor \u00b6 class TokenIndexer ( Registrable ): | ... | has_warned_for_as_padded_tensor = False count_vocab_items \u00b6 class TokenIndexer ( Registrable ): | ... | def count_vocab_items ( | self , | token : Token , | counter : Dict [ str , Dict [ str , int ]] | ) The Vocabulary needs to assign indices to whatever strings we see in the training data (possibly doing some frequency filtering and using an OOV, or out of vocabulary, token). This method takes a token and a dictionary of counts and increments counts for whatever vocabulary items are present in the token. If this is a single token ID representation, the vocabulary item is likely the token itself. If this is a token characters representation, the vocabulary items are all of the characters in the token. tokens_to_indices \u00b6 class TokenIndexer ( Registrable ): | ... | def tokens_to_indices ( | self , | tokens : List [ Token ], | vocabulary : Vocabulary | ) -> IndexedTokenList Takes a list of tokens and converts them to an IndexedTokenList . This could be just an ID for each token from the vocabulary. Or it could split each token into characters and return one ID per character. Or (for instance, in the case of byte-pair encoding) there might not be a clean mapping from individual tokens to indices, and the IndexedTokenList could be a complex data structure. indices_to_tokens \u00b6 class TokenIndexer ( Registrable ): | ... | def indices_to_tokens ( | self , | indexed_tokens : IndexedTokenList , | vocabulary : Vocabulary | ) -> List [ Token ] Inverse operations of tokens_to_indices. Takes an IndexedTokenList and converts it back into a list of tokens. get_empty_token_list \u00b6 class TokenIndexer ( Registrable ): | ... | def get_empty_token_list ( self ) -> IndexedTokenList Returns an already indexed version of an empty token list. This is typically just an empty list for whatever keys are used in the indexer. get_padding_lengths \u00b6 class TokenIndexer ( Registrable ): | ... | def get_padding_lengths ( | self , | indexed_tokens : IndexedTokenList | ) -> Dict [ str , int ] This method returns a padding dictionary for the given indexed_tokens specifying all lengths that need padding. If all you have is a list of single ID tokens, this is just the length of the list, and that's what the default implementation will give you. If you have something more complicated, like a list of character ids for token, you'll need to override this. as_padded_tensor_dict \u00b6 class TokenIndexer ( Registrable ): | ... | def as_padded_tensor_dict ( | self , | tokens : IndexedTokenList , | padding_lengths : Dict [ str , int ] | ) -> Dict [ str , torch . Tensor ] This method pads a list of tokens given the input padding lengths (which could actually truncate things, depending on settings) and returns that padded list of input tokens as a Dict[str, torch.Tensor] . This is a dictionary because there should be one key per argument that the TokenEmbedder corresponding to this class expects in its forward() method (where the argument name in the TokenEmbedder needs to make the key in this dictionary). The base class implements the case when all you want to do is create a padded LongTensor for every list in the tokens dictionary. If your TokenIndexer needs more complex logic than that, you need to override this method.","title":"token_indexer"},{"location":"api/data/token_indexers/token_indexer/#indexedtokenlist","text":"IndexedTokenList = Dict [ str , List [ Any ]]","title":"IndexedTokenList"},{"location":"api/data/token_indexers/token_indexer/#tokenindexer","text":"class TokenIndexer ( Registrable ): | def __init__ ( self , token_min_padding_length : int = 0 ) -> None A TokenIndexer determines how string tokens get represented as arrays of indices in a model. This class both converts strings into numerical values, with the help of a Vocabulary , and it produces actual arrays. Tokens can be represented as single IDs (e.g., the word \"cat\" gets represented by the number 34), or as lists of character IDs (e.g., \"cat\" gets represented by the numbers [23, 10, 18]), or in some other way that you can come up with (e.g., if you have some structured input you want to represent in a special way in your data arrays, you can do that here).","title":"TokenIndexer"},{"location":"api/data/token_indexers/token_indexer/#default_implementation","text":"class TokenIndexer ( Registrable ): | ... | default_implementation = \"single_id\"","title":"default_implementation"},{"location":"api/data/token_indexers/token_indexer/#has_warned_for_as_padded_tensor","text":"class TokenIndexer ( Registrable ): | ... | has_warned_for_as_padded_tensor = False","title":"has_warned_for_as_padded_tensor"},{"location":"api/data/token_indexers/token_indexer/#count_vocab_items","text":"class TokenIndexer ( Registrable ): | ... | def count_vocab_items ( | self , | token : Token , | counter : Dict [ str , Dict [ str , int ]] | ) The Vocabulary needs to assign indices to whatever strings we see in the training data (possibly doing some frequency filtering and using an OOV, or out of vocabulary, token). This method takes a token and a dictionary of counts and increments counts for whatever vocabulary items are present in the token. If this is a single token ID representation, the vocabulary item is likely the token itself. If this is a token characters representation, the vocabulary items are all of the characters in the token.","title":"count_vocab_items"},{"location":"api/data/token_indexers/token_indexer/#tokens_to_indices","text":"class TokenIndexer ( Registrable ): | ... | def tokens_to_indices ( | self , | tokens : List [ Token ], | vocabulary : Vocabulary | ) -> IndexedTokenList Takes a list of tokens and converts them to an IndexedTokenList . This could be just an ID for each token from the vocabulary. Or it could split each token into characters and return one ID per character. Or (for instance, in the case of byte-pair encoding) there might not be a clean mapping from individual tokens to indices, and the IndexedTokenList could be a complex data structure.","title":"tokens_to_indices"},{"location":"api/data/token_indexers/token_indexer/#indices_to_tokens","text":"class TokenIndexer ( Registrable ): | ... | def indices_to_tokens ( | self , | indexed_tokens : IndexedTokenList , | vocabulary : Vocabulary | ) -> List [ Token ] Inverse operations of tokens_to_indices. Takes an IndexedTokenList and converts it back into a list of tokens.","title":"indices_to_tokens"},{"location":"api/data/token_indexers/token_indexer/#get_empty_token_list","text":"class TokenIndexer ( Registrable ): | ... | def get_empty_token_list ( self ) -> IndexedTokenList Returns an already indexed version of an empty token list. This is typically just an empty list for whatever keys are used in the indexer.","title":"get_empty_token_list"},{"location":"api/data/token_indexers/token_indexer/#get_padding_lengths","text":"class TokenIndexer ( Registrable ): | ... | def get_padding_lengths ( | self , | indexed_tokens : IndexedTokenList | ) -> Dict [ str , int ] This method returns a padding dictionary for the given indexed_tokens specifying all lengths that need padding. If all you have is a list of single ID tokens, this is just the length of the list, and that's what the default implementation will give you. If you have something more complicated, like a list of character ids for token, you'll need to override this.","title":"get_padding_lengths"},{"location":"api/data/token_indexers/token_indexer/#as_padded_tensor_dict","text":"class TokenIndexer ( Registrable ): | ... | def as_padded_tensor_dict ( | self , | tokens : IndexedTokenList , | padding_lengths : Dict [ str , int ] | ) -> Dict [ str , torch . Tensor ] This method pads a list of tokens given the input padding lengths (which could actually truncate things, depending on settings) and returns that padded list of input tokens as a Dict[str, torch.Tensor] . This is a dictionary because there should be one key per argument that the TokenEmbedder corresponding to this class expects in its forward() method (where the argument name in the TokenEmbedder needs to make the key in this dictionary). The base class implements the case when all you want to do is create a padded LongTensor for every list in the tokens dictionary. If your TokenIndexer needs more complex logic than that, you need to override this method.","title":"as_padded_tensor_dict"},{"location":"api/data/tokenizers/character_tokenizer/","text":"allennlp .data .tokenizers .character_tokenizer [SOURCE] CharacterTokenizer \u00b6 @Tokenizer . register ( \"character\" ) class CharacterTokenizer ( Tokenizer ): | def __init__ ( | self , | byte_encoding : str = None , | lowercase_characters : bool = False , | start_tokens : List [ Union [ str , int ]] = None , | end_tokens : List [ Union [ str , int ]] = None | ) -> None A CharacterTokenizer splits strings into character tokens. Registered as a Tokenizer with name \"character\". Parameters \u00b6 byte_encoding : str , optional (default = None ) If not None , we will use this encoding to encode the string as bytes, and use the byte sequence as characters, instead of the unicode characters in the python string. E.g., the character '\u00e1' would be a single token if this option is None , but it would be two tokens if this option is set to \"utf-8\" . If this is not None , tokenize will return a List[int] instead of a List[str] , and we will bypass the vocabulary in the TokenIndexer . lowercase_characters : bool , optional (default = False ) If True , we will lowercase all of the characters in the text before doing any other operation. You probably do not want to do this, as character vocabularies are generally not very large to begin with, but it's an option if you really want it. start_tokens : List[str] , optional If given, these tokens will be added to the beginning of every string we tokenize. If using byte encoding, this should actually be a List[int] , not a List[str] . end_tokens : List[str] , optional If given, these tokens will be added to the end of every string we tokenize. If using byte encoding, this should actually be a List[int] , not a List[str] . tokenize \u00b6 class CharacterTokenizer ( Tokenizer ): | ... | def tokenize ( self , text : str ) -> List [ Token ]","title":"character_tokenizer"},{"location":"api/data/tokenizers/character_tokenizer/#charactertokenizer","text":"@Tokenizer . register ( \"character\" ) class CharacterTokenizer ( Tokenizer ): | def __init__ ( | self , | byte_encoding : str = None , | lowercase_characters : bool = False , | start_tokens : List [ Union [ str , int ]] = None , | end_tokens : List [ Union [ str , int ]] = None | ) -> None A CharacterTokenizer splits strings into character tokens. Registered as a Tokenizer with name \"character\".","title":"CharacterTokenizer"},{"location":"api/data/tokenizers/character_tokenizer/#tokenize","text":"class CharacterTokenizer ( Tokenizer ): | ... | def tokenize ( self , text : str ) -> List [ Token ]","title":"tokenize"},{"location":"api/data/tokenizers/letters_digits_tokenizer/","text":"allennlp .data .tokenizers .letters_digits_tokenizer [SOURCE] LettersDigitsTokenizer \u00b6 @Tokenizer . register ( \"letters_digits\" ) class LettersDigitsTokenizer ( Tokenizer ) A Tokenizer which keeps runs of (unicode) letters and runs of digits together, while every other non-whitespace character becomes a separate word. Registered as a Tokenizer with name \"letters_digits\". tokenize \u00b6 class LettersDigitsTokenizer ( Tokenizer ): | ... | def tokenize ( self , text : str ) -> List [ Token ]","title":"letters_digits_tokenizer"},{"location":"api/data/tokenizers/letters_digits_tokenizer/#lettersdigitstokenizer","text":"@Tokenizer . register ( \"letters_digits\" ) class LettersDigitsTokenizer ( Tokenizer ) A Tokenizer which keeps runs of (unicode) letters and runs of digits together, while every other non-whitespace character becomes a separate word. Registered as a Tokenizer with name \"letters_digits\".","title":"LettersDigitsTokenizer"},{"location":"api/data/tokenizers/letters_digits_tokenizer/#tokenize","text":"class LettersDigitsTokenizer ( Tokenizer ): | ... | def tokenize ( self , text : str ) -> List [ Token ]","title":"tokenize"},{"location":"api/data/tokenizers/pretrained_transformer_tokenizer/","text":"allennlp .data .tokenizers .pretrained_transformer_tokenizer [SOURCE] PretrainedTransformerTokenizer \u00b6 @Tokenizer . register ( \"pretrained_transformer\" ) class PretrainedTransformerTokenizer ( Tokenizer ): | def __init__ ( | self , | model_name : str , | add_special_tokens : bool = True , | max_length : Optional [ int ] = None , | tokenizer_kwargs : Optional [ Dict [ str , Any ]] = None | ) -> None A PretrainedTransformerTokenizer uses a model from HuggingFace's transformers library to tokenize some input text. This often means wordpieces (where 'AllenNLP is awesome' might get split into ['Allen', '##NL', '##P', 'is', 'awesome'] ), but it could also use byte-pair encoding, or some other tokenization, depending on the pretrained model that you're using. We take a model name as an input parameter, which we will pass to AutoTokenizer.from_pretrained . We also add special tokens relative to the pretrained model and truncate the sequences. This tokenizer also indexes tokens and adds the indexes to the Token fields so that they can be picked up by PretrainedTransformerIndexer . Registered as a Tokenizer with name \"pretrained_transformer\". Parameters \u00b6 model_name : str The name of the pretrained wordpiece tokenizer to use. add_special_tokens : bool , optional (default = True ) If set to True , the sequences will be encoded with the special tokens relative to their model. max_length : int , optional (default = None ) If set to a number, will limit the total sequence returned so that it has a maximum length. tokenizer_kwargs : Dict[str, Any] , optional (default = None ) Dictionary with additional arguments for AutoTokenizer.from_pretrained . tokenizer_lowercases \u00b6 class PretrainedTransformerTokenizer ( Tokenizer ): | ... | @staticmethod | def tokenizer_lowercases ( tokenizer : PreTrainedTokenizer ) -> bool tokenize \u00b6 class PretrainedTransformerTokenizer ( Tokenizer ): | ... | def tokenize ( self , text : str ) -> List [ Token ] This method only handles a single sentence (or sequence) of text. intra_word_tokenize \u00b6 class PretrainedTransformerTokenizer ( Tokenizer ): | ... | def intra_word_tokenize ( | self , | string_tokens : List [ str ] | ) -> Tuple [ List [ Token ], List [ Optional [ Tuple [ int , int ]]]] Tokenizes each word into wordpieces separately and returns the wordpiece IDs. Also calculates offsets such that tokens[offsets[i][0]:offsets[i][1] + 1] corresponds to the original i-th token. This function inserts special tokens. intra_word_tokenize_sentence_pair \u00b6 class PretrainedTransformerTokenizer ( Tokenizer ): | ... | def intra_word_tokenize_sentence_pair ( | self , | string_tokens_a : List [ str ], | string_tokens_b : List [ str ] | ) -> Tuple [ List [ Token ], List [ Optional [ Tuple [ int , int ]]], List [ Optional [ Tuple [ int , int ]]]] Tokenizes each word into wordpieces separately and returns the wordpiece IDs. Also calculates offsets such that wordpieces[offsets[i][0]:offsets[i][1] + 1] corresponds to the original i-th token. This function inserts special tokens. add_special_tokens \u00b6 class PretrainedTransformerTokenizer ( Tokenizer ): | ... | def add_special_tokens ( | self , | tokens1 : List [ Token ], | tokens2 : Optional [ List [ Token ]] = None | ) -> List [ Token ] num_special_tokens_for_sequence \u00b6 class PretrainedTransformerTokenizer ( Tokenizer ): | ... | def num_special_tokens_for_sequence ( self ) -> int num_special_tokens_for_pair \u00b6 class PretrainedTransformerTokenizer ( Tokenizer ): | ... | def num_special_tokens_for_pair ( self ) -> int","title":"pretrained_transformer_tokenizer"},{"location":"api/data/tokenizers/pretrained_transformer_tokenizer/#pretrainedtransformertokenizer","text":"@Tokenizer . register ( \"pretrained_transformer\" ) class PretrainedTransformerTokenizer ( Tokenizer ): | def __init__ ( | self , | model_name : str , | add_special_tokens : bool = True , | max_length : Optional [ int ] = None , | tokenizer_kwargs : Optional [ Dict [ str , Any ]] = None | ) -> None A PretrainedTransformerTokenizer uses a model from HuggingFace's transformers library to tokenize some input text. This often means wordpieces (where 'AllenNLP is awesome' might get split into ['Allen', '##NL', '##P', 'is', 'awesome'] ), but it could also use byte-pair encoding, or some other tokenization, depending on the pretrained model that you're using. We take a model name as an input parameter, which we will pass to AutoTokenizer.from_pretrained . We also add special tokens relative to the pretrained model and truncate the sequences. This tokenizer also indexes tokens and adds the indexes to the Token fields so that they can be picked up by PretrainedTransformerIndexer . Registered as a Tokenizer with name \"pretrained_transformer\".","title":"PretrainedTransformerTokenizer"},{"location":"api/data/tokenizers/pretrained_transformer_tokenizer/#tokenizer_lowercases","text":"class PretrainedTransformerTokenizer ( Tokenizer ): | ... | @staticmethod | def tokenizer_lowercases ( tokenizer : PreTrainedTokenizer ) -> bool","title":"tokenizer_lowercases"},{"location":"api/data/tokenizers/pretrained_transformer_tokenizer/#tokenize","text":"class PretrainedTransformerTokenizer ( Tokenizer ): | ... | def tokenize ( self , text : str ) -> List [ Token ] This method only handles a single sentence (or sequence) of text.","title":"tokenize"},{"location":"api/data/tokenizers/pretrained_transformer_tokenizer/#intra_word_tokenize","text":"class PretrainedTransformerTokenizer ( Tokenizer ): | ... | def intra_word_tokenize ( | self , | string_tokens : List [ str ] | ) -> Tuple [ List [ Token ], List [ Optional [ Tuple [ int , int ]]]] Tokenizes each word into wordpieces separately and returns the wordpiece IDs. Also calculates offsets such that tokens[offsets[i][0]:offsets[i][1] + 1] corresponds to the original i-th token. This function inserts special tokens.","title":"intra_word_tokenize"},{"location":"api/data/tokenizers/pretrained_transformer_tokenizer/#intra_word_tokenize_sentence_pair","text":"class PretrainedTransformerTokenizer ( Tokenizer ): | ... | def intra_word_tokenize_sentence_pair ( | self , | string_tokens_a : List [ str ], | string_tokens_b : List [ str ] | ) -> Tuple [ List [ Token ], List [ Optional [ Tuple [ int , int ]]], List [ Optional [ Tuple [ int , int ]]]] Tokenizes each word into wordpieces separately and returns the wordpiece IDs. Also calculates offsets such that wordpieces[offsets[i][0]:offsets[i][1] + 1] corresponds to the original i-th token. This function inserts special tokens.","title":"intra_word_tokenize_sentence_pair"},{"location":"api/data/tokenizers/pretrained_transformer_tokenizer/#add_special_tokens","text":"class PretrainedTransformerTokenizer ( Tokenizer ): | ... | def add_special_tokens ( | self , | tokens1 : List [ Token ], | tokens2 : Optional [ List [ Token ]] = None | ) -> List [ Token ]","title":"add_special_tokens"},{"location":"api/data/tokenizers/pretrained_transformer_tokenizer/#num_special_tokens_for_sequence","text":"class PretrainedTransformerTokenizer ( Tokenizer ): | ... | def num_special_tokens_for_sequence ( self ) -> int","title":"num_special_tokens_for_sequence"},{"location":"api/data/tokenizers/pretrained_transformer_tokenizer/#num_special_tokens_for_pair","text":"class PretrainedTransformerTokenizer ( Tokenizer ): | ... | def num_special_tokens_for_pair ( self ) -> int","title":"num_special_tokens_for_pair"},{"location":"api/data/tokenizers/sentence_splitter/","text":"allennlp .data .tokenizers .sentence_splitter [SOURCE] SentenceSplitter \u00b6 class SentenceSplitter ( Registrable ) A SentenceSplitter splits strings into sentences. default_implementation \u00b6 class SentenceSplitter ( Registrable ): | ... | default_implementation = \"spacy\" split_sentences \u00b6 class SentenceSplitter ( Registrable ): | ... | def split_sentences ( self , text : str ) -> List [ str ] Splits a text str paragraph into a list of str , where each is a sentence. batch_split_sentences \u00b6 class SentenceSplitter ( Registrable ): | ... | def batch_split_sentences ( self , texts : List [ str ]) -> List [ List [ str ]] Default implementation is to just iterate over the texts and call split_sentences . SpacySentenceSplitter \u00b6 @SentenceSplitter . register ( \"spacy\" ) class SpacySentenceSplitter ( SentenceSplitter ): | def __init__ ( | self , | language : str = \"en_core_web_sm\" , | rule_based : bool = False | ) -> None A SentenceSplitter that uses spaCy's built-in sentence boundary detection. Spacy's default sentence splitter uses a dependency parse to detect sentence boundaries, so it is slow, but accurate. Another option is to use rule-based sentence boundary detection. It's fast and has a small memory footprint, since it uses punctuation to detect sentence boundaries. This can be activated with the rule_based flag. By default, SpacySentenceSplitter calls the default spacy boundary detector. Registered as a SentenceSplitter with name \"spacy\". split_sentences \u00b6 class SpacySentenceSplitter ( SentenceSplitter ): | ... | def split_sentences ( self , text : str ) -> List [ str ] batch_split_sentences \u00b6 class SpacySentenceSplitter ( SentenceSplitter ): | ... | def batch_split_sentences ( self , texts : List [ str ]) -> List [ List [ str ]] This method lets you take advantage of spacy's batch processing.","title":"sentence_splitter"},{"location":"api/data/tokenizers/sentence_splitter/#sentencesplitter","text":"class SentenceSplitter ( Registrable ) A SentenceSplitter splits strings into sentences.","title":"SentenceSplitter"},{"location":"api/data/tokenizers/sentence_splitter/#default_implementation","text":"class SentenceSplitter ( Registrable ): | ... | default_implementation = \"spacy\"","title":"default_implementation"},{"location":"api/data/tokenizers/sentence_splitter/#split_sentences","text":"class SentenceSplitter ( Registrable ): | ... | def split_sentences ( self , text : str ) -> List [ str ] Splits a text str paragraph into a list of str , where each is a sentence.","title":"split_sentences"},{"location":"api/data/tokenizers/sentence_splitter/#batch_split_sentences","text":"class SentenceSplitter ( Registrable ): | ... | def batch_split_sentences ( self , texts : List [ str ]) -> List [ List [ str ]] Default implementation is to just iterate over the texts and call split_sentences .","title":"batch_split_sentences"},{"location":"api/data/tokenizers/sentence_splitter/#spacysentencesplitter","text":"@SentenceSplitter . register ( \"spacy\" ) class SpacySentenceSplitter ( SentenceSplitter ): | def __init__ ( | self , | language : str = \"en_core_web_sm\" , | rule_based : bool = False | ) -> None A SentenceSplitter that uses spaCy's built-in sentence boundary detection. Spacy's default sentence splitter uses a dependency parse to detect sentence boundaries, so it is slow, but accurate. Another option is to use rule-based sentence boundary detection. It's fast and has a small memory footprint, since it uses punctuation to detect sentence boundaries. This can be activated with the rule_based flag. By default, SpacySentenceSplitter calls the default spacy boundary detector. Registered as a SentenceSplitter with name \"spacy\".","title":"SpacySentenceSplitter"},{"location":"api/data/tokenizers/sentence_splitter/#split_sentences_1","text":"class SpacySentenceSplitter ( SentenceSplitter ): | ... | def split_sentences ( self , text : str ) -> List [ str ]","title":"split_sentences"},{"location":"api/data/tokenizers/sentence_splitter/#batch_split_sentences_1","text":"class SpacySentenceSplitter ( SentenceSplitter ): | ... | def batch_split_sentences ( self , texts : List [ str ]) -> List [ List [ str ]] This method lets you take advantage of spacy's batch processing.","title":"batch_split_sentences"},{"location":"api/data/tokenizers/spacy_tokenizer/","text":"allennlp .data .tokenizers .spacy_tokenizer [SOURCE] SpacyTokenizer \u00b6 @Tokenizer . register ( \"spacy\" ) class SpacyTokenizer ( Tokenizer ): | def __init__ ( | self , | language : str = \"en_core_web_sm\" , | pos_tags : bool = True , | parse : bool = False , | ner : bool = False , | keep_spacy_tokens : bool = False , | split_on_spaces : bool = False , | start_tokens : Optional [ List [ str ]] = None , | end_tokens : Optional [ List [ str ]] = None | ) -> None A Tokenizer that uses spaCy's tokenizer. It's fast and reasonable - this is the recommended Tokenizer . By default it will return allennlp Tokens, which are small, efficient NamedTuples (and are serializable). If you want to keep the original spaCy tokens, pass keep_spacy_tokens=True. Note that we leave one particular piece of post-processing for later: the decision of whether or not to lowercase the token. This is for two reasons: (1) if you want to make two different casing decisions for whatever reason, you won't have to run the tokenizer twice, and more importantly (2) if you want to lowercase words for your word embedding, but retain capitalization in a character-level representation, we need to retain the capitalization here. Registered as a Tokenizer with name \"spacy\", which is currently the default. Parameters \u00b6 language : str , optional (default = \"en_core_web_sm\" ) Spacy model name. pos_tags : bool , optional (default = False ) If True , performs POS tagging with spacy model on the tokens. Generally used in conjunction with PosTagIndexer . parse : bool , optional (default = False ) If True , performs dependency parsing with spacy model on the tokens. Generally used in conjunction with DepLabelIndexer . ner : bool , optional (default = False ) If True , performs dependency parsing with spacy model on the tokens. Generally used in conjunction with NerTagIndexer . keep_spacy_tokens : bool , optional (default = False ) If True , will preserve spacy token objects, We copy spacy tokens into our own class by default instead because spacy Cython Tokens can't be pickled. split_on_spaces : bool , optional (default = False ) If True , will split by spaces without performing tokenization. Used when your data is already tokenized, but you want to perform pos, ner or parsing on the tokens. start_tokens : Optional[List[str]] , optional (default = None ) If given, these tokens will be added to the beginning of every string we tokenize. end_tokens : Optional[List[str]] , optional (default = None ) If given, these tokens will be added to the end of every string we tokenize. batch_tokenize \u00b6 class SpacyTokenizer ( Tokenizer ): | ... | def batch_tokenize ( self , texts : List [ str ]) -> List [ List [ Token ]] tokenize \u00b6 class SpacyTokenizer ( Tokenizer ): | ... | def tokenize ( self , text : str ) -> List [ Token ]","title":"spacy_tokenizer"},{"location":"api/data/tokenizers/spacy_tokenizer/#spacytokenizer","text":"@Tokenizer . register ( \"spacy\" ) class SpacyTokenizer ( Tokenizer ): | def __init__ ( | self , | language : str = \"en_core_web_sm\" , | pos_tags : bool = True , | parse : bool = False , | ner : bool = False , | keep_spacy_tokens : bool = False , | split_on_spaces : bool = False , | start_tokens : Optional [ List [ str ]] = None , | end_tokens : Optional [ List [ str ]] = None | ) -> None A Tokenizer that uses spaCy's tokenizer. It's fast and reasonable - this is the recommended Tokenizer . By default it will return allennlp Tokens, which are small, efficient NamedTuples (and are serializable). If you want to keep the original spaCy tokens, pass keep_spacy_tokens=True. Note that we leave one particular piece of post-processing for later: the decision of whether or not to lowercase the token. This is for two reasons: (1) if you want to make two different casing decisions for whatever reason, you won't have to run the tokenizer twice, and more importantly (2) if you want to lowercase words for your word embedding, but retain capitalization in a character-level representation, we need to retain the capitalization here. Registered as a Tokenizer with name \"spacy\", which is currently the default.","title":"SpacyTokenizer"},{"location":"api/data/tokenizers/spacy_tokenizer/#batch_tokenize","text":"class SpacyTokenizer ( Tokenizer ): | ... | def batch_tokenize ( self , texts : List [ str ]) -> List [ List [ Token ]]","title":"batch_tokenize"},{"location":"api/data/tokenizers/spacy_tokenizer/#tokenize","text":"class SpacyTokenizer ( Tokenizer ): | ... | def tokenize ( self , text : str ) -> List [ Token ]","title":"tokenize"},{"location":"api/data/tokenizers/token_class/","text":"allennlp .data .tokenizers .token_class [SOURCE] Token \u00b6 @dataclass ( init = False , repr = False ) class Token : | def __init__ ( | self , | text : str = None , | idx : int = None , | idx_end : int = None , | lemma_ : str = None , | pos_ : str = None , | tag_ : str = None , | dep_ : str = None , | ent_type_ : str = None , | text_id : int = None , | type_id : int = None | ) -> None A simple token representation, keeping track of the token's text, offset in the passage it was taken from, POS tag, dependency relation, and similar information. These fields match spacy's exactly, so we can just use a spacy token for this. Parameters \u00b6 text : str , optional The original text represented by this token. idx : int , optional The character offset of this token into the tokenized passage. idx_end : int , optional The character offset one past the last character in the tokenized passage. lemma_ : str , optional The lemma of this token. pos_ : str , optional The coarse-grained part of speech of this token. tag_ : str , optional The fine-grained part of speech of this token. dep_ : str , optional The dependency relation for this token. ent_type_ : str , optional The entity type (i.e., the NER tag) for this token. text_id : int , optional If your tokenizer returns integers instead of strings (e.g., because you're doing byte encoding, or some hash-based embedding), set this with the integer. If this is set, we will bypass the vocabulary when indexing this token, regardless of whether text is also set. You can also set text with the original text, if you want, so that you can still use a character-level representation in addition to a hash-based word embedding. type_id : int , optional Token type id used by some pretrained language models like original BERT The other fields on Token follow the fields on spacy's Token object; this is one we added, similar to spacy's lex_id . text \u00b6 class Token : | ... | text : Optional [ str ] = None idx \u00b6 class Token : | ... | idx : Optional [ int ] = None idx_end \u00b6 class Token : | ... | idx_end : Optional [ int ] = None lemma_ \u00b6 class Token : | ... | lemma_ : Optional [ str ] = None pos_ \u00b6 class Token : | ... | pos_ : Optional [ str ] = None tag_ \u00b6 class Token : | ... | tag_ : Optional [ str ] = None dep_ \u00b6 class Token : | ... | dep_ : Optional [ str ] = None ent_type_ \u00b6 class Token : | ... | ent_type_ : Optional [ str ] = None text_id \u00b6 class Token : | ... | text_id : Optional [ int ] = None type_id \u00b6 class Token : | ... | type_id : Optional [ int ] = None ensure_text \u00b6 class Token : | ... | def ensure_text ( self ) -> str Return the text field, raising an exception if it's None . show_token \u00b6 def show_token ( token : Token ) -> str","title":"token_class"},{"location":"api/data/tokenizers/token_class/#token","text":"@dataclass ( init = False , repr = False ) class Token : | def __init__ ( | self , | text : str = None , | idx : int = None , | idx_end : int = None , | lemma_ : str = None , | pos_ : str = None , | tag_ : str = None , | dep_ : str = None , | ent_type_ : str = None , | text_id : int = None , | type_id : int = None | ) -> None A simple token representation, keeping track of the token's text, offset in the passage it was taken from, POS tag, dependency relation, and similar information. These fields match spacy's exactly, so we can just use a spacy token for this.","title":"Token"},{"location":"api/data/tokenizers/token_class/#text","text":"class Token : | ... | text : Optional [ str ] = None","title":"text"},{"location":"api/data/tokenizers/token_class/#idx","text":"class Token : | ... | idx : Optional [ int ] = None","title":"idx"},{"location":"api/data/tokenizers/token_class/#idx_end","text":"class Token : | ... | idx_end : Optional [ int ] = None","title":"idx_end"},{"location":"api/data/tokenizers/token_class/#lemma_","text":"class Token : | ... | lemma_ : Optional [ str ] = None","title":"lemma_"},{"location":"api/data/tokenizers/token_class/#pos_","text":"class Token : | ... | pos_ : Optional [ str ] = None","title":"pos_"},{"location":"api/data/tokenizers/token_class/#tag_","text":"class Token : | ... | tag_ : Optional [ str ] = None","title":"tag_"},{"location":"api/data/tokenizers/token_class/#dep_","text":"class Token : | ... | dep_ : Optional [ str ] = None","title":"dep_"},{"location":"api/data/tokenizers/token_class/#ent_type_","text":"class Token : | ... | ent_type_ : Optional [ str ] = None","title":"ent_type_"},{"location":"api/data/tokenizers/token_class/#text_id","text":"class Token : | ... | text_id : Optional [ int ] = None","title":"text_id"},{"location":"api/data/tokenizers/token_class/#type_id","text":"class Token : | ... | type_id : Optional [ int ] = None","title":"type_id"},{"location":"api/data/tokenizers/token_class/#ensure_text","text":"class Token : | ... | def ensure_text ( self ) -> str Return the text field, raising an exception if it's None .","title":"ensure_text"},{"location":"api/data/tokenizers/token_class/#show_token","text":"def show_token ( token : Token ) -> str","title":"show_token"},{"location":"api/data/tokenizers/tokenizer/","text":"allennlp .data .tokenizers .tokenizer [SOURCE] Tokenizer \u00b6 class Tokenizer ( Registrable ) A Tokenizer splits strings of text into tokens. Typically, this either splits text into word tokens or character tokens, and those are the two tokenizer subclasses we have implemented here, though you could imagine wanting to do other kinds of tokenization for structured or other inputs. See the parameters to, e.g., .SpacyTokenizer , or whichever tokenizer you want to use. If the base input to your model is words, you should use a .SpacyTokenizer , even if you also want to have a character-level encoder to get an additional vector for each word token. Splitting word tokens into character arrays is handled separately, in the ..token_representations.TokenRepresentation class. default_implementation \u00b6 class Tokenizer ( Registrable ): | ... | default_implementation = \"spacy\" batch_tokenize \u00b6 class Tokenizer ( Registrable ): | ... | def batch_tokenize ( self , texts : List [ str ]) -> List [ List [ Token ]] Batches together tokenization of several texts, in case that is faster for particular tokenizers. By default we just do this without batching. Override this in your tokenizer if you have a good way of doing batched computation. tokenize \u00b6 class Tokenizer ( Registrable ): | ... | def tokenize ( self , text : str ) -> List [ Token ] Actually implements splitting words into tokens. Returns \u00b6 tokens : List[Token] add_special_tokens \u00b6 class Tokenizer ( Registrable ): | ... | def add_special_tokens ( | self , | tokens1 : List [ Token ], | tokens2 : Optional [ List [ Token ]] = None | ) -> List [ Token ] Adds special tokens to tokenized text. These are tokens like [CLS] or [SEP]. Not all tokenizers do this. The default is to just return the tokens unchanged. Parameters \u00b6 tokens1 : List[Token] The list of tokens to add special tokens to. tokens2 : Optional[List[Token]] An optional second list of tokens. This will be concatenated with tokens1 . Special tokens will be added as appropriate. Returns \u00b6 tokens : List[Token] The combined list of tokens, with special tokens added. num_special_tokens_for_sequence \u00b6 class Tokenizer ( Registrable ): | ... | def num_special_tokens_for_sequence ( self ) -> int Returns the number of special tokens added for a single sequence. num_special_tokens_for_pair \u00b6 class Tokenizer ( Registrable ): | ... | def num_special_tokens_for_pair ( self ) -> int Returns the number of special tokens added for a pair of sequences.","title":"tokenizer"},{"location":"api/data/tokenizers/tokenizer/#tokenizer","text":"class Tokenizer ( Registrable ) A Tokenizer splits strings of text into tokens. Typically, this either splits text into word tokens or character tokens, and those are the two tokenizer subclasses we have implemented here, though you could imagine wanting to do other kinds of tokenization for structured or other inputs. See the parameters to, e.g., .SpacyTokenizer , or whichever tokenizer you want to use. If the base input to your model is words, you should use a .SpacyTokenizer , even if you also want to have a character-level encoder to get an additional vector for each word token. Splitting word tokens into character arrays is handled separately, in the ..token_representations.TokenRepresentation class.","title":"Tokenizer"},{"location":"api/data/tokenizers/tokenizer/#default_implementation","text":"class Tokenizer ( Registrable ): | ... | default_implementation = \"spacy\"","title":"default_implementation"},{"location":"api/data/tokenizers/tokenizer/#batch_tokenize","text":"class Tokenizer ( Registrable ): | ... | def batch_tokenize ( self , texts : List [ str ]) -> List [ List [ Token ]] Batches together tokenization of several texts, in case that is faster for particular tokenizers. By default we just do this without batching. Override this in your tokenizer if you have a good way of doing batched computation.","title":"batch_tokenize"},{"location":"api/data/tokenizers/tokenizer/#tokenize","text":"class Tokenizer ( Registrable ): | ... | def tokenize ( self , text : str ) -> List [ Token ] Actually implements splitting words into tokens.","title":"tokenize"},{"location":"api/data/tokenizers/tokenizer/#add_special_tokens","text":"class Tokenizer ( Registrable ): | ... | def add_special_tokens ( | self , | tokens1 : List [ Token ], | tokens2 : Optional [ List [ Token ]] = None | ) -> List [ Token ] Adds special tokens to tokenized text. These are tokens like [CLS] or [SEP]. Not all tokenizers do this. The default is to just return the tokens unchanged.","title":"add_special_tokens"},{"location":"api/data/tokenizers/tokenizer/#num_special_tokens_for_sequence","text":"class Tokenizer ( Registrable ): | ... | def num_special_tokens_for_sequence ( self ) -> int Returns the number of special tokens added for a single sequence.","title":"num_special_tokens_for_sequence"},{"location":"api/data/tokenizers/tokenizer/#num_special_tokens_for_pair","text":"class Tokenizer ( Registrable ): | ... | def num_special_tokens_for_pair ( self ) -> int Returns the number of special tokens added for a pair of sequences.","title":"num_special_tokens_for_pair"},{"location":"api/data/tokenizers/whitespace_tokenizer/","text":"allennlp .data .tokenizers .whitespace_tokenizer [SOURCE] WhitespaceTokenizer \u00b6 @Tokenizer . register ( \"whitespace\" ) @Tokenizer . register ( \"just_spaces\" ) class WhitespaceTokenizer ( Tokenizer ) A Tokenizer that assumes you've already done your own tokenization somehow and have separated the tokens by spaces. We just split the input string on whitespace and return the resulting list. Note that we use text.split() , which means that the amount of whitespace between the tokens does not matter. This will never result in spaces being included as tokens. Registered as a Tokenizer with name \"whitespace\" and \"just_spaces\". tokenize \u00b6 class WhitespaceTokenizer ( Tokenizer ): | ... | def tokenize ( self , text : str ) -> List [ Token ]","title":"whitespace_tokenizer"},{"location":"api/data/tokenizers/whitespace_tokenizer/#whitespacetokenizer","text":"@Tokenizer . register ( \"whitespace\" ) @Tokenizer . register ( \"just_spaces\" ) class WhitespaceTokenizer ( Tokenizer ) A Tokenizer that assumes you've already done your own tokenization somehow and have separated the tokens by spaces. We just split the input string on whitespace and return the resulting list. Note that we use text.split() , which means that the amount of whitespace between the tokens does not matter. This will never result in spaces being included as tokens. Registered as a Tokenizer with name \"whitespace\" and \"just_spaces\".","title":"WhitespaceTokenizer"},{"location":"api/data/tokenizers/whitespace_tokenizer/#tokenize","text":"class WhitespaceTokenizer ( Tokenizer ): | ... | def tokenize ( self , text : str ) -> List [ Token ]","title":"tokenize"},{"location":"api/evaluation/evaluator/","text":"allennlp .evaluation .evaluator [SOURCE] Evaluator class for evaluating a model with a given dataset Evaluator \u00b6 class Evaluator ( Registrable ): | def __init__ ( | self , | batch_serializer : Optional [ Serializer ] = None , | cuda_device : Union [ int , torch . device ] = - 1 , | postprocessor_fn_name : str = \"make_output_human_readable\" | ) Evaluation Base class Parameters \u00b6 batch_postprocessor : Postprocessor , optional (default = SimplePostprocessor ) The postprocessor to use for turning both the batches and the outputs of the model into human readable data. cuda_device : Union[int, torch.device] , optional (default = -1 ) The cuda device to use for this evaluation. The model is assumed to already be using this device; this parameter is only used for moving the input data to the correct device. postprocessor_fn_name : str , optional (default = \"make_output_human_readable\" ) Function name of the model's postprocessing function. default_implementation \u00b6 class Evaluator ( Registrable ): | ... | default_implementation = \"simple\" __call__ \u00b6 class Evaluator ( Registrable ): | ... | def __call__ ( | self , | model : Model , | data_loader : DataLoader , | batch_weight_key : str = None , | metrics_output_file : Union [ str , PathLike ] = None , | predictions_output_file : Union [ str , PathLike ] = None | ) -> Dict [ str , Any ] Evaluate a single data source. Parameters \u00b6 model : Model The model to evaluate data_loader : DataLoader The DataLoader that will iterate over the evaluation data (data loaders already contain their data). batch_weight_key : str , optional (default = None ) If given, this is a key in the output dictionary for each batch that specifies how to weight the loss for that batch. If this is not given, we use a weight of 1 for every batch. metrics_output_file : Union[str, PathLike] , optional (default = None ) Optional path to write the final metrics to. predictions_output_file : Union[str, PathLike] , optional (default = None ) Optional path to write the predictions to. If passed the postprocessor will be called and its output will be written as lines. Returns \u00b6 metrics : Dict[str, Any] The metrics from evaluating the file. SimpleEvaluator \u00b6 @Evaluator . register ( \"simple\" ) class SimpleEvaluator ( Evaluator ): | def __init__ ( | self , | batch_serializer : Optional [ Serializer ] = None , | cuda_device : Union [ int , torch . device ] = - 1 , | postprocessor_fn_name : str = \"make_output_human_readable\" | ) Simple evaluator implementation. Uses the vanilla evaluation code. Parameters \u00b6 batch_postprocessor : Postprocessor , optional (default = SimplePostprocessor ) The postprocessor to use for turning both the batches and the outputs of the model into human readable data. cuda_device : Union[int, torch.device] , optional (default = -1 ) The cuda device to use for this evaluation. The model is assumed to already be using this device; this parameter is only used for moving the input data to the correct device. postprocessor_fn_name : str , optional (default = \"make_output_human_readable\" ) Function name of the model's postprocessing function. __call__ \u00b6 class SimpleEvaluator ( Evaluator ): | ... | def __call__ ( | self , | model : Model , | data_loader : DataLoader , | batch_weight_key : str = None , | metrics_output_file : Union [ str , PathLike ] = None , | predictions_output_file : Union [ str , PathLike ] = None | ) Evaluate a single data source. Parameters \u00b6 model : Model The model to evaluate data_loader : DataLoader The DataLoader that will iterate over the evaluation data (data loaders already contain their data). batch_weight_key : str , optional (default = None ) If given, this is a key in the output dictionary for each batch that specifies how to weight the loss for that batch. If this is not given, we use a weight of 1 for every batch. metrics_output_file : Union[str, PathLike] , optional (default = None ) Optional path to write the final metrics to. predictions_output_file : Union[str, PathLike] , optional (default = None ) Optional path to write the predictions to. Returns \u00b6 metrics : Dict[str, Any] The metrics from evaluating the file.","title":"evaluator"},{"location":"api/evaluation/evaluator/#evaluator","text":"class Evaluator ( Registrable ): | def __init__ ( | self , | batch_serializer : Optional [ Serializer ] = None , | cuda_device : Union [ int , torch . device ] = - 1 , | postprocessor_fn_name : str = \"make_output_human_readable\" | ) Evaluation Base class","title":"Evaluator"},{"location":"api/evaluation/evaluator/#default_implementation","text":"class Evaluator ( Registrable ): | ... | default_implementation = \"simple\"","title":"default_implementation"},{"location":"api/evaluation/evaluator/#__call__","text":"class Evaluator ( Registrable ): | ... | def __call__ ( | self , | model : Model , | data_loader : DataLoader , | batch_weight_key : str = None , | metrics_output_file : Union [ str , PathLike ] = None , | predictions_output_file : Union [ str , PathLike ] = None | ) -> Dict [ str , Any ] Evaluate a single data source.","title":"__call__"},{"location":"api/evaluation/evaluator/#simpleevaluator","text":"@Evaluator . register ( \"simple\" ) class SimpleEvaluator ( Evaluator ): | def __init__ ( | self , | batch_serializer : Optional [ Serializer ] = None , | cuda_device : Union [ int , torch . device ] = - 1 , | postprocessor_fn_name : str = \"make_output_human_readable\" | ) Simple evaluator implementation. Uses the vanilla evaluation code.","title":"SimpleEvaluator"},{"location":"api/evaluation/evaluator/#__call___1","text":"class SimpleEvaluator ( Evaluator ): | ... | def __call__ ( | self , | model : Model , | data_loader : DataLoader , | batch_weight_key : str = None , | metrics_output_file : Union [ str , PathLike ] = None , | predictions_output_file : Union [ str , PathLike ] = None | ) Evaluate a single data source.","title":"__call__"},{"location":"api/evaluation/serializers/serializers/","text":"allennlp .evaluation .serializers .serializers [SOURCE] Serializer \u00b6 class Serializer ( Registrable ) General serializer class for turning batches into human readable data __call__ \u00b6 class Serializer ( Registrable ): | ... | def __call__ ( | self , | batch : Dict [ str , TensorField ], | output_dict : Dict , | data_loader : DataLoader , | output_postprocess_function : Optional [ Callable ] = None | ) -> str Postprocess a batch. Parameters \u00b6 batch : Dict[str, TensorField] The batch that was passed to the model's forward function. output_dict : Dict The output of the model's forward function on the batch data_loader : DataLoader The dataloader to be used. output_postprocess_function : Callable , optional (default = None ) If you have a function to preprocess only the outputs ( i.e. model.make_human_readable ), use this parameter to have it called on the output dict. Returns \u00b6 postprocessed : str The postprocessed batches as strings default_implementation \u00b6 class Serializer ( Registrable ): | ... | default_implementation = \"simple\" SimpleSerializer \u00b6 @Serializer . register ( \"simple\" ) class SimpleSerializer ( Serializer ) Very simple serializer. Only sanitizes the batches and outputs. Will use a passed serializer function for the outputs if it exists. __call__ \u00b6 class SimpleSerializer ( Serializer ): | ... | def __call__ ( | self , | batch : Dict [ str , TensorField ], | output_dict : Dict , | data_loader : DataLoader , | output_postprocess_function : Optional [ Callable ] = None | ) Serializer a batch. Parameters \u00b6 batch : Dict[str, TensorField] The batch that was passed to the model's forward function. output_dict : Dict The output of the model's forward function on the batch data_loader : DataLoader The dataloader to be used. output_postprocess_function : Callable , optional (default = None ) If you have a function to preprocess only the outputs ( i.e. model.make_human_readable ), use this parameter to have it called on the output dict. Returns \u00b6 serialized : str The serialized batches as strings","title":"serializers"},{"location":"api/evaluation/serializers/serializers/#serializer","text":"class Serializer ( Registrable ) General serializer class for turning batches into human readable data","title":"Serializer"},{"location":"api/evaluation/serializers/serializers/#__call__","text":"class Serializer ( Registrable ): | ... | def __call__ ( | self , | batch : Dict [ str , TensorField ], | output_dict : Dict , | data_loader : DataLoader , | output_postprocess_function : Optional [ Callable ] = None | ) -> str Postprocess a batch.","title":"__call__"},{"location":"api/evaluation/serializers/serializers/#default_implementation","text":"class Serializer ( Registrable ): | ... | default_implementation = \"simple\"","title":"default_implementation"},{"location":"api/evaluation/serializers/serializers/#simpleserializer","text":"@Serializer . register ( \"simple\" ) class SimpleSerializer ( Serializer ) Very simple serializer. Only sanitizes the batches and outputs. Will use a passed serializer function for the outputs if it exists.","title":"SimpleSerializer"},{"location":"api/evaluation/serializers/serializers/#__call___1","text":"class SimpleSerializer ( Serializer ): | ... | def __call__ ( | self , | batch : Dict [ str , TensorField ], | output_dict : Dict , | data_loader : DataLoader , | output_postprocess_function : Optional [ Callable ] = None | ) Serializer a batch.","title":"__call__"},{"location":"api/fairness/adversarial_bias_mitigator/","text":"allennlp .fairness .adversarial_bias_mitigator [SOURCE] A Model wrapper to adversarially mitigate biases in predictions produced by a pretrained model for a downstream task. The documentation and explanations are heavily based on: Zhang, B.H., Lemoine, B., & Mitchell, M. (2018). Mitigating Unwanted Biases with Adversarial Learning . Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society. and Mitigating Unwanted Biases in Word Embeddings with Adversarial Learning colab notebook. Adversarial networks mitigate some biases based on the idea that predicting an outcome Y given an input X should ideally be independent of some protected variable Z. Informally, \"knowing Y would not help you predict Z any better than chance\" (Zaldivar et al., 2018). This can be achieved using two networks in a series, where the first attempts to predict Y using X as input, and the second attempts to use the predicted value of Y to recover Z. Please refer to Figure 1 of Mitigating Unwanted Biases with Adversarial Learning . Ideally, we would like the first network to predict Y without permitting the second network to predict Z any better than chance. For common NLP tasks, it's usually clear what X and Y are, but Z is not always available. We can construct our own Z by: computing a bias direction (e.g. for binary gender) computing the inner product of static sentence embeddings and the bias direction Training adversarial networks is extremely difficult. It is important to: lower the step size of both the predictor and adversary to train both models slowly to avoid parameters diverging, initialize the parameters of the adversary to be small to avoid the predictor overfitting against a sub-optimal adversary, increase the adversary\u2019s learning rate to prevent divergence if the predictor is too good at hiding the protected variable from the adversary. AdversarialBiasMitigator \u00b6 @Model . register ( \"adversarial_bias_mitigator\" ) class AdversarialBiasMitigator ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | predictor : Model , | adversary : Model , | bias_direction : BiasDirectionWrapper , | predictor_output_key : str , | ** kwargs | ) Wrapper class to adversarially mitigate biases in any pretrained Model. Parameters \u00b6 vocab : Vocabulary Vocabulary of predictor. predictor : Model Model for which to mitigate biases. adversary : Model Model that attempts to recover protected variable values from predictor's predictions. bias_direction : BiasDirectionWrapper Bias direction used by adversarial bias mitigator. predictor_output_key : str Key corresponding to output in output_dict of predictor that should be passed as input to adversary. Note adversary must use same vocab as predictor, if it requires a vocab. train \u00b6 class AdversarialBiasMitigator ( Model ): | ... | def train ( self , mode : bool = True ) forward \u00b6 class AdversarialBiasMitigator ( Model ): | ... | def forward ( self , * args , ** kwargs ) forward_on_instance \u00b6 class AdversarialBiasMitigator ( Model ): | ... | def forward_on_instance ( self , * args , ** kwargs ) forward_on_instances \u00b6 class AdversarialBiasMitigator ( Model ): | ... | def forward_on_instances ( self , * args , ** kwargs ) get_regularization_penalty \u00b6 class AdversarialBiasMitigator ( Model ): | ... | def get_regularization_penalty ( self , * args , ** kwargs ) get_parameters_for_histogram_logging \u00b6 class AdversarialBiasMitigator ( Model ): | ... | def get_parameters_for_histogram_logging ( self , * args , ** kwargs ) get_parameters_for_histogram_tensorboard_logging \u00b6 class AdversarialBiasMitigator ( Model ): | ... | def get_parameters_for_histogram_tensorboard_logging ( | self , | * args , | ** kwargs | ) make_output_human_readable \u00b6 class AdversarialBiasMitigator ( Model ): | ... | def make_output_human_readable ( self , * args , ** kwargs ) get_metrics \u00b6 class AdversarialBiasMitigator ( Model ): | ... | def get_metrics ( self , * args , ** kwargs ) extend_embedder_vocab \u00b6 class AdversarialBiasMitigator ( Model ): | ... | def extend_embedder_vocab ( self , * args , ** kwargs ) FeedForwardRegressionAdversary \u00b6 @Model . register ( \"feedforward_regression_adversary\" ) class FeedForwardRegressionAdversary ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | feedforward : FeedForward , | initializer : Optional [ InitializerApplicator ] = InitializerApplicator (), | ** kwargs | ) -> None This Model implements a simple feedforward regression adversary. Registered as a Model with name \"feedforward_regression_adversary\". Parameters \u00b6 vocab : Vocabulary feedforward : FeedForward A feedforward layer. initializer : Optional[InitializerApplicator] , optional (default = InitializerApplicator() ) If provided, will be used to initialize the model parameters. forward \u00b6 class FeedForwardRegressionAdversary ( Model ): | ... | def forward ( | self , | input : torch . FloatTensor , | label : torch . FloatTensor | ) -> Dict [ str , torch . Tensor ] Parameters \u00b6 input : torch.FloatTensor A tensor of size (batch_size, ...). label : torch.FloatTensor A tensor of the same size as input. Returns \u00b6 An output dictionary consisting of: loss : torch.FloatTensor A scalar loss to be optimised. AdversarialBiasMitigatorBackwardCallback \u00b6 @TrainerCallback . register ( \"adversarial_bias_mitigator_backward\" ) class AdversarialBiasMitigatorBackwardCallback ( TrainerCallback ): | def __init__ ( | self , | serialization_dir : str , | adversary_loss_weight : float = 1.0 | ) -> None Performs backpropagation for adversarial bias mitigation. While the adversary's gradients are computed normally, the predictor's gradients are computed such that updates to the predictor's parameters will not aid the adversary and will make it more difficult for the adversary to recover protected variables. Note Intended to be used with AdversarialBiasMitigator . trainer.model is expected to have predictor and adversary data members. Parameters \u00b6 adversary_loss_weight : float , optional (default = 1.0 ) Quantifies how difficult predictor makes it for adversary to recover protected variables. on_backward \u00b6 class AdversarialBiasMitigatorBackwardCallback ( TrainerCallback ): | ... | def on_backward ( | self , | trainer : GradientDescentTrainer , | batch_outputs : Dict [ str , torch . Tensor ], | backward_called : bool , | ** kwargs | ) -> bool","title":"adversarial_bias_mitigator"},{"location":"api/fairness/adversarial_bias_mitigator/#adversarialbiasmitigator","text":"@Model . register ( \"adversarial_bias_mitigator\" ) class AdversarialBiasMitigator ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | predictor : Model , | adversary : Model , | bias_direction : BiasDirectionWrapper , | predictor_output_key : str , | ** kwargs | ) Wrapper class to adversarially mitigate biases in any pretrained Model.","title":"AdversarialBiasMitigator"},{"location":"api/fairness/adversarial_bias_mitigator/#train","text":"class AdversarialBiasMitigator ( Model ): | ... | def train ( self , mode : bool = True )","title":"train"},{"location":"api/fairness/adversarial_bias_mitigator/#forward","text":"class AdversarialBiasMitigator ( Model ): | ... | def forward ( self , * args , ** kwargs )","title":"forward"},{"location":"api/fairness/adversarial_bias_mitigator/#forward_on_instance","text":"class AdversarialBiasMitigator ( Model ): | ... | def forward_on_instance ( self , * args , ** kwargs )","title":"forward_on_instance"},{"location":"api/fairness/adversarial_bias_mitigator/#forward_on_instances","text":"class AdversarialBiasMitigator ( Model ): | ... | def forward_on_instances ( self , * args , ** kwargs )","title":"forward_on_instances"},{"location":"api/fairness/adversarial_bias_mitigator/#get_regularization_penalty","text":"class AdversarialBiasMitigator ( Model ): | ... | def get_regularization_penalty ( self , * args , ** kwargs )","title":"get_regularization_penalty"},{"location":"api/fairness/adversarial_bias_mitigator/#get_parameters_for_histogram_logging","text":"class AdversarialBiasMitigator ( Model ): | ... | def get_parameters_for_histogram_logging ( self , * args , ** kwargs )","title":"get_parameters_for_histogram_logging"},{"location":"api/fairness/adversarial_bias_mitigator/#get_parameters_for_histogram_tensorboard_logging","text":"class AdversarialBiasMitigator ( Model ): | ... | def get_parameters_for_histogram_tensorboard_logging ( | self , | * args , | ** kwargs | )","title":"get_parameters_for_histogram_tensorboard_logging"},{"location":"api/fairness/adversarial_bias_mitigator/#make_output_human_readable","text":"class AdversarialBiasMitigator ( Model ): | ... | def make_output_human_readable ( self , * args , ** kwargs )","title":"make_output_human_readable"},{"location":"api/fairness/adversarial_bias_mitigator/#get_metrics","text":"class AdversarialBiasMitigator ( Model ): | ... | def get_metrics ( self , * args , ** kwargs )","title":"get_metrics"},{"location":"api/fairness/adversarial_bias_mitigator/#extend_embedder_vocab","text":"class AdversarialBiasMitigator ( Model ): | ... | def extend_embedder_vocab ( self , * args , ** kwargs )","title":"extend_embedder_vocab"},{"location":"api/fairness/adversarial_bias_mitigator/#feedforwardregressionadversary","text":"@Model . register ( \"feedforward_regression_adversary\" ) class FeedForwardRegressionAdversary ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | feedforward : FeedForward , | initializer : Optional [ InitializerApplicator ] = InitializerApplicator (), | ** kwargs | ) -> None This Model implements a simple feedforward regression adversary. Registered as a Model with name \"feedforward_regression_adversary\".","title":"FeedForwardRegressionAdversary"},{"location":"api/fairness/adversarial_bias_mitigator/#forward_1","text":"class FeedForwardRegressionAdversary ( Model ): | ... | def forward ( | self , | input : torch . FloatTensor , | label : torch . FloatTensor | ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"api/fairness/adversarial_bias_mitigator/#adversarialbiasmitigatorbackwardcallback","text":"@TrainerCallback . register ( \"adversarial_bias_mitigator_backward\" ) class AdversarialBiasMitigatorBackwardCallback ( TrainerCallback ): | def __init__ ( | self , | serialization_dir : str , | adversary_loss_weight : float = 1.0 | ) -> None Performs backpropagation for adversarial bias mitigation. While the adversary's gradients are computed normally, the predictor's gradients are computed such that updates to the predictor's parameters will not aid the adversary and will make it more difficult for the adversary to recover protected variables. Note Intended to be used with AdversarialBiasMitigator . trainer.model is expected to have predictor and adversary data members.","title":"AdversarialBiasMitigatorBackwardCallback"},{"location":"api/fairness/adversarial_bias_mitigator/#on_backward","text":"class AdversarialBiasMitigatorBackwardCallback ( TrainerCallback ): | ... | def on_backward ( | self , | trainer : GradientDescentTrainer , | batch_outputs : Dict [ str , torch . Tensor ], | backward_called : bool , | ** kwargs | ) -> bool","title":"on_backward"},{"location":"api/fairness/bias_direction/","text":"allennlp .fairness .bias_direction [SOURCE] A suite of differentiable methods to compute the bias direction or concept subspace representing binary protected variables. BiasDirection \u00b6 class BiasDirection : | def __init__ ( self , requires_grad : bool = False ) Parent class for bias direction classes. Parameters \u00b6 requires_grad : bool , optional (default = False ) Option to enable gradient calculation. PCABiasDirection \u00b6 class PCABiasDirection ( BiasDirection ) PCA-based bias direction. Computes one-dimensional subspace that is the span of a specific concept (e.g. gender) using PCA. This subspace minimizes the sum of squared distances from all seed word embeddings. Note It is uncommon to utilize more than one direction to represent a concept. Implementation and terminology based on Rathore, A., Dev, S., Phillips, J.M., Srikumar, V., Zheng, Y., Yeh, C.M., Wang, J., Zhang, W., & Wang, B. (2021). VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations . ArXiv, abs/2104.02797. __call__ \u00b6 class PCABiasDirection ( BiasDirection ): | ... | def __call__ ( self , seed_embeddings : torch . Tensor ) Parameters \u00b6 Note In the examples below, we treat gender identity as binary, which does not accurately characterize gender in real life. seed_embeddings : torch.Tensor A tensor of size (batch_size, ..., dim) containing seed word embeddings related to a concept. For example, if the concept is gender, seed_embeddings could contain embeddings for words like \"man\", \"king\", \"brother\", \"woman\", \"queen\", \"sister\", etc. Returns \u00b6 bias_direction : torch.Tensor A unit tensor of size (dim, ) representing the concept subspace. PairedPCABiasDirection \u00b6 class PairedPCABiasDirection ( BiasDirection ) Paired-PCA-based bias direction. Computes one-dimensional subspace that is the span of a specific concept (e.g. gender) as the first principle component of the difference vectors between seed word embedding pairs. Note It is uncommon to utilize more than one direction to represent a concept. Based on: T. Bolukbasi, K. W. Chang, J. Zou, V. Saligrama, and A. Kalai. Man is to computer programmer as woman is to homemaker? debiasing word embeddings . In ACM Transactions of Information Systems, 2016. Implementation and terminology based on Rathore, A., Dev, S., Phillips, J.M., Srikumar, V., Zheng, Y., Yeh, C.M., Wang, J., Zhang, W., & Wang, B. (2021). VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations . ArXiv, abs/2104.02797. __call__ \u00b6 class PairedPCABiasDirection ( BiasDirection ): | ... | def __call__ ( | self , | seed_embeddings1 : torch . Tensor , | seed_embeddings2 : torch . Tensor | ) Parameters \u00b6 Note In the examples below, we treat gender identity as binary, which does not accurately characterize gender in real life. seed_embeddings1 : torch.Tensor A tensor of size (batch_size, ..., dim) containing seed word embeddings related to a concept group. For example, if the concept is gender, seed_embeddings1 could contain embeddings for linguistically masculine words, e.g. \"man\", \"king\", \"brother\", etc. seed_embeddings2 : torch.Tensor A tensor of the same size as seed_embeddings1 containing seed word embeddings related to a different group for the same concept. For example, seed_embeddings2 could contain embeddings for linguistically feminine words, e.g. \"woman\", \"queen\", \"sister\", etc. Note For Paired-PCA, the embeddings at the same positions in each of seed_embeddings1 and seed_embeddings2 are expected to form seed word pairs. For example, if the concept is gender, the embeddings for (\"man\", \"woman\"), (\"king\", \"queen\"), (\"brother\", \"sister\"), etc. should be at the same positions in seed_embeddings1 and seed_embeddings2. Note All tensors are expected to be on the same device. Returns \u00b6 bias_direction : torch.Tensor A unit tensor of size (dim, ) representing the concept subspace. TwoMeansBiasDirection \u00b6 class TwoMeansBiasDirection ( BiasDirection ) Two-means bias direction. Computes one-dimensional subspace that is the span of a specific concept (e.g. gender) as the normalized difference vector of the averages of seed word embedding sets. Note It is uncommon to utilize more than one direction to represent a concept. Based on: Dev, S., & Phillips, J.M. (2019). Attenuating Bias in Word Vectors . AISTATS. Implementation and terminology based on Rathore, A., Dev, S., Phillips, J.M., Srikumar, V., Zheng, Y., Yeh, C.M., Wang, J., Zhang, W., & Wang, B. (2021). VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations . ArXiv, abs/2104.02797. __call__ \u00b6 class TwoMeansBiasDirection ( BiasDirection ): | ... | def __call__ ( | self , | seed_embeddings1 : torch . Tensor , | seed_embeddings2 : torch . Tensor | ) Parameters \u00b6 Note In the examples below, we treat gender identity as binary, which does not accurately characterize gender in real life. seed_embeddings1 : torch.Tensor A tensor of size (embeddings1_batch_size, ..., dim) containing seed word embeddings related to a specific concept group. For example, if the concept is gender, seed_embeddings1 could contain embeddings for linguistically masculine words, e.g. \"man\", \"king\", \"brother\", etc. seed_embeddings2 : torch.Tensor A tensor of size (embeddings2_batch_size, ..., dim) containing seed word embeddings related to a different group for the same concept. For example, seed_embeddings2 could contain embeddings for linguistically feminine words, , e.g. \"woman\", \"queen\", \"sister\", etc. Note seed_embeddings1 and seed_embeddings2 need NOT be the same size. Furthermore, the embeddings at the same positions in each of seed_embeddings1 and seed_embeddings2 are NOT expected to form seed word pairs. Note All tensors are expected to be on the same device. Returns \u00b6 bias_direction : torch.Tensor A unit tensor of size (dim, ) representing the concept subspace. ClassificationNormalBiasDirection \u00b6 class ClassificationNormalBiasDirection ( BiasDirection ): | def __init__ ( self ) Classification normal bias direction. Computes one-dimensional subspace that is the span of a specific concept (e.g. gender) as the direction perpendicular to the classification boundary of a linear support vector machine fit to classify seed word embedding sets. Note It is uncommon to utilize more than one direction to represent a concept. Based on: Ravfogel, S., Elazar, Y., Gonen, H., Twiton, M., & Goldberg, Y. (2020). Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection . ArXiv, abs/2004.07667. Implementation and terminology based on Rathore, A., Dev, S., Phillips, J.M., Srikumar, V., Zheng, Y., Yeh, C.M., Wang, J., Zhang, W., & Wang, B. (2021). VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations . ArXiv, abs/2104.02797. __call__ \u00b6 class ClassificationNormalBiasDirection ( BiasDirection ): | ... | def __call__ ( | self , | seed_embeddings1 : torch . Tensor , | seed_embeddings2 : torch . Tensor | ) Parameters \u00b6 Note In the examples below, we treat gender identity as binary, which does not accurately characterize gender in real life. seed_embeddings1 : torch.Tensor A tensor of size (embeddings1_batch_size, ..., dim) containing seed word embeddings related to a specific concept group. For example, if the concept is gender, seed_embeddings1 could contain embeddings for linguistically masculine words, e.g. \"man\", \"king\", \"brother\", etc. seed_embeddings2 : torch.Tensor A tensor of size (embeddings2_batch_size, ..., dim) containing seed word embeddings related to a different group for the same concept. For example, seed_embeddings2 could contain embeddings for linguistically feminine words, , e.g. \"woman\", \"queen\", \"sister\", etc. Note seed_embeddings1 and seed_embeddings2 need NOT be the same size. Furthermore, the embeddings at the same positions in each of seed_embeddings1 and seed_embeddings2 are NOT expected to form seed word pairs. Note All tensors are expected to be on the same device. Note This bias direction method is NOT differentiable. Returns \u00b6 bias_direction : torch.Tensor A unit tensor of size (dim, ) representing the concept subspace.","title":"bias_direction"},{"location":"api/fairness/bias_direction/#biasdirection","text":"class BiasDirection : | def __init__ ( self , requires_grad : bool = False ) Parent class for bias direction classes.","title":"BiasDirection"},{"location":"api/fairness/bias_direction/#pcabiasdirection","text":"class PCABiasDirection ( BiasDirection ) PCA-based bias direction. Computes one-dimensional subspace that is the span of a specific concept (e.g. gender) using PCA. This subspace minimizes the sum of squared distances from all seed word embeddings. Note It is uncommon to utilize more than one direction to represent a concept. Implementation and terminology based on Rathore, A., Dev, S., Phillips, J.M., Srikumar, V., Zheng, Y., Yeh, C.M., Wang, J., Zhang, W., & Wang, B. (2021). VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations . ArXiv, abs/2104.02797.","title":"PCABiasDirection"},{"location":"api/fairness/bias_direction/#__call__","text":"class PCABiasDirection ( BiasDirection ): | ... | def __call__ ( self , seed_embeddings : torch . Tensor )","title":"__call__"},{"location":"api/fairness/bias_direction/#pairedpcabiasdirection","text":"class PairedPCABiasDirection ( BiasDirection ) Paired-PCA-based bias direction. Computes one-dimensional subspace that is the span of a specific concept (e.g. gender) as the first principle component of the difference vectors between seed word embedding pairs. Note It is uncommon to utilize more than one direction to represent a concept. Based on: T. Bolukbasi, K. W. Chang, J. Zou, V. Saligrama, and A. Kalai. Man is to computer programmer as woman is to homemaker? debiasing word embeddings . In ACM Transactions of Information Systems, 2016. Implementation and terminology based on Rathore, A., Dev, S., Phillips, J.M., Srikumar, V., Zheng, Y., Yeh, C.M., Wang, J., Zhang, W., & Wang, B. (2021). VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations . ArXiv, abs/2104.02797.","title":"PairedPCABiasDirection"},{"location":"api/fairness/bias_direction/#__call___1","text":"class PairedPCABiasDirection ( BiasDirection ): | ... | def __call__ ( | self , | seed_embeddings1 : torch . Tensor , | seed_embeddings2 : torch . Tensor | )","title":"__call__"},{"location":"api/fairness/bias_direction/#twomeansbiasdirection","text":"class TwoMeansBiasDirection ( BiasDirection ) Two-means bias direction. Computes one-dimensional subspace that is the span of a specific concept (e.g. gender) as the normalized difference vector of the averages of seed word embedding sets. Note It is uncommon to utilize more than one direction to represent a concept. Based on: Dev, S., & Phillips, J.M. (2019). Attenuating Bias in Word Vectors . AISTATS. Implementation and terminology based on Rathore, A., Dev, S., Phillips, J.M., Srikumar, V., Zheng, Y., Yeh, C.M., Wang, J., Zhang, W., & Wang, B. (2021). VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations . ArXiv, abs/2104.02797.","title":"TwoMeansBiasDirection"},{"location":"api/fairness/bias_direction/#__call___2","text":"class TwoMeansBiasDirection ( BiasDirection ): | ... | def __call__ ( | self , | seed_embeddings1 : torch . Tensor , | seed_embeddings2 : torch . Tensor | )","title":"__call__"},{"location":"api/fairness/bias_direction/#classificationnormalbiasdirection","text":"class ClassificationNormalBiasDirection ( BiasDirection ): | def __init__ ( self ) Classification normal bias direction. Computes one-dimensional subspace that is the span of a specific concept (e.g. gender) as the direction perpendicular to the classification boundary of a linear support vector machine fit to classify seed word embedding sets. Note It is uncommon to utilize more than one direction to represent a concept. Based on: Ravfogel, S., Elazar, Y., Gonen, H., Twiton, M., & Goldberg, Y. (2020). Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection . ArXiv, abs/2004.07667. Implementation and terminology based on Rathore, A., Dev, S., Phillips, J.M., Srikumar, V., Zheng, Y., Yeh, C.M., Wang, J., Zhang, W., & Wang, B. (2021). VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations . ArXiv, abs/2104.02797.","title":"ClassificationNormalBiasDirection"},{"location":"api/fairness/bias_direction/#__call___3","text":"class ClassificationNormalBiasDirection ( BiasDirection ): | ... | def __call__ ( | self , | seed_embeddings1 : torch . Tensor , | seed_embeddings2 : torch . Tensor | )","title":"__call__"},{"location":"api/fairness/bias_direction_wrappers/","text":"allennlp .fairness .bias_direction_wrappers [SOURCE] BiasDirectionWrapper \u00b6 class BiasDirectionWrapper ( Registrable ): | def __init__ ( self ) Parent class for bias direction wrappers. __call__ \u00b6 class BiasDirectionWrapper ( Registrable ): | ... | def __call__ ( self , module ) train \u00b6 class BiasDirectionWrapper ( Registrable ): | ... | def train ( self , mode : bool = True ) Parameters \u00b6 mode : bool , optional (default = True ) Sets requires_grad to value of mode for bias direction. add_noise \u00b6 class BiasDirectionWrapper ( Registrable ): | ... | def add_noise ( self , t : torch . Tensor ) Parameters \u00b6 t : torch.Tensor Tensor to which to add small amount of Gaussian noise. PCABiasDirectionWrapper \u00b6 @BiasDirectionWrapper . register ( \"pca\" ) class PCABiasDirectionWrapper ( BiasDirectionWrapper ): | def __init__ ( | self , | seed_words_file : Union [ PathLike , str ], | tokenizer : Tokenizer , | direction_vocab : Optional [ Vocabulary ] = None , | namespace : str = \"tokens\" , | requires_grad : bool = False , | noise : float = 1e-10 | ) Parameters \u00b6 seed_words_file : Union[PathLike, str] Path of file containing seed words. tokenizer : Tokenizer Tokenizer used to tokenize seed words. direction_vocab : Vocabulary , optional (default = None ) Vocabulary of tokenizer. If None , assumes tokenizer is of type PreTrainedTokenizer and uses tokenizer's vocab attribute. namespace : str , optional (default = \"tokens\" ) Namespace of direction_vocab to use when tokenizing. Disregarded when direction_vocab is None . requires_grad : bool , optional (default = False ) Option to enable gradient calculation for bias direction. noise : float , optional (default = 1e-10 ) To avoid numerical instability if embeddings are initialized uniformly. __call__ \u00b6 class PCABiasDirectionWrapper ( BiasDirectionWrapper ): | ... | def __call__ ( self , module ) PairedPCABiasDirectionWrapper \u00b6 @BiasDirectionWrapper . register ( \"paired_pca\" ) class PairedPCABiasDirectionWrapper ( BiasDirectionWrapper ): | def __init__ ( | self , | seed_word_pairs_file : Union [ PathLike , str ], | tokenizer : Tokenizer , | direction_vocab : Optional [ Vocabulary ] = None , | namespace : str = \"tokens\" , | requires_grad : bool = False , | noise : float = 1e-10 | ) Parameters \u00b6 seed_word_pairs_file : Union[PathLike, str] Path of file containing seed word pairs. tokenizer : Tokenizer Tokenizer used to tokenize seed words. direction_vocab : Vocabulary , optional (default = None ) Vocabulary of tokenizer. If None , assumes tokenizer is of type PreTrainedTokenizer and uses tokenizer's vocab attribute. namespace : str , optional (default = \"tokens\" ) Namespace of direction_vocab to use when tokenizing. Disregarded when direction_vocab is None . requires_grad : bool , optional (default = False ) Option to enable gradient calculation for bias direction. noise : float , optional (default = 1e-10 ) To avoid numerical instability if embeddings are initialized uniformly. __call__ \u00b6 class PairedPCABiasDirectionWrapper ( BiasDirectionWrapper ): | ... | def __call__ ( self , module ) TwoMeansBiasDirectionWrapper \u00b6 @BiasDirectionWrapper . register ( \"two_means\" ) class TwoMeansBiasDirectionWrapper ( BiasDirectionWrapper ): | def __init__ ( | self , | seed_word_pairs_file : Union [ PathLike , str ], | tokenizer : Tokenizer , | direction_vocab : Optional [ Vocabulary ] = None , | namespace : str = \"tokens\" , | requires_grad : bool = False , | noise : float = 1e-10 | ) Parameters \u00b6 seed_word_pairs_file : Union[PathLike, str] Path of file containing seed word pairs. tokenizer : Tokenizer Tokenizer used to tokenize seed words. direction_vocab : Vocabulary , optional (default = None ) Vocabulary of tokenizer. If None , assumes tokenizer is of type PreTrainedTokenizer and uses tokenizer's vocab attribute. namespace : str , optional (default = \"tokens\" ) Namespace of direction_vocab to use when tokenizing. Disregarded when direction_vocab is None . requires_grad : bool , optional (default = False ) Option to enable gradient calculation for bias direction. noise : float , optional (default = 1e-10 ) To avoid numerical instability if embeddings are initialized uniformly. __call__ \u00b6 class TwoMeansBiasDirectionWrapper ( BiasDirectionWrapper ): | ... | def __call__ ( self , module ) ClassificationNormalBiasDirectionWrapper \u00b6 @BiasDirectionWrapper . register ( \"classification_normal\" ) class ClassificationNormalBiasDirectionWrapper ( BiasDirectionWrapper ): | def __init__ ( | self , | seed_word_pairs_file : Union [ PathLike , str ], | tokenizer : Tokenizer , | direction_vocab : Optional [ Vocabulary ] = None , | namespace : str = \"tokens\" , | noise : float = 1e-10 | ) Parameters \u00b6 seed_word_pairs_file : Union[PathLike, str] Path of file containing seed word pairs. tokenizer : Tokenizer Tokenizer used to tokenize seed words. direction_vocab : Vocabulary , optional (default = None ) Vocabulary of tokenizer. If None , assumes tokenizer is of type PreTrainedTokenizer and uses tokenizer's vocab attribute. namespace : str , optional (default = \"tokens\" ) Namespace of direction_vocab to use when tokenizing. Disregarded when direction_vocab is None . noise : float , optional (default = 1e-10 ) To avoid numerical instability if embeddings are initialized uniformly. __call__ \u00b6 class ClassificationNormalBiasDirectionWrapper ( BiasDirectionWrapper ): | ... | def __call__ ( self , module )","title":"bias_direction_wrappers"},{"location":"api/fairness/bias_direction_wrappers/#biasdirectionwrapper","text":"class BiasDirectionWrapper ( Registrable ): | def __init__ ( self ) Parent class for bias direction wrappers.","title":"BiasDirectionWrapper"},{"location":"api/fairness/bias_direction_wrappers/#__call__","text":"class BiasDirectionWrapper ( Registrable ): | ... | def __call__ ( self , module )","title":"__call__"},{"location":"api/fairness/bias_direction_wrappers/#train","text":"class BiasDirectionWrapper ( Registrable ): | ... | def train ( self , mode : bool = True )","title":"train"},{"location":"api/fairness/bias_direction_wrappers/#add_noise","text":"class BiasDirectionWrapper ( Registrable ): | ... | def add_noise ( self , t : torch . Tensor )","title":"add_noise"},{"location":"api/fairness/bias_direction_wrappers/#pcabiasdirectionwrapper","text":"@BiasDirectionWrapper . register ( \"pca\" ) class PCABiasDirectionWrapper ( BiasDirectionWrapper ): | def __init__ ( | self , | seed_words_file : Union [ PathLike , str ], | tokenizer : Tokenizer , | direction_vocab : Optional [ Vocabulary ] = None , | namespace : str = \"tokens\" , | requires_grad : bool = False , | noise : float = 1e-10 | )","title":"PCABiasDirectionWrapper"},{"location":"api/fairness/bias_direction_wrappers/#__call___1","text":"class PCABiasDirectionWrapper ( BiasDirectionWrapper ): | ... | def __call__ ( self , module )","title":"__call__"},{"location":"api/fairness/bias_direction_wrappers/#pairedpcabiasdirectionwrapper","text":"@BiasDirectionWrapper . register ( \"paired_pca\" ) class PairedPCABiasDirectionWrapper ( BiasDirectionWrapper ): | def __init__ ( | self , | seed_word_pairs_file : Union [ PathLike , str ], | tokenizer : Tokenizer , | direction_vocab : Optional [ Vocabulary ] = None , | namespace : str = \"tokens\" , | requires_grad : bool = False , | noise : float = 1e-10 | )","title":"PairedPCABiasDirectionWrapper"},{"location":"api/fairness/bias_direction_wrappers/#__call___2","text":"class PairedPCABiasDirectionWrapper ( BiasDirectionWrapper ): | ... | def __call__ ( self , module )","title":"__call__"},{"location":"api/fairness/bias_direction_wrappers/#twomeansbiasdirectionwrapper","text":"@BiasDirectionWrapper . register ( \"two_means\" ) class TwoMeansBiasDirectionWrapper ( BiasDirectionWrapper ): | def __init__ ( | self , | seed_word_pairs_file : Union [ PathLike , str ], | tokenizer : Tokenizer , | direction_vocab : Optional [ Vocabulary ] = None , | namespace : str = \"tokens\" , | requires_grad : bool = False , | noise : float = 1e-10 | )","title":"TwoMeansBiasDirectionWrapper"},{"location":"api/fairness/bias_direction_wrappers/#__call___3","text":"class TwoMeansBiasDirectionWrapper ( BiasDirectionWrapper ): | ... | def __call__ ( self , module )","title":"__call__"},{"location":"api/fairness/bias_direction_wrappers/#classificationnormalbiasdirectionwrapper","text":"@BiasDirectionWrapper . register ( \"classification_normal\" ) class ClassificationNormalBiasDirectionWrapper ( BiasDirectionWrapper ): | def __init__ ( | self , | seed_word_pairs_file : Union [ PathLike , str ], | tokenizer : Tokenizer , | direction_vocab : Optional [ Vocabulary ] = None , | namespace : str = \"tokens\" , | noise : float = 1e-10 | )","title":"ClassificationNormalBiasDirectionWrapper"},{"location":"api/fairness/bias_direction_wrappers/#__call___4","text":"class ClassificationNormalBiasDirectionWrapper ( BiasDirectionWrapper ): | ... | def __call__ ( self , module )","title":"__call__"},{"location":"api/fairness/bias_metrics/","text":"allennlp .fairness .bias_metrics [SOURCE] A suite of metrics to quantify how much bias is encoded by word embeddings and determine the effectiveness of bias mitigation. Bias metrics are based on: Caliskan, A., Bryson, J., & Narayanan, A. (2017). Semantics derived automatically from language corpora contain human-like biases . Science, 356, 183 - 186. Dev, S., & Phillips, J.M. (2019). Attenuating Bias in Word Vectors . AISTATS. Dev, S., Li, T., Phillips, J.M., & Srikumar, V. (2020). On Measuring and Mitigating Biased Inferences of Word Embeddings . ArXiv, abs/1908.09369. Rathore, A., Dev, S., Phillips, J.M., Srikumar, V., Zheng, Y., Yeh, C.M., Wang, J., Zhang, W., & Wang, B. (2021). VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations . ArXiv, abs/2104.02797. Aka, O.; Burke, K.; B\u00e4uerle, A.; Greer, C.; and Mitchell, M. 2021. Measuring model biases in the absence of ground truth . arXiv preprint arXiv:2103.03417. WordEmbeddingAssociationTest \u00b6 class WordEmbeddingAssociationTest Word Embedding Association Test (WEAT) score measures the unlikelihood there is no difference between two sets of target words in terms of their relative similarity to two sets of attribute words by computing the probability that a random permutation of attribute words would produce the observed (or greater) difference in sample means. Analog of Implicit Association Test from psychology for word embeddings. Based on: Caliskan, A., Bryson, J., & Narayanan, A. (2017). Semantics derived automatically from language corpora contain human-like biases . Science, 356, 183 - 186. __call__ \u00b6 class WordEmbeddingAssociationTest : | ... | def __call__ ( | self , | target_embeddings1 : torch . Tensor , | target_embeddings2 : torch . Tensor , | attribute_embeddings1 : torch . Tensor , | attribute_embeddings2 : torch . Tensor | ) -> torch . FloatTensor Parameters \u00b6 Note In the examples below, we treat gender identity as binary, which does not accurately characterize gender in real life. target_embeddings1 : torch.Tensor A tensor of size (target_embeddings_batch_size, ..., dim) containing target word embeddings related to a concept group. For example, if the concept is gender, target_embeddings1 could contain embeddings for linguistically masculine words, e.g. \"man\", \"king\", \"brother\", etc. Represented as X. target_embeddings2 : torch.Tensor A tensor of the same size as target_embeddings1 containing target word embeddings related to a different group for the same concept. For example, target_embeddings2 could contain embeddings for linguistically feminine words, e.g. \"woman\", \"queen\", \"sister\", etc. Represented as Y. attribute_embeddings1 : torch.Tensor A tensor of size (attribute_embeddings1_batch_size, ..., dim) containing attribute word embeddings related to a concept group associated with the concept group for target_embeddings1. For example, if the concept is professions, attribute_embeddings1 could contain embeddings for stereotypically male professions, e.g. \"doctor\", \"banker\", \"engineer\", etc. Represented as A. attribute_embeddings2 : torch.Tensor A tensor of size (attribute_embeddings2_batch_size, ..., dim) containing attribute word embeddings related to a concept group associated with the concept group for target_embeddings2. For example, if the concept is professions, attribute_embeddings2 could contain embeddings for stereotypically female professions, e.g. \"nurse\", \"receptionist\", \"homemaker\", etc. Represented as B. Note While target_embeddings1 and target_embeddings2 must be the same size, attribute_embeddings1 and attribute_embeddings2 need not be the same size. Returns \u00b6 weat_score : torch.FloatTensor The unlikelihood there is no difference between target_embeddings1 and target_embeddings2 in terms of their relative similarity to attribute_embeddings1 and attribute_embeddings2. Typical values are around [-1, 1], with values closer to 0 indicating less biased associations. EmbeddingCoherenceTest \u00b6 class EmbeddingCoherenceTest Embedding Coherence Test (ECT) score measures if groups of words have stereotypical associations by computing the Spearman Coefficient of lists of attribute embeddings sorted based on their similarity to target embeddings. Based on: Dev, S., & Phillips, J.M. (2019). Attenuating Bias in Word Vectors . AISTATS. __call__ \u00b6 class EmbeddingCoherenceTest : | ... | def __call__ ( | self , | target_embeddings1 : torch . Tensor , | target_embeddings2 : torch . Tensor , | attribute_embeddings : torch . Tensor | ) -> torch . FloatTensor Parameters \u00b6 Note In the examples below, we treat gender identity as binary, which does not accurately characterize gender in real life. target_embeddings1 : torch.Tensor A tensor of size (target_embeddings_batch_size, ..., dim) containing target word embeddings related to a concept group. For example, if the concept is gender, target_embeddings1 could contain embeddings for linguistically masculine words, e.g. \"man\", \"king\", \"brother\", etc. Represented as X. target_embeddings2 : torch.Tensor A tensor of the same size as target_embeddings1 containing target word embeddings related to a different group for the same concept. For example, target_embeddings2 could contain embeddings for linguistically feminine words, e.g. \"woman\", \"queen\", \"sister\", etc. Represented as Y. attribute_embeddings : torch.Tensor A tensor of size (attribute_embeddings_batch_size, ..., dim) containing attribute word embeddings related to a concept associated with target_embeddings1 and target_embeddings2. For example, if the concept is professions, attribute_embeddings could contain embeddings for \"doctor\", \"banker\", \"engineer\", etc. Represented as AB. Returns \u00b6 ect_score : torch.FloatTensor The Spearman Coefficient measuring the similarity of lists of attribute embeddings sorted based on their similarity to the target embeddings. Ranges from [-1, 1], with values closer to 1 indicating less biased associations. spearman_correlation \u00b6 class EmbeddingCoherenceTest : | ... | def spearman_correlation ( self , x : torch . Tensor , y : torch . Tensor ) NaturalLanguageInference \u00b6 @Metric . register ( \"nli\" ) class NaturalLanguageInference ( Metric ): | def __init__ ( | self , | neutral_label : int = 2 , | taus : List [ float ] = [ 0.5 , 0.7 ] | ) Natural language inference scores measure the effect biased associations have on decisions made downstream, given neutrally-constructed pairs of sentences differing only in the subject. Net Neutral (NN): The average probability of the neutral label across all sentence pairs. Fraction Neutral (FN): The fraction of sentence pairs predicted neutral. Threshold:tau (T:tau): A parameterized measure that reports the fraction of examples whose probability of neutral is above tau. Parameters \u00b6 neutral_label : int , optional (default = 2 ) The discrete integer label corresponding to a neutral entailment prediction. taus : List[float] , optional (default = [0.5, 0.7] ) All the taus for which to compute Threshold:tau. Based on: Dev, S., Li, T., Phillips, J.M., & Srikumar, V. (2020). On Measuring and Mitigating Biased Inferences of Word Embeddings . ArXiv, abs/1908.09369. __call__ \u00b6 class NaturalLanguageInference ( Metric ): | ... | def __call__ ( self , nli_probabilities : torch . Tensor ) -> None Parameters \u00b6 Note In the examples below, we treat gender identity as binary, which does not accurately characterize gender in real life. nli_probabilities : torch.Tensor A tensor of size (batch_size, ..., 3) containing natural language inference (i.e. entailment, contradiction, and neutral) probabilities for neutrally-constructed pairs of sentences differing only in the subject. For example, if the concept is gender, nli_probabilities could contain the natural language inference probabilities of: \"The driver owns a cabinet.\" -> \"The man owns a cabinet.\" \"The driver owns a cabinet.\" -> \"The woman owns a cabinet.\" \"The doctor eats an apple.\" -> \"The man eats an apple.\" \"The doctor eats an apple.\" -> \"The woman eats an apple.\" get_metric \u00b6 class NaturalLanguageInference ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns \u00b6 nli_scores : Dict[str, float] Contains the following keys: \" net_neutral \" : The average probability of the neutral label across all sentence pairs. A value closer to 1 suggests lower bias, as bias will result in a higher probability of entailment or contradiction. \" fraction_neutral \" : The fraction of sentence pairs predicted neutral. A value closer to 1 suggests lower bias, as bias will result in a higher probability of entailment or contradiction. \" threshold_{taus} \" : For each tau, the fraction of examples whose probability of neutral is above tau. For each tau, a value closer to 1 suggests lower bias, as bias will result in a higher probability of entailment or contradiction. reset \u00b6 class NaturalLanguageInference ( Metric ): | ... | def reset ( self ) AssociationWithoutGroundTruth \u00b6 @Metric . register ( \"association_without_ground_truth\" ) class AssociationWithoutGroundTruth ( Metric ): | def __init__ ( | self , | num_classes : int , | num_protected_variable_labels : int , | association_metric : str = \"npmixy\" , | gap_type : str = \"ova\" | ) -> None Association without ground truth, from: Aka, O.; Burke, K.; B\u00e4uerle, A.; Greer, C.; and Mitchell, M. 2021. Measuring model biases in the absence of ground truth. arXiv preprint arXiv:2103.03417. Parameters \u00b6 num_classes : int Number of classes. num_protected_variable_labels : int Number of protected variable labels. association_metric : str , optional (default = \"npmixy\" ) A generic association metric A(x, y), where x is an identity label and y is any other label. Examples include: nPMIxy ( \"npmixy\" ), nPMIy ( \"npmiy\" ), PMI^2 ( \"pmisq\" ), PMI ( \"pmi\" ) Empirically, nPMIxy and nPMIy are more capable of capturing labels across a range of marginal frequencies. gap_type : str , optional (default = \"ova\" ) Either one-vs-all ( \"ova\" ) or pairwise ( \"pairwise\" ). One-vs-all gap is equivalent to A(x, y) - E[A(x', y)], where x' is in the set of all protected variable labels setminus {x}. Pairwise gaps are A(x, y) - A(x', y), for all x' in the set of all protected variable labels setminus {x}. Note Assumes integer predictions, with each item to be classified having a single correct class. __call__ \u00b6 class AssociationWithoutGroundTruth ( Metric ): | ... | def __call__ ( | self , | predicted_labels : torch . Tensor , | protected_variable_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) -> None Parameters \u00b6 predicted_labels : torch.Tensor A tensor of predicted integer class labels of shape (batch_size, ...). Represented as Y. protected_variable_labels : torch.Tensor A tensor of integer protected variable labels of shape (batch_size, ...). It must be the same shape as the predicted_labels tensor. Represented as X. mask : torch.BoolTensor , optional (default = None ) A tensor of the same shape as predicted_labels . Note All tensors are expected to be on the same device. get_metric \u00b6 class AssociationWithoutGroundTruth ( Metric ): | ... | def get_metric ( | self , | reset : bool = False | ) -> Dict [ int , Union [ torch . Tensor , Dict [ int , torch . Tensor ]]] Returns \u00b6 gaps : Dict[int, Union[torch.FloatTensor, Dict[int, torch.FloatTensor]]] A dictionary mapping each protected variable label x to either: a tensor of the one-vs-all gaps (where the gap corresponding to prediction label i is at index i), another dictionary mapping protected variable labels x' to a tensor of the pairwise gaps (where the gap corresponding to prediction label i is at index i). A gap of nearly 0 implies fairness on the basis of Association in the Absence of Ground Truth. Note If a possible class label is not present in Y, the expected behavior is that the gaps corresponding to this class label are NaN. If a possible (class label, protected variable label) pair is not present in the joint of Y and X, the expected behavior is that the gap corresponding to this (class label, protected variable label) pair is NaN. reset \u00b6 class AssociationWithoutGroundTruth ( Metric ): | ... | def reset ( self ) -> None","title":"bias_metrics"},{"location":"api/fairness/bias_metrics/#wordembeddingassociationtest","text":"class WordEmbeddingAssociationTest Word Embedding Association Test (WEAT) score measures the unlikelihood there is no difference between two sets of target words in terms of their relative similarity to two sets of attribute words by computing the probability that a random permutation of attribute words would produce the observed (or greater) difference in sample means. Analog of Implicit Association Test from psychology for word embeddings. Based on: Caliskan, A., Bryson, J., & Narayanan, A. (2017). Semantics derived automatically from language corpora contain human-like biases . Science, 356, 183 - 186.","title":"WordEmbeddingAssociationTest"},{"location":"api/fairness/bias_metrics/#__call__","text":"class WordEmbeddingAssociationTest : | ... | def __call__ ( | self , | target_embeddings1 : torch . Tensor , | target_embeddings2 : torch . Tensor , | attribute_embeddings1 : torch . Tensor , | attribute_embeddings2 : torch . Tensor | ) -> torch . FloatTensor","title":"__call__"},{"location":"api/fairness/bias_metrics/#embeddingcoherencetest","text":"class EmbeddingCoherenceTest Embedding Coherence Test (ECT) score measures if groups of words have stereotypical associations by computing the Spearman Coefficient of lists of attribute embeddings sorted based on their similarity to target embeddings. Based on: Dev, S., & Phillips, J.M. (2019). Attenuating Bias in Word Vectors . AISTATS.","title":"EmbeddingCoherenceTest"},{"location":"api/fairness/bias_metrics/#__call___1","text":"class EmbeddingCoherenceTest : | ... | def __call__ ( | self , | target_embeddings1 : torch . Tensor , | target_embeddings2 : torch . Tensor , | attribute_embeddings : torch . Tensor | ) -> torch . FloatTensor","title":"__call__"},{"location":"api/fairness/bias_metrics/#spearman_correlation","text":"class EmbeddingCoherenceTest : | ... | def spearman_correlation ( self , x : torch . Tensor , y : torch . Tensor )","title":"spearman_correlation"},{"location":"api/fairness/bias_metrics/#naturallanguageinference","text":"@Metric . register ( \"nli\" ) class NaturalLanguageInference ( Metric ): | def __init__ ( | self , | neutral_label : int = 2 , | taus : List [ float ] = [ 0.5 , 0.7 ] | ) Natural language inference scores measure the effect biased associations have on decisions made downstream, given neutrally-constructed pairs of sentences differing only in the subject. Net Neutral (NN): The average probability of the neutral label across all sentence pairs. Fraction Neutral (FN): The fraction of sentence pairs predicted neutral. Threshold:tau (T:tau): A parameterized measure that reports the fraction of examples whose probability of neutral is above tau.","title":"NaturalLanguageInference"},{"location":"api/fairness/bias_metrics/#__call___2","text":"class NaturalLanguageInference ( Metric ): | ... | def __call__ ( self , nli_probabilities : torch . Tensor ) -> None","title":"__call__"},{"location":"api/fairness/bias_metrics/#get_metric","text":"class NaturalLanguageInference ( Metric ): | ... | def get_metric ( self , reset : bool = False )","title":"get_metric"},{"location":"api/fairness/bias_metrics/#reset","text":"class NaturalLanguageInference ( Metric ): | ... | def reset ( self )","title":"reset"},{"location":"api/fairness/bias_metrics/#associationwithoutgroundtruth","text":"@Metric . register ( \"association_without_ground_truth\" ) class AssociationWithoutGroundTruth ( Metric ): | def __init__ ( | self , | num_classes : int , | num_protected_variable_labels : int , | association_metric : str = \"npmixy\" , | gap_type : str = \"ova\" | ) -> None Association without ground truth, from: Aka, O.; Burke, K.; B\u00e4uerle, A.; Greer, C.; and Mitchell, M. 2021. Measuring model biases in the absence of ground truth. arXiv preprint arXiv:2103.03417.","title":"AssociationWithoutGroundTruth"},{"location":"api/fairness/bias_metrics/#__call___3","text":"class AssociationWithoutGroundTruth ( Metric ): | ... | def __call__ ( | self , | predicted_labels : torch . Tensor , | protected_variable_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) -> None","title":"__call__"},{"location":"api/fairness/bias_metrics/#get_metric_1","text":"class AssociationWithoutGroundTruth ( Metric ): | ... | def get_metric ( | self , | reset : bool = False | ) -> Dict [ int , Union [ torch . Tensor , Dict [ int , torch . Tensor ]]]","title":"get_metric"},{"location":"api/fairness/bias_metrics/#reset_1","text":"class AssociationWithoutGroundTruth ( Metric ): | ... | def reset ( self ) -> None","title":"reset"},{"location":"api/fairness/bias_mitigator_applicator/","text":"allennlp .fairness .bias_mitigator_applicator [SOURCE] A Model wrapper to mitigate biases in contextual embeddings during finetuning on a downstream task and test time. Based on: Dev, S., Li, T., Phillips, J.M., & Srikumar, V. (2020). On Measuring and Mitigating Biased Inferences of Word Embeddings . ArXiv, abs/1908.09369. BiasMitigatorApplicator \u00b6 @Model . register ( \"bias_mitigator_applicator\" ) class BiasMitigatorApplicator ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | base_model : Model , | bias_mitigator : Lazy [ BiasMitigatorWrapper ], | ** kwargs | ) Wrapper class to apply bias mitigation to any pretrained Model. Parameters \u00b6 vocab : Vocabulary Vocabulary of base model. base_model : Model Base model for which to mitigate biases. bias_mitigator : Lazy[BiasMitigatorWrapper] Bias mitigator to apply to base model. train \u00b6 class BiasMitigatorApplicator ( Model ): | ... | def train ( self , mode : bool = True ) forward \u00b6 class BiasMitigatorApplicator ( Model ): | ... | def forward ( self , * args , ** kwargs ) forward_on_instance \u00b6 class BiasMitigatorApplicator ( Model ): | ... | def forward_on_instance ( self , * args , ** kwargs ) forward_on_instances \u00b6 class BiasMitigatorApplicator ( Model ): | ... | def forward_on_instances ( self , * args , ** kwargs ) get_regularization_penalty \u00b6 class BiasMitigatorApplicator ( Model ): | ... | def get_regularization_penalty ( self , * args , ** kwargs ) get_parameters_for_histogram_logging \u00b6 class BiasMitigatorApplicator ( Model ): | ... | def get_parameters_for_histogram_logging ( self , * args , ** kwargs ) get_parameters_for_histogram_tensorboard_logging \u00b6 class BiasMitigatorApplicator ( Model ): | ... | def get_parameters_for_histogram_tensorboard_logging ( | self , | * args , | ** kwargs | ) make_output_human_readable \u00b6 class BiasMitigatorApplicator ( Model ): | ... | def make_output_human_readable ( self , * args , ** kwargs ) get_metrics \u00b6 class BiasMitigatorApplicator ( Model ): | ... | def get_metrics ( self , * args , ** kwargs ) extend_embedder_vocab \u00b6 class BiasMitigatorApplicator ( Model ): | ... | def extend_embedder_vocab ( self , * args , ** kwargs )","title":"bias_mitigator_applicator"},{"location":"api/fairness/bias_mitigator_applicator/#biasmitigatorapplicator","text":"@Model . register ( \"bias_mitigator_applicator\" ) class BiasMitigatorApplicator ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | base_model : Model , | bias_mitigator : Lazy [ BiasMitigatorWrapper ], | ** kwargs | ) Wrapper class to apply bias mitigation to any pretrained Model.","title":"BiasMitigatorApplicator"},{"location":"api/fairness/bias_mitigator_applicator/#train","text":"class BiasMitigatorApplicator ( Model ): | ... | def train ( self , mode : bool = True )","title":"train"},{"location":"api/fairness/bias_mitigator_applicator/#forward","text":"class BiasMitigatorApplicator ( Model ): | ... | def forward ( self , * args , ** kwargs )","title":"forward"},{"location":"api/fairness/bias_mitigator_applicator/#forward_on_instance","text":"class BiasMitigatorApplicator ( Model ): | ... | def forward_on_instance ( self , * args , ** kwargs )","title":"forward_on_instance"},{"location":"api/fairness/bias_mitigator_applicator/#forward_on_instances","text":"class BiasMitigatorApplicator ( Model ): | ... | def forward_on_instances ( self , * args , ** kwargs )","title":"forward_on_instances"},{"location":"api/fairness/bias_mitigator_applicator/#get_regularization_penalty","text":"class BiasMitigatorApplicator ( Model ): | ... | def get_regularization_penalty ( self , * args , ** kwargs )","title":"get_regularization_penalty"},{"location":"api/fairness/bias_mitigator_applicator/#get_parameters_for_histogram_logging","text":"class BiasMitigatorApplicator ( Model ): | ... | def get_parameters_for_histogram_logging ( self , * args , ** kwargs )","title":"get_parameters_for_histogram_logging"},{"location":"api/fairness/bias_mitigator_applicator/#get_parameters_for_histogram_tensorboard_logging","text":"class BiasMitigatorApplicator ( Model ): | ... | def get_parameters_for_histogram_tensorboard_logging ( | self , | * args , | ** kwargs | )","title":"get_parameters_for_histogram_tensorboard_logging"},{"location":"api/fairness/bias_mitigator_applicator/#make_output_human_readable","text":"class BiasMitigatorApplicator ( Model ): | ... | def make_output_human_readable ( self , * args , ** kwargs )","title":"make_output_human_readable"},{"location":"api/fairness/bias_mitigator_applicator/#get_metrics","text":"class BiasMitigatorApplicator ( Model ): | ... | def get_metrics ( self , * args , ** kwargs )","title":"get_metrics"},{"location":"api/fairness/bias_mitigator_applicator/#extend_embedder_vocab","text":"class BiasMitigatorApplicator ( Model ): | ... | def extend_embedder_vocab ( self , * args , ** kwargs )","title":"extend_embedder_vocab"},{"location":"api/fairness/bias_mitigator_wrappers/","text":"allennlp .fairness .bias_mitigator_wrappers [SOURCE] BiasMitigatorWrapper \u00b6 class BiasMitigatorWrapper ( Registrable ) Parent class for bias mitigator wrappers. train \u00b6 class BiasMitigatorWrapper ( Registrable ): | ... | def train ( self , mode : bool = True ) Parameters \u00b6 mode : bool , optional (default = True ) Sets requires_grad to value of mode for bias mitigator and associated bias direction. HardBiasMitigatorWrapper \u00b6 @BiasMitigatorWrapper . register ( \"hard\" ) class HardBiasMitigatorWrapper ( BiasMitigatorWrapper ): | def __init__ ( | self , | bias_direction : BiasDirectionWrapper , | embedding_layer : torch . nn . Embedding , | equalize_word_pairs_file : Union [ PathLike , str ], | tokenizer : Tokenizer , | mitigator_vocab : Optional [ Vocabulary ] = None , | namespace : str = \"tokens\" , | requires_grad : bool = True | ) Parameters \u00b6 bias_direction : BiasDirectionWrapper Bias direction used by mitigator. embedding_layer : torch.nn.Embedding Embedding layer of base model. equalize_word_pairs_file : Union[PathLike, str] Path of file containing equalize word pairs. tokenizer : Tokenizer Tokenizer used to tokenize equalize words. mitigator_vocab : Vocabulary , optional (default = None ) Vocabulary of tokenizer. If None , assumes tokenizer is of type PreTrainedTokenizer and uses tokenizer's vocab attribute. namespace : str , optional (default = \"tokens\" ) Namespace of mitigator_vocab to use when tokenizing. Disregarded when mitigator_vocab is None . requires_grad : bool , optional (default = True ) Option to enable gradient calculation for bias mitigator. __call__ \u00b6 class HardBiasMitigatorWrapper ( BiasMitigatorWrapper ): | ... | def __call__ ( self , module , module_in , module_out ) Called as forward hook. train \u00b6 class HardBiasMitigatorWrapper ( BiasMitigatorWrapper ): | ... | def train ( self , mode : bool = True ) LinearBiasMitigatorWrapper \u00b6 @BiasMitigatorWrapper . register ( \"linear\" ) class LinearBiasMitigatorWrapper ( BiasMitigatorWrapper ): | def __init__ ( | self , | bias_direction : BiasDirectionWrapper , | embedding_layer : torch . nn . Embedding , | requires_grad : bool = True | ) Parameters \u00b6 bias_direction : BiasDirectionWrapper Bias direction used by mitigator. embedding_layer : torch.nn.Embedding Embedding layer of base model. requires_grad : bool , optional (default = True ) Option to enable gradient calculation for bias mitigator. __call__ \u00b6 class LinearBiasMitigatorWrapper ( BiasMitigatorWrapper ): | ... | def __call__ ( self , module , module_in , module_out ) Called as forward hook. train \u00b6 class LinearBiasMitigatorWrapper ( BiasMitigatorWrapper ): | ... | def train ( self , mode : bool = True ) INLPBiasMitigatorWrapper \u00b6 @BiasMitigatorWrapper . register ( \"inlp\" ) class INLPBiasMitigatorWrapper ( BiasMitigatorWrapper ): | def __init__ ( | self , | embedding_layer : torch . nn . Embedding , | seed_word_pairs_file : Union [ PathLike , str ], | tokenizer : Tokenizer , | mitigator_vocab : Optional [ Vocabulary ] = None , | namespace : str = \"tokens\" | ) Parameters \u00b6 embedding_layer : torch.nn.Embedding Embedding layer of base model. seed_word_pairs_file : Union[PathLike, str] Path of file containing seed word pairs. tokenizer : Tokenizer Tokenizer used to tokenize seed words. mitigator_vocab : Vocabulary , optional (default = None ) Vocabulary of tokenizer. If None , assumes tokenizer is of type PreTrainedTokenizer and uses tokenizer's vocab attribute. namespace : str , optional (default = \"tokens\" ) Namespace of mitigator_vocab to use when tokenizing. Disregarded when mitigator_vocab is None . __call__ \u00b6 class INLPBiasMitigatorWrapper ( BiasMitigatorWrapper ): | ... | def __call__ ( self , module , module_in , module_out ) Called as forward hook. train \u00b6 class INLPBiasMitigatorWrapper ( BiasMitigatorWrapper ): | ... | def train ( self , mode : bool = True ) OSCaRBiasMitigatorWrapper \u00b6 @BiasMitigatorWrapper . register ( \"oscar\" ) class OSCaRBiasMitigatorWrapper ( BiasMitigatorWrapper ): | def __init__ ( | self , | bias_direction1 : BiasDirectionWrapper , | bias_direction2 : BiasDirectionWrapper , | embedding_layer : torch . nn . Embedding , | requires_grad : bool = True | ) Parameters \u00b6 bias_direction1 : BiasDirectionWrapper Bias direction of first concept subspace used by mitigator. bias_direction2 : BiasDirectionWrapper Bias direction of second concept subspace used by mitigator. embedding_layer : torch.nn.Embedding Embedding layer of base model. requires_grad : bool , optional (default = True ) Option to enable gradient calculation for bias mitigator. __call__ \u00b6 class OSCaRBiasMitigatorWrapper ( BiasMitigatorWrapper ): | ... | def __call__ ( self , module , module_in , module_out ) Called as forward hook. train \u00b6 class OSCaRBiasMitigatorWrapper ( BiasMitigatorWrapper ): | ... | def train ( self , mode : bool = True )","title":"bias_mitigator_wrappers"},{"location":"api/fairness/bias_mitigator_wrappers/#biasmitigatorwrapper","text":"class BiasMitigatorWrapper ( Registrable ) Parent class for bias mitigator wrappers.","title":"BiasMitigatorWrapper"},{"location":"api/fairness/bias_mitigator_wrappers/#train","text":"class BiasMitigatorWrapper ( Registrable ): | ... | def train ( self , mode : bool = True )","title":"train"},{"location":"api/fairness/bias_mitigator_wrappers/#hardbiasmitigatorwrapper","text":"@BiasMitigatorWrapper . register ( \"hard\" ) class HardBiasMitigatorWrapper ( BiasMitigatorWrapper ): | def __init__ ( | self , | bias_direction : BiasDirectionWrapper , | embedding_layer : torch . nn . Embedding , | equalize_word_pairs_file : Union [ PathLike , str ], | tokenizer : Tokenizer , | mitigator_vocab : Optional [ Vocabulary ] = None , | namespace : str = \"tokens\" , | requires_grad : bool = True | )","title":"HardBiasMitigatorWrapper"},{"location":"api/fairness/bias_mitigator_wrappers/#__call__","text":"class HardBiasMitigatorWrapper ( BiasMitigatorWrapper ): | ... | def __call__ ( self , module , module_in , module_out ) Called as forward hook.","title":"__call__"},{"location":"api/fairness/bias_mitigator_wrappers/#train_1","text":"class HardBiasMitigatorWrapper ( BiasMitigatorWrapper ): | ... | def train ( self , mode : bool = True )","title":"train"},{"location":"api/fairness/bias_mitigator_wrappers/#linearbiasmitigatorwrapper","text":"@BiasMitigatorWrapper . register ( \"linear\" ) class LinearBiasMitigatorWrapper ( BiasMitigatorWrapper ): | def __init__ ( | self , | bias_direction : BiasDirectionWrapper , | embedding_layer : torch . nn . Embedding , | requires_grad : bool = True | )","title":"LinearBiasMitigatorWrapper"},{"location":"api/fairness/bias_mitigator_wrappers/#__call___1","text":"class LinearBiasMitigatorWrapper ( BiasMitigatorWrapper ): | ... | def __call__ ( self , module , module_in , module_out ) Called as forward hook.","title":"__call__"},{"location":"api/fairness/bias_mitigator_wrappers/#train_2","text":"class LinearBiasMitigatorWrapper ( BiasMitigatorWrapper ): | ... | def train ( self , mode : bool = True )","title":"train"},{"location":"api/fairness/bias_mitigator_wrappers/#inlpbiasmitigatorwrapper","text":"@BiasMitigatorWrapper . register ( \"inlp\" ) class INLPBiasMitigatorWrapper ( BiasMitigatorWrapper ): | def __init__ ( | self , | embedding_layer : torch . nn . Embedding , | seed_word_pairs_file : Union [ PathLike , str ], | tokenizer : Tokenizer , | mitigator_vocab : Optional [ Vocabulary ] = None , | namespace : str = \"tokens\" | )","title":"INLPBiasMitigatorWrapper"},{"location":"api/fairness/bias_mitigator_wrappers/#__call___2","text":"class INLPBiasMitigatorWrapper ( BiasMitigatorWrapper ): | ... | def __call__ ( self , module , module_in , module_out ) Called as forward hook.","title":"__call__"},{"location":"api/fairness/bias_mitigator_wrappers/#train_3","text":"class INLPBiasMitigatorWrapper ( BiasMitigatorWrapper ): | ... | def train ( self , mode : bool = True )","title":"train"},{"location":"api/fairness/bias_mitigator_wrappers/#oscarbiasmitigatorwrapper","text":"@BiasMitigatorWrapper . register ( \"oscar\" ) class OSCaRBiasMitigatorWrapper ( BiasMitigatorWrapper ): | def __init__ ( | self , | bias_direction1 : BiasDirectionWrapper , | bias_direction2 : BiasDirectionWrapper , | embedding_layer : torch . nn . Embedding , | requires_grad : bool = True | )","title":"OSCaRBiasMitigatorWrapper"},{"location":"api/fairness/bias_mitigator_wrappers/#__call___3","text":"class OSCaRBiasMitigatorWrapper ( BiasMitigatorWrapper ): | ... | def __call__ ( self , module , module_in , module_out ) Called as forward hook.","title":"__call__"},{"location":"api/fairness/bias_mitigator_wrappers/#train_4","text":"class OSCaRBiasMitigatorWrapper ( BiasMitigatorWrapper ): | ... | def train ( self , mode : bool = True )","title":"train"},{"location":"api/fairness/bias_mitigators/","text":"allennlp .fairness .bias_mitigators [SOURCE] A suite of differentiable methods to mitigate biases for binary concepts in embeddings. BiasMitigator \u00b6 class BiasMitigator : | def __init__ ( self , requires_grad : bool = False ) Parent class for bias mitigator classes. Parameters \u00b6 requires_grad : bool , optional (default = False ) Option to enable gradient calculation. HardBiasMitigator \u00b6 class HardBiasMitigator ( BiasMitigator ) Hard bias mitigator. Mitigates bias in embeddings by: Neutralizing: ensuring protected variable-neutral words remain equidistant from the bias direction by removing component of embeddings in the bias direction. Equalizing: ensuring that protected variable-related words are averaged out to have the same norm. Note For a detailed walkthrough and visual descriptions of the steps, please refer to Figure 4 in VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations . Based on: T. Bolukbasi, K. W. Chang, J. Zou, V. Saligrama, and A. Kalai. Man is to computer programmer as woman is to homemaker? debiasing word embeddings . In ACM Transactions of Information Systems, 2016. Description taken from: Goenka, D. (2020). Tackling Gender Bias in Word Embeddings . Implementation and terminology based on Rathore, A., Dev, S., Phillips, J.M., Srikumar, V., Zheng, Y., Yeh, C.M., Wang, J., Zhang, W., & Wang, B. (2021). VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations . ArXiv, abs/2104.02797. __call__ \u00b6 class HardBiasMitigator ( BiasMitigator ): | ... | def __call__ ( | self , | evaluation_embeddings : torch . Tensor , | bias_direction : torch . Tensor , | equalize_embeddings1 : torch . Tensor , | equalize_embeddings2 : torch . Tensor | ) Note In the examples below, we treat gender identity as binary, which does not accurately characterize gender in real life. Parameters \u00b6 evaluation_embeddings : torch.Tensor A tensor of size (evaluation_batch_size, ..., dim) of embeddings for which to mitigate bias. bias_direction : torch.Tensor A unit tensor of size (dim, ) representing the concept subspace. The words that are used to define the bias direction are considered definitionally gendered and not modified. equalize_embeddings1 : torch.Tensor A tensor of size (equalize_batch_size, ..., dim) containing equalize word embeddings related to a group from the concept represented by bias_direction. For example, if the concept is gender, equalize_embeddings1 could contain embeddings for \"boy\", \"man\", \"dad\", \"brother\", etc. equalize_embeddings2 : torch.Tensor A tensor of size (equalize_batch_size, ..., dim) containing equalize word embeddings related to a different group for the same concept. For example, equalize_embeddings2 could contain embeddings for \"girl\", \"woman\", \"mom\", \"sister\", etc. Note The embeddings at the same positions in each of equalize_embeddings1 and equalize_embeddings2 are expected to form equalize word pairs. For example, if the concept is gender, the embeddings for (\"boy\", \"girl\"), (\"man\", \"woman\"), (\"dad\", \"mom\"), (\"brother\", \"sister\"), etc. should be at the same positions in equalize_embeddings1 and equalize_embeddings2. Note evaluation_embeddings, equalize_embeddings1, and equalize_embeddings2 must have same size except for 0th dim (i.e. batch dimension). Note Please ensure that the words in evaluation_embeddings, equalize_embeddings1, and equalize_embeddings2 and those used to compute bias_direction are disjoint. Note All tensors are expected to be on the same device. Returns \u00b6 bias_mitigated_embeddings : torch.Tensor A tensor of the same size as evaluation_embeddings, equalize_embeddings1, and equalize_embeddings2 (in this order) stacked. LinearBiasMitigator \u00b6 class LinearBiasMitigator ( BiasMitigator ) Linear bias mitigator. Mitigates bias in embeddings by removing component in the bias direction. Note For a detailed walkthrough and visual descriptions of the steps, please refer to Figure 3 in VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations . Based on: S. Dev and J. M. Phillips. Attenuating bias in word vectors . In International Conference on Artificial Intelligence and Statistics, Proceedings of Machine Learning Research, pages 879\u2013887. PMLR, 2019. Implementation and terminology based on Rathore, A., Dev, S., Phillips, J.M., Srikumar, V., Zheng, Y., Yeh, C.M., Wang, J., Zhang, W., & Wang, B. (2021). VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations . ArXiv, abs/2104.02797. __call__ \u00b6 class LinearBiasMitigator ( BiasMitigator ): | ... | def __call__ ( | self , | evaluation_embeddings : torch . Tensor , | bias_direction : torch . Tensor | ) Note In the examples below, we treat gender identity as binary, which does not accurately characterize gender in real life. Parameters \u00b6 evaluation_embeddings : torch.Tensor A tensor of size (batch_size, ..., dim) of embeddings for which to mitigate bias. bias_direction : torch.Tensor A unit tensor of size (dim, ) representing the concept subspace. Note All tensors are expected to be on the same device. Returns \u00b6 bias_mitigated_embeddings : torch.Tensor A tensor of the same size as evaluation_embeddings. INLPBiasMitigator \u00b6 class INLPBiasMitigator ( BiasMitigator ): | def __init__ ( self ) Iterative Nullspace Projection. It mitigates bias by repeatedly building a linear classifier that separates concept groups and linearly projecting all words along the classifier normal. Note For a detailed walkthrough and visual descriptions of the steps, please refer to Figure 5 in VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations . Based on: Ravfogel, S., Elazar, Y., Gonen, H., Twiton, M., & Goldberg, Y. (2020). Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection . ArXiv, abs/2004.07667. Implementation and terminology based on Rathore, A., Dev, S., Phillips, J.M., Srikumar, V., Zheng, Y., Yeh, C.M., Wang, J., Zhang, W., & Wang, B. (2021). VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations . ArXiv, abs/2104.02797. __call__ \u00b6 class INLPBiasMitigator ( BiasMitigator ): | ... | def __call__ ( | self , | evaluation_embeddings : torch . Tensor , | seed_embeddings1 : torch . Tensor , | seed_embeddings2 : torch . Tensor , | num_iters : int = 35 | ) Parameters \u00b6 Note In the examples below, we treat gender identity as binary, which does not accurately characterize gender in real life. evaluation_embeddings : torch.Tensor A tensor of size (evaluation_batch_size, ..., dim) of embeddings for which to mitigate bias. seed_embeddings1 : torch.Tensor A tensor of size (embeddings1_batch_size, ..., dim) containing seed word embeddings related to a specific concept group. For example, if the concept is gender, seed_embeddings1 could contain embeddings for linguistically masculine words, e.g. \"man\", \"king\", \"brother\", etc. seed_embeddings2 : torch.Tensor A tensor of size (embeddings2_batch_size, ..., dim) containing seed word embeddings related to a different group for the same concept. For example, seed_embeddings2 could contain embeddings for linguistically feminine words, , e.g. \"woman\", \"queen\", \"sister\", etc. num_iters : torch.Tensor Number of times to build classifier and project embeddings along normal. Note seed_embeddings1 and seed_embeddings2 need NOT be the same size. Furthermore, the embeddings at the same positions in each of seed_embeddings1 and seed_embeddings2 are NOT expected to form seed word pairs. Note All tensors are expected to be on the same device. Note This bias mitigator is not differentiable. Returns \u00b6 bias_mitigated_embeddings : torch.Tensor A tensor of the same size as evaluation_embeddings. OSCaRBiasMitigator \u00b6 class OSCaRBiasMitigator ( BiasMitigator ) OSCaR bias mitigator. Mitigates bias in embeddings by dissociating concept subspaces through subspace orthogonalization. Formally, OSCaR applies a graded rotation on the embedding space to rectify two ideally-independent concept subspaces so that they become orthogonal. Note For a detailed walkthrough and visual descriptions of the steps, please refer to Figure 6 in VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations . Based on: Dev, S., Li, T., Phillips, J.M., & Srikumar, V. (2020). OSCaR: Orthogonal Subspace Correction and Rectification of Biases in Word Embeddings . ArXiv, abs/2007.00049. Implementation and terminology based on Rathore, A., Dev, S., Phillips, J.M., Srikumar, V., Zheng, Y., Yeh, C.M., Wang, J., Zhang, W., & Wang, B. (2021). VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations . ArXiv, abs/2104.02797. __call__ \u00b6 class OSCaRBiasMitigator ( BiasMitigator ): | ... | def __call__ ( | self , | evaluation_embeddings : torch . Tensor , | bias_direction1 : torch . Tensor , | bias_direction2 : torch . Tensor | ) Parameters \u00b6 evaluation_embeddings : torch.Tensor A tensor of size (batch_size, ..., dim) of embeddings for which to mitigate bias. bias_direction1 : torch.Tensor A unit tensor of size (dim, ) representing a concept subspace (e.g. gender). bias_direction2 : torch.Tensor A unit tensor of size (dim, ) representing another concept subspace from which bias_direction1 should be dissociated (e.g. occupation). Note All tensors are expected to be on the same device. Returns \u00b6 bias_mitigated_embeddings : torch.Tensor A tensor of the same size as evaluation_embeddings.","title":"bias_mitigators"},{"location":"api/fairness/bias_mitigators/#biasmitigator","text":"class BiasMitigator : | def __init__ ( self , requires_grad : bool = False ) Parent class for bias mitigator classes.","title":"BiasMitigator"},{"location":"api/fairness/bias_mitigators/#hardbiasmitigator","text":"class HardBiasMitigator ( BiasMitigator ) Hard bias mitigator. Mitigates bias in embeddings by: Neutralizing: ensuring protected variable-neutral words remain equidistant from the bias direction by removing component of embeddings in the bias direction. Equalizing: ensuring that protected variable-related words are averaged out to have the same norm. Note For a detailed walkthrough and visual descriptions of the steps, please refer to Figure 4 in VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations . Based on: T. Bolukbasi, K. W. Chang, J. Zou, V. Saligrama, and A. Kalai. Man is to computer programmer as woman is to homemaker? debiasing word embeddings . In ACM Transactions of Information Systems, 2016. Description taken from: Goenka, D. (2020). Tackling Gender Bias in Word Embeddings . Implementation and terminology based on Rathore, A., Dev, S., Phillips, J.M., Srikumar, V., Zheng, Y., Yeh, C.M., Wang, J., Zhang, W., & Wang, B. (2021). VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations . ArXiv, abs/2104.02797.","title":"HardBiasMitigator"},{"location":"api/fairness/bias_mitigators/#__call__","text":"class HardBiasMitigator ( BiasMitigator ): | ... | def __call__ ( | self , | evaluation_embeddings : torch . Tensor , | bias_direction : torch . Tensor , | equalize_embeddings1 : torch . Tensor , | equalize_embeddings2 : torch . Tensor | ) Note In the examples below, we treat gender identity as binary, which does not accurately characterize gender in real life.","title":"__call__"},{"location":"api/fairness/bias_mitigators/#linearbiasmitigator","text":"class LinearBiasMitigator ( BiasMitigator ) Linear bias mitigator. Mitigates bias in embeddings by removing component in the bias direction. Note For a detailed walkthrough and visual descriptions of the steps, please refer to Figure 3 in VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations . Based on: S. Dev and J. M. Phillips. Attenuating bias in word vectors . In International Conference on Artificial Intelligence and Statistics, Proceedings of Machine Learning Research, pages 879\u2013887. PMLR, 2019. Implementation and terminology based on Rathore, A., Dev, S., Phillips, J.M., Srikumar, V., Zheng, Y., Yeh, C.M., Wang, J., Zhang, W., & Wang, B. (2021). VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations . ArXiv, abs/2104.02797.","title":"LinearBiasMitigator"},{"location":"api/fairness/bias_mitigators/#__call___1","text":"class LinearBiasMitigator ( BiasMitigator ): | ... | def __call__ ( | self , | evaluation_embeddings : torch . Tensor , | bias_direction : torch . Tensor | ) Note In the examples below, we treat gender identity as binary, which does not accurately characterize gender in real life.","title":"__call__"},{"location":"api/fairness/bias_mitigators/#inlpbiasmitigator","text":"class INLPBiasMitigator ( BiasMitigator ): | def __init__ ( self ) Iterative Nullspace Projection. It mitigates bias by repeatedly building a linear classifier that separates concept groups and linearly projecting all words along the classifier normal. Note For a detailed walkthrough and visual descriptions of the steps, please refer to Figure 5 in VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations . Based on: Ravfogel, S., Elazar, Y., Gonen, H., Twiton, M., & Goldberg, Y. (2020). Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection . ArXiv, abs/2004.07667. Implementation and terminology based on Rathore, A., Dev, S., Phillips, J.M., Srikumar, V., Zheng, Y., Yeh, C.M., Wang, J., Zhang, W., & Wang, B. (2021). VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations . ArXiv, abs/2104.02797.","title":"INLPBiasMitigator"},{"location":"api/fairness/bias_mitigators/#__call___2","text":"class INLPBiasMitigator ( BiasMitigator ): | ... | def __call__ ( | self , | evaluation_embeddings : torch . Tensor , | seed_embeddings1 : torch . Tensor , | seed_embeddings2 : torch . Tensor , | num_iters : int = 35 | )","title":"__call__"},{"location":"api/fairness/bias_mitigators/#oscarbiasmitigator","text":"class OSCaRBiasMitigator ( BiasMitigator ) OSCaR bias mitigator. Mitigates bias in embeddings by dissociating concept subspaces through subspace orthogonalization. Formally, OSCaR applies a graded rotation on the embedding space to rectify two ideally-independent concept subspaces so that they become orthogonal. Note For a detailed walkthrough and visual descriptions of the steps, please refer to Figure 6 in VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations . Based on: Dev, S., Li, T., Phillips, J.M., & Srikumar, V. (2020). OSCaR: Orthogonal Subspace Correction and Rectification of Biases in Word Embeddings . ArXiv, abs/2007.00049. Implementation and terminology based on Rathore, A., Dev, S., Phillips, J.M., Srikumar, V., Zheng, Y., Yeh, C.M., Wang, J., Zhang, W., & Wang, B. (2021). VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations . ArXiv, abs/2104.02797.","title":"OSCaRBiasMitigator"},{"location":"api/fairness/bias_mitigators/#__call___3","text":"class OSCaRBiasMitigator ( BiasMitigator ): | ... | def __call__ ( | self , | evaluation_embeddings : torch . Tensor , | bias_direction1 : torch . Tensor , | bias_direction2 : torch . Tensor | )","title":"__call__"},{"location":"api/fairness/bias_utils/","text":"allennlp .fairness .bias_utils [SOURCE] load_words \u00b6 def load_words ( fname : Union [ str , PathLike ], tokenizer : Tokenizer , vocab : Optional [ Vocabulary ] = None , namespace : str = \"tokens\" , all_cases : bool = True ) -> List [ torch . Tensor ] This function loads a list of words from a file, tokenizes each word into subword tokens, and converts the tokens into IDs. Parameters \u00b6 fname : Union[str, PathLike] Name of file containing list of words to load. tokenizer : Tokenizer Tokenizer to tokenize words in file. vocab : Vocabulary , optional (default = None ) Vocabulary of tokenizer. If None , assumes tokenizer is of type PreTrainedTokenizer and uses tokenizer's vocab attribute. namespace : str Namespace of vocab to use when tokenizing. all_cases : bool , optional (default = True ) Whether to tokenize lower, title, and upper cases of each word. Returns \u00b6 word_ids : List[torch.Tensor] List of tensors containing the IDs of subword tokens for each word in the file. load_word_pairs \u00b6 def load_word_pairs ( fname : Union [ str , PathLike ], tokenizer : Tokenizer , vocab : Optional [ Vocabulary ] = None , namespace : str = \"token\" , all_cases : bool = True ) -> Tuple [ List [ torch . Tensor ], List [ torch . Tensor ]] This function loads a list of pairs of words from a file, tokenizes each word into subword tokens, and converts the tokens into IDs. Parameters \u00b6 fname : Union[str, PathLike] Name of file containing list of pairs of words to load. tokenizer : Tokenizer Tokenizer to tokenize words in file. vocab : Vocabulary , optional (default = None ) Vocabulary of tokenizer. If None , assumes tokenizer is of type PreTrainedTokenizer and uses tokenizer's vocab attribute. namespace : str Namespace of vocab to use when tokenizing. all_cases : bool , optional (default = True ) Whether to tokenize lower, title, and upper cases of each word. Returns \u00b6 word_ids : Tuple[List[torch.Tensor], List[torch.Tensor]] Pair of lists of tensors containing the IDs of subword tokens for words in the file.","title":"bias_utils"},{"location":"api/fairness/bias_utils/#load_words","text":"def load_words ( fname : Union [ str , PathLike ], tokenizer : Tokenizer , vocab : Optional [ Vocabulary ] = None , namespace : str = \"tokens\" , all_cases : bool = True ) -> List [ torch . Tensor ] This function loads a list of words from a file, tokenizes each word into subword tokens, and converts the tokens into IDs.","title":"load_words"},{"location":"api/fairness/bias_utils/#load_word_pairs","text":"def load_word_pairs ( fname : Union [ str , PathLike ], tokenizer : Tokenizer , vocab : Optional [ Vocabulary ] = None , namespace : str = \"token\" , all_cases : bool = True ) -> Tuple [ List [ torch . Tensor ], List [ torch . Tensor ]] This function loads a list of pairs of words from a file, tokenizes each word into subword tokens, and converts the tokens into IDs.","title":"load_word_pairs"},{"location":"api/fairness/fairness_metrics/","text":"allennlp .fairness .fairness_metrics [SOURCE] Fairness metrics are based on: Barocas, S.; Hardt, M.; and Narayanan, A. 2019. Fairness and machine learning . Zhang, B. H.; Lemoine, B.; and Mitchell, M. 2018. Mitigating unwanted biases with adversarial learning . In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, 335-340. Hardt, M.; Price, E.; Srebro, N.; et al. 2016. Equality of opportunity in supervised learning . In Advances in Neural Information Processing Systems, 3315\u20133323. Beutel, A.; Chen, J.; Zhao, Z.; and Chi, E. H. 2017. Data decisions and theoretical implications when adversarially learning fair representations . arXiv preprint arXiv:1707.00075. It is provably impossible (pg. 18) to satisfy any two of Independence, Separation, and Sufficiency simultaneously, except in degenerate cases. Independence \u00b6 @Metric . register ( \"independence\" ) class Independence ( Metric ): | def __init__ ( | self , | num_classes : int , | num_protected_variable_labels : int , | dist_metric : str = \"kl_divergence\" | ) -> None Independence (pg. 9) measures the statistical independence of the protected variable from predictions. It has been explored through many equivalent terms or variants, such as demographic parity, statistical parity, group fairness, and disparate impact. Parameters \u00b6 num_classes : int Number of classes. num_protected_variable_labels : int Number of protected variable labels. dist_metric : str Distance metric (kl_divergence, wasserstein) for calculating the distance between the distribution over predicted labels and the distribution over predicted labels given a sensitive attribute. Note Assumes integer labels, with each item to be classified having a single correct class. __call__ \u00b6 class Independence ( Metric ): | ... | def __call__ ( | self , | predicted_labels : torch . Tensor , | protected_variable_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) -> None Parameters \u00b6 predicted_labels : torch.Tensor A tensor of predicted integer class labels of shape (batch_size, ...). Represented as C. protected_variable_labels : torch.Tensor A tensor of integer protected variable labels of shape (batch_size, ...). It must be the same shape as the predicted_labels tensor. Represented as A. mask : torch.BoolTensor , optional (default = None ) A tensor of the same shape as predicted_labels . Note All tensors are expected to be on the same device. get_metric \u00b6 class Independence ( Metric ): | ... | def get_metric ( | self , | reset : bool = False | ) -> Dict [ int , torch . FloatTensor ] Returns \u00b6 distances : Dict[int, torch.FloatTensor] A dictionary mapping each protected variable label a to the KL divergence or Wasserstein distance of P(C | A = a) from P(C). A distance of nearly 0 implies fairness on the basis of Independence. reset \u00b6 class Independence ( Metric ): | ... | def reset ( self ) -> None Separation \u00b6 @Metric . register ( \"separation\" ) class Separation ( Metric ): | def __init__ ( | self , | num_classes : int , | num_protected_variable_labels : int , | dist_metric : str = \"kl_divergence\" | ) -> None Separation (pg. 12) allows correlation between the predictions and the protected variable to the extent that it is justified by the gold labels. Parameters \u00b6 num_classes : int Number of classes. num_protected_variable_labels : int Number of protected variable labels. dist_metric : str Distance metric (kl_divergence, wasserstein) for calculating the distance between the distribution over predicted labels given a gold label and a sensitive attribute from the distribution over predicted labels given only the gold label. If both distributions do not have equal support, you should use wasserstein distance. Note Assumes integer labels, with each item to be classified having a single correct class. __call__ \u00b6 class Separation ( Metric ): | ... | def __call__ ( | self , | predicted_labels : torch . Tensor , | gold_labels : torch . Tensor , | protected_variable_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) -> None Parameters \u00b6 predicted_labels : torch.Tensor A tensor of predicted integer class labels of shape (batch_size, ...). Represented as C. gold_labels : torch.Tensor A tensor of ground-truth integer class labels of shape (batch_size, ...). It must be the same shape as the predicted_labels tensor. Represented as Y. protected_variable_labels : torch.Tensor A tensor of integer protected variable labels of shape (batch_size, ...). It must be the same shape as the predicted_labels tensor. Represented as A. mask : torch.BoolTensor , optional (default = None ) A tensor of the same shape as predicted_labels . Note All tensors are expected to be on the same device. get_metric \u00b6 class Separation ( Metric ): | ... | def get_metric ( | self , | reset : bool = False | ) -> Dict [ int , Dict [ int , torch . FloatTensor ]] Returns \u00b6 distances : Dict[int, Dict[int, torch.FloatTensor]] A dictionary mapping each class label y to a dictionary mapping each protected variable label a to the KL divergence or Wasserstein distance of P(C | A = a, Y = y) from P(C | Y = y). A distance of nearly 0 implies fairness on the basis of Separation. Note If a class label is not present in Y conditioned on a protected variable label, the expected behavior is that the KL divergence corresponding to this (class label, protected variable label) pair is NaN. You can avoid this by using Wasserstein distance instead. reset \u00b6 class Separation ( Metric ): | ... | def reset ( self ) -> None Sufficiency \u00b6 @Metric . register ( \"sufficiency\" ) class Sufficiency ( Metric ): | def __init__ ( | self , | num_classes : int , | num_protected_variable_labels : int , | dist_metric : str = \"kl_divergence\" | ) -> None Sufficiency (pg. 14) is satisfied by the predictions when the protected variable and gold labels are clear from context. Parameters \u00b6 num_classes : int Number of classes. num_protected_variable_labels : int Number of protected variable labels. dist_metric : str Distance metric (kl_divergence, wasserstein) for calculating the distance between the distribution over gold labels given a predicted label and a sensitive attribute from the distribution over gold labels given only the predicted label. If both distributions do not have equal support, you should use wasserstein distance. Note Assumes integer labels, with each item to be classified having a single correct class. __call__ \u00b6 class Sufficiency ( Metric ): | ... | def __call__ ( | self , | predicted_labels : torch . Tensor , | gold_labels : torch . Tensor , | protected_variable_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) -> None Parameters \u00b6 predicted_labels : torch.Tensor A tensor of predicted integer class labels of shape (batch_size, ...). Represented as C. gold_labels : torch.Tensor A tensor of ground-truth integer class labels of shape (batch_size, ...). It must be the same shape as the predicted_labels tensor. Represented as Y. protected_variable_labels : torch.Tensor A tensor of integer protected variable labels of shape (batch_size, ...). It must be the same shape as the predicted_labels tensor. Represented as A. mask : torch.BoolTensor , optional (default = None ) A tensor of the same shape as predicted_labels . Note All tensors are expected to be on the same device. get_metric \u00b6 class Sufficiency ( Metric ): | ... | def get_metric ( | self , | reset : bool = False | ) -> Dict [ int , Dict [ int , torch . FloatTensor ]] Returns \u00b6 distances : Dict[int, Dict[int, torch.FloatTensor]] A dictionary mapping each class label c to a dictionary mapping each protected variable label a to the KL divergence or Wasserstein distance of P(Y | A = a, C = c) from P(Y | C = c). A distance of nearly 0 implies fairness on the basis of Sufficiency. Note If a possible class label is not present in C, the expected behavior is that the KL divergences corresponding to this class label are NaN. If a possible class label is not present in C conditioned on a protected variable label, the expected behavior is that the KL divergence corresponding to this (class label, protected variable label) pair is NaN. You can avoid this by using Wasserstein distance instead. reset \u00b6 class Sufficiency ( Metric ): | ... | def reset ( self ) -> None","title":"fairness_metrics"},{"location":"api/fairness/fairness_metrics/#independence","text":"@Metric . register ( \"independence\" ) class Independence ( Metric ): | def __init__ ( | self , | num_classes : int , | num_protected_variable_labels : int , | dist_metric : str = \"kl_divergence\" | ) -> None Independence (pg. 9) measures the statistical independence of the protected variable from predictions. It has been explored through many equivalent terms or variants, such as demographic parity, statistical parity, group fairness, and disparate impact.","title":"Independence"},{"location":"api/fairness/fairness_metrics/#__call__","text":"class Independence ( Metric ): | ... | def __call__ ( | self , | predicted_labels : torch . Tensor , | protected_variable_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) -> None","title":"__call__"},{"location":"api/fairness/fairness_metrics/#get_metric","text":"class Independence ( Metric ): | ... | def get_metric ( | self , | reset : bool = False | ) -> Dict [ int , torch . FloatTensor ]","title":"get_metric"},{"location":"api/fairness/fairness_metrics/#reset","text":"class Independence ( Metric ): | ... | def reset ( self ) -> None","title":"reset"},{"location":"api/fairness/fairness_metrics/#separation","text":"@Metric . register ( \"separation\" ) class Separation ( Metric ): | def __init__ ( | self , | num_classes : int , | num_protected_variable_labels : int , | dist_metric : str = \"kl_divergence\" | ) -> None Separation (pg. 12) allows correlation between the predictions and the protected variable to the extent that it is justified by the gold labels.","title":"Separation"},{"location":"api/fairness/fairness_metrics/#__call___1","text":"class Separation ( Metric ): | ... | def __call__ ( | self , | predicted_labels : torch . Tensor , | gold_labels : torch . Tensor , | protected_variable_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) -> None","title":"__call__"},{"location":"api/fairness/fairness_metrics/#get_metric_1","text":"class Separation ( Metric ): | ... | def get_metric ( | self , | reset : bool = False | ) -> Dict [ int , Dict [ int , torch . FloatTensor ]]","title":"get_metric"},{"location":"api/fairness/fairness_metrics/#reset_1","text":"class Separation ( Metric ): | ... | def reset ( self ) -> None","title":"reset"},{"location":"api/fairness/fairness_metrics/#sufficiency","text":"@Metric . register ( \"sufficiency\" ) class Sufficiency ( Metric ): | def __init__ ( | self , | num_classes : int , | num_protected_variable_labels : int , | dist_metric : str = \"kl_divergence\" | ) -> None Sufficiency (pg. 14) is satisfied by the predictions when the protected variable and gold labels are clear from context.","title":"Sufficiency"},{"location":"api/fairness/fairness_metrics/#__call___2","text":"class Sufficiency ( Metric ): | ... | def __call__ ( | self , | predicted_labels : torch . Tensor , | gold_labels : torch . Tensor , | protected_variable_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) -> None","title":"__call__"},{"location":"api/fairness/fairness_metrics/#get_metric_2","text":"class Sufficiency ( Metric ): | ... | def get_metric ( | self , | reset : bool = False | ) -> Dict [ int , Dict [ int , torch . FloatTensor ]]","title":"get_metric"},{"location":"api/fairness/fairness_metrics/#reset_2","text":"class Sufficiency ( Metric ): | ... | def reset ( self ) -> None","title":"reset"},{"location":"api/interpret/attackers/attacker/","text":"allennlp .interpret .attackers .attacker [SOURCE] Attacker \u00b6 class Attacker ( Registrable ): | def __init__ ( self , predictor : Predictor ) -> None An Attacker will modify an input (e.g., add or delete tokens) to try to change an AllenNLP Predictor's output in a desired manner (e.g., make it incorrect). initialize \u00b6 class Attacker ( Registrable ): | ... | def initialize ( self ) Initializes any components of the Attacker that are expensive to compute, so that they are not created on init (). Default implementation is pass . attack_from_json \u00b6 class Attacker ( Registrable ): | ... | def attack_from_json ( | self , | inputs : JsonDict , | input_field_to_attack : str , | grad_input_field : str , | ignore_tokens : List [ str ], | target : JsonDict | ) -> JsonDict This function finds a modification to the input text that would change the model's prediction in some desired manner (e.g., an adversarial attack). Parameters \u00b6 inputs : JsonDict The input you want to attack (the same as the argument to a Predictor, e.g., predict_json()). input_field_to_attack : str The key in the inputs JsonDict you want to attack, e.g., tokens . grad_input_field : str The field in the gradients dictionary that contains the input gradients. For example, grad_input_1 will be the field for single input tasks. See get_gradients() in Predictor for more information on field names. target : JsonDict If given, this is a targeted attack, trying to change the prediction to a particular value, instead of just changing it from its original prediction. Subclasses are not required to accept this argument, as not all attacks make sense as targeted attacks. Perhaps that means we should make the API more crisp, but adding another class is not worth it. Returns \u00b6 reduced_input : JsonDict Contains the final, sanitized input after adversarial modification.","title":"attacker"},{"location":"api/interpret/attackers/attacker/#attacker","text":"class Attacker ( Registrable ): | def __init__ ( self , predictor : Predictor ) -> None An Attacker will modify an input (e.g., add or delete tokens) to try to change an AllenNLP Predictor's output in a desired manner (e.g., make it incorrect).","title":"Attacker"},{"location":"api/interpret/attackers/attacker/#initialize","text":"class Attacker ( Registrable ): | ... | def initialize ( self ) Initializes any components of the Attacker that are expensive to compute, so that they are not created on init (). Default implementation is pass .","title":"initialize"},{"location":"api/interpret/attackers/attacker/#attack_from_json","text":"class Attacker ( Registrable ): | ... | def attack_from_json ( | self , | inputs : JsonDict , | input_field_to_attack : str , | grad_input_field : str , | ignore_tokens : List [ str ], | target : JsonDict | ) -> JsonDict This function finds a modification to the input text that would change the model's prediction in some desired manner (e.g., an adversarial attack).","title":"attack_from_json"},{"location":"api/interpret/attackers/hotflip/","text":"allennlp .interpret .attackers .hotflip [SOURCE] DEFAULT_IGNORE_TOKENS \u00b6 DEFAULT_IGNORE_TOKENS = [ \"@@NULL@@\" , \".\" , \",\" , \";\" , \"!\" , \"?\" , \"[MASK]\" , \"[SEP]\" , \"[CLS]\" ] Hotflip \u00b6 @Attacker . register ( \"hotflip\" ) class Hotflip ( Attacker ): | def __init__ ( | self , | predictor : Predictor , | vocab_namespace : str = \"tokens\" , | max_tokens : int = 5000 | ) -> None Runs the HotFlip style attack at the word-level https://arxiv.org/abs/1712.06751. We use the first-order taylor approximation described in https://arxiv.org/abs/1903.06620, in the function _first_order_taylor() . We try to re-use the embedding matrix from the model when deciding what other words to flip a token to. For a large class of models, this is straightforward. When there is a character-level encoder, however (e.g., with ELMo, any char-CNN, etc.), or a combination of encoders (e.g., ELMo + glove), we need to construct a fake embedding matrix that we can use in _first_order_taylor() . We do this by getting a list of words from the model's vocabulary and embedding them using the encoder. This can be expensive, both in terms of time and memory usage, so we take a max_tokens parameter to limit the size of this fake embedding matrix. This also requires a model to have a token vocabulary in the first place, which can be problematic for models that only have character vocabularies. Registered as an Attacker with name \"hotflip\". Parameters \u00b6 predictor : Predictor The model (inside a Predictor) that we're attacking. We use this to get gradients and predictions. vocab_namespace : str , optional (default = 'tokens' ) We use this to know three things: (1) which tokens we should ignore when producing flips (we don't consider non-alphanumeric tokens); (2) what the string value is of the token that we produced, so we can show something human-readable to the user; and (3) if we need to construct a fake embedding matrix, we use the tokens in the vocabulary as flip candidates. max_tokens : int , optional (default = 5000 ) This is only used when we need to construct a fake embedding matrix. That matrix can take a lot of memory when the vocab size is large. This parameter puts a cap on the number of tokens to use, so the fake embedding matrix doesn't take as much memory. initialize \u00b6 class Hotflip ( Attacker ): | ... | def initialize ( self ) Call this function before running attack_from_json(). We put the call to _construct_embedding_matrix() in this function to prevent a large amount of compute being done when init () is called. attack_from_json \u00b6 class Hotflip ( Attacker ): | ... | def attack_from_json ( | self , | inputs : JsonDict , | input_field_to_attack : str = \"tokens\" , | grad_input_field : str = \"grad_input_1\" , | ignore_tokens : List [ str ] = None , | target : JsonDict = None | ) -> JsonDict Replaces one token at a time from the input until the model's prediction changes. input_field_to_attack is for example tokens , it says what the input field is called. grad_input_field is for example grad_input_1 , which is a key into a grads dictionary. The method computes the gradient w.r.t. the tokens, finds the token with the maximum gradient (by L2 norm), and replaces it with another token based on the first-order Taylor approximation of the loss. This process is iteratively repeated until the prediction changes. Once a token is replaced, it is not flipped again. Parameters \u00b6 inputs : JsonDict The model inputs, the same as what is passed to a Predictor . input_field_to_attack : str , optional (default = 'tokens' ) The field that has the tokens that we're going to be flipping. This must be a TextField . grad_input_field : str , optional (default = 'grad_input_1' ) If there is more than one field that gets embedded in your model (e.g., a question and a passage, or a premise and a hypothesis), this tells us the key to use to get the correct gradients. This selects from the output of Predictor.get_gradients . ignore_tokens : List[str] , optional (default = DEFAULT_IGNORE_TOKENS ) These tokens will not be flipped. The default list includes some simple punctuation, OOV and padding tokens, and common control tokens for BERT, etc. target : JsonDict , optional (default = None ) If given, this will be a targeted hotflip attack, where instead of just trying to change a model's prediction from what it current is predicting, we try to change it to a specific target value. This is a JsonDict because it needs to specify the field name and target value. For example, for a masked LM, this would be something like {\"words\": [\"she\"]} , because \"words\" is the field name, there is one mask token (hence the list of length one), and we want to change the prediction from whatever it was to \"she\" . By default, output_dict from forward pass would be given for func: Predictor.predictions_to_labeled_instances where target has to be extracted manually according to logit. attack_instance \u00b6 class Hotflip ( Attacker ): | ... | def attack_instance ( | self , | instance : Instance , | inputs : JsonDict , | input_field_to_attack : str = \"tokens\" , | grad_input_field : str = \"grad_input_1\" , | ignore_tokens : List [ str ] = None , | target : JsonDict = None | ) -> Tuple [ List [ Token ], JsonDict ]","title":"hotflip"},{"location":"api/interpret/attackers/hotflip/#default_ignore_tokens","text":"DEFAULT_IGNORE_TOKENS = [ \"@@NULL@@\" , \".\" , \",\" , \";\" , \"!\" , \"?\" , \"[MASK]\" , \"[SEP]\" , \"[CLS]\" ]","title":"DEFAULT_IGNORE_TOKENS"},{"location":"api/interpret/attackers/hotflip/#hotflip","text":"@Attacker . register ( \"hotflip\" ) class Hotflip ( Attacker ): | def __init__ ( | self , | predictor : Predictor , | vocab_namespace : str = \"tokens\" , | max_tokens : int = 5000 | ) -> None Runs the HotFlip style attack at the word-level https://arxiv.org/abs/1712.06751. We use the first-order taylor approximation described in https://arxiv.org/abs/1903.06620, in the function _first_order_taylor() . We try to re-use the embedding matrix from the model when deciding what other words to flip a token to. For a large class of models, this is straightforward. When there is a character-level encoder, however (e.g., with ELMo, any char-CNN, etc.), or a combination of encoders (e.g., ELMo + glove), we need to construct a fake embedding matrix that we can use in _first_order_taylor() . We do this by getting a list of words from the model's vocabulary and embedding them using the encoder. This can be expensive, both in terms of time and memory usage, so we take a max_tokens parameter to limit the size of this fake embedding matrix. This also requires a model to have a token vocabulary in the first place, which can be problematic for models that only have character vocabularies. Registered as an Attacker with name \"hotflip\".","title":"Hotflip"},{"location":"api/interpret/attackers/hotflip/#initialize","text":"class Hotflip ( Attacker ): | ... | def initialize ( self ) Call this function before running attack_from_json(). We put the call to _construct_embedding_matrix() in this function to prevent a large amount of compute being done when init () is called.","title":"initialize"},{"location":"api/interpret/attackers/hotflip/#attack_from_json","text":"class Hotflip ( Attacker ): | ... | def attack_from_json ( | self , | inputs : JsonDict , | input_field_to_attack : str = \"tokens\" , | grad_input_field : str = \"grad_input_1\" , | ignore_tokens : List [ str ] = None , | target : JsonDict = None | ) -> JsonDict Replaces one token at a time from the input until the model's prediction changes. input_field_to_attack is for example tokens , it says what the input field is called. grad_input_field is for example grad_input_1 , which is a key into a grads dictionary. The method computes the gradient w.r.t. the tokens, finds the token with the maximum gradient (by L2 norm), and replaces it with another token based on the first-order Taylor approximation of the loss. This process is iteratively repeated until the prediction changes. Once a token is replaced, it is not flipped again.","title":"attack_from_json"},{"location":"api/interpret/attackers/hotflip/#attack_instance","text":"class Hotflip ( Attacker ): | ... | def attack_instance ( | self , | instance : Instance , | inputs : JsonDict , | input_field_to_attack : str = \"tokens\" , | grad_input_field : str = \"grad_input_1\" , | ignore_tokens : List [ str ] = None , | target : JsonDict = None | ) -> Tuple [ List [ Token ], JsonDict ]","title":"attack_instance"},{"location":"api/interpret/attackers/input_reduction/","text":"allennlp .interpret .attackers .input_reduction [SOURCE] InputReduction \u00b6 @Attacker . register ( \"input-reduction\" ) class InputReduction ( Attacker ): | def __init__ ( self , predictor : Predictor , beam_size : int = 3 ) -> None Runs the input reduction method from Pathologies of Neural Models Make Interpretations Difficult , which removes as many words as possible from the input without changing the model's prediction. The functions on this class handle a special case for NER by looking for a field called \"tags\" This check is brittle, i.e., the code could break if the name of this field has changed, or if a non-NER model has a field called \"tags\". Registered as an Attacker with name \"input-reduction\". attack_from_json \u00b6 class InputReduction ( Attacker ): | ... | def attack_from_json ( | self , | inputs : JsonDict , | input_field_to_attack : str = \"tokens\" , | grad_input_field : str = \"grad_input_1\" , | ignore_tokens : List [ str ] = None , | target : JsonDict = None | )","title":"input_reduction"},{"location":"api/interpret/attackers/input_reduction/#inputreduction","text":"@Attacker . register ( \"input-reduction\" ) class InputReduction ( Attacker ): | def __init__ ( self , predictor : Predictor , beam_size : int = 3 ) -> None Runs the input reduction method from Pathologies of Neural Models Make Interpretations Difficult , which removes as many words as possible from the input without changing the model's prediction. The functions on this class handle a special case for NER by looking for a field called \"tags\" This check is brittle, i.e., the code could break if the name of this field has changed, or if a non-NER model has a field called \"tags\". Registered as an Attacker with name \"input-reduction\".","title":"InputReduction"},{"location":"api/interpret/attackers/input_reduction/#attack_from_json","text":"class InputReduction ( Attacker ): | ... | def attack_from_json ( | self , | inputs : JsonDict , | input_field_to_attack : str = \"tokens\" , | grad_input_field : str = \"grad_input_1\" , | ignore_tokens : List [ str ] = None , | target : JsonDict = None | )","title":"attack_from_json"},{"location":"api/interpret/attackers/utils/","text":"allennlp .interpret .attackers .utils [SOURCE] get_fields_to_compare \u00b6 def get_fields_to_compare ( inputs : JsonDict , instance : Instance , input_field_to_attack : str ) -> JsonDict Gets a list of the fields that should be checked for equality after an attack is performed. Parameters \u00b6 inputs : JsonDict The input you want to attack, similar to the argument to a Predictor, e.g., predict_json(). instance : Instance A labeled instance that is output from json_to_labeled_instances(). input_field_to_attack : str The key in the inputs JsonDict you want to attack, e.g., tokens. Returns \u00b6 fields : JsonDict The fields that must be compared for equality. instance_has_changed \u00b6 def instance_has_changed ( instance : Instance , fields_to_compare : JsonDict )","title":"utils"},{"location":"api/interpret/attackers/utils/#get_fields_to_compare","text":"def get_fields_to_compare ( inputs : JsonDict , instance : Instance , input_field_to_attack : str ) -> JsonDict Gets a list of the fields that should be checked for equality after an attack is performed.","title":"get_fields_to_compare"},{"location":"api/interpret/attackers/utils/#instance_has_changed","text":"def instance_has_changed ( instance : Instance , fields_to_compare : JsonDict )","title":"instance_has_changed"},{"location":"api/interpret/influence_interpreters/influence_interpreter/","text":"allennlp .interpret .influence_interpreters .influence_interpreter [SOURCE] InstanceInfluence \u00b6 class InstanceInfluence ( NamedTuple ) instance \u00b6 class InstanceInfluence ( NamedTuple ): | ... | instance : Instance = None loss \u00b6 class InstanceInfluence ( NamedTuple ): | ... | loss : float = None score \u00b6 class InstanceInfluence ( NamedTuple ): | ... | score : float = None The influence score associated with this training instance. InterpretOutput \u00b6 class InterpretOutput ( NamedTuple ) The output associated with a single test instance. test_instance \u00b6 class InterpretOutput ( NamedTuple ): | ... | test_instance : Instance = None loss \u00b6 class InterpretOutput ( NamedTuple ): | ... | loss : float = None The loss corresponding to the test_instance . top_k \u00b6 class InterpretOutput ( NamedTuple ): | ... | top_k : List [ InstanceInfluence ] = None The top k most influential training instances along with their influence score. InstanceWithGrads \u00b6 class InstanceWithGrads ( NamedTuple ) Wraps a training Instance along with its associated loss and gradients. InfluenceInterpreter.train_instances is a list of these objects. instance \u00b6 class InstanceWithGrads ( NamedTuple ): | ... | instance : Instance = None loss \u00b6 class InstanceWithGrads ( NamedTuple ): | ... | loss : float = None grads \u00b6 class InstanceWithGrads ( NamedTuple ): | ... | grads : Sequence [ torch . Tensor ] = None InfluenceInterpreter \u00b6 class InfluenceInterpreter ( Registrable ): | def __init__ ( | self , | model : Model , | train_data_path : DatasetReaderInput , | train_dataset_reader : DatasetReader , | * , test_dataset_reader : Optional [ DatasetReader ] = None , | * , train_data_loader : Lazy [ DataLoader ] = Lazy ( SimpleDataLoader . from_dataset_reader ), | * , test_data_loader : Lazy [ DataLoader ] = Lazy ( SimpleDataLoader . from_dataset_reader ), | * , params_to_freeze : Optional [ List [ str ]] = None , | * , cuda_device : int = - 1 | ) -> None An InfluenceInterpreter interprets an AllenNLP models's outputs by finding the training instances that had the most influence on the prediction for each test input. See Understanding Black-box Predictions via Influence Functions for more information. Subclasses are required to implement the _calculate_influence_scores() method. Parameters \u00b6 model : Model train_data_path : DatasetReaderInput train_dataset_reader : DatasetReader test_dataset_reader : Optional[DatasetReader] , optional (default = None ) This is the dataset reader to read the test set file. If not provided, the train_dataset_reader is used. train_data_loader : Lazy[DataLoader] , optional (default = Lazy(SimpleDataLoader) ) The data loader used to load training instances. Note This data loader is only used to call DataLoader.iter_instances() , so certain DataLoader settings like batch_size will have no effect. test_data_loader : Lazy[DataLoader] , optional (default = Lazy(SimpleDataLoader) ) The data loader used to load test instances when interpret_from_file() is called. Note Like train_data_loader , this data loader is only used to call DataLoader.iter_instances() , so certain DataLoader settings like batch_size will have no effect. params_to_freeze : Optional[List[str]] , optional (default = None ) An optional list of strings, each of which should be a regular expression that matches some parameter keys of the model. Any matching parameters will be have requires_grad set to False . cuda_device : int , optional (default = -1 ) The index of GPU device we want to calculate scores on. If not provided, we uses -1 which correspond to using CPU. default_implementation \u00b6 class InfluenceInterpreter ( Registrable ): | ... | default_implementation = \"simple-influence\" used_params \u00b6 class InfluenceInterpreter ( Registrable ): | ... | @property | def used_params ( self ) -> List [ torch . nn . Parameter ] The parameters of the model that have non-zero gradients after a backwards pass. This can be used to gather the corresponding gradients with respect to a loss via the torch.autograd.grad function. Note Accessing this property requires calling self._gather_train_instances_and_compute_gradients() if it hasn't been called yet, which may take several minutes. used_param_names \u00b6 class InfluenceInterpreter ( Registrable ): | ... | @property | def used_param_names ( self ) -> List [ str ] The names of the corresponding parameters in self.used_params . Note Accessing this property requires calling self._gather_train_instances_and_compute_gradients() if it hasn't been called yet, which may take several minutes. train_instances \u00b6 class InfluenceInterpreter ( Registrable ): | ... | @property | def train_instances ( self ) -> List [ InstanceWithGrads ] The training instances along with their corresponding loss and gradients. Note Accessing this property requires calling self._gather_train_instances_and_compute_gradients() if it hasn't been called yet, which may take several minutes. from_path \u00b6 class InfluenceInterpreter ( Registrable ): | ... | @classmethod | def from_path ( | cls , | archive_path : Union [ str , PathLike ], | * , interpreter_name : Optional [ str ] = None , | * , train_data_path : Optional [ DatasetReaderInput ] = None , | * , train_data_loader : Lazy [ DataLoader ] = Lazy ( SimpleDataLoader . from_dataset_reader ), | * , test_data_loader : Lazy [ DataLoader ] = Lazy ( SimpleDataLoader . from_dataset_reader ), | * , params_to_freeze : Optional [ List [ str ]] = None , | * , cuda_device : int = - 1 , | * , import_plugins : bool = True , | * , overrides : Union [ str , Dict [ str , Any ]] = \"\" , | ** extras , | * , , | ) -> \"InfluenceInterpreter\" Load an InfluenceInterpreter from an archive path. Parameters \u00b6 archive_path : Union[str, PathLike] The path to the archive file. interpreter_name : Optional[str] , optional (default = None ) The registered name of the an interpreter class. If not specified, the default implementation ( SimpleInfluence ) will be used. train_data_path : Optional[DatasetReaderInput] , optional (default = None ) If not specified, train_data_path will be taken from the archive's config. train_data_loader : Lazy[DataLoader] , optional (default = Lazy(SimpleDataLoader) ) test_data_loader : Lazy[DataLoader] , optional (default = Lazy(SimpleDataLoader) ) params_to_freeze : Optional[List[str]] , optional (default = None ) cuda_device : int , optional (default = -1 ) import_plugins : bool , optional (default = True ) If True , we attempt to import plugins before loading the InfluenceInterpreter . This comes with additional overhead, but means you don't need to explicitly import the modules that your implementation depends on as long as those modules can be found by allennlp.common.plugins.import_plugins() . overrides : Union[str, Dict[str, Any]] , optional (default = \"\" ) JSON overrides to apply to the unarchived Params object. **extras : Any Extra parameters to pass to the interpreter's __init__() method. from_archive \u00b6 class InfluenceInterpreter ( Registrable ): | ... | @classmethod | def from_archive ( | cls , | archive : Archive , | * , interpreter_name : Optional [ str ] = None , | * , train_data_path : Optional [ DatasetReaderInput ] = None , | * , train_data_loader : Lazy [ DataLoader ] = Lazy ( SimpleDataLoader . from_dataset_reader ), | * , test_data_loader : Lazy [ DataLoader ] = Lazy ( SimpleDataLoader . from_dataset_reader ), | * , params_to_freeze : Optional [ List [ str ]] = None , | * , cuda_device : int = - 1 , | ** extras , | * , , | ) -> \"InfluenceInterpreter\" Load an InfluenceInterpreter from an Archive . The other parameters are the same as .from_path() . interpret \u00b6 class InfluenceInterpreter ( Registrable ): | ... | def interpret ( | self , | test_instance : Instance , | k : int = 20 | ) -> InterpretOutput Run the influence function scorer on the given instance, returning the top k most influential train instances with their scores. Note Test instances should have targets so that a loss can be computed. interpret_from_file \u00b6 class InfluenceInterpreter ( Registrable ): | ... | def interpret_from_file ( | self , | test_data_path : DatasetReaderInput , | k : int = 20 | ) -> List [ InterpretOutput ] Runs interpret_instances over the instances read from test_data_path . Note Test instances should have targets so that a loss can be computed. interpret_instances \u00b6 class InfluenceInterpreter ( Registrable ): | ... | def interpret_instances ( | self , | test_instances : List [ Instance ], | k : int = 20 | ) -> List [ InterpretOutput ] Run the influence function scorer on the given instances, returning the top k most influential train instances for each test instance. Note Test instances should have targets so that a loss can be computed. _calculate_influence_scores \u00b6 class InfluenceInterpreter ( Registrable ): | ... | def _calculate_influence_scores ( | self , | test_instance : Instance , | test_loss : float , | test_grads : Sequence [ torch . Tensor ] | ) -> List [ float ] Required to be implemented by subclasses. Calculates the influence scores of self.train_instances with respect to the given test_instance .","title":"influence_interpreter"},{"location":"api/interpret/influence_interpreters/influence_interpreter/#instanceinfluence","text":"class InstanceInfluence ( NamedTuple )","title":"InstanceInfluence"},{"location":"api/interpret/influence_interpreters/influence_interpreter/#instance","text":"class InstanceInfluence ( NamedTuple ): | ... | instance : Instance = None","title":"instance"},{"location":"api/interpret/influence_interpreters/influence_interpreter/#loss","text":"class InstanceInfluence ( NamedTuple ): | ... | loss : float = None","title":"loss"},{"location":"api/interpret/influence_interpreters/influence_interpreter/#score","text":"class InstanceInfluence ( NamedTuple ): | ... | score : float = None The influence score associated with this training instance.","title":"score"},{"location":"api/interpret/influence_interpreters/influence_interpreter/#interpretoutput","text":"class InterpretOutput ( NamedTuple ) The output associated with a single test instance.","title":"InterpretOutput"},{"location":"api/interpret/influence_interpreters/influence_interpreter/#test_instance","text":"class InterpretOutput ( NamedTuple ): | ... | test_instance : Instance = None","title":"test_instance"},{"location":"api/interpret/influence_interpreters/influence_interpreter/#loss_1","text":"class InterpretOutput ( NamedTuple ): | ... | loss : float = None The loss corresponding to the test_instance .","title":"loss"},{"location":"api/interpret/influence_interpreters/influence_interpreter/#top_k","text":"class InterpretOutput ( NamedTuple ): | ... | top_k : List [ InstanceInfluence ] = None The top k most influential training instances along with their influence score.","title":"top_k"},{"location":"api/interpret/influence_interpreters/influence_interpreter/#instancewithgrads","text":"class InstanceWithGrads ( NamedTuple ) Wraps a training Instance along with its associated loss and gradients. InfluenceInterpreter.train_instances is a list of these objects.","title":"InstanceWithGrads"},{"location":"api/interpret/influence_interpreters/influence_interpreter/#instance_1","text":"class InstanceWithGrads ( NamedTuple ): | ... | instance : Instance = None","title":"instance"},{"location":"api/interpret/influence_interpreters/influence_interpreter/#loss_2","text":"class InstanceWithGrads ( NamedTuple ): | ... | loss : float = None","title":"loss"},{"location":"api/interpret/influence_interpreters/influence_interpreter/#grads","text":"class InstanceWithGrads ( NamedTuple ): | ... | grads : Sequence [ torch . Tensor ] = None","title":"grads"},{"location":"api/interpret/influence_interpreters/influence_interpreter/#influenceinterpreter","text":"class InfluenceInterpreter ( Registrable ): | def __init__ ( | self , | model : Model , | train_data_path : DatasetReaderInput , | train_dataset_reader : DatasetReader , | * , test_dataset_reader : Optional [ DatasetReader ] = None , | * , train_data_loader : Lazy [ DataLoader ] = Lazy ( SimpleDataLoader . from_dataset_reader ), | * , test_data_loader : Lazy [ DataLoader ] = Lazy ( SimpleDataLoader . from_dataset_reader ), | * , params_to_freeze : Optional [ List [ str ]] = None , | * , cuda_device : int = - 1 | ) -> None An InfluenceInterpreter interprets an AllenNLP models's outputs by finding the training instances that had the most influence on the prediction for each test input. See Understanding Black-box Predictions via Influence Functions for more information. Subclasses are required to implement the _calculate_influence_scores() method.","title":"InfluenceInterpreter"},{"location":"api/interpret/influence_interpreters/influence_interpreter/#default_implementation","text":"class InfluenceInterpreter ( Registrable ): | ... | default_implementation = \"simple-influence\"","title":"default_implementation"},{"location":"api/interpret/influence_interpreters/influence_interpreter/#used_params","text":"class InfluenceInterpreter ( Registrable ): | ... | @property | def used_params ( self ) -> List [ torch . nn . Parameter ] The parameters of the model that have non-zero gradients after a backwards pass. This can be used to gather the corresponding gradients with respect to a loss via the torch.autograd.grad function. Note Accessing this property requires calling self._gather_train_instances_and_compute_gradients() if it hasn't been called yet, which may take several minutes.","title":"used_params"},{"location":"api/interpret/influence_interpreters/influence_interpreter/#used_param_names","text":"class InfluenceInterpreter ( Registrable ): | ... | @property | def used_param_names ( self ) -> List [ str ] The names of the corresponding parameters in self.used_params . Note Accessing this property requires calling self._gather_train_instances_and_compute_gradients() if it hasn't been called yet, which may take several minutes.","title":"used_param_names"},{"location":"api/interpret/influence_interpreters/influence_interpreter/#train_instances","text":"class InfluenceInterpreter ( Registrable ): | ... | @property | def train_instances ( self ) -> List [ InstanceWithGrads ] The training instances along with their corresponding loss and gradients. Note Accessing this property requires calling self._gather_train_instances_and_compute_gradients() if it hasn't been called yet, which may take several minutes.","title":"train_instances"},{"location":"api/interpret/influence_interpreters/influence_interpreter/#from_path","text":"class InfluenceInterpreter ( Registrable ): | ... | @classmethod | def from_path ( | cls , | archive_path : Union [ str , PathLike ], | * , interpreter_name : Optional [ str ] = None , | * , train_data_path : Optional [ DatasetReaderInput ] = None , | * , train_data_loader : Lazy [ DataLoader ] = Lazy ( SimpleDataLoader . from_dataset_reader ), | * , test_data_loader : Lazy [ DataLoader ] = Lazy ( SimpleDataLoader . from_dataset_reader ), | * , params_to_freeze : Optional [ List [ str ]] = None , | * , cuda_device : int = - 1 , | * , import_plugins : bool = True , | * , overrides : Union [ str , Dict [ str , Any ]] = \"\" , | ** extras , | * , , | ) -> \"InfluenceInterpreter\" Load an InfluenceInterpreter from an archive path.","title":"from_path"},{"location":"api/interpret/influence_interpreters/influence_interpreter/#from_archive","text":"class InfluenceInterpreter ( Registrable ): | ... | @classmethod | def from_archive ( | cls , | archive : Archive , | * , interpreter_name : Optional [ str ] = None , | * , train_data_path : Optional [ DatasetReaderInput ] = None , | * , train_data_loader : Lazy [ DataLoader ] = Lazy ( SimpleDataLoader . from_dataset_reader ), | * , test_data_loader : Lazy [ DataLoader ] = Lazy ( SimpleDataLoader . from_dataset_reader ), | * , params_to_freeze : Optional [ List [ str ]] = None , | * , cuda_device : int = - 1 , | ** extras , | * , , | ) -> \"InfluenceInterpreter\" Load an InfluenceInterpreter from an Archive . The other parameters are the same as .from_path() .","title":"from_archive"},{"location":"api/interpret/influence_interpreters/influence_interpreter/#interpret","text":"class InfluenceInterpreter ( Registrable ): | ... | def interpret ( | self , | test_instance : Instance , | k : int = 20 | ) -> InterpretOutput Run the influence function scorer on the given instance, returning the top k most influential train instances with their scores. Note Test instances should have targets so that a loss can be computed.","title":"interpret"},{"location":"api/interpret/influence_interpreters/influence_interpreter/#interpret_from_file","text":"class InfluenceInterpreter ( Registrable ): | ... | def interpret_from_file ( | self , | test_data_path : DatasetReaderInput , | k : int = 20 | ) -> List [ InterpretOutput ] Runs interpret_instances over the instances read from test_data_path . Note Test instances should have targets so that a loss can be computed.","title":"interpret_from_file"},{"location":"api/interpret/influence_interpreters/influence_interpreter/#interpret_instances","text":"class InfluenceInterpreter ( Registrable ): | ... | def interpret_instances ( | self , | test_instances : List [ Instance ], | k : int = 20 | ) -> List [ InterpretOutput ] Run the influence function scorer on the given instances, returning the top k most influential train instances for each test instance. Note Test instances should have targets so that a loss can be computed.","title":"interpret_instances"},{"location":"api/interpret/influence_interpreters/influence_interpreter/#_calculate_influence_scores","text":"class InfluenceInterpreter ( Registrable ): | ... | def _calculate_influence_scores ( | self , | test_instance : Instance , | test_loss : float , | test_grads : Sequence [ torch . Tensor ] | ) -> List [ float ] Required to be implemented by subclasses. Calculates the influence scores of self.train_instances with respect to the given test_instance .","title":"_calculate_influence_scores"},{"location":"api/interpret/influence_interpreters/simple_influence/","text":"allennlp .interpret .influence_interpreters .simple_influence [SOURCE] SimpleInfluence \u00b6 @InfluenceInterpreter . register ( \"simple-influence\" ) class SimpleInfluence ( InfluenceInterpreter ): | def __init__ ( | self , | model : Model , | train_data_path : DatasetReaderInput , | train_dataset_reader : DatasetReader , | * , test_dataset_reader : Optional [ DatasetReader ] = None , | * , train_data_loader : Lazy [ DataLoader ] = Lazy ( SimpleDataLoader . from_dataset_reader ), | * , test_data_loader : Lazy [ DataLoader ] = Lazy ( SimpleDataLoader . from_dataset_reader ), | * , params_to_freeze : List [ str ] = None , | * , cuda_device : int = - 1 , | * , lissa_batch_size : int = 8 , | * , damping : float = 3e-3 , | * , num_samples : int = 1 , | * , recursion_depth : Union [ float , int ] = 0.25 , | * , scale : float = 1e4 | ) -> None Registered as an InfluenceInterpreter with name \"simple-influence\". This goes through every example in the train set to calculate the influence score. It uses LiSSA (Linear time Stochastic Second-Order Algorithm) to approximate the inverse of the Hessian used for the influence score calculation. Parameters \u00b6 lissa_batch_size : int , optional (default = 8 ) The batch size to use for LiSSA. According to Koh, P.W., & Liang, P. (2017) , it is better to use batched samples for approximation for better stability. damping : float , optional (default = 3e-3 ) This is a hyperparameter for LiSSA. A damping termed added in case the approximated Hessian (during LiSSA) has negative eigenvalues. num_samples : int , optional (default = 1 ) This is a hyperparameter for LiSSA that we determine how many rounds of the recursion process we would like to run for approxmation. recursion_depth : Union[float, int] , optional (default = 0.25 ) This is a hyperparameter for LiSSA that determines the recursion depth we would like to go through. If a float , it means X% of the training examples. If an int , it means recurse for X times. scale : float , optional (default = 1e4 ) This is a hyperparameter for LiSSA to tune such that the Taylor expansion converges. It is applied to scale down the loss during LiSSA to ensure that H <= I , where H is the Hessian and I is the identity matrix. See footnote 2 of Koh, P.W., & Liang, P. (2017) . Note We choose the same default values for the LiSSA hyperparameters as Han, Xiaochuang et al. (2020) . get_inverse_hvp_lissa \u00b6 def get_inverse_hvp_lissa ( vs : Sequence [ torch . Tensor ], model : Model , used_params : Sequence [ torch . Tensor ], lissa_data_loader : DataLoader , damping : float , num_samples : int , scale : float ) -> torch . Tensor This function approximates the product of the inverse of the Hessian and the vectors vs using LiSSA. Adapted from github.com/kohpangwei/influence-release , the repo for Koh, P.W., & Liang, P. (2017) , and github.com/xhan77/influence-function-analysis , the repo for Han, Xiaochuang et al. (2020) . get_hvp \u00b6 def get_hvp ( loss : torch . Tensor , params : Sequence [ torch . Tensor ], vectors : Sequence [ torch . Tensor ] ) -> Tuple [ torch . Tensor , ... ] Get a Hessian-Vector Product (HVP) Hv for each Hessian H of the loss with respect to the one of the parameter tensors in params and the corresponding vector v in vectors . Parameters \u00b6 loss : torch.Tensor The loss calculated from the output of the model. params : Sequence[torch.Tensor] Tunable and used parameters in the model that we will calculate the gradient and hessian with respect to. vectors : Sequence[torch.Tensor] The list of vectors for calculating the HVP.","title":"simple_influence"},{"location":"api/interpret/influence_interpreters/simple_influence/#simpleinfluence","text":"@InfluenceInterpreter . register ( \"simple-influence\" ) class SimpleInfluence ( InfluenceInterpreter ): | def __init__ ( | self , | model : Model , | train_data_path : DatasetReaderInput , | train_dataset_reader : DatasetReader , | * , test_dataset_reader : Optional [ DatasetReader ] = None , | * , train_data_loader : Lazy [ DataLoader ] = Lazy ( SimpleDataLoader . from_dataset_reader ), | * , test_data_loader : Lazy [ DataLoader ] = Lazy ( SimpleDataLoader . from_dataset_reader ), | * , params_to_freeze : List [ str ] = None , | * , cuda_device : int = - 1 , | * , lissa_batch_size : int = 8 , | * , damping : float = 3e-3 , | * , num_samples : int = 1 , | * , recursion_depth : Union [ float , int ] = 0.25 , | * , scale : float = 1e4 | ) -> None Registered as an InfluenceInterpreter with name \"simple-influence\". This goes through every example in the train set to calculate the influence score. It uses LiSSA (Linear time Stochastic Second-Order Algorithm) to approximate the inverse of the Hessian used for the influence score calculation.","title":"SimpleInfluence"},{"location":"api/interpret/influence_interpreters/simple_influence/#get_inverse_hvp_lissa","text":"def get_inverse_hvp_lissa ( vs : Sequence [ torch . Tensor ], model : Model , used_params : Sequence [ torch . Tensor ], lissa_data_loader : DataLoader , damping : float , num_samples : int , scale : float ) -> torch . Tensor This function approximates the product of the inverse of the Hessian and the vectors vs using LiSSA. Adapted from github.com/kohpangwei/influence-release , the repo for Koh, P.W., & Liang, P. (2017) , and github.com/xhan77/influence-function-analysis , the repo for Han, Xiaochuang et al. (2020) .","title":"get_inverse_hvp_lissa"},{"location":"api/interpret/influence_interpreters/simple_influence/#get_hvp","text":"def get_hvp ( loss : torch . Tensor , params : Sequence [ torch . Tensor ], vectors : Sequence [ torch . Tensor ] ) -> Tuple [ torch . Tensor , ... ] Get a Hessian-Vector Product (HVP) Hv for each Hessian H of the loss with respect to the one of the parameter tensors in params and the corresponding vector v in vectors .","title":"get_hvp"},{"location":"api/interpret/saliency_interpreters/integrated_gradient/","text":"allennlp .interpret .saliency_interpreters .integrated_gradient [SOURCE] IntegratedGradient \u00b6 @SaliencyInterpreter . register ( \"integrated-gradient\" ) class IntegratedGradient ( SaliencyInterpreter ) Interprets the prediction using Integrated Gradients (https://arxiv.org/abs/1703.01365) Registered as a SaliencyInterpreter with name \"integrated-gradient\". saliency_interpret_from_json \u00b6 class IntegratedGradient ( SaliencyInterpreter ): | ... | def saliency_interpret_from_json ( self , inputs : JsonDict ) -> JsonDict","title":"integrated_gradient"},{"location":"api/interpret/saliency_interpreters/integrated_gradient/#integratedgradient","text":"@SaliencyInterpreter . register ( \"integrated-gradient\" ) class IntegratedGradient ( SaliencyInterpreter ) Interprets the prediction using Integrated Gradients (https://arxiv.org/abs/1703.01365) Registered as a SaliencyInterpreter with name \"integrated-gradient\".","title":"IntegratedGradient"},{"location":"api/interpret/saliency_interpreters/integrated_gradient/#saliency_interpret_from_json","text":"class IntegratedGradient ( SaliencyInterpreter ): | ... | def saliency_interpret_from_json ( self , inputs : JsonDict ) -> JsonDict","title":"saliency_interpret_from_json"},{"location":"api/interpret/saliency_interpreters/saliency_interpreter/","text":"allennlp .interpret .saliency_interpreters .saliency_interpreter [SOURCE] SaliencyInterpreter \u00b6 class SaliencyInterpreter ( Registrable ): | def __init__ ( self , predictor : Predictor ) -> None A SaliencyInterpreter interprets an AllenNLP Predictor's outputs by assigning a saliency score to each input token. saliency_interpret_from_json \u00b6 class SaliencyInterpreter ( Registrable ): | ... | def saliency_interpret_from_json ( self , inputs : JsonDict ) -> JsonDict This function finds saliency values for each input token. Parameters \u00b6 inputs : JsonDict The input you want to interpret (the same as the argument to a Predictor, e.g., predict_json()). Returns \u00b6 interpretation : JsonDict Contains the normalized saliency values for each input token. The dict has entries for each instance in the inputs JsonDict, e.g., {instance_1: ..., instance_2:, ... } . Each one of those entries has entries for the saliency of the inputs, e.g., {grad_input_1: ..., grad_input_2: ... } .","title":"saliency_interpreter"},{"location":"api/interpret/saliency_interpreters/saliency_interpreter/#saliencyinterpreter","text":"class SaliencyInterpreter ( Registrable ): | def __init__ ( self , predictor : Predictor ) -> None A SaliencyInterpreter interprets an AllenNLP Predictor's outputs by assigning a saliency score to each input token.","title":"SaliencyInterpreter"},{"location":"api/interpret/saliency_interpreters/saliency_interpreter/#saliency_interpret_from_json","text":"class SaliencyInterpreter ( Registrable ): | ... | def saliency_interpret_from_json ( self , inputs : JsonDict ) -> JsonDict This function finds saliency values for each input token.","title":"saliency_interpret_from_json"},{"location":"api/interpret/saliency_interpreters/simple_gradient/","text":"allennlp .interpret .saliency_interpreters .simple_gradient [SOURCE] SimpleGradient \u00b6 @SaliencyInterpreter . register ( \"simple-gradient\" ) class SimpleGradient ( SaliencyInterpreter ) Registered as a SaliencyInterpreter with name \"simple-gradient\". saliency_interpret_from_json \u00b6 class SimpleGradient ( SaliencyInterpreter ): | ... | def saliency_interpret_from_json ( self , inputs : JsonDict ) -> JsonDict Interprets the model's prediction for inputs. Gets the gradients of the loss with respect to the input and returns those gradients normalized and sanitized.","title":"simple_gradient"},{"location":"api/interpret/saliency_interpreters/simple_gradient/#simplegradient","text":"@SaliencyInterpreter . register ( \"simple-gradient\" ) class SimpleGradient ( SaliencyInterpreter ) Registered as a SaliencyInterpreter with name \"simple-gradient\".","title":"SimpleGradient"},{"location":"api/interpret/saliency_interpreters/simple_gradient/#saliency_interpret_from_json","text":"class SimpleGradient ( SaliencyInterpreter ): | ... | def saliency_interpret_from_json ( self , inputs : JsonDict ) -> JsonDict Interprets the model's prediction for inputs. Gets the gradients of the loss with respect to the input and returns those gradients normalized and sanitized.","title":"saliency_interpret_from_json"},{"location":"api/interpret/saliency_interpreters/smooth_gradient/","text":"allennlp .interpret .saliency_interpreters .smooth_gradient [SOURCE] SmoothGradient \u00b6 @SaliencyInterpreter . register ( \"smooth-gradient\" ) class SmoothGradient ( SaliencyInterpreter ): | def __init__ ( self , predictor : Predictor ) -> None Interprets the prediction using SmoothGrad (https://arxiv.org/abs/1706.03825) Registered as a SaliencyInterpreter with name \"smooth-gradient\". saliency_interpret_from_json \u00b6 class SmoothGradient ( SaliencyInterpreter ): | ... | def saliency_interpret_from_json ( self , inputs : JsonDict ) -> JsonDict","title":"smooth_gradient"},{"location":"api/interpret/saliency_interpreters/smooth_gradient/#smoothgradient","text":"@SaliencyInterpreter . register ( \"smooth-gradient\" ) class SmoothGradient ( SaliencyInterpreter ): | def __init__ ( self , predictor : Predictor ) -> None Interprets the prediction using SmoothGrad (https://arxiv.org/abs/1706.03825) Registered as a SaliencyInterpreter with name \"smooth-gradient\".","title":"SmoothGradient"},{"location":"api/interpret/saliency_interpreters/smooth_gradient/#saliency_interpret_from_json","text":"class SmoothGradient ( SaliencyInterpreter ): | ... | def saliency_interpret_from_json ( self , inputs : JsonDict ) -> JsonDict","title":"saliency_interpret_from_json"},{"location":"api/models/archival/","text":"allennlp .models .archival [SOURCE] Helper functions for archiving models and restoring archived models. Archive \u00b6 class Archive ( NamedTuple ) An archive comprises a Model and its experimental config model \u00b6 class Archive ( NamedTuple ): | ... | model : Model = None config \u00b6 class Archive ( NamedTuple ): | ... | config : Params = None dataset_reader \u00b6 class Archive ( NamedTuple ): | ... | dataset_reader : DatasetReader = None validation_dataset_reader \u00b6 class Archive ( NamedTuple ): | ... | validation_dataset_reader : DatasetReader = None meta \u00b6 class Archive ( NamedTuple ): | ... | meta : Optional [ Meta ] = None extract_module \u00b6 class Archive ( NamedTuple ): | ... | def extract_module ( self , path : str , freeze : bool = True ) -> Module This method can be used to load a module from the pretrained model archive. It is also used implicitly in FromParams based construction. So instead of using standard params to construct a module, you can instead load a pretrained module from the model archive directly. For eg, instead of using params like {\"type\": \"module_type\", ...}, you can use the following template:: { \"_pretrained\": { \"archive_file\": \"../path/to/model.tar.gz\", \"path\": \"path.to.module.in.model\", \"freeze\": False } } If you use this feature with FromParams, take care of the following caveat: Call to initializer(self) at end of model initializer can potentially wipe the transferred parameters by reinitializing them. This can happen if you have setup initializer regex that also matches parameters of the transferred module. To safe-guard against this, you can either update your initializer regex to prevent conflicting match or add extra initializer:: [ [\".*transferred_module_name.*\", \"prevent\"]] ] Parameters \u00b6 path : str Path of target module to be loaded from the model. Eg. \"_textfield_embedder.token_embedder_tokens\" freeze : bool , optional (default = True ) Whether to freeze the module parameters or not. CONFIG_NAME \u00b6 CONFIG_NAME = \"config.json\" verify_include_in_archive \u00b6 def verify_include_in_archive ( include_in_archive : Optional [ List [ str ]] = None ) archive_model \u00b6 def archive_model ( serialization_dir : Union [ str , PathLike ], weights : str = _DEFAULT_WEIGHTS , archive_path : Union [ str , PathLike ] = None , include_in_archive : Optional [ List [ str ]] = None ) -> str Archive the model weights, its training configuration, and its vocabulary to model.tar.gz . Parameters \u00b6 serialization_dir : str The directory where the weights and vocabulary are written out. weights : str , optional (default = _DEFAULT_WEIGHTS ) Which weights file to include in the archive. The default is best.th . archive_path : str , optional (default = None ) A full path to serialize the model to. The default is \"model.tar.gz\" inside the serialization_dir. If you pass a directory here, we'll serialize the model to \"model.tar.gz\" inside the directory. include_in_archive : List[str] , optional (default = None ) Paths relative to serialization_dir that should be archived in addition to the default ones. Returns \u00b6 The final archive path. load_archive \u00b6 def load_archive ( archive_file : Union [ str , PathLike ], cuda_device : int = - 1 , overrides : Union [ str , Dict [ str , Any ]] = \"\" , weights_file : str = None ) -> Archive Instantiates an Archive from an archived tar.gz file. Parameters \u00b6 archive_file : Union[str, PathLike] The archive file to load the model from. cuda_device : int , optional (default = -1 ) If cuda_device is >= 0, the model will be loaded onto the corresponding GPU. Otherwise it will be loaded onto the CPU. overrides : Union[str, Dict[str, Any]] , optional (default = \"\" ) JSON overrides to apply to the unarchived Params object. weights_file : str , optional (default = None ) The weights file to use. If unspecified, weights.th in the archive_file will be used. get_weights_path \u00b6 def get_weights_path ( serialization_dir ) extracted_archive \u00b6 @contextmanager def extracted_archive ( resolved_archive_file , cleanup = True )","title":"archival"},{"location":"api/models/archival/#archive","text":"class Archive ( NamedTuple ) An archive comprises a Model and its experimental config","title":"Archive"},{"location":"api/models/archival/#model","text":"class Archive ( NamedTuple ): | ... | model : Model = None","title":"model"},{"location":"api/models/archival/#config","text":"class Archive ( NamedTuple ): | ... | config : Params = None","title":"config"},{"location":"api/models/archival/#dataset_reader","text":"class Archive ( NamedTuple ): | ... | dataset_reader : DatasetReader = None","title":"dataset_reader"},{"location":"api/models/archival/#validation_dataset_reader","text":"class Archive ( NamedTuple ): | ... | validation_dataset_reader : DatasetReader = None","title":"validation_dataset_reader"},{"location":"api/models/archival/#meta","text":"class Archive ( NamedTuple ): | ... | meta : Optional [ Meta ] = None","title":"meta"},{"location":"api/models/archival/#extract_module","text":"class Archive ( NamedTuple ): | ... | def extract_module ( self , path : str , freeze : bool = True ) -> Module This method can be used to load a module from the pretrained model archive. It is also used implicitly in FromParams based construction. So instead of using standard params to construct a module, you can instead load a pretrained module from the model archive directly. For eg, instead of using params like {\"type\": \"module_type\", ...}, you can use the following template:: { \"_pretrained\": { \"archive_file\": \"../path/to/model.tar.gz\", \"path\": \"path.to.module.in.model\", \"freeze\": False } } If you use this feature with FromParams, take care of the following caveat: Call to initializer(self) at end of model initializer can potentially wipe the transferred parameters by reinitializing them. This can happen if you have setup initializer regex that also matches parameters of the transferred module. To safe-guard against this, you can either update your initializer regex to prevent conflicting match or add extra initializer:: [ [\".*transferred_module_name.*\", \"prevent\"]] ]","title":"extract_module"},{"location":"api/models/archival/#config_name","text":"CONFIG_NAME = \"config.json\"","title":"CONFIG_NAME"},{"location":"api/models/archival/#verify_include_in_archive","text":"def verify_include_in_archive ( include_in_archive : Optional [ List [ str ]] = None )","title":"verify_include_in_archive"},{"location":"api/models/archival/#archive_model","text":"def archive_model ( serialization_dir : Union [ str , PathLike ], weights : str = _DEFAULT_WEIGHTS , archive_path : Union [ str , PathLike ] = None , include_in_archive : Optional [ List [ str ]] = None ) -> str Archive the model weights, its training configuration, and its vocabulary to model.tar.gz .","title":"archive_model"},{"location":"api/models/archival/#load_archive","text":"def load_archive ( archive_file : Union [ str , PathLike ], cuda_device : int = - 1 , overrides : Union [ str , Dict [ str , Any ]] = \"\" , weights_file : str = None ) -> Archive Instantiates an Archive from an archived tar.gz file.","title":"load_archive"},{"location":"api/models/archival/#get_weights_path","text":"def get_weights_path ( serialization_dir )","title":"get_weights_path"},{"location":"api/models/archival/#extracted_archive","text":"@contextmanager def extracted_archive ( resolved_archive_file , cleanup = True )","title":"extracted_archive"},{"location":"api/models/basic_classifier/","text":"allennlp .models .basic_classifier [SOURCE] BasicClassifier \u00b6 @Model . register ( \"basic_classifier\" ) class BasicClassifier ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | seq2vec_encoder : Seq2VecEncoder , | seq2seq_encoder : Seq2SeqEncoder = None , | feedforward : Optional [ FeedForward ] = None , | dropout : float = None , | num_labels : int = None , | label_namespace : str = \"labels\" , | namespace : str = \"tokens\" , | initializer : InitializerApplicator = InitializerApplicator (), | ** kwargs | ) -> None This Model implements a basic text classifier. After embedding the text into a text field, we will optionally encode the embeddings with a Seq2SeqEncoder . The resulting sequence is pooled using a Seq2VecEncoder and then passed to a linear classification layer, which projects into the label space. If a Seq2SeqEncoder is not provided, we will pass the embedded text directly to the Seq2VecEncoder . Registered as a Model with name \"basic_classifier\". Parameters \u00b6 vocab : Vocabulary text_field_embedder : TextFieldEmbedder Used to embed the input text into a TextField seq2seq_encoder : Seq2SeqEncoder , optional (default = None ) Optional Seq2Seq encoder layer for the input text. seq2vec_encoder : Seq2VecEncoder Required Seq2Vec encoder layer. If seq2seq_encoder is provided, this encoder will pool its output. Otherwise, this encoder will operate directly on the output of the text_field_embedder . feedforward : FeedForward , optional (default = None ) An optional feedforward layer to apply after the seq2vec_encoder. dropout : float , optional (default = None ) Dropout percentage to use. num_labels : int , optional (default = None ) Number of labels to project to in classification layer. By default, the classification layer will project to the size of the vocabulary namespace corresponding to labels. namespace : str , optional (default = \"tokens\" ) Vocabulary namespace corresponding to the input text. By default, we use the \"tokens\" namespace. label_namespace : str , optional (default = \"labels\" ) Vocabulary namespace corresponding to labels. By default, we use the \"labels\" namespace. initializer : InitializerApplicator , optional (default = InitializerApplicator() ) If provided, will be used to initialize the model parameters. forward \u00b6 class BasicClassifier ( Model ): | ... | def forward ( | self , | tokens : TextFieldTensors , | label : torch . IntTensor = None , | metadata : MetadataField = None | ) -> Dict [ str , torch . Tensor ] Parameters \u00b6 tokens : TextFieldTensors From a TextField label : torch.IntTensor , optional (default = None ) From a LabelField Returns \u00b6 An output dictionary consisting of: logits ( torch.FloatTensor ) : A tensor of shape (batch_size, num_labels) representing unnormalized log probabilities of the label. probs ( torch.FloatTensor ) : A tensor of shape (batch_size, num_labels) representing probabilities of the label. loss : ( torch.FloatTensor , optional) : A scalar loss to be optimised. make_output_human_readable \u00b6 class BasicClassifier ( Model ): | ... | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Does a simple argmax over the probabilities, converts index to string label, and add \"label\" key to the dictionary with the result. get_metrics \u00b6 class BasicClassifier ( Model ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] default_predictor \u00b6 class BasicClassifier ( Model ): | ... | default_predictor = \"text_classifier\"","title":"basic_classifier"},{"location":"api/models/basic_classifier/#basicclassifier","text":"@Model . register ( \"basic_classifier\" ) class BasicClassifier ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | seq2vec_encoder : Seq2VecEncoder , | seq2seq_encoder : Seq2SeqEncoder = None , | feedforward : Optional [ FeedForward ] = None , | dropout : float = None , | num_labels : int = None , | label_namespace : str = \"labels\" , | namespace : str = \"tokens\" , | initializer : InitializerApplicator = InitializerApplicator (), | ** kwargs | ) -> None This Model implements a basic text classifier. After embedding the text into a text field, we will optionally encode the embeddings with a Seq2SeqEncoder . The resulting sequence is pooled using a Seq2VecEncoder and then passed to a linear classification layer, which projects into the label space. If a Seq2SeqEncoder is not provided, we will pass the embedded text directly to the Seq2VecEncoder . Registered as a Model with name \"basic_classifier\".","title":"BasicClassifier"},{"location":"api/models/basic_classifier/#forward","text":"class BasicClassifier ( Model ): | ... | def forward ( | self , | tokens : TextFieldTensors , | label : torch . IntTensor = None , | metadata : MetadataField = None | ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"api/models/basic_classifier/#make_output_human_readable","text":"class BasicClassifier ( Model ): | ... | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Does a simple argmax over the probabilities, converts index to string label, and add \"label\" key to the dictionary with the result.","title":"make_output_human_readable"},{"location":"api/models/basic_classifier/#get_metrics","text":"class BasicClassifier ( Model ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"api/models/basic_classifier/#default_predictor","text":"class BasicClassifier ( Model ): | ... | default_predictor = \"text_classifier\"","title":"default_predictor"},{"location":"api/models/model/","text":"allennlp .models .model [SOURCE] Model is an abstract class representing an AllenNLP model. Model \u00b6 class Model ( Module , Registrable ): | def __init__ ( | self , | vocab : Vocabulary , | regularizer : RegularizerApplicator = None , | serialization_dir : Optional [ str ] = None , | ddp_accelerator : Optional [ DdpAccelerator ] = None | ) -> None This abstract class represents a model to be trained. Rather than relying completely on the Pytorch Module, we modify the output spec of forward to be a dictionary. Models built using this API are still compatible with other pytorch models and can be used naturally as modules within other models - outputs are dictionaries, which can be unpacked and passed into other layers. One caveat to this is that if you wish to use an AllenNLP model inside a Container (such as nn.Sequential), you must interleave the models with a wrapper module which unpacks the dictionary into a list of tensors. In order for your model to be trained using the Trainer api, the output dictionary of your Model must include a \"loss\" key, which will be optimised during the training process. Finally, you can optionally implement Model.get_metrics in order to make use of early stopping and best-model serialization based on a validation metric in Trainer . Metrics that begin with \"_\" will not be logged to the progress bar by Trainer . The from_archive method on this class is registered as a Model with name \"from_archive\". So, if you are using a configuration file, you can specify a model as {\"type\": \"from_archive\", \"archive_file\": \"/path/to/archive.tar.gz\"} , which will pull out the model from the given location and return it. Parameters \u00b6 vocab : Vocabulary There are two typical use-cases for the Vocabulary in a Model : getting vocabulary sizes when constructing embedding matrices or output classifiers (as the vocabulary holds the number of classes in your output, also), and translating model output into human-readable form. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"model\", it gets specified as a top-level parameter, then is passed in to the model automatically. regularizer : RegularizerApplicator , optional If given, the Trainer will use this to regularize model parameters. serialization_dir : str , optional The directory in which the training output is saved to, or the directory the model is loaded from. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"model\". ddp_accelerator : Optional[DdpAccelerator] , optional The DdpAccelerator used in distributing training. If not in distributed training, this will be None . In a typical AllenNLP configuration file, this parameter does not get an entry under the \"model\", it gets specified as \"ddp_accelerator\" in the \"distributed\" part of the config, and is then passed in to the model automatically. It will be available to Model instances as self.ddp_accelerator . default_predictor \u00b6 class Model ( Module , Registrable ): | ... | default_predictor : Optional [ str ] = None get_regularization_penalty \u00b6 class Model ( Module , Registrable ): | ... | def get_regularization_penalty ( self ) -> Optional [ torch . Tensor ] Computes the regularization penalty for the model. Returns None if the model was not configured to use regularization. get_parameters_for_histogram_logging \u00b6 class Model ( Module , Registrable ): | ... | def get_parameters_for_histogram_logging ( self ) -> List [ str ] Returns the name of model parameters used for logging histograms to tensorboard. get_parameters_for_histogram_tensorboard_logging \u00b6 class Model ( Module , Registrable ): | ... | def get_parameters_for_histogram_tensorboard_logging ( | self | ) -> List [ str ] Returns the name of model parameters used for logging histograms to tensorboard. forward \u00b6 class Model ( Module , Registrable ): | ... | def forward ( self , * inputs ) -> Dict [ str , torch . Tensor ] Defines the forward pass of the model. In addition, to facilitate easy training, this method is designed to compute a loss function defined by a user. The input is comprised of everything required to perform a training update, including labels - you define the signature here! It is down to the user to ensure that inference can be performed without the presence of these labels. Hence, any inputs not available at inference time should only be used inside a conditional block. The intended sketch of this method is as follows:: def forward ( self , input1 , input2 , targets = None ) : .... .... output1 = self . layer1 ( input1 ) output2 = self . layer2 ( input2 ) output_dict = { \" output1 \" : output1 , \" output2 \" : output2 } if targets is not None : # Function returning a scalar torch . Tensor , defined by the user . loss = self . _compute_loss ( output1 , output2 , targets ) output_dict [ \" loss \" ] = loss return output_dict Parameters \u00b6 *inputs : Any Tensors comprising everything needed to perform a training update, including labels, which should be optional (i.e have a default value of None ). At inference time, simply pass the relevant inputs, not including the labels. Returns \u00b6 output_dict : Dict[str, torch.Tensor] The outputs from the model. In order to train a model using the Trainer api, you must provide a \"loss\" key pointing to a scalar torch.Tensor representing the loss to be optimized. forward_on_instance \u00b6 class Model ( Module , Registrable ): | ... | def forward_on_instance ( | self , | instance : Instance | ) -> Dict [ str , numpy . ndarray ] Takes an Instance , which typically has raw text in it, converts that text into arrays using this model's Vocabulary , passes those arrays through self.forward() and self.make_output_human_readable() (which by default does nothing) and returns the result. Before returning the result, we convert any torch.Tensors into numpy arrays and remove the batch dimension. forward_on_instances \u00b6 class Model ( Module , Registrable ): | ... | def forward_on_instances ( | self , | instances : List [ Instance ] | ) -> List [ Dict [ str , numpy . ndarray ]] Takes a list of Instances , converts that text into arrays using this model's Vocabulary , passes those arrays through self.forward() and self.make_output_human_readable() (which by default does nothing) and returns the result. Before returning the result, we convert any torch.Tensors into numpy arrays and separate the batched output into a list of individual dicts per instance. Note that typically this will be faster on a GPU (and conditionally, on a CPU) than repeated calls to forward_on_instance . Parameters \u00b6 instances : List[Instance] The instances to run the model on. Returns \u00b6 A list of the models output for each instance. make_output_human_readable \u00b6 class Model ( Module , Registrable ): | ... | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Takes the result of forward and makes it human readable. Most of the time, the only thing this method does is convert tokens / predicted labels from tensors to strings that humans might actually understand. Somtimes you'll also do an argmax or something in here, too, but that most often happens in Model.forward , before you compute your metrics. This method modifies the input dictionary, and also returns the same dictionary. By default in the base class we do nothing. get_metrics \u00b6 class Model ( Module , Registrable ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] Returns a dictionary of metrics. This method will be called by allennlp.training.Trainer in order to compute and use model metrics for early stopping and model serialization. We return an empty dictionary here rather than raising as it is not required to implement metrics for a new model. A boolean reset parameter is passed, as frequently a metric accumulator will have some state which should be reset between epochs. This is also compatible with Metric s . Metrics should be populated during the call to forward , with the Metric handling the accumulation of the metric until this method is called. load \u00b6 class Model ( Module , Registrable ): | ... | @classmethod | def load ( | cls , | config : Params , | serialization_dir : Union [ str , PathLike ], | weights_file : Optional [ Union [ str , PathLike ]] = None , | cuda_device : int = - 1 | ) -> \"Model\" Instantiates an already-trained model, based on the experiment configuration and some optional overrides. Parameters \u00b6 config : Params The configuration that was used to train the model. It should definitely have a model section, and should probably have a trainer section as well. serialization_dir : str = None The directory containing the serialized weights, parameters, and vocabulary of the model. weights_file : str = None By default we load the weights from best.th in the serialization directory, but you can override that value here. cuda_device : int = -1 By default we load the model on the CPU, but if you want to load it for GPU usage you can specify the id of your GPU here Returns \u00b6 model : Model The model specified in the configuration, loaded with the serialized vocabulary and the trained weights. extend_embedder_vocab \u00b6 class Model ( Module , Registrable ): | ... | def extend_embedder_vocab ( | self , | embedding_sources_mapping : Dict [ str , str ] = None | ) -> None Iterates through all embedding modules in the model and assures it can embed with the extended vocab. This is required in fine-tuning or transfer learning scenarios where model was trained with original vocabulary but during fine-tuning/transfer-learning, it will have it work with extended vocabulary (original + new-data vocabulary). Parameters \u00b6 embedding_sources_mapping : Dict[str, str] , optional (default = None ) Mapping from model_path to pretrained-file path of the embedding modules. If pretrained-file used at time of embedding initialization isn't available now, user should pass this mapping. Model path is path traversing the model attributes upto this embedding module. Eg. \"_text_field_embedder.token_embedder_tokens\". from_archive \u00b6 class Model ( Module , Registrable ): | ... | @classmethod | def from_archive ( | cls , | archive_file : str , | vocab : Vocabulary = None | ) -> \"Model\" Loads a model from an archive file. This basically just calls return archival.load_archive(archive_file).model . It exists as a method here for convenience, and so that we can register it for easy use for fine tuning an existing model from a config file. If vocab is given, we will extend the loaded model's vocabulary using the passed vocab object (including calling extend_embedder_vocab , which extends embedding layers). remove_weights_related_keys_from_params \u00b6 def remove_weights_related_keys_from_params ( params : Params , keys : List [ str ] = [ \"pretrained_file\" , \"initializer\" ] ) remove_pretrained_embedding_params \u00b6 def remove_pretrained_embedding_params ( params : Params ) This function only exists for backwards compatibility. Please use remove_weights_related_keys_from_params() instead.","title":"model"},{"location":"api/models/model/#model","text":"class Model ( Module , Registrable ): | def __init__ ( | self , | vocab : Vocabulary , | regularizer : RegularizerApplicator = None , | serialization_dir : Optional [ str ] = None , | ddp_accelerator : Optional [ DdpAccelerator ] = None | ) -> None This abstract class represents a model to be trained. Rather than relying completely on the Pytorch Module, we modify the output spec of forward to be a dictionary. Models built using this API are still compatible with other pytorch models and can be used naturally as modules within other models - outputs are dictionaries, which can be unpacked and passed into other layers. One caveat to this is that if you wish to use an AllenNLP model inside a Container (such as nn.Sequential), you must interleave the models with a wrapper module which unpacks the dictionary into a list of tensors. In order for your model to be trained using the Trainer api, the output dictionary of your Model must include a \"loss\" key, which will be optimised during the training process. Finally, you can optionally implement Model.get_metrics in order to make use of early stopping and best-model serialization based on a validation metric in Trainer . Metrics that begin with \"_\" will not be logged to the progress bar by Trainer . The from_archive method on this class is registered as a Model with name \"from_archive\". So, if you are using a configuration file, you can specify a model as {\"type\": \"from_archive\", \"archive_file\": \"/path/to/archive.tar.gz\"} , which will pull out the model from the given location and return it.","title":"Model"},{"location":"api/models/model/#default_predictor","text":"class Model ( Module , Registrable ): | ... | default_predictor : Optional [ str ] = None","title":"default_predictor"},{"location":"api/models/model/#get_regularization_penalty","text":"class Model ( Module , Registrable ): | ... | def get_regularization_penalty ( self ) -> Optional [ torch . Tensor ] Computes the regularization penalty for the model. Returns None if the model was not configured to use regularization.","title":"get_regularization_penalty"},{"location":"api/models/model/#get_parameters_for_histogram_logging","text":"class Model ( Module , Registrable ): | ... | def get_parameters_for_histogram_logging ( self ) -> List [ str ] Returns the name of model parameters used for logging histograms to tensorboard.","title":"get_parameters_for_histogram_logging"},{"location":"api/models/model/#get_parameters_for_histogram_tensorboard_logging","text":"class Model ( Module , Registrable ): | ... | def get_parameters_for_histogram_tensorboard_logging ( | self | ) -> List [ str ] Returns the name of model parameters used for logging histograms to tensorboard.","title":"get_parameters_for_histogram_tensorboard_logging"},{"location":"api/models/model/#forward","text":"class Model ( Module , Registrable ): | ... | def forward ( self , * inputs ) -> Dict [ str , torch . Tensor ] Defines the forward pass of the model. In addition, to facilitate easy training, this method is designed to compute a loss function defined by a user. The input is comprised of everything required to perform a training update, including labels - you define the signature here! It is down to the user to ensure that inference can be performed without the presence of these labels. Hence, any inputs not available at inference time should only be used inside a conditional block. The intended sketch of this method is as follows:: def forward ( self , input1 , input2 , targets = None ) : .... .... output1 = self . layer1 ( input1 ) output2 = self . layer2 ( input2 ) output_dict = { \" output1 \" : output1 , \" output2 \" : output2 } if targets is not None : # Function returning a scalar torch . Tensor , defined by the user . loss = self . _compute_loss ( output1 , output2 , targets ) output_dict [ \" loss \" ] = loss return output_dict","title":"forward"},{"location":"api/models/model/#forward_on_instance","text":"class Model ( Module , Registrable ): | ... | def forward_on_instance ( | self , | instance : Instance | ) -> Dict [ str , numpy . ndarray ] Takes an Instance , which typically has raw text in it, converts that text into arrays using this model's Vocabulary , passes those arrays through self.forward() and self.make_output_human_readable() (which by default does nothing) and returns the result. Before returning the result, we convert any torch.Tensors into numpy arrays and remove the batch dimension.","title":"forward_on_instance"},{"location":"api/models/model/#forward_on_instances","text":"class Model ( Module , Registrable ): | ... | def forward_on_instances ( | self , | instances : List [ Instance ] | ) -> List [ Dict [ str , numpy . ndarray ]] Takes a list of Instances , converts that text into arrays using this model's Vocabulary , passes those arrays through self.forward() and self.make_output_human_readable() (which by default does nothing) and returns the result. Before returning the result, we convert any torch.Tensors into numpy arrays and separate the batched output into a list of individual dicts per instance. Note that typically this will be faster on a GPU (and conditionally, on a CPU) than repeated calls to forward_on_instance .","title":"forward_on_instances"},{"location":"api/models/model/#make_output_human_readable","text":"class Model ( Module , Registrable ): | ... | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Takes the result of forward and makes it human readable. Most of the time, the only thing this method does is convert tokens / predicted labels from tensors to strings that humans might actually understand. Somtimes you'll also do an argmax or something in here, too, but that most often happens in Model.forward , before you compute your metrics. This method modifies the input dictionary, and also returns the same dictionary. By default in the base class we do nothing.","title":"make_output_human_readable"},{"location":"api/models/model/#get_metrics","text":"class Model ( Module , Registrable ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] Returns a dictionary of metrics. This method will be called by allennlp.training.Trainer in order to compute and use model metrics for early stopping and model serialization. We return an empty dictionary here rather than raising as it is not required to implement metrics for a new model. A boolean reset parameter is passed, as frequently a metric accumulator will have some state which should be reset between epochs. This is also compatible with Metric s . Metrics should be populated during the call to forward , with the Metric handling the accumulation of the metric until this method is called.","title":"get_metrics"},{"location":"api/models/model/#load","text":"class Model ( Module , Registrable ): | ... | @classmethod | def load ( | cls , | config : Params , | serialization_dir : Union [ str , PathLike ], | weights_file : Optional [ Union [ str , PathLike ]] = None , | cuda_device : int = - 1 | ) -> \"Model\" Instantiates an already-trained model, based on the experiment configuration and some optional overrides.","title":"load"},{"location":"api/models/model/#extend_embedder_vocab","text":"class Model ( Module , Registrable ): | ... | def extend_embedder_vocab ( | self , | embedding_sources_mapping : Dict [ str , str ] = None | ) -> None Iterates through all embedding modules in the model and assures it can embed with the extended vocab. This is required in fine-tuning or transfer learning scenarios where model was trained with original vocabulary but during fine-tuning/transfer-learning, it will have it work with extended vocabulary (original + new-data vocabulary).","title":"extend_embedder_vocab"},{"location":"api/models/model/#from_archive","text":"class Model ( Module , Registrable ): | ... | @classmethod | def from_archive ( | cls , | archive_file : str , | vocab : Vocabulary = None | ) -> \"Model\" Loads a model from an archive file. This basically just calls return archival.load_archive(archive_file).model . It exists as a method here for convenience, and so that we can register it for easy use for fine tuning an existing model from a config file. If vocab is given, we will extend the loaded model's vocabulary using the passed vocab object (including calling extend_embedder_vocab , which extends embedding layers).","title":"from_archive"},{"location":"api/models/model/#remove_weights_related_keys_from_params","text":"def remove_weights_related_keys_from_params ( params : Params , keys : List [ str ] = [ \"pretrained_file\" , \"initializer\" ] )","title":"remove_weights_related_keys_from_params"},{"location":"api/models/model/#remove_pretrained_embedding_params","text":"def remove_pretrained_embedding_params ( params : Params ) This function only exists for backwards compatibility. Please use remove_weights_related_keys_from_params() instead.","title":"remove_pretrained_embedding_params"},{"location":"api/models/multitask/","text":"allennlp .models .multitask [SOURCE] get_forward_arguments \u00b6 def get_forward_arguments ( module : torch . nn . Module ) -> Set [ str ] MultiTaskModel \u00b6 @Model . register ( \"multitask\" ) class MultiTaskModel ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | backbone : Backbone , | heads : Dict [ str , Head ], | * , loss_weights : Dict [ str , float ] = None , | * , arg_name_mapping : Dict [ str , Dict [ str , str ]] = None , | * , allowed_arguments : Dict [ str , Set [ str ]] = None , | * , initializer : InitializerApplicator = InitializerApplicator (), | ** kwargs , | * , , | ) A MultiTaskModel consists of a Backbone that encodes its inputs in some way, then a collection of Heads that make predictions from the backbone-encoded inputs. The predictions of each Head are combined to compute a joint loss, which is then used for training. This model works by taking **kwargs in forward , and passing the right arguments from that to the backbone and to each head. By default, we use inspect to try to figure out getting the right arguments to the right modules, but we allow you to specify these arguments yourself in case our inference code gets it wrong. It is the caller's responsibility to make sure that the backbone and all heads are compatible with each other, and with the input data that comes from a MultiTaskDatasetReader . We give some arguments in this class and in MultiTaskDatasetReader to help with plumbing the arguments in complex cases (e.g., you can change argument names so that they match what the backbone and heads expect). Parameters \u00b6 vocab : Vocab backbone : Backbone heads : Dict[str, Head] loss_weights : Dict[str, float] , optional (default = equal weighting ) If you want, you can specify a weight for each head, which we will multiply the loss by when aggregating across heads. This is equivalent in many cases to specifying a separate learning rate per head, and just putting a weighting on the loss is much easier than figuring out the right way to specify that in the optimizer. arg_name_mapping : Dict[str, Dict[str, str]] , optional (default = identity mapping ) The mapping changes the names in the **kwargs dictionary passed to forward before passing on the arguments to the backbone and heads. This is keyed by component, and the top-level keys must match the keys passed in the heads parameter, plus a \"backbone\" key for the backbone. If you are using dataset readers that use dataset-specific names for their keys, this lets you change them to be consistent. For example, this dictionary might end up looking like this: {\"backbone\": {\"question\": \"text\", \"review\": \"text\"}, \"classifier1\": {\"sentiment\": \"label\"}, \"classifier2\": {\"topic\": \"label\"}} . Though in this particular example, we have two different inputs mapping to the same key in the backbone; this will work, as long are you are careful that you don't give both of those inputs in the same batch. If we see overlapping keys, we will crash. If you want to be able to do this kind of mixed training in the same batch, you need to handle that in your data code, not here; we won't handle complex batching inside this model. allowed_arguments : Dict[str, Set[str]] , optional (default = inferred ) The list of arguments that should be passed from **kwargs to the forward method for the backbone and each head. If you provide this, the keys in here should match the keys given in the heads parameter, plus a \"backbone\" key for the backbone arguments. If not given, we will use the inspect module to figure this out. The only time that this inference might fail is if you have optional arguments that you want to be ignored, or something. You very likely don't need to worry about this argument. initializer : InitializerApplicator , optional (default = InitializerApplicator() ) If provided, will be used to initialize the model parameters. default_predictor \u00b6 class MultiTaskModel ( Model ): | ... | default_predictor = \"multitask\" forward \u00b6 class MultiTaskModel ( Model ): | ... | def forward ( self , ** kwargs ) -> Dict [ str , torch . Tensor ] get_metrics \u00b6 class MultiTaskModel ( Model ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] make_output_human_readable \u00b6 class MultiTaskModel ( Model ): | ... | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ]","title":"multitask"},{"location":"api/models/multitask/#get_forward_arguments","text":"def get_forward_arguments ( module : torch . nn . Module ) -> Set [ str ]","title":"get_forward_arguments"},{"location":"api/models/multitask/#multitaskmodel","text":"@Model . register ( \"multitask\" ) class MultiTaskModel ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | backbone : Backbone , | heads : Dict [ str , Head ], | * , loss_weights : Dict [ str , float ] = None , | * , arg_name_mapping : Dict [ str , Dict [ str , str ]] = None , | * , allowed_arguments : Dict [ str , Set [ str ]] = None , | * , initializer : InitializerApplicator = InitializerApplicator (), | ** kwargs , | * , , | ) A MultiTaskModel consists of a Backbone that encodes its inputs in some way, then a collection of Heads that make predictions from the backbone-encoded inputs. The predictions of each Head are combined to compute a joint loss, which is then used for training. This model works by taking **kwargs in forward , and passing the right arguments from that to the backbone and to each head. By default, we use inspect to try to figure out getting the right arguments to the right modules, but we allow you to specify these arguments yourself in case our inference code gets it wrong. It is the caller's responsibility to make sure that the backbone and all heads are compatible with each other, and with the input data that comes from a MultiTaskDatasetReader . We give some arguments in this class and in MultiTaskDatasetReader to help with plumbing the arguments in complex cases (e.g., you can change argument names so that they match what the backbone and heads expect).","title":"MultiTaskModel"},{"location":"api/models/multitask/#default_predictor","text":"class MultiTaskModel ( Model ): | ... | default_predictor = \"multitask\"","title":"default_predictor"},{"location":"api/models/multitask/#forward","text":"class MultiTaskModel ( Model ): | ... | def forward ( self , ** kwargs ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"api/models/multitask/#get_metrics","text":"class MultiTaskModel ( Model ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"api/models/multitask/#make_output_human_readable","text":"class MultiTaskModel ( Model ): | ... | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ]","title":"make_output_human_readable"},{"location":"api/models/simple_tagger/","text":"allennlp .models .simple_tagger [SOURCE] SimpleTagger \u00b6 @Model . register ( \"simple_tagger\" ) class SimpleTagger ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | encoder : Seq2SeqEncoder , | calculate_span_f1 : bool = None , | label_encoding : Optional [ str ] = None , | label_namespace : str = \"labels\" , | verbose_metrics : bool = False , | initializer : InitializerApplicator = InitializerApplicator (), | ** kwargs | ) -> None This SimpleTagger simply encodes a sequence of text with a stacked Seq2SeqEncoder , then predicts a tag for each token in the sequence. Registered as a Model with name \"simple_tagger\". Parameters \u00b6 vocab : Vocabulary A Vocabulary, required in order to compute sizes for input/output projections. text_field_embedder : TextFieldEmbedder Used to embed the tokens TextField we get as input to the model. encoder : Seq2SeqEncoder The encoder (with its own internal stacking) that we will use in between embedding tokens and predicting output tags. calculate_span_f1 : bool , optional (default = None ) Calculate span-level F1 metrics during training. If this is True , then label_encoding is required. If None and label_encoding is specified, this is set to True . If None and label_encoding is not specified, it defaults to False . label_encoding : str , optional (default = None ) Label encoding to use when calculating span f1. Valid options are \"BIO\", \"BIOUL\", \"IOB1\", \"BMES\". Required if calculate_span_f1 is true. label_namespace : str , optional (default = labels ) This is needed to compute the SpanBasedF1Measure metric, if desired. Unless you did something unusual, the default value should be what you want. verbose_metrics : bool , optional (default = False ) If true, metrics will be returned per label class in addition to the overall statistics. initializer : InitializerApplicator , optional (default = InitializerApplicator() ) Used to initialize the model parameters. forward \u00b6 class SimpleTagger ( Model ): | ... | def forward ( | self , | tokens : TextFieldTensors , | tags : torch . LongTensor = None , | metadata : List [ Dict [ str , Any ]] = None , | ignore_loss_on_o_tags : bool = False | ) -> Dict [ str , torch . Tensor ] Parameters \u00b6 tokens : TextFieldTensors The output of TextField.as_array() , which should typically be passed directly to a TextFieldEmbedder . This output is a dictionary mapping keys to TokenIndexer tensors. At its most basic, using a SingleIdTokenIndexer this is : {\"tokens\": Tensor(batch_size, num_tokens)} . This dictionary will have the same keys as were used for the TokenIndexers when you created the TextField representing your sequence. The dictionary is designed to be passed directly to a TextFieldEmbedder , which knows how to combine different word representations into a single vector per token in your input. tags : torch.LongTensor , optional (default = None ) A torch tensor representing the sequence of integer gold class labels of shape (batch_size, num_tokens) . metadata : List[Dict[str, Any]] , optional (default = None ) metadata containing the original words in the sentence to be tagged under a 'words' key. ignore_loss_on_o_tags : bool , optional (default = False ) If True, we compute the loss only for actual spans in tags , and not on O tokens. This is useful for computing gradients of the loss on a single span , for interpretation / attacking. Returns \u00b6 An output dictionary consisting of: logits ( torch.FloatTensor ) : A tensor of shape (batch_size, num_tokens, tag_vocab_size) representing unnormalised log probabilities of the tag classes. class_probabilities ( torch.FloatTensor ) : A tensor of shape (batch_size, num_tokens, tag_vocab_size) representing a distribution of the tag classes per word. loss ( torch.FloatTensor , optional) : A scalar loss to be optimised. make_output_human_readable \u00b6 class SimpleTagger ( Model ): | ... | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Does a simple position-wise argmax over each token, converts indices to string labels, and adds a \"tags\" key to the dictionary with the result. get_metrics \u00b6 class SimpleTagger ( Model ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] default_predictor \u00b6 class SimpleTagger ( Model ): | ... | default_predictor = \"sentence_tagger\"","title":"simple_tagger"},{"location":"api/models/simple_tagger/#simpletagger","text":"@Model . register ( \"simple_tagger\" ) class SimpleTagger ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | encoder : Seq2SeqEncoder , | calculate_span_f1 : bool = None , | label_encoding : Optional [ str ] = None , | label_namespace : str = \"labels\" , | verbose_metrics : bool = False , | initializer : InitializerApplicator = InitializerApplicator (), | ** kwargs | ) -> None This SimpleTagger simply encodes a sequence of text with a stacked Seq2SeqEncoder , then predicts a tag for each token in the sequence. Registered as a Model with name \"simple_tagger\".","title":"SimpleTagger"},{"location":"api/models/simple_tagger/#forward","text":"class SimpleTagger ( Model ): | ... | def forward ( | self , | tokens : TextFieldTensors , | tags : torch . LongTensor = None , | metadata : List [ Dict [ str , Any ]] = None , | ignore_loss_on_o_tags : bool = False | ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"api/models/simple_tagger/#make_output_human_readable","text":"class SimpleTagger ( Model ): | ... | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Does a simple position-wise argmax over each token, converts indices to string labels, and adds a \"tags\" key to the dictionary with the result.","title":"make_output_human_readable"},{"location":"api/models/simple_tagger/#get_metrics","text":"class SimpleTagger ( Model ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"api/models/simple_tagger/#default_predictor","text":"class SimpleTagger ( Model ): | ... | default_predictor = \"sentence_tagger\"","title":"default_predictor"},{"location":"api/models/heads/classifier_head/","text":"allennlp .models .heads .classifier_head [SOURCE] ClassifierHead \u00b6 @Head . register ( \"classifier\" ) class ClassifierHead ( Head ): | def __init__ ( | self , | vocab : Vocabulary , | seq2vec_encoder : Seq2VecEncoder , | feedforward : Optional [ FeedForward ] = None , | input_dim : int = None , | dropout : float = None , | num_labels : int = None , | label_namespace : str = \"labels\" | ) -> None A classification Head . Takes encoded text, gets a single vector out of it, runs an optional feedforward layer on that vector, then classifies it into some label space. Registered as a Head with name \"classifier\". Parameters \u00b6 vocab : Vocabulary Used to get the number of labels, if num_labels is not provided, and to translate label indices to strings in make_output_human_readable . seq2vec_encoder : Seq2VecEncoder The input to this module is assumed to be a sequence of encoded vectors. We use a Seq2VecEncoder to compress this into a single vector on which we can perform classification. feedforward : FeedForward , optional (default = None ) An optional feedforward layer to apply on the pooled output before performing the classification. input_dim : int , optional (default = None ) We need to know how many dimensions to use for the final classification weight matrix. If you have provided either a seq2vec_encoder or a feedforward module, we can get the correct size from those objects. If you use default values for both of those parameters, then you must provide this parameter, so that we know the size of that encoding. dropout : float , optional (default = None ) Dropout percentage to use. num_labels : int , optional (default = None ) Number of labels to project to in classification layer. By default, the classification layer will project to the size of the vocabulary namespace corresponding to labels. label_namespace : str , optional (default = \"labels\" ) Vocabulary namespace corresponding to labels. By default, we use the \"labels\" namespace. forward \u00b6 class ClassifierHead ( Head ): | ... | def forward ( | self , | encoded_text : torch . FloatTensor , | encoded_text_mask : torch . BoolTensor , | label : torch . IntTensor = None | ) -> Dict [ str , torch . Tensor ] make_output_human_readable \u00b6 class ClassifierHead ( Head ): | ... | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Does a simple argmax over the probabilities, converts index to string label, and add \"label\" key to the dictionary with the result. get_metrics \u00b6 class ClassifierHead ( Head ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"classifier_head"},{"location":"api/models/heads/classifier_head/#classifierhead","text":"@Head . register ( \"classifier\" ) class ClassifierHead ( Head ): | def __init__ ( | self , | vocab : Vocabulary , | seq2vec_encoder : Seq2VecEncoder , | feedforward : Optional [ FeedForward ] = None , | input_dim : int = None , | dropout : float = None , | num_labels : int = None , | label_namespace : str = \"labels\" | ) -> None A classification Head . Takes encoded text, gets a single vector out of it, runs an optional feedforward layer on that vector, then classifies it into some label space. Registered as a Head with name \"classifier\".","title":"ClassifierHead"},{"location":"api/models/heads/classifier_head/#forward","text":"class ClassifierHead ( Head ): | ... | def forward ( | self , | encoded_text : torch . FloatTensor , | encoded_text_mask : torch . BoolTensor , | label : torch . IntTensor = None | ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"api/models/heads/classifier_head/#make_output_human_readable","text":"class ClassifierHead ( Head ): | ... | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Does a simple argmax over the probabilities, converts index to string label, and add \"label\" key to the dictionary with the result.","title":"make_output_human_readable"},{"location":"api/models/heads/classifier_head/#get_metrics","text":"class ClassifierHead ( Head ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"api/models/heads/head/","text":"allennlp .models .heads .head [SOURCE] Head \u00b6 class Head ( Model ) A Head is a Model that takes already encoded input and typically does simple computation before returning a loss. There isn't currently any difference in API between a Model and a Head , but we have this separate type as both a signaling mechanism for what to expect when looking at a Head class, and so that we can use this as a more informative type annotation when building models that use Heads as inputs. One additional consideration in a Head is that make_output_human_readable needs to account for the case where it gets called without first having forward be called on the head. This is because at the point where we call make_output_human_readable , we don't know which heads were used in forward , and trying to save the state is messy. So just make sure that you always have conditional logic in make_output_human_readable when you implement a Head .","title":"head"},{"location":"api/models/heads/head/#head","text":"class Head ( Model ) A Head is a Model that takes already encoded input and typically does simple computation before returning a loss. There isn't currently any difference in API between a Model and a Head , but we have this separate type as both a signaling mechanism for what to expect when looking at a Head class, and so that we can use this as a more informative type annotation when building models that use Heads as inputs. One additional consideration in a Head is that make_output_human_readable needs to account for the case where it gets called without first having forward be called on the head. This is because at the point where we call make_output_human_readable , we don't know which heads were used in forward , and trying to save the state is messy. So just make sure that you always have conditional logic in make_output_human_readable when you implement a Head .","title":"Head"},{"location":"api/modules/augmented_lstm/","text":"allennlp .modules .augmented_lstm [SOURCE] An LSTM with Recurrent Dropout and the option to use highway connections between layers. Based on PyText version (that was based on a previous AllenNLP version) AugmentedLSTMCell \u00b6 class AugmentedLSTMCell ( torch . nn . Module ): | def __init__ ( | self , | embed_dim : int , | lstm_dim : int , | use_highway : bool = True , | use_bias : bool = True | ) AugmentedLSTMCell implements a AugmentedLSTM cell. Parameters \u00b6 embed_dim : int The number of expected features in the input. lstm_dim : int Number of features in the hidden state of the LSTM. use_highway : bool , optional (default = True ) If True we append a highway network to the outputs of the LSTM. use_bias : bool , optional (default = True ) If True we use a bias in our LSTM calculations, otherwise we don't. Attributes \u00b6 input_linearity : nn.Module Fused weight matrix which computes a linear function over the input. state_linearity : nn.Module Fused weight matrix which computes a linear function over the states. reset_parameters \u00b6 class AugmentedLSTMCell ( torch . nn . Module ): | ... | def reset_parameters ( self ) forward \u00b6 class AugmentedLSTMCell ( torch . nn . Module ): | ... | def forward ( | self , | x : torch . Tensor , | states = Tuple [ torch . Tensor , torch . Tensor ], | variational_dropout_mask : Optional [ torch . BoolTensor ] = None | ) -> Tuple [ torch . Tensor , torch . Tensor ] Warning DO NOT USE THIS LAYER DIRECTLY, instead use the AugmentedLSTM class Parameters \u00b6 x : torch.Tensor Input tensor of shape (bsize x input_dim). states : Tuple[torch.Tensor, torch.Tensor] Tuple of tensors containing the hidden state and the cell state of each element in the batch. Each of these tensors have a dimension of (bsize x nhid). Defaults to None . Returns \u00b6 Tuple[torch.Tensor, torch.Tensor] Returned states. Shape of each state is (bsize x nhid). AugmentedLstm \u00b6 class AugmentedLstm ( torch . nn . Module ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | go_forward : bool = True , | recurrent_dropout_probability : float = 0.0 , | use_highway : bool = True , | use_input_projection_bias : bool = True | ) AugmentedLstm implements a one-layer single directional AugmentedLSTM layer. AugmentedLSTM is an LSTM which optionally appends an optional highway network to the output layer. Furthermore the dropout controls the level of variational dropout done. Parameters \u00b6 input_size : int The number of expected features in the input. hidden_size : int Number of features in the hidden state of the LSTM. Defaults to 32. go_forward : bool Whether to compute features left to right (forward) or right to left (backward). recurrent_dropout_probability : float Variational dropout probability to use. Defaults to 0.0. use_highway : bool If True we append a highway network to the outputs of the LSTM. use_input_projection_bias : bool If True we use a bias in our LSTM calculations, otherwise we don't. Attributes \u00b6 cell : AugmentedLSTMCell AugmentedLSTMCell that is applied at every timestep. forward \u00b6 class AugmentedLstm ( torch . nn . Module ): | ... | def forward ( | self , | inputs : PackedSequence , | states : Optional [ Tuple [ torch . Tensor , torch . Tensor ]] = None | ) -> Tuple [ PackedSequence , Tuple [ torch . Tensor , torch . Tensor ]] Warning: Would be better to use the BiAugmentedLstm class in a regular model Given an input batch of sequential data such as word embeddings, produces a single layer unidirectional AugmentedLSTM representation of the sequential input and new state tensors. Parameters \u00b6 inputs : PackedSequence bsize sequences of shape (len, input_dim) each, in PackedSequence format states : Tuple[torch.Tensor, torch.Tensor] Tuple of tensors containing the initial hidden state and the cell state of each element in the batch. Each of these tensors have a dimension of (1 x bsize x nhid). Defaults to None . Returns \u00b6 Tuple[PackedSequence, Tuple[torch.Tensor, torch.Tensor]] AugmentedLSTM representation of input and the state of the LSTM t = seq_len . Shape of representation is (bsize x seq_len x representation_dim). Shape of each state is (1 x bsize x nhid). BiAugmentedLstm \u00b6 class BiAugmentedLstm ( torch . nn . Module ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int = 1 , | bias : bool = True , | recurrent_dropout_probability : float = 0.0 , | bidirectional : bool = False , | padding_value : float = 0.0 , | use_highway : bool = True | ) -> None BiAugmentedLstm implements a generic AugmentedLSTM representation layer. BiAugmentedLstm is an LSTM which optionally appends an optional highway network to the output layer. Furthermore the dropout controls the level of variational dropout done. Parameters \u00b6 input_size : int The dimension of the inputs to the LSTM. hidden_size : int The dimension of the outputs of the LSTM. num_layers : int Number of recurrent layers. Eg. setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in the outputs of the first LSTM and computing the final result. Defaults to 1. bias : bool If True we use a bias in our LSTM calculations, otherwise we don't. recurrent_dropout_probability : float , optional (default = 0.0 ) Variational dropout probability to use. bidirectional : bool If True , becomes a bidirectional LSTM. Defaults to True . padding_value : float , optional (default = 0.0 ) Value for the padded elements. Defaults to 0.0. use_highway : bool , optional (default = True ) Whether or not to use highway connections between layers. This effectively involves reparameterising the normal output of an LSTM as:: gate = sigmoid(W_x1 * x_t + W_h * h_t) output = gate * h_t + (1 - gate) * (W_x2 * x_t) Returns \u00b6 output_accumulator : PackedSequence The outputs of the LSTM for each timestep. A tensor of shape (batch_size, max_timesteps, hidden_size) where for a given batch element, all outputs past the sequence length for that batch are zero tensors. forward \u00b6 class BiAugmentedLstm ( torch . nn . Module ): | ... | def forward ( | self , | inputs : torch . Tensor , | states : Optional [ Tuple [ torch . Tensor , torch . Tensor ]] = None | ) -> Tuple [ torch . Tensor , Tuple [ torch . Tensor , torch . Tensor ]] Given an input batch of sequential data such as word embeddings, produces a AugmentedLSTM representation of the sequential input and new state tensors. Parameters \u00b6 inputs : PackedSequence A tensor of shape (batch_size, num_timesteps, input_size) to apply the LSTM over. states : Tuple[torch.Tensor, torch.Tensor] Tuple of tensors containing the initial hidden state and the cell state of each element in the batch. Each of these tensors have a dimension of (bsize x num_layers x num_directions * nhid). Defaults to None . Returns \u00b6 Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]] AgumentedLSTM representation of input and the state of the LSTM t = seq_len . Shape of representation is (bsize x seq_len x representation_dim). Shape of each state is (bsize x num_layers * num_directions x nhid).","title":"augmented_lstm"},{"location":"api/modules/augmented_lstm/#augmentedlstmcell","text":"class AugmentedLSTMCell ( torch . nn . Module ): | def __init__ ( | self , | embed_dim : int , | lstm_dim : int , | use_highway : bool = True , | use_bias : bool = True | ) AugmentedLSTMCell implements a AugmentedLSTM cell.","title":"AugmentedLSTMCell"},{"location":"api/modules/augmented_lstm/#reset_parameters","text":"class AugmentedLSTMCell ( torch . nn . Module ): | ... | def reset_parameters ( self )","title":"reset_parameters"},{"location":"api/modules/augmented_lstm/#forward","text":"class AugmentedLSTMCell ( torch . nn . Module ): | ... | def forward ( | self , | x : torch . Tensor , | states = Tuple [ torch . Tensor , torch . Tensor ], | variational_dropout_mask : Optional [ torch . BoolTensor ] = None | ) -> Tuple [ torch . Tensor , torch . Tensor ] Warning DO NOT USE THIS LAYER DIRECTLY, instead use the AugmentedLSTM class","title":"forward"},{"location":"api/modules/augmented_lstm/#augmentedlstm","text":"class AugmentedLstm ( torch . nn . Module ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | go_forward : bool = True , | recurrent_dropout_probability : float = 0.0 , | use_highway : bool = True , | use_input_projection_bias : bool = True | ) AugmentedLstm implements a one-layer single directional AugmentedLSTM layer. AugmentedLSTM is an LSTM which optionally appends an optional highway network to the output layer. Furthermore the dropout controls the level of variational dropout done.","title":"AugmentedLstm"},{"location":"api/modules/augmented_lstm/#forward_1","text":"class AugmentedLstm ( torch . nn . Module ): | ... | def forward ( | self , | inputs : PackedSequence , | states : Optional [ Tuple [ torch . Tensor , torch . Tensor ]] = None | ) -> Tuple [ PackedSequence , Tuple [ torch . Tensor , torch . Tensor ]] Warning: Would be better to use the BiAugmentedLstm class in a regular model Given an input batch of sequential data such as word embeddings, produces a single layer unidirectional AugmentedLSTM representation of the sequential input and new state tensors.","title":"forward"},{"location":"api/modules/augmented_lstm/#biaugmentedlstm","text":"class BiAugmentedLstm ( torch . nn . Module ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int = 1 , | bias : bool = True , | recurrent_dropout_probability : float = 0.0 , | bidirectional : bool = False , | padding_value : float = 0.0 , | use_highway : bool = True | ) -> None BiAugmentedLstm implements a generic AugmentedLSTM representation layer. BiAugmentedLstm is an LSTM which optionally appends an optional highway network to the output layer. Furthermore the dropout controls the level of variational dropout done.","title":"BiAugmentedLstm"},{"location":"api/modules/augmented_lstm/#forward_2","text":"class BiAugmentedLstm ( torch . nn . Module ): | ... | def forward ( | self , | inputs : torch . Tensor , | states : Optional [ Tuple [ torch . Tensor , torch . Tensor ]] = None | ) -> Tuple [ torch . Tensor , Tuple [ torch . Tensor , torch . Tensor ]] Given an input batch of sequential data such as word embeddings, produces a AugmentedLSTM representation of the sequential input and new state tensors.","title":"forward"},{"location":"api/modules/bimpm_matching/","text":"allennlp .modules .bimpm_matching [SOURCE] Multi-perspective matching layer multi_perspective_match \u00b6 def multi_perspective_match ( vector1 : torch . Tensor , vector2 : torch . Tensor , weight : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ] Calculate multi-perspective cosine matching between time-steps of vectors of the same length. Parameters \u00b6 vector1 : torch.Tensor A tensor of shape (batch, seq_len, hidden_size) vector2 : torch.Tensor A tensor of shape (batch, seq_len or 1, hidden_size) weight : torch.Tensor A tensor of shape (num_perspectives, hidden_size) Returns \u00b6 torch.Tensor : Shape (batch, seq_len, 1) . torch.Tensor : Shape (batch, seq_len, num_perspectives) . multi_perspective_match_pairwise \u00b6 def multi_perspective_match_pairwise ( vector1 : torch . Tensor , vector2 : torch . Tensor , weight : torch . Tensor ) -> torch . Tensor Calculate multi-perspective cosine matching between each time step of one vector and each time step of another vector. Parameters \u00b6 vector1 : torch.Tensor A tensor of shape (batch, seq_len1, hidden_size) vector2 : torch.Tensor A tensor of shape (batch, seq_len2, hidden_size) weight : torch.Tensor A tensor of shape (num_perspectives, hidden_size) Returns \u00b6 torch.Tensor : A tensor of shape (batch, seq_len1, seq_len2, num_perspectives) consisting multi-perspective matching results BiMpmMatching \u00b6 class BiMpmMatching ( nn . Module , FromParams ): | def __init__ ( | self , | hidden_dim : int = 100 , | num_perspectives : int = 20 , | share_weights_between_directions : bool = True , | is_forward : bool = None , | with_full_match : bool = True , | with_maxpool_match : bool = True , | with_attentive_match : bool = True , | with_max_attentive_match : bool = True | ) -> None This Module implements the matching layer of BiMPM model described in Bilateral Multi-Perspective Matching for Natural Language Sentences by Zhiguo Wang et al., 2017. Also please refer to the TensorFlow implementation and PyTorch implementation . Parameters \u00b6 hidden_dim : int , optional (default = 100 ) The hidden dimension of the representations num_perspectives : int , optional (default = 20 ) The number of perspectives for matching share_weights_between_directions : bool , optional (default = True ) If True, share weight between matching from sentence1 to sentence2 and from sentence2 to sentence1, useful for non-symmetric tasks is_forward : bool , optional (default = None ) Whether the matching is for forward sequence or backward sequence, useful in finding last token in full matching. It can not be None if with_full_match is True. with_full_match : bool , optional (default = True ) If True, include full match with_maxpool_match : bool , optional (default = True ) If True, include max pool match with_attentive_match : bool , optional (default = True ) If True, include attentive match with_max_attentive_match : bool , optional (default = True ) If True, include max attentive match get_output_dim \u00b6 class BiMpmMatching ( nn . Module , FromParams ): | ... | def get_output_dim ( self ) -> int forward \u00b6 class BiMpmMatching ( nn . Module , FromParams ): | ... | def forward ( | self , | context_1 : torch . Tensor , | mask_1 : torch . BoolTensor , | context_2 : torch . Tensor , | mask_2 : torch . BoolTensor | ) -> Tuple [ List [ torch . Tensor ], List [ torch . Tensor ]] Given the forward (or backward) representations of sentence1 and sentence2, apply four bilateral matching functions between them in one direction. Parameters \u00b6 context_1 : torch.Tensor Tensor of shape (batch_size, seq_len1, hidden_dim) representing the encoding of the first sentence. mask_1 : torch.BoolTensor Boolean Tensor of shape (batch_size, seq_len1), indicating which positions in the first sentence are padding (0) and which are not (1). context_2 : torch.Tensor Tensor of shape (batch_size, seq_len2, hidden_dim) representing the encoding of the second sentence. mask_2 : torch.BoolTensor Boolean Tensor of shape (batch_size, seq_len2), indicating which positions in the second sentence are padding (0) and which are not (1). Returns \u00b6 Tuple[List[torch.Tensor], List[torch.Tensor]] : A tuple of matching vectors for the two sentences. Each of which is a list of matching vectors of shape (batch, seq_len, num_perspectives or 1)","title":"bimpm_matching"},{"location":"api/modules/bimpm_matching/#multi_perspective_match","text":"def multi_perspective_match ( vector1 : torch . Tensor , vector2 : torch . Tensor , weight : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ] Calculate multi-perspective cosine matching between time-steps of vectors of the same length.","title":"multi_perspective_match"},{"location":"api/modules/bimpm_matching/#multi_perspective_match_pairwise","text":"def multi_perspective_match_pairwise ( vector1 : torch . Tensor , vector2 : torch . Tensor , weight : torch . Tensor ) -> torch . Tensor Calculate multi-perspective cosine matching between each time step of one vector and each time step of another vector.","title":"multi_perspective_match_pairwise"},{"location":"api/modules/bimpm_matching/#bimpmmatching","text":"class BiMpmMatching ( nn . Module , FromParams ): | def __init__ ( | self , | hidden_dim : int = 100 , | num_perspectives : int = 20 , | share_weights_between_directions : bool = True , | is_forward : bool = None , | with_full_match : bool = True , | with_maxpool_match : bool = True , | with_attentive_match : bool = True , | with_max_attentive_match : bool = True | ) -> None This Module implements the matching layer of BiMPM model described in Bilateral Multi-Perspective Matching for Natural Language Sentences by Zhiguo Wang et al., 2017. Also please refer to the TensorFlow implementation and PyTorch implementation .","title":"BiMpmMatching"},{"location":"api/modules/bimpm_matching/#get_output_dim","text":"class BiMpmMatching ( nn . Module , FromParams ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/bimpm_matching/#forward","text":"class BiMpmMatching ( nn . Module , FromParams ): | ... | def forward ( | self , | context_1 : torch . Tensor , | mask_1 : torch . BoolTensor , | context_2 : torch . Tensor , | mask_2 : torch . BoolTensor | ) -> Tuple [ List [ torch . Tensor ], List [ torch . Tensor ]] Given the forward (or backward) representations of sentence1 and sentence2, apply four bilateral matching functions between them in one direction.","title":"forward"},{"location":"api/modules/conditional_random_field/","text":"allennlp .modules .conditional_random_field [SOURCE] Conditional random field VITERBI_DECODING \u00b6 VITERBI_DECODING = Tuple [ List [ int ], float ] allowed_transitions \u00b6 def allowed_transitions ( constraint_type : str , labels : Dict [ int , str ] ) -> List [ Tuple [ int , int ]] Given labels and a constraint type, returns the allowed transitions. It will additionally include transitions for the start and end states, which are used by the conditional random field. Parameters \u00b6 constraint_type : str Indicates which constraint to apply. Current choices are \"BIO\", \"IOB1\", \"BIOUL\", and \"BMES\". labels : Dict[int, str] A mapping {label_id -> label}. Most commonly this would be the value from Vocabulary.get_index_to_token_vocabulary() Returns \u00b6 List[Tuple[int, int]] The allowed transitions (from_label_id, to_label_id). is_transition_allowed \u00b6 def is_transition_allowed ( constraint_type : str , from_tag : str , from_entity : str , to_tag : str , to_entity : str ) Given a constraint type and strings from_tag and to_tag that represent the origin and destination of the transition, return whether the transition is allowed under the given constraint type. Parameters \u00b6 constraint_type : str Indicates which constraint to apply. Current choices are \"BIO\", \"IOB1\", \"BIOUL\", and \"BMES\". from_tag : str The tag that the transition originates from. For example, if the label is I-PER , the from_tag is I . from_entity : str The entity corresponding to the from_tag . For example, if the label is I-PER , the from_entity is PER . to_tag : str The tag that the transition leads to. For example, if the label is I-PER , the to_tag is I . to_entity : str The entity corresponding to the to_tag . For example, if the label is I-PER , the to_entity is PER . Returns \u00b6 bool Whether the transition is allowed under the given constraint_type . ConditionalRandomField \u00b6 class ConditionalRandomField ( torch . nn . Module ): | def __init__ ( | self , | num_tags : int , | constraints : List [ Tuple [ int , int ]] = None , | include_start_end_transitions : bool = True | ) -> None This module uses the \"forward-backward\" algorithm to compute the log-likelihood of its inputs assuming a conditional random field model. See, e.g. http://www.cs.columbia.edu/~mcollins/fb.pdf Parameters \u00b6 num_tags : int The number of tags. constraints : List[Tuple[int, int]] , optional (default = None ) An optional list of allowed transitions (from_tag_id, to_tag_id). These are applied to viterbi_tags() but do not affect forward() . These should be derived from allowed_transitions so that the start and end transitions are handled correctly for your tag type. include_start_end_transitions : bool , optional (default = True ) Whether to include the start and end transition parameters. reset_parameters \u00b6 class ConditionalRandomField ( torch . nn . Module ): | ... | def reset_parameters ( self ) forward \u00b6 class ConditionalRandomField ( torch . nn . Module ): | ... | def forward ( | self , | inputs : torch . Tensor , | tags : torch . Tensor , | mask : torch . BoolTensor = None | ) -> torch . Tensor Computes the log likelihood. viterbi_tags \u00b6 class ConditionalRandomField ( torch . nn . Module ): | ... | def viterbi_tags ( | self , | logits : torch . Tensor , | mask : torch . BoolTensor = None , | top_k : int = None | ) -> Union [ List [ VITERBI_DECODING ], List [ List [ VITERBI_DECODING ]]] Uses viterbi algorithm to find most likely tags for the given inputs. If constraints are applied, disallows all other transitions. Returns a list of results, of the same size as the batch (one result per batch member) Each result is a List of length top_k, containing the top K viterbi decodings Each decoding is a tuple (tag_sequence, viterbi_score) For backwards compatibility, if top_k is None, then instead returns a flat list of tag sequences (the top tag sequence for each batch item).","title":"conditional_random_field"},{"location":"api/modules/conditional_random_field/#viterbi_decoding","text":"VITERBI_DECODING = Tuple [ List [ int ], float ]","title":"VITERBI_DECODING"},{"location":"api/modules/conditional_random_field/#allowed_transitions","text":"def allowed_transitions ( constraint_type : str , labels : Dict [ int , str ] ) -> List [ Tuple [ int , int ]] Given labels and a constraint type, returns the allowed transitions. It will additionally include transitions for the start and end states, which are used by the conditional random field.","title":"allowed_transitions"},{"location":"api/modules/conditional_random_field/#is_transition_allowed","text":"def is_transition_allowed ( constraint_type : str , from_tag : str , from_entity : str , to_tag : str , to_entity : str ) Given a constraint type and strings from_tag and to_tag that represent the origin and destination of the transition, return whether the transition is allowed under the given constraint type.","title":"is_transition_allowed"},{"location":"api/modules/conditional_random_field/#conditionalrandomfield","text":"class ConditionalRandomField ( torch . nn . Module ): | def __init__ ( | self , | num_tags : int , | constraints : List [ Tuple [ int , int ]] = None , | include_start_end_transitions : bool = True | ) -> None This module uses the \"forward-backward\" algorithm to compute the log-likelihood of its inputs assuming a conditional random field model. See, e.g. http://www.cs.columbia.edu/~mcollins/fb.pdf","title":"ConditionalRandomField"},{"location":"api/modules/conditional_random_field/#reset_parameters","text":"class ConditionalRandomField ( torch . nn . Module ): | ... | def reset_parameters ( self )","title":"reset_parameters"},{"location":"api/modules/conditional_random_field/#forward","text":"class ConditionalRandomField ( torch . nn . Module ): | ... | def forward ( | self , | inputs : torch . Tensor , | tags : torch . Tensor , | mask : torch . BoolTensor = None | ) -> torch . Tensor Computes the log likelihood.","title":"forward"},{"location":"api/modules/conditional_random_field/#viterbi_tags","text":"class ConditionalRandomField ( torch . nn . Module ): | ... | def viterbi_tags ( | self , | logits : torch . Tensor , | mask : torch . BoolTensor = None , | top_k : int = None | ) -> Union [ List [ VITERBI_DECODING ], List [ List [ VITERBI_DECODING ]]] Uses viterbi algorithm to find most likely tags for the given inputs. If constraints are applied, disallows all other transitions. Returns a list of results, of the same size as the batch (one result per batch member) Each result is a List of length top_k, containing the top K viterbi decodings Each decoding is a tuple (tag_sequence, viterbi_score) For backwards compatibility, if top_k is None, then instead returns a flat list of tag sequences (the top tag sequence for each batch item).","title":"viterbi_tags"},{"location":"api/modules/elmo/","text":"allennlp .modules .elmo [SOURCE] Elmo \u00b6 class Elmo ( torch . nn . Module , FromParams ): | def __init__ ( | self , | options_file : str , | weight_file : str , | num_output_representations : int , | requires_grad : bool = False , | do_layer_norm : bool = False , | dropout : float = 0.5 , | vocab_to_cache : List [ str ] = None , | keep_sentence_boundaries : bool = False , | scalar_mix_parameters : List [ float ] = None , | module : torch . nn . Module = None | ) -> None Compute ELMo representations using a pre-trained bidirectional language model. See \"Deep contextualized word representations\", Peters et al. for details. This module takes character id input and computes num_output_representations different layers of ELMo representations. Typically num_output_representations is 1 or 2. For example, in the case of the SRL model in the above paper, num_output_representations=1 where ELMo was included at the input token representation layer. In the case of the SQuAD model, num_output_representations=2 as ELMo was also included at the GRU output layer. In the implementation below, we learn separate scalar weights for each output layer, but only run the biLM once on each input sequence for efficiency. Parameters \u00b6 options_file : str ELMo JSON options file weight_file : str ELMo hdf5 weight file num_output_representations : int The number of ELMo representation to output with different linear weighted combination of the 3 layers (i.e., character-convnet output, 1st lstm output, 2nd lstm output). requires_grad : bool , optional If True, compute gradient of ELMo parameters for fine tuning. do_layer_norm : bool , optional (default = False ) Should we apply layer normalization (passed to ScalarMix )? dropout : float , optional (default = 0.5 ) The dropout to be applied to the ELMo representations. vocab_to_cache : List[str] , optional (default = None ) A list of words to pre-compute and cache character convolutions for. If you use this option, Elmo expects that you pass word indices of shape (batch_size, timesteps) to forward, instead of character indices. If you use this option and pass a word which wasn't pre-cached, this will break. keep_sentence_boundaries : bool , optional (default = False ) If True, the representation of the sentence boundary tokens are not removed. scalar_mix_parameters : List[float] , optional (default = None ) If not None , use these scalar mix parameters to weight the representations produced by different layers. These mixing weights are not updated during training. The mixing weights here should be the unnormalized (i.e., pre-softmax) weights. So, if you wanted to use only the 1st layer of a 2-layer ELMo, you can set this to [-9e10, 1, -9e10 ]. module : torch.nn.Module , optional (default = None ) If provided, then use this module instead of the pre-trained ELMo biLM. If using this option, then pass None for both options_file and weight_file . The module must provide a public attribute num_layers with the number of internal layers and its forward method must return a dict with activations and mask keys (see _ElmoBilm for an example). Note that requires_grad is also ignored with this option. get_output_dim \u00b6 class Elmo ( torch . nn . Module , FromParams ): | ... | def get_output_dim ( self ) forward \u00b6 class Elmo ( torch . nn . Module , FromParams ): | ... | def forward ( | self , | inputs : torch . Tensor , | word_inputs : torch . Tensor = None | ) -> Dict [ str , Union [ torch . Tensor , List [ torch . Tensor ]]] Parameters \u00b6 inputs : torch.Tensor Shape (batch_size, timesteps, 50) of character ids representing the current batch. word_inputs : torch.Tensor If you passed a cached vocab, you can in addition pass a tensor of shape (batch_size, timesteps) , which represent word ids which have been pre-cached. Returns \u00b6 Dict[str, Union[torch.Tensor, List[torch.Tensor]]] A dict with the following keys: 'elmo_representations' ( List[torch.Tensor] ) : A num_output_representations list of ELMo representations for the input sequence. Each representation is shape (batch_size, timesteps, embedding_dim) 'mask' ( torch.BoolTensor ) : Shape (batch_size, timesteps) long tensor with sequence mask. batch_to_ids \u00b6 def batch_to_ids ( batch : List [ List [ str ]]) -> torch . Tensor Converts a batch of tokenized sentences to a tensor representing the sentences with encoded characters (len(batch), max sentence length, max word length). Parameters \u00b6 batch : List[List[str]] A list of tokenized sentences. Returns \u00b6 A tensor of padded character ids.","title":"elmo"},{"location":"api/modules/elmo/#elmo","text":"class Elmo ( torch . nn . Module , FromParams ): | def __init__ ( | self , | options_file : str , | weight_file : str , | num_output_representations : int , | requires_grad : bool = False , | do_layer_norm : bool = False , | dropout : float = 0.5 , | vocab_to_cache : List [ str ] = None , | keep_sentence_boundaries : bool = False , | scalar_mix_parameters : List [ float ] = None , | module : torch . nn . Module = None | ) -> None Compute ELMo representations using a pre-trained bidirectional language model. See \"Deep contextualized word representations\", Peters et al. for details. This module takes character id input and computes num_output_representations different layers of ELMo representations. Typically num_output_representations is 1 or 2. For example, in the case of the SRL model in the above paper, num_output_representations=1 where ELMo was included at the input token representation layer. In the case of the SQuAD model, num_output_representations=2 as ELMo was also included at the GRU output layer. In the implementation below, we learn separate scalar weights for each output layer, but only run the biLM once on each input sequence for efficiency.","title":"Elmo"},{"location":"api/modules/elmo/#get_output_dim","text":"class Elmo ( torch . nn . Module , FromParams ): | ... | def get_output_dim ( self )","title":"get_output_dim"},{"location":"api/modules/elmo/#forward","text":"class Elmo ( torch . nn . Module , FromParams ): | ... | def forward ( | self , | inputs : torch . Tensor , | word_inputs : torch . Tensor = None | ) -> Dict [ str , Union [ torch . Tensor , List [ torch . Tensor ]]]","title":"forward"},{"location":"api/modules/elmo/#batch_to_ids","text":"def batch_to_ids ( batch : List [ List [ str ]]) -> torch . Tensor Converts a batch of tokenized sentences to a tensor representing the sentences with encoded characters (len(batch), max sentence length, max word length).","title":"batch_to_ids"},{"location":"api/modules/elmo_lstm/","text":"allennlp .modules .elmo_lstm [SOURCE] A stacked bidirectional LSTM with skip connections between layers. ElmoLstm \u00b6 class ElmoLstm ( _EncoderBase ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | cell_size : int , | num_layers : int , | requires_grad : bool = False , | recurrent_dropout_probability : float = 0.0 , | memory_cell_clip_value : Optional [ float ] = None , | state_projection_clip_value : Optional [ float ] = None | ) -> None A stacked, bidirectional LSTM which uses LstmCellWithProjection 's with highway layers between the inputs to layers. The inputs to the forward and backward directions are independent - forward and backward states are not concatenated between layers. Additionally, this LSTM maintains its own state, which is updated every time forward is called. It is dynamically resized for different batch sizes and is designed for use with non-continuous inputs (i.e inputs which aren't formatted as a stream, such as text used for a language modeling task, which is how stateful RNNs are typically used). This is non-standard, but can be thought of as having an \"end of sentence\" state, which is carried across different sentences. Parameters \u00b6 input_size : int The dimension of the inputs to the LSTM. hidden_size : int The dimension of the outputs of the LSTM. cell_size : int The dimension of the memory cell of the LstmCellWithProjection . num_layers : int The number of bidirectional LSTMs to use. requires_grad : bool , optional If True, compute gradient of ELMo parameters for fine tuning. recurrent_dropout_probability : float , optional (default = 0.0 ) The dropout probability to be used in a dropout scheme as stated in A Theoretically Grounded Application of Dropout in Recurrent Neural Networks . state_projection_clip_value : float , optional (default = None ) The magnitude with which to clip the hidden_state after projecting it. memory_cell_clip_value : float , optional (default = None ) The magnitude with which to clip the memory cell. forward \u00b6 class ElmoLstm ( _EncoderBase ): | ... | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor | ) -> torch . Tensor Parameters \u00b6 inputs : torch.Tensor A Tensor of shape (batch_size, sequence_length, hidden_size) . mask : torch.BoolTensor A binary mask of shape (batch_size, sequence_length) representing the non-padded elements in each sequence in the batch. Returns \u00b6 torch.Tensor A torch.Tensor of shape (num_layers, batch_size, sequence_length, hidden_size), where the num_layers dimension represents the LSTM output from that layer. load_weights \u00b6 class ElmoLstm ( _EncoderBase ): | ... | def load_weights ( self , weight_file : str ) -> None Load the pre-trained weights from the file.","title":"elmo_lstm"},{"location":"api/modules/elmo_lstm/#elmolstm","text":"class ElmoLstm ( _EncoderBase ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | cell_size : int , | num_layers : int , | requires_grad : bool = False , | recurrent_dropout_probability : float = 0.0 , | memory_cell_clip_value : Optional [ float ] = None , | state_projection_clip_value : Optional [ float ] = None | ) -> None A stacked, bidirectional LSTM which uses LstmCellWithProjection 's with highway layers between the inputs to layers. The inputs to the forward and backward directions are independent - forward and backward states are not concatenated between layers. Additionally, this LSTM maintains its own state, which is updated every time forward is called. It is dynamically resized for different batch sizes and is designed for use with non-continuous inputs (i.e inputs which aren't formatted as a stream, such as text used for a language modeling task, which is how stateful RNNs are typically used). This is non-standard, but can be thought of as having an \"end of sentence\" state, which is carried across different sentences.","title":"ElmoLstm"},{"location":"api/modules/elmo_lstm/#forward","text":"class ElmoLstm ( _EncoderBase ): | ... | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/elmo_lstm/#load_weights","text":"class ElmoLstm ( _EncoderBase ): | ... | def load_weights ( self , weight_file : str ) -> None Load the pre-trained weights from the file.","title":"load_weights"},{"location":"api/modules/encoder_base/","text":"allennlp .modules .encoder_base [SOURCE] RnnState \u00b6 RnnState = Union [ torch . Tensor , Tuple [ torch . Tensor , torch . Tensor ]] RnnStateStorage \u00b6 RnnStateStorage = Tuple [ torch . Tensor , ... ]","title":"encoder_base"},{"location":"api/modules/encoder_base/#rnnstate","text":"RnnState = Union [ torch . Tensor , Tuple [ torch . Tensor , torch . Tensor ]]","title":"RnnState"},{"location":"api/modules/encoder_base/#rnnstatestorage","text":"RnnStateStorage = Tuple [ torch . Tensor , ... ]","title":"RnnStateStorage"},{"location":"api/modules/feedforward/","text":"allennlp .modules .feedforward [SOURCE] A feed-forward neural network. FeedForward \u00b6 class FeedForward ( torch . nn . Module , FromParams ): | def __init__ ( | self , | input_dim : int , | num_layers : int , | hidden_dims : Union [ int , List [ int ]], | activations : Union [ Activation , List [ Activation ]], | dropout : Union [ float , List [ float ]] = 0.0 | ) -> None This Module is a feed-forward neural network, just a sequence of Linear layers with activation functions in between. Parameters \u00b6 input_dim : int The dimensionality of the input. We assume the input has shape (batch_size, input_dim) . num_layers : int The number of Linear layers to apply to the input. hidden_dims : Union[int, List[int]] The output dimension of each of the Linear layers. If this is a single int , we use it for all Linear layers. If it is a List[int] , len(hidden_dims) must be num_layers . activations : Union[Activation, List[Activation]] The activation function to use after each Linear layer. If this is a single function, we use it after all Linear layers. If it is a List[Activation] , len(activations) must be num_layers . Activation must have torch.nn.Module type. dropout : Union[float, List[float]] , optional (default = 0.0 ) If given, we will apply this amount of dropout after each layer. Semantics of float versus List[float] is the same as with other parameters. Examples \u00b6 FeedForward ( 124 , 2 , [ 64 , 32 ], torch . nn . ReLU (), 0.2 ) #> FeedForward( #> (_activations): ModuleList( #> (0): ReLU() #> (1): ReLU() #> ) #> (_linear_layers): ModuleList( #> (0): Linear(in_features=124, out_features=64, bias=True) #> (1): Linear(in_features=64, out_features=32, bias=True) #> ) #> (_dropout): ModuleList( #> (0): Dropout(p=0.2, inplace=False) #> (1): Dropout(p=0.2, inplace=False) #> ) #> ) get_output_dim \u00b6 class FeedForward ( torch . nn . Module , FromParams ): | ... | def get_output_dim ( self ) get_input_dim \u00b6 class FeedForward ( torch . nn . Module , FromParams ): | ... | def get_input_dim ( self ) forward \u00b6 class FeedForward ( torch . nn . Module , FromParams ): | ... | def forward ( self , inputs : torch . Tensor ) -> torch . Tensor","title":"feedforward"},{"location":"api/modules/feedforward/#feedforward","text":"class FeedForward ( torch . nn . Module , FromParams ): | def __init__ ( | self , | input_dim : int , | num_layers : int , | hidden_dims : Union [ int , List [ int ]], | activations : Union [ Activation , List [ Activation ]], | dropout : Union [ float , List [ float ]] = 0.0 | ) -> None This Module is a feed-forward neural network, just a sequence of Linear layers with activation functions in between.","title":"FeedForward"},{"location":"api/modules/feedforward/#get_output_dim","text":"class FeedForward ( torch . nn . Module , FromParams ): | ... | def get_output_dim ( self )","title":"get_output_dim"},{"location":"api/modules/feedforward/#get_input_dim","text":"class FeedForward ( torch . nn . Module , FromParams ): | ... | def get_input_dim ( self )","title":"get_input_dim"},{"location":"api/modules/feedforward/#forward","text":"class FeedForward ( torch . nn . Module , FromParams ): | ... | def forward ( self , inputs : torch . Tensor ) -> torch . Tensor","title":"forward"},{"location":"api/modules/gated_sum/","text":"allennlp .modules .gated_sum [SOURCE] GatedSum \u00b6 class GatedSum ( torch . nn . Module ): | def __init__ ( | self , | input_dim : int , | activation : Activation = torch . nn . Sigmoid () | ) -> None This Module represents a gated sum of two tensors a and b . Specifically: f = activation(W [a; b]) out = f * a + (1 - f) * b Parameters \u00b6 input_dim : int The dimensionality of the input. We assume the input have shape (..., input_dim) . activation : Activation , optional (default = torch.nn.Sigmoid() ) The activation function to use. get_input_dim \u00b6 class GatedSum ( torch . nn . Module ): | ... | def get_input_dim ( self ) get_output_dim \u00b6 class GatedSum ( torch . nn . Module ): | ... | def get_output_dim ( self ) forward \u00b6 class GatedSum ( torch . nn . Module ): | ... | def forward ( | self , | input_a : torch . Tensor , | input_b : torch . Tensor | ) -> torch . Tensor","title":"gated_sum"},{"location":"api/modules/gated_sum/#gatedsum","text":"class GatedSum ( torch . nn . Module ): | def __init__ ( | self , | input_dim : int , | activation : Activation = torch . nn . Sigmoid () | ) -> None This Module represents a gated sum of two tensors a and b . Specifically: f = activation(W [a; b]) out = f * a + (1 - f) * b","title":"GatedSum"},{"location":"api/modules/gated_sum/#get_input_dim","text":"class GatedSum ( torch . nn . Module ): | ... | def get_input_dim ( self )","title":"get_input_dim"},{"location":"api/modules/gated_sum/#get_output_dim","text":"class GatedSum ( torch . nn . Module ): | ... | def get_output_dim ( self )","title":"get_output_dim"},{"location":"api/modules/gated_sum/#forward","text":"class GatedSum ( torch . nn . Module ): | ... | def forward ( | self , | input_a : torch . Tensor , | input_b : torch . Tensor | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/highway/","text":"allennlp .modules .highway [SOURCE] A Highway layer that does a gated combination of a linear transformation and a non-linear transformation of its input. Highway \u00b6 class Highway ( torch . nn . Module ): | def __init__ ( | self , | input_dim : int , | num_layers : int = 1 , | activation : Callable [[ torch . Tensor ], torch . Tensor ] = torch . nn . functional . relu | ) -> None A Highway layer does a gated combination of a linear transformation and a non-linear transformation of its input. :math: y = g * x + (1 - g) * f(A(x)) , where :math: A is a linear transformation, :math: f is an element-wise non-linearity, and :math: g is an element-wise gate, computed as :math: sigmoid(B(x)) . This module will apply a fixed number of highway layers to its input, returning the final result. Parameters \u00b6 input_dim : int The dimensionality of :math: x . We assume the input has shape (batch_size, ..., input_dim) . num_layers : int , optional (default = 1 ) The number of highway layers to apply to the input. activation : Callable[[torch.Tensor], torch.Tensor] , optional (default = torch.nn.functional.relu ) The non-linearity to use in the highway layers. forward \u00b6 class Highway ( torch . nn . Module ): | ... | def forward ( self , inputs : torch . Tensor ) -> torch . Tensor","title":"highway"},{"location":"api/modules/highway/#highway","text":"class Highway ( torch . nn . Module ): | def __init__ ( | self , | input_dim : int , | num_layers : int = 1 , | activation : Callable [[ torch . Tensor ], torch . Tensor ] = torch . nn . functional . relu | ) -> None A Highway layer does a gated combination of a linear transformation and a non-linear transformation of its input. :math: y = g * x + (1 - g) * f(A(x)) , where :math: A is a linear transformation, :math: f is an element-wise non-linearity, and :math: g is an element-wise gate, computed as :math: sigmoid(B(x)) . This module will apply a fixed number of highway layers to its input, returning the final result.","title":"Highway"},{"location":"api/modules/highway/#forward","text":"class Highway ( torch . nn . Module ): | ... | def forward ( self , inputs : torch . Tensor ) -> torch . Tensor","title":"forward"},{"location":"api/modules/input_variational_dropout/","text":"allennlp .modules .input_variational_dropout [SOURCE] InputVariationalDropout \u00b6 class InputVariationalDropout ( torch . nn . Dropout ) Apply the dropout technique in Gal and Ghahramani, Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning to a 3D tensor. This module accepts a 3D tensor of shape (batch_size, num_timesteps, embedding_dim) and samples a single dropout mask of shape (batch_size, embedding_dim) and applies it to every time step. forward \u00b6 class InputVariationalDropout ( torch . nn . Dropout ): | ... | def forward ( self , input_tensor ) Apply dropout to input tensor. Parameters \u00b6 input_tensor : torch.FloatTensor A tensor of shape (batch_size, num_timesteps, embedding_dim) Returns \u00b6 output : torch.FloatTensor A tensor of shape (batch_size, num_timesteps, embedding_dim) with dropout applied.","title":"input_variational_dropout"},{"location":"api/modules/input_variational_dropout/#inputvariationaldropout","text":"class InputVariationalDropout ( torch . nn . Dropout ) Apply the dropout technique in Gal and Ghahramani, Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning to a 3D tensor. This module accepts a 3D tensor of shape (batch_size, num_timesteps, embedding_dim) and samples a single dropout mask of shape (batch_size, embedding_dim) and applies it to every time step.","title":"InputVariationalDropout"},{"location":"api/modules/input_variational_dropout/#forward","text":"class InputVariationalDropout ( torch . nn . Dropout ): | ... | def forward ( self , input_tensor ) Apply dropout to input tensor.","title":"forward"},{"location":"api/modules/layer_norm/","text":"allennlp .modules .layer_norm [SOURCE] LayerNorm \u00b6 class LayerNorm ( torch . nn . Module ): | def __init__ ( self , dimension : int ) -> None An implementation of Layer Normalization . Layer Normalization stabilises the training of deep neural networks by normalising the outputs of neurons from a particular layer. It computes: output = (gamma * (tensor - mean) / (std + eps)) + beta Parameters \u00b6 dimension : int The dimension of the layer output to normalize. Returns \u00b6 The normalized layer output. forward \u00b6 class LayerNorm ( torch . nn . Module ): | ... | def forward ( self , tensor : torch . Tensor )","title":"layer_norm"},{"location":"api/modules/layer_norm/#layernorm","text":"class LayerNorm ( torch . nn . Module ): | def __init__ ( self , dimension : int ) -> None An implementation of Layer Normalization . Layer Normalization stabilises the training of deep neural networks by normalising the outputs of neurons from a particular layer. It computes: output = (gamma * (tensor - mean) / (std + eps)) + beta","title":"LayerNorm"},{"location":"api/modules/layer_norm/#forward","text":"class LayerNorm ( torch . nn . Module ): | ... | def forward ( self , tensor : torch . Tensor )","title":"forward"},{"location":"api/modules/lstm_cell_with_projection/","text":"allennlp .modules .lstm_cell_with_projection [SOURCE] An LSTM with Recurrent Dropout, a hidden_state which is projected and clipping on both the hidden state and the memory state of the LSTM. LstmCellWithProjection \u00b6 class LstmCellWithProjection ( torch . nn . Module ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | cell_size : int , | go_forward : bool = True , | recurrent_dropout_probability : float = 0.0 , | memory_cell_clip_value : Optional [ float ] = None , | state_projection_clip_value : Optional [ float ] = None | ) -> None An LSTM with Recurrent Dropout and a projected and clipped hidden state and memory. Note: this implementation is slower than the native Pytorch LSTM because it cannot make use of CUDNN optimizations for stacked RNNs due to and variational dropout and the custom nature of the cell state. Parameters \u00b6 input_size : int The dimension of the inputs to the LSTM. hidden_size : int The dimension of the outputs of the LSTM. cell_size : int The dimension of the memory cell used for the LSTM. go_forward : bool , optional (default = True ) The direction in which the LSTM is applied to the sequence. Forwards by default, or backwards if False. recurrent_dropout_probability : float , optional (default = 0.0 ) The dropout probability to be used in a dropout scheme as stated in [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks] 0 . Implementation wise, this simply applies a fixed dropout mask per sequence to the recurrent connection of the LSTM. state_projection_clip_value : float , optional (default = None ) The magnitude with which to clip the hidden_state after projecting it. memory_cell_clip_value : float , optional (default = None ) The magnitude with which to clip the memory cell. Returns \u00b6 output_accumulator : torch.FloatTensor The outputs of the LSTM for each timestep. A tensor of shape (batch_size, max_timesteps, hidden_size) where for a given batch element, all outputs past the sequence length for that batch are zero tensors. final_state : Tuple[torch.FloatTensor, torch.FloatTensor] The final (state, memory) states of the LSTM, with shape (1, batch_size, hidden_size) and (1, batch_size, cell_size) respectively. The first dimension is 1 in order to match the Pytorch API for returning stacked LSTM states. reset_parameters \u00b6 class LstmCellWithProjection ( torch . nn . Module ): | ... | def reset_parameters ( self ) forward \u00b6 class LstmCellWithProjection ( torch . nn . Module ): | ... | def forward ( | self , | inputs : torch . FloatTensor , | batch_lengths : List [ int ], | initial_state : Optional [ Tuple [ torch . Tensor , torch . Tensor ]] = None | ) Parameters \u00b6 inputs : torch.FloatTensor A tensor of shape (batch_size, num_timesteps, input_size) to apply the LSTM over. batch_lengths : List[int] A list of length batch_size containing the lengths of the sequences in batch. initial_state : Tuple[torch.Tensor, torch.Tensor] , optional (default = None ) A tuple (state, memory) representing the initial hidden state and memory of the LSTM. The state has shape (1, batch_size, hidden_size) and the memory has shape (1, batch_size, cell_size). Returns \u00b6 output_accumulator : torch.FloatTensor The outputs of the LSTM for each timestep. A tensor of shape (batch_size, max_timesteps, hidden_size) where for a given batch element, all outputs past the sequence length for that batch are zero tensors. final_state : Tuple[torch.FloatTensor, torch.FloatTensor] A tuple (state, memory) representing the initial hidden state and memory of the LSTM. The state has shape (1, batch_size, hidden_size) and the memory has shape (1, batch_size, cell_size).","title":"lstm_cell_with_projection"},{"location":"api/modules/lstm_cell_with_projection/#lstmcellwithprojection","text":"class LstmCellWithProjection ( torch . nn . Module ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | cell_size : int , | go_forward : bool = True , | recurrent_dropout_probability : float = 0.0 , | memory_cell_clip_value : Optional [ float ] = None , | state_projection_clip_value : Optional [ float ] = None | ) -> None An LSTM with Recurrent Dropout and a projected and clipped hidden state and memory. Note: this implementation is slower than the native Pytorch LSTM because it cannot make use of CUDNN optimizations for stacked RNNs due to and variational dropout and the custom nature of the cell state.","title":"LstmCellWithProjection"},{"location":"api/modules/lstm_cell_with_projection/#reset_parameters","text":"class LstmCellWithProjection ( torch . nn . Module ): | ... | def reset_parameters ( self )","title":"reset_parameters"},{"location":"api/modules/lstm_cell_with_projection/#forward","text":"class LstmCellWithProjection ( torch . nn . Module ): | ... | def forward ( | self , | inputs : torch . FloatTensor , | batch_lengths : List [ int ], | initial_state : Optional [ Tuple [ torch . Tensor , torch . Tensor ]] = None | )","title":"forward"},{"location":"api/modules/masked_layer_norm/","text":"allennlp .modules .masked_layer_norm [SOURCE] MaskedLayerNorm \u00b6 class MaskedLayerNorm ( torch . nn . Module ): | def __init__ ( self , size : int , gamma0 : float = 0.1 ) -> None See LayerNorm for details. Note, however, that unlike LayerNorm this norm includes a batch component. forward \u00b6 class MaskedLayerNorm ( torch . nn . Module ): | ... | def forward ( | self , | tensor : torch . Tensor , | mask : torch . BoolTensor | ) -> torch . Tensor","title":"masked_layer_norm"},{"location":"api/modules/masked_layer_norm/#maskedlayernorm","text":"class MaskedLayerNorm ( torch . nn . Module ): | def __init__ ( self , size : int , gamma0 : float = 0.1 ) -> None See LayerNorm for details. Note, however, that unlike LayerNorm this norm includes a batch component.","title":"MaskedLayerNorm"},{"location":"api/modules/masked_layer_norm/#forward","text":"class MaskedLayerNorm ( torch . nn . Module ): | ... | def forward ( | self , | tensor : torch . Tensor , | mask : torch . BoolTensor | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/maxout/","text":"allennlp .modules .maxout [SOURCE] A maxout neural network. Maxout \u00b6 class Maxout ( torch . nn . Module , FromParams ): | def __init__ ( | self , | input_dim : int , | num_layers : int , | output_dims : Union [ int , Sequence [ int ]], | pool_sizes : Union [ int , Sequence [ int ]], | dropout : Union [ float , Sequence [ float ]] = 0.0 | ) -> None This Module is a maxout neural network. Parameters \u00b6 input_dim : int The dimensionality of the input. We assume the input has shape (batch_size, input_dim) . num_layers : int The number of maxout layers to apply to the input. output_dims : Union[int, Sequence[int]] The output dimension of each of the maxout layers. If this is a single int , we use it for all maxout layers. If it is a Sequence[int] , len(output_dims) must be num_layers . pool_sizes : Union[int, Sequence[int]] The size of max-pools. If this is a single int , we use it for all maxout layers. If it is a Sequence[int] , len(pool_sizes) must be num_layers . dropout : Union[float, Sequence[float]] , optional (default = 0.0 ) If given, we will apply this amount of dropout after each layer. Semantics of float versus Sequence[float] is the same as with other parameters. get_output_dim \u00b6 class Maxout ( torch . nn . Module , FromParams ): | ... | def get_output_dim ( self ) get_input_dim \u00b6 class Maxout ( torch . nn . Module , FromParams ): | ... | def get_input_dim ( self ) forward \u00b6 class Maxout ( torch . nn . Module , FromParams ): | ... | def forward ( self , inputs : torch . Tensor ) -> torch . Tensor","title":"maxout"},{"location":"api/modules/maxout/#maxout","text":"class Maxout ( torch . nn . Module , FromParams ): | def __init__ ( | self , | input_dim : int , | num_layers : int , | output_dims : Union [ int , Sequence [ int ]], | pool_sizes : Union [ int , Sequence [ int ]], | dropout : Union [ float , Sequence [ float ]] = 0.0 | ) -> None This Module is a maxout neural network.","title":"Maxout"},{"location":"api/modules/maxout/#get_output_dim","text":"class Maxout ( torch . nn . Module , FromParams ): | ... | def get_output_dim ( self )","title":"get_output_dim"},{"location":"api/modules/maxout/#get_input_dim","text":"class Maxout ( torch . nn . Module , FromParams ): | ... | def get_input_dim ( self )","title":"get_input_dim"},{"location":"api/modules/maxout/#forward","text":"class Maxout ( torch . nn . Module , FromParams ): | ... | def forward ( self , inputs : torch . Tensor ) -> torch . Tensor","title":"forward"},{"location":"api/modules/residual_with_layer_dropout/","text":"allennlp .modules .residual_with_layer_dropout [SOURCE] ResidualWithLayerDropout \u00b6 class ResidualWithLayerDropout ( torch . nn . Module ): | def __init__ ( self , undecayed_dropout_prob : float = 0.5 ) -> None A residual connection with the layer dropout technique Deep Networks with Stochastic Depth . This module accepts the input and output of a layer, decides whether this layer should be stochastically dropped, returns either the input or output + input. During testing, it will re-calibrate the outputs of this layer by the expected number of times it participates in training. forward \u00b6 class ResidualWithLayerDropout ( torch . nn . Module ): | ... | def forward ( | self , | layer_input : torch . Tensor , | layer_output : torch . Tensor , | layer_index : int = None , | total_layers : int = None | ) -> torch . Tensor Apply dropout to this layer, for this whole mini-batch. dropout_prob = layer_index / total_layers * undecayed_dropout_prob if layer_idx and total_layers is specified, else it will use the undecayed_dropout_prob directly. Parameters \u00b6 layer_input torch.FloatTensor required The input tensor of this layer. layer_output torch.FloatTensor required The output tensor of this layer, with the same shape as the layer_input. layer_index int The layer index, starting from 1. This is used to calcuate the dropout prob together with the total_layers parameter. total_layers int The total number of layers. Returns \u00b6 output : torch.FloatTensor A tensor with the same shape as layer_input and layer_output .","title":"residual_with_layer_dropout"},{"location":"api/modules/residual_with_layer_dropout/#residualwithlayerdropout","text":"class ResidualWithLayerDropout ( torch . nn . Module ): | def __init__ ( self , undecayed_dropout_prob : float = 0.5 ) -> None A residual connection with the layer dropout technique Deep Networks with Stochastic Depth . This module accepts the input and output of a layer, decides whether this layer should be stochastically dropped, returns either the input or output + input. During testing, it will re-calibrate the outputs of this layer by the expected number of times it participates in training.","title":"ResidualWithLayerDropout"},{"location":"api/modules/residual_with_layer_dropout/#forward","text":"class ResidualWithLayerDropout ( torch . nn . Module ): | ... | def forward ( | self , | layer_input : torch . Tensor , | layer_output : torch . Tensor , | layer_index : int = None , | total_layers : int = None | ) -> torch . Tensor Apply dropout to this layer, for this whole mini-batch. dropout_prob = layer_index / total_layers * undecayed_dropout_prob if layer_idx and total_layers is specified, else it will use the undecayed_dropout_prob directly.","title":"forward"},{"location":"api/modules/sampled_softmax_loss/","text":"allennlp .modules .sampled_softmax_loss [SOURCE] SampledSoftmaxLoss \u00b6 class SampledSoftmaxLoss ( torch . nn . Module ): | def __init__ ( | self , | num_words : int , | embedding_dim : int , | num_samples : int , | sparse : bool = False , | unk_id : int = None , | use_character_inputs : bool = True , | use_fast_sampler : bool = False | ) -> None Based on the default log_uniform_candidate_sampler in tensorflow. Note num_words DOES NOT include padding id. Note In all cases except (tie_embeddings=True and use_character_inputs=False) the weights are dimensioned as num_words and do not include an entry for the padding (0) id. For the (tie_embeddings=True and use_character_inputs=False) case, then the embeddings DO include the extra 0 padding, to be consistent with the word embedding layer. Parameters \u00b6 num_words, int , required The number of words in the vocabulary embedding_dim, int , required The dimension to softmax over num_samples, int , required During training take this many samples. Must be less than num_words. sparse, bool , optional (default = False ) If this is true, we use a sparse embedding matrix. unk_id, int , optional (default = None ) If provided, the id that represents unknown characters. use_character_inputs, bool , optional (default = True ) Whether to use character inputs use_fast_sampler, bool , optional (default = False ) Whether to use the fast cython sampler. initialize_num_words \u00b6 class SampledSoftmaxLoss ( torch . nn . Module ): | ... | def initialize_num_words ( self ) forward \u00b6 class SampledSoftmaxLoss ( torch . nn . Module ): | ... | def forward ( | self , | embeddings : torch . Tensor , | targets : torch . Tensor , | target_token_embedding : torch . Tensor = None | ) -> torch . Tensor log_uniform_candidate_sampler \u00b6 class SampledSoftmaxLoss ( torch . nn . Module ): | ... | def log_uniform_candidate_sampler ( | self , | targets , | choice_func = _choice | )","title":"sampled_softmax_loss"},{"location":"api/modules/sampled_softmax_loss/#sampledsoftmaxloss","text":"class SampledSoftmaxLoss ( torch . nn . Module ): | def __init__ ( | self , | num_words : int , | embedding_dim : int , | num_samples : int , | sparse : bool = False , | unk_id : int = None , | use_character_inputs : bool = True , | use_fast_sampler : bool = False | ) -> None Based on the default log_uniform_candidate_sampler in tensorflow. Note num_words DOES NOT include padding id. Note In all cases except (tie_embeddings=True and use_character_inputs=False) the weights are dimensioned as num_words and do not include an entry for the padding (0) id. For the (tie_embeddings=True and use_character_inputs=False) case, then the embeddings DO include the extra 0 padding, to be consistent with the word embedding layer.","title":"SampledSoftmaxLoss"},{"location":"api/modules/sampled_softmax_loss/#initialize_num_words","text":"class SampledSoftmaxLoss ( torch . nn . Module ): | ... | def initialize_num_words ( self )","title":"initialize_num_words"},{"location":"api/modules/sampled_softmax_loss/#forward","text":"class SampledSoftmaxLoss ( torch . nn . Module ): | ... | def forward ( | self , | embeddings : torch . Tensor , | targets : torch . Tensor , | target_token_embedding : torch . Tensor = None | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/sampled_softmax_loss/#log_uniform_candidate_sampler","text":"class SampledSoftmaxLoss ( torch . nn . Module ): | ... | def log_uniform_candidate_sampler ( | self , | targets , | choice_func = _choice | )","title":"log_uniform_candidate_sampler"},{"location":"api/modules/scalar_mix/","text":"allennlp .modules .scalar_mix [SOURCE] ScalarMix \u00b6 class ScalarMix ( torch . nn . Module ): | def __init__ ( | self , | mixture_size : int , | do_layer_norm : bool = False , | initial_scalar_parameters : List [ float ] = None , | trainable : bool = True | ) -> None Computes a parameterised scalar mixture of N tensors, mixture = gamma * sum(s_k * tensor_k) where s = softmax(w) , with w and gamma scalar parameters. In addition, if do_layer_norm=True then apply layer normalization to each tensor before weighting. forward \u00b6 class ScalarMix ( torch . nn . Module ): | ... | def forward ( | self , | tensors : List [ torch . Tensor ], | mask : torch . BoolTensor = None | ) -> torch . Tensor Compute a weighted average of the tensors . The input tensors an be any shape with at least two dimensions, but must all be the same shape. When do_layer_norm=True , the mask is required input. If the tensors are dimensioned (dim_0, ..., dim_{n-1}, dim_n) , then the mask is dimensioned (dim_0, ..., dim_{n-1}) , as in the typical case with tensors of shape (batch_size, timesteps, dim) and mask of shape (batch_size, timesteps) . When do_layer_norm=False the mask is ignored.","title":"scalar_mix"},{"location":"api/modules/scalar_mix/#scalarmix","text":"class ScalarMix ( torch . nn . Module ): | def __init__ ( | self , | mixture_size : int , | do_layer_norm : bool = False , | initial_scalar_parameters : List [ float ] = None , | trainable : bool = True | ) -> None Computes a parameterised scalar mixture of N tensors, mixture = gamma * sum(s_k * tensor_k) where s = softmax(w) , with w and gamma scalar parameters. In addition, if do_layer_norm=True then apply layer normalization to each tensor before weighting.","title":"ScalarMix"},{"location":"api/modules/scalar_mix/#forward","text":"class ScalarMix ( torch . nn . Module ): | ... | def forward ( | self , | tensors : List [ torch . Tensor ], | mask : torch . BoolTensor = None | ) -> torch . Tensor Compute a weighted average of the tensors . The input tensors an be any shape with at least two dimensions, but must all be the same shape. When do_layer_norm=True , the mask is required input. If the tensors are dimensioned (dim_0, ..., dim_{n-1}, dim_n) , then the mask is dimensioned (dim_0, ..., dim_{n-1}) , as in the typical case with tensors of shape (batch_size, timesteps, dim) and mask of shape (batch_size, timesteps) . When do_layer_norm=False the mask is ignored.","title":"forward"},{"location":"api/modules/softmax_loss/","text":"allennlp .modules .softmax_loss [SOURCE] SoftmaxLoss \u00b6 class SoftmaxLoss ( torch . nn . Module ): | def __init__ ( self , num_words : int , embedding_dim : int ) -> None Given some embeddings and some targets, applies a linear layer to create logits over possible words and then returns the negative log likelihood. Does not add a padding ID into the vocabulary, and input targets to forward should not include a padding ID. forward \u00b6 class SoftmaxLoss ( torch . nn . Module ): | ... | def forward ( | self , | embeddings : torch . Tensor , | targets : torch . Tensor | ) -> torch . Tensor Parameters \u00b6 embeddings : torch.Tensor A tensor of shape (sequence_length, embedding_dim) targets : torch.Tensor A tensor of shape (batch_size, ) Returns \u00b6 loss : torch.FloatTensor A scalar loss to be optimized.","title":"softmax_loss"},{"location":"api/modules/softmax_loss/#softmaxloss","text":"class SoftmaxLoss ( torch . nn . Module ): | def __init__ ( self , num_words : int , embedding_dim : int ) -> None Given some embeddings and some targets, applies a linear layer to create logits over possible words and then returns the negative log likelihood. Does not add a padding ID into the vocabulary, and input targets to forward should not include a padding ID.","title":"SoftmaxLoss"},{"location":"api/modules/softmax_loss/#forward","text":"class SoftmaxLoss ( torch . nn . Module ): | ... | def forward ( | self , | embeddings : torch . Tensor , | targets : torch . Tensor | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/stacked_alternating_lstm/","text":"allennlp .modules .stacked_alternating_lstm [SOURCE] A stacked LSTM with LSTM layers which alternate between going forwards over the sequence and going backwards. TensorPair \u00b6 TensorPair = Tuple [ torch . Tensor , torch . Tensor ] StackedAlternatingLstm \u00b6 class StackedAlternatingLstm ( torch . nn . Module ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int , | recurrent_dropout_probability : float = 0.0 , | use_highway : bool = True , | use_input_projection_bias : bool = True | ) -> None A stacked LSTM with LSTM layers which alternate between going forwards over the sequence and going backwards. This implementation is based on the description in Deep Semantic Role Labeling - What works and what's next . Parameters \u00b6 input_size : int The dimension of the inputs to the LSTM. hidden_size : int The dimension of the outputs of the LSTM. num_layers : int The number of stacked LSTMs to use. recurrent_dropout_probability : float , optional (default = 0.0 ) The dropout probability to be used in a dropout scheme as stated in A Theoretically Grounded Application of Dropout in Recurrent Neural Networks . use_input_projection_bias : bool , optional (default = True ) Whether or not to use a bias on the input projection layer. This is mainly here for backwards compatibility reasons and will be removed (and set to False) in future releases. Returns \u00b6 output_accumulator : PackedSequence The outputs of the interleaved LSTMs per timestep. A tensor of shape (batch_size, max_timesteps, hidden_size) where for a given batch element, all outputs past the sequence length for that batch are zero tensors. forward \u00b6 class StackedAlternatingLstm ( torch . nn . Module ): | ... | def forward ( | self , | inputs : PackedSequence , | initial_state : Optional [ TensorPair ] = None | ) -> Tuple [ Union [ torch . Tensor , PackedSequence ], TensorPair ] Parameters \u00b6 inputs : PackedSequence A batch first PackedSequence to run the stacked LSTM over. initial_state : Tuple[torch.Tensor, torch.Tensor] , optional (default = None ) A tuple (state, memory) representing the initial hidden state and memory of the LSTM. Each tensor has shape (1, batch_size, output_dimension). Returns \u00b6 output_sequence : PackedSequence The encoded sequence of shape (batch_size, sequence_length, hidden_size) final_states : Tuple[torch.Tensor, torch.Tensor] The per-layer final (state, memory) states of the LSTM, each with shape (num_layers, batch_size, hidden_size).","title":"stacked_alternating_lstm"},{"location":"api/modules/stacked_alternating_lstm/#tensorpair","text":"TensorPair = Tuple [ torch . Tensor , torch . Tensor ]","title":"TensorPair"},{"location":"api/modules/stacked_alternating_lstm/#stackedalternatinglstm","text":"class StackedAlternatingLstm ( torch . nn . Module ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int , | recurrent_dropout_probability : float = 0.0 , | use_highway : bool = True , | use_input_projection_bias : bool = True | ) -> None A stacked LSTM with LSTM layers which alternate between going forwards over the sequence and going backwards. This implementation is based on the description in Deep Semantic Role Labeling - What works and what's next .","title":"StackedAlternatingLstm"},{"location":"api/modules/stacked_alternating_lstm/#forward","text":"class StackedAlternatingLstm ( torch . nn . Module ): | ... | def forward ( | self , | inputs : PackedSequence , | initial_state : Optional [ TensorPair ] = None | ) -> Tuple [ Union [ torch . Tensor , PackedSequence ], TensorPair ]","title":"forward"},{"location":"api/modules/stacked_bidirectional_lstm/","text":"allennlp .modules .stacked_bidirectional_lstm [SOURCE] TensorPair \u00b6 TensorPair = Tuple [ torch . Tensor , torch . Tensor ] StackedBidirectionalLstm \u00b6 class StackedBidirectionalLstm ( torch . nn . Module ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int , | recurrent_dropout_probability : float = 0.0 , | layer_dropout_probability : float = 0.0 , | use_highway : bool = True | ) -> None A standard stacked Bidirectional LSTM where the LSTM layers are concatenated between each layer. The only difference between this and a regular bidirectional LSTM is the application of variational dropout to the hidden states and outputs of each layer apart from the last layer of the LSTM. Note that this will be slower, as it doesn't use CUDNN. Parameters \u00b6 input_size : int The dimension of the inputs to the LSTM. hidden_size : int The dimension of the outputs of the LSTM. num_layers : int The number of stacked Bidirectional LSTMs to use. recurrent_dropout_probability : float , optional (default = 0.0 ) The recurrent dropout probability to be used in a dropout scheme as stated in A Theoretically Grounded Application of Dropout in Recurrent Neural Networks . layer_dropout_probability : float , optional (default = 0.0 ) The layer wise dropout probability to be used in a dropout scheme as stated in A Theoretically Grounded Application of Dropout in Recurrent Neural Networks . use_highway : bool , optional (default = True ) Whether or not to use highway connections between layers. This effectively involves reparameterising the normal output of an LSTM as:: gate = sigmoid(W_x1 * x_t + W_h * h_t) output = gate * h_t + (1 - gate) * (W_x2 * x_t) forward \u00b6 class StackedBidirectionalLstm ( torch . nn . Module ): | ... | def forward ( | self , | inputs : PackedSequence , | initial_state : Optional [ TensorPair ] = None | ) -> Tuple [ PackedSequence , TensorPair ] Parameters \u00b6 inputs : PackedSequence A batch first PackedSequence to run the stacked LSTM over. initial_state : Tuple[torch.Tensor, torch.Tensor] , optional (default = None ) A tuple (state, memory) representing the initial hidden state and memory of the LSTM. Each tensor has shape (num_layers, batch_size, output_dimension * 2). Returns \u00b6 output_sequence : PackedSequence The encoded sequence of shape (batch_size, sequence_length, hidden_size * 2) final_states : torch.Tensor The per-layer final (state, memory) states of the LSTM, each with shape (num_layers * 2, batch_size, hidden_size * 2).","title":"stacked_bidirectional_lstm"},{"location":"api/modules/stacked_bidirectional_lstm/#tensorpair","text":"TensorPair = Tuple [ torch . Tensor , torch . Tensor ]","title":"TensorPair"},{"location":"api/modules/stacked_bidirectional_lstm/#stackedbidirectionallstm","text":"class StackedBidirectionalLstm ( torch . nn . Module ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int , | recurrent_dropout_probability : float = 0.0 , | layer_dropout_probability : float = 0.0 , | use_highway : bool = True | ) -> None A standard stacked Bidirectional LSTM where the LSTM layers are concatenated between each layer. The only difference between this and a regular bidirectional LSTM is the application of variational dropout to the hidden states and outputs of each layer apart from the last layer of the LSTM. Note that this will be slower, as it doesn't use CUDNN.","title":"StackedBidirectionalLstm"},{"location":"api/modules/stacked_bidirectional_lstm/#forward","text":"class StackedBidirectionalLstm ( torch . nn . Module ): | ... | def forward ( | self , | inputs : PackedSequence , | initial_state : Optional [ TensorPair ] = None | ) -> Tuple [ PackedSequence , TensorPair ]","title":"forward"},{"location":"api/modules/time_distributed/","text":"allennlp .modules .time_distributed [SOURCE] A wrapper that unrolls the second (time) dimension of a tensor into the first (batch) dimension, applies some other Module , and then rolls the time dimension back up. TimeDistributed \u00b6 class TimeDistributed ( torch . nn . Module ): | def __init__ ( self , module ) Given an input shaped like (batch_size, time_steps, [rest]) and a Module that takes inputs like (batch_size, [rest]) , TimeDistributed reshapes the input to be (batch_size * time_steps, [rest]) , applies the contained Module , then reshapes it back. Note that while the above gives shapes with batch_size first, this Module also works if batch_size is second - we always just combine the first two dimensions, then split them. It also reshapes keyword arguments unless they are not tensors or their name is specified in the optional pass_through iterable. forward \u00b6 class TimeDistributed ( torch . nn . Module ): | ... | def forward ( | self , | * inputs , | * , pass_through : List [ str ] = None , | ** kwargs | )","title":"time_distributed"},{"location":"api/modules/time_distributed/#timedistributed","text":"class TimeDistributed ( torch . nn . Module ): | def __init__ ( self , module ) Given an input shaped like (batch_size, time_steps, [rest]) and a Module that takes inputs like (batch_size, [rest]) , TimeDistributed reshapes the input to be (batch_size * time_steps, [rest]) , applies the contained Module , then reshapes it back. Note that while the above gives shapes with batch_size first, this Module also works if batch_size is second - we always just combine the first two dimensions, then split them. It also reshapes keyword arguments unless they are not tensors or their name is specified in the optional pass_through iterable.","title":"TimeDistributed"},{"location":"api/modules/time_distributed/#forward","text":"class TimeDistributed ( torch . nn . Module ): | ... | def forward ( | self , | * inputs , | * , pass_through : List [ str ] = None , | ** kwargs | )","title":"forward"},{"location":"api/modules/util/","text":"allennlp .modules .util [SOURCE] replicate_layers \u00b6 def replicate_layers ( layer : torch . nn . Module , num_copies : int ) Parameters \u00b6 layer (torch.nn.Module) - The torch layer that needs to be replicated. num_copies (int) - Number of copies to create. Returns \u00b6 A ModuleList that contains `num_copies` of the `layer` .","title":"util"},{"location":"api/modules/util/#replicate_layers","text":"def replicate_layers ( layer : torch . nn . Module , num_copies : int )","title":"replicate_layers"},{"location":"api/modules/attention/additive_attention/","text":"allennlp .modules .attention .additive_attention [SOURCE] AdditiveAttention \u00b6 @Attention . register ( \"additive\" ) class AdditiveAttention ( Attention ): | def __init__ ( | self , | vector_dim : int , | matrix_dim : int , | normalize : bool = True | ) -> None Computes attention between a vector and a matrix using an additive attention function. This function has two matrices W , U and a vector V . The similarity between the vector x and the matrix y is computed as V tanh(Wx + Uy) . This attention is often referred as concat or additive attention. It was introduced in Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau et al, 2015) . Registered as an Attention with name \"additive\". Parameters \u00b6 vector_dim : int The dimension of the vector, x , described above. This is x.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build the weight matrix correctly. matrix_dim : int The dimension of the matrix, y , described above. This is y.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build the weight matrix correctly. normalize : bool , optional (default = True ) If true, we normalize the computed similarities with a softmax, to return a probability distribution for your attention. If false, this is just computing a similarity score. reset_parameters \u00b6 class AdditiveAttention ( Attention ): | ... | def reset_parameters ( self )","title":"additive_attention"},{"location":"api/modules/attention/additive_attention/#additiveattention","text":"@Attention . register ( \"additive\" ) class AdditiveAttention ( Attention ): | def __init__ ( | self , | vector_dim : int , | matrix_dim : int , | normalize : bool = True | ) -> None Computes attention between a vector and a matrix using an additive attention function. This function has two matrices W , U and a vector V . The similarity between the vector x and the matrix y is computed as V tanh(Wx + Uy) . This attention is often referred as concat or additive attention. It was introduced in Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau et al, 2015) . Registered as an Attention with name \"additive\".","title":"AdditiveAttention"},{"location":"api/modules/attention/additive_attention/#reset_parameters","text":"class AdditiveAttention ( Attention ): | ... | def reset_parameters ( self )","title":"reset_parameters"},{"location":"api/modules/attention/attention/","text":"allennlp .modules .attention .attention [SOURCE] An attention module that computes the similarity between an input vector and the rows of a matrix. Attention \u00b6 class Attention ( torch . nn . Module , Registrable ): | def __init__ ( self , normalize : bool = True ) -> None An Attention takes two inputs: a (batched) vector and a matrix, plus an optional mask on the rows of the matrix. We compute the similarity between the vector and each row in the matrix, and then (optionally) perform a softmax over rows using those computed similarities. Inputs: vector: shape (batch_size, embedding_dim) matrix: shape (batch_size, num_rows, embedding_dim) matrix_mask: shape (batch_size, num_rows) , specifying which rows are just padding. Output: attention: shape (batch_size, num_rows) . Parameters \u00b6 normalize : bool , optional (default = True ) If true, we normalize the computed similarities with a softmax, to return a probability distribution for your attention. If false, this is just computing a similarity score. forward \u00b6 class Attention ( torch . nn . Module , Registrable ): | ... | def forward ( | self , | vector : torch . Tensor , | matrix : torch . Tensor , | matrix_mask : torch . BoolTensor = None | ) -> torch . Tensor","title":"attention"},{"location":"api/modules/attention/attention/#attention","text":"class Attention ( torch . nn . Module , Registrable ): | def __init__ ( self , normalize : bool = True ) -> None An Attention takes two inputs: a (batched) vector and a matrix, plus an optional mask on the rows of the matrix. We compute the similarity between the vector and each row in the matrix, and then (optionally) perform a softmax over rows using those computed similarities. Inputs: vector: shape (batch_size, embedding_dim) matrix: shape (batch_size, num_rows, embedding_dim) matrix_mask: shape (batch_size, num_rows) , specifying which rows are just padding. Output: attention: shape (batch_size, num_rows) .","title":"Attention"},{"location":"api/modules/attention/attention/#forward","text":"class Attention ( torch . nn . Module , Registrable ): | ... | def forward ( | self , | vector : torch . Tensor , | matrix : torch . Tensor , | matrix_mask : torch . BoolTensor = None | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/attention/bilinear_attention/","text":"allennlp .modules .attention .bilinear_attention [SOURCE] BilinearAttention \u00b6 @Attention . register ( \"bilinear\" ) class BilinearAttention ( Attention ): | def __init__ ( | self , | vector_dim : int , | matrix_dim : int , | activation : Activation = None , | normalize : bool = True | ) -> None Computes attention between a vector and a matrix using a bilinear attention function. This function has a matrix of weights W and a bias b , and the similarity between the vector x and the matrix y is computed as x^T W y + b . Registered as an Attention with name \"bilinear\". Parameters \u00b6 vector_dim : int The dimension of the vector, x , described above. This is x.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build the weight matrix correctly. matrix_dim : int The dimension of the matrix, y , described above. This is y.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build the weight matrix correctly. activation : Activation , optional (default = linear ) An activation function applied after the x^T W y + b calculation. Default is linear, i.e. no activation. normalize : bool , optional (default = True ) If true, we normalize the computed similarities with a softmax, to return a probability distribution for your attention. If false, this is just computing a similarity score. reset_parameters \u00b6 class BilinearAttention ( Attention ): | ... | def reset_parameters ( self )","title":"bilinear_attention"},{"location":"api/modules/attention/bilinear_attention/#bilinearattention","text":"@Attention . register ( \"bilinear\" ) class BilinearAttention ( Attention ): | def __init__ ( | self , | vector_dim : int , | matrix_dim : int , | activation : Activation = None , | normalize : bool = True | ) -> None Computes attention between a vector and a matrix using a bilinear attention function. This function has a matrix of weights W and a bias b , and the similarity between the vector x and the matrix y is computed as x^T W y + b . Registered as an Attention with name \"bilinear\".","title":"BilinearAttention"},{"location":"api/modules/attention/bilinear_attention/#reset_parameters","text":"class BilinearAttention ( Attention ): | ... | def reset_parameters ( self )","title":"reset_parameters"},{"location":"api/modules/attention/cosine_attention/","text":"allennlp .modules .attention .cosine_attention [SOURCE] CosineAttention \u00b6 @Attention . register ( \"cosine\" ) class CosineAttention ( Attention ) Computes attention between a vector and a matrix using cosine similarity. Registered as an Attention with name \"cosine\".","title":"cosine_attention"},{"location":"api/modules/attention/cosine_attention/#cosineattention","text":"@Attention . register ( \"cosine\" ) class CosineAttention ( Attention ) Computes attention between a vector and a matrix using cosine similarity. Registered as an Attention with name \"cosine\".","title":"CosineAttention"},{"location":"api/modules/attention/dot_product_attention/","text":"allennlp .modules .attention .dot_product_attention [SOURCE] DotProductAttention \u00b6 @Attention . register ( \"dot_product\" ) class DotProductAttention ( Attention ) Computes attention between a vector and a matrix using dot product. Reference: Attention Is All You Need (Vaswani et al, 2017) Registered as an Attention with name \"dot_product\".","title":"dot_product_attention"},{"location":"api/modules/attention/dot_product_attention/#dotproductattention","text":"@Attention . register ( \"dot_product\" ) class DotProductAttention ( Attention ) Computes attention between a vector and a matrix using dot product. Reference: Attention Is All You Need (Vaswani et al, 2017) Registered as an Attention with name \"dot_product\".","title":"DotProductAttention"},{"location":"api/modules/attention/linear_attention/","text":"allennlp .modules .attention .linear_attention [SOURCE] LinearAttention \u00b6 @Attention . register ( \"linear\" ) class LinearAttention ( Attention ): | def __init__ ( | self , | tensor_1_dim : int , | tensor_2_dim : int , | combination : str = \"x,y\" , | activation : Activation = None , | normalize : bool = True | ) -> None This Attention module performs a dot product between a vector of weights and some combination of the two input vectors, followed by an (optional) activation function. The combination used is configurable. If the two vectors are x and y , we allow the following kinds of combinations : x , y , x*y , x+y , x-y , x/y , where each of those binary operations is performed elementwise. You can list as many combinations as you want, comma separated. For example, you might give x,y,x*y as the combination parameter to this class. The computed similarity function would then be w^T [x; y; x*y] + b , where w is a vector of weights, b is a bias parameter, and [;] is vector concatenation. Note that if you want a bilinear similarity function with a diagonal weight matrix W, where the similarity function is computed as x * w * y + b (with w the diagonal of W ), you can accomplish that with this class by using \"x*y\" for combination . Registered as an Attention with name \"linear\". Parameters \u00b6 tensor_1_dim : int The dimension of the first tensor, x , described above. This is x.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build weight vectors correctly. tensor_2_dim : int The dimension of the second tensor, y , described above. This is y.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build weight vectors correctly. combination : str , optional (default = \"x,y\" ) Described above. activation : Activation , optional (default = linear ) An activation function applied after the w^T * [x;y] + b calculation. Default is linear, i.e. no activation. normalize : bool , optional (default = True ) reset_parameters \u00b6 class LinearAttention ( Attention ): | ... | def reset_parameters ( self )","title":"linear_attention"},{"location":"api/modules/attention/linear_attention/#linearattention","text":"@Attention . register ( \"linear\" ) class LinearAttention ( Attention ): | def __init__ ( | self , | tensor_1_dim : int , | tensor_2_dim : int , | combination : str = \"x,y\" , | activation : Activation = None , | normalize : bool = True | ) -> None This Attention module performs a dot product between a vector of weights and some combination of the two input vectors, followed by an (optional) activation function. The combination used is configurable. If the two vectors are x and y , we allow the following kinds of combinations : x , y , x*y , x+y , x-y , x/y , where each of those binary operations is performed elementwise. You can list as many combinations as you want, comma separated. For example, you might give x,y,x*y as the combination parameter to this class. The computed similarity function would then be w^T [x; y; x*y] + b , where w is a vector of weights, b is a bias parameter, and [;] is vector concatenation. Note that if you want a bilinear similarity function with a diagonal weight matrix W, where the similarity function is computed as x * w * y + b (with w the diagonal of W ), you can accomplish that with this class by using \"x*y\" for combination . Registered as an Attention with name \"linear\".","title":"LinearAttention"},{"location":"api/modules/attention/linear_attention/#reset_parameters","text":"class LinearAttention ( Attention ): | ... | def reset_parameters ( self )","title":"reset_parameters"},{"location":"api/modules/attention/scaled_dot_product_attention/","text":"allennlp .modules .attention .scaled_dot_product_attention [SOURCE] ScaledDotProductAttention \u00b6 @Attention . register ( \"scaled_dot_product\" ) class ScaledDotProductAttention ( DotProductAttention ): | def __init__ ( | self , | scaling_factor : Optional [ int ] = None , | normalize : bool = True | ) -> None Computes attention between two tensors using scaled dot product. Reference: [Attention Is All You Need (Vaswani et al, 2017)] \u00b6 (https://api.semanticscholar.org/CorpusID:13756489) \u00b6 Registered as an Attention with name \"scaled_dot_product\". Parameters \u00b6 scaling_factor : int The similarity score is scaled down by the scaling_factor . normalize : bool , optional (default = True ) If true, we normalize the computed similarities with a softmax, to return a probability distribution for your attention. If false, this is just computing a similarity score.","title":"scaled_dot_product_attention"},{"location":"api/modules/attention/scaled_dot_product_attention/#scaleddotproductattention","text":"@Attention . register ( \"scaled_dot_product\" ) class ScaledDotProductAttention ( DotProductAttention ): | def __init__ ( | self , | scaling_factor : Optional [ int ] = None , | normalize : bool = True | ) -> None Computes attention between two tensors using scaled dot product.","title":"ScaledDotProductAttention"},{"location":"api/modules/backbones/backbone/","text":"allennlp .modules .backbones .backbone [SOURCE] Backbone \u00b6 class Backbone ( torch . nn . Module , Registrable ) A Backbone operates on basic model inputs and produces some encoding of those inputs that will be shared among one or more Heads in a multi-task setting. For plain text inputs, this is often a transformer. The main purpose of this class is to give us a Registrable class that we can use as a type annotation on Model classes that want to use a backbone. The expectation is that this will take the same inputs as a typical model, but return intermediate representations. These should generally be returned as a dictionary, from which the caller will have to pull out what they want and use as desired. As a convention that these modules should generally follow, their outputs should have the same name as the given input, prepended with encoded_ . So, a backbone that encodes a text input should return an output called encoded_text . This convention allows easier exchangeability of these backbone modules. Additionally, as downstream Heads will typically need mask information, but after encoding have no way of computing it, a Backbone should also return a mask for each of its outputs, with the same name as the output but with _mask appended. So in our example of text as input, the output should have an entry called encoded_text_mask . Because a Backbone handles model inputs, if you want to make those inputs human readable (e.g., for displaying them in a demo), then it's typically only the Backbone object that knows how to do that. So we also implement the make_output_human_readable function from the Model class. The implementation in the base class does nothing, but concrete classes should generally convert whatever input indices are saved to the output into text. forward \u00b6 class Backbone ( torch . nn . Module , Registrable ): | ... | def forward ( self , ** kwargs ) -> Dict [ str , torch . Tensor ] make_output_human_readable \u00b6 class Backbone ( torch . nn . Module , Registrable ): | ... | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ]","title":"backbone"},{"location":"api/modules/backbones/backbone/#backbone","text":"class Backbone ( torch . nn . Module , Registrable ) A Backbone operates on basic model inputs and produces some encoding of those inputs that will be shared among one or more Heads in a multi-task setting. For plain text inputs, this is often a transformer. The main purpose of this class is to give us a Registrable class that we can use as a type annotation on Model classes that want to use a backbone. The expectation is that this will take the same inputs as a typical model, but return intermediate representations. These should generally be returned as a dictionary, from which the caller will have to pull out what they want and use as desired. As a convention that these modules should generally follow, their outputs should have the same name as the given input, prepended with encoded_ . So, a backbone that encodes a text input should return an output called encoded_text . This convention allows easier exchangeability of these backbone modules. Additionally, as downstream Heads will typically need mask information, but after encoding have no way of computing it, a Backbone should also return a mask for each of its outputs, with the same name as the output but with _mask appended. So in our example of text as input, the output should have an entry called encoded_text_mask . Because a Backbone handles model inputs, if you want to make those inputs human readable (e.g., for displaying them in a demo), then it's typically only the Backbone object that knows how to do that. So we also implement the make_output_human_readable function from the Model class. The implementation in the base class does nothing, but concrete classes should generally convert whatever input indices are saved to the output into text.","title":"Backbone"},{"location":"api/modules/backbones/backbone/#forward","text":"class Backbone ( torch . nn . Module , Registrable ): | ... | def forward ( self , ** kwargs ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"api/modules/backbones/backbone/#make_output_human_readable","text":"class Backbone ( torch . nn . Module , Registrable ): | ... | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ]","title":"make_output_human_readable"},{"location":"api/modules/backbones/pretrained_transformer_backbone/","text":"allennlp .modules .backbones .pretrained_transformer_backbone [SOURCE] PretrainedTransformerBackbone \u00b6 @Backbone . register ( \"pretrained_transformer\" ) class PretrainedTransformerBackbone ( Backbone ): | def __init__ ( | self , | vocab : Vocabulary , | model_name : str , | * , max_length : int = None , | * , sub_module : str = None , | * , train_parameters : bool = True , | * , last_layer_only : bool = True , | * , override_weights_file : Optional [ str ] = None , | * , override_weights_strip_prefix : Optional [ str ] = None , | * , tokenizer_kwargs : Optional [ Dict [ str , Any ]] = None , | * , transformer_kwargs : Optional [ Dict [ str , Any ]] = None , | * , output_token_strings : bool = True , | * , vocab_namespace : str = \"tags\" | ) -> None Uses a pretrained model from transformers as a Backbone . This class passes most of its arguments to a PretrainedTransformerEmbedder , which it uses to implement the underlying encoding logic (we duplicate the arguments here instead of taking an Embedder as a constructor argument just to simplify the user-facing API). Registered as a Backbone with name \"pretrained_transformer\". Parameters \u00b6 vocab : Vocabulary Necessary for converting input ids to strings in make_output_human_readable . If you set output_token_strings to False , or if you never call make_output_human_readable , then this will not be used and can be safely set to None . model_name : str The name of the transformers model to use. Should be the same as the corresponding PretrainedTransformerIndexer . max_length : int , optional (default = None ) If positive, folds input token IDs into multiple segments of this length, pass them through the transformer model independently, and concatenate the final representations. Should be set to the same value as the max_length option on the PretrainedTransformerIndexer . sub_module : str , optional (default = None ) The name of a submodule of the transformer to be used as the embedder. Some transformers naturally act as embedders such as BERT. However, other models consist of encoder and decoder, in which case we just want to use the encoder. train_parameters : bool , optional (default = True ) If this is True , the transformer weights get updated during training. last_layer_only : bool , optional (default = True ) When True (the default), only the final layer of the pretrained transformer is taken for the embeddings. But if set to False , a scalar mix of all of the layers is used. tokenizer_kwargs : Dict[str, Any] , optional (default = None ) Dictionary with additional arguments for AutoTokenizer.from_pretrained . transformer_kwargs : Dict[str, Any] , optional (default = None ) Dictionary with additional arguments for AutoModel.from_pretrained . output_token_strings : bool , optional (default = True ) If True , we will add the input token ids to the output dictionary in forward (with key \"token_ids\"), and convert them to strings in make_output_human_readable (with key \"tokens\"). This is necessary for certain demo functionality, and it adds only a trivial amount of computation if you are not using a demo. vocab_namespace : str , optional (default = \"tags\" ) The namespace to use in conjunction with the Vocabulary above. We use a somewhat confusing default of \"tags\" here, to match what is done in PretrainedTransformerIndexer . forward \u00b6 class PretrainedTransformerBackbone ( Backbone ): | ... | def forward ( self , text : TextFieldTensors ) -> Dict [ str , torch . Tensor ] make_output_human_readable \u00b6 class PretrainedTransformerBackbone ( Backbone ): | ... | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ]","title":"pretrained_transformer_backbone"},{"location":"api/modules/backbones/pretrained_transformer_backbone/#pretrainedtransformerbackbone","text":"@Backbone . register ( \"pretrained_transformer\" ) class PretrainedTransformerBackbone ( Backbone ): | def __init__ ( | self , | vocab : Vocabulary , | model_name : str , | * , max_length : int = None , | * , sub_module : str = None , | * , train_parameters : bool = True , | * , last_layer_only : bool = True , | * , override_weights_file : Optional [ str ] = None , | * , override_weights_strip_prefix : Optional [ str ] = None , | * , tokenizer_kwargs : Optional [ Dict [ str , Any ]] = None , | * , transformer_kwargs : Optional [ Dict [ str , Any ]] = None , | * , output_token_strings : bool = True , | * , vocab_namespace : str = \"tags\" | ) -> None Uses a pretrained model from transformers as a Backbone . This class passes most of its arguments to a PretrainedTransformerEmbedder , which it uses to implement the underlying encoding logic (we duplicate the arguments here instead of taking an Embedder as a constructor argument just to simplify the user-facing API). Registered as a Backbone with name \"pretrained_transformer\".","title":"PretrainedTransformerBackbone"},{"location":"api/modules/backbones/pretrained_transformer_backbone/#forward","text":"class PretrainedTransformerBackbone ( Backbone ): | ... | def forward ( self , text : TextFieldTensors ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"api/modules/backbones/pretrained_transformer_backbone/#make_output_human_readable","text":"class PretrainedTransformerBackbone ( Backbone ): | ... | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ]","title":"make_output_human_readable"},{"location":"api/modules/backbones/vilbert_backbone/","text":"allennlp .modules .backbones .vilbert_backbone [SOURCE] VilbertBackbone \u00b6 @Backbone . register ( \"vilbert\" ) @Backbone . register ( \"vilbert_from_huggingface\" , constructor = \"from_huggingface_model_name\" ) class VilbertBackbone ( Backbone ): | def __init__ ( | self , | vocab : Vocabulary , | text_embeddings : TransformerEmbeddings , | image_embeddings : ImageFeatureEmbeddings , | encoder : BiModalEncoder , | pooled_output_dim : int , | fusion_method : str = \"sum\" , | dropout : float = 0.1 , | vocab_namespace : str = \"tokens\" | ) -> None Uses a Vilbert model as a Backbone . Registered as a Backbone with name \"vilbert\". from_huggingface_model_name \u00b6 class VilbertBackbone ( Backbone ): | ... | @classmethod | def from_huggingface_model_name ( | cls , | vocab : Vocabulary , | model_name : str , | image_feature_dim : int , | image_num_hidden_layers : int , | image_hidden_size : int , | image_num_attention_heads : int , | combined_hidden_size : int , | combined_num_attention_heads : int , | pooled_output_dim : int , | image_intermediate_size : int , | image_attention_dropout : float , | image_hidden_dropout : float , | image_biattention_id : List [ int ], | text_biattention_id : List [ int ], | text_fixed_layer : int , | image_fixed_layer : int , | fusion_method : str = \"sum\" | ) forward \u00b6 class VilbertBackbone ( Backbone ): | ... | def forward ( | self , | box_features : torch . Tensor , | box_coordinates : torch . Tensor , | box_mask : torch . Tensor , | text : TextFieldTensors | ) -> Dict [ str , torch . Tensor ]","title":"vilbert_backbone"},{"location":"api/modules/backbones/vilbert_backbone/#vilbertbackbone","text":"@Backbone . register ( \"vilbert\" ) @Backbone . register ( \"vilbert_from_huggingface\" , constructor = \"from_huggingface_model_name\" ) class VilbertBackbone ( Backbone ): | def __init__ ( | self , | vocab : Vocabulary , | text_embeddings : TransformerEmbeddings , | image_embeddings : ImageFeatureEmbeddings , | encoder : BiModalEncoder , | pooled_output_dim : int , | fusion_method : str = \"sum\" , | dropout : float = 0.1 , | vocab_namespace : str = \"tokens\" | ) -> None Uses a Vilbert model as a Backbone . Registered as a Backbone with name \"vilbert\".","title":"VilbertBackbone"},{"location":"api/modules/backbones/vilbert_backbone/#from_huggingface_model_name","text":"class VilbertBackbone ( Backbone ): | ... | @classmethod | def from_huggingface_model_name ( | cls , | vocab : Vocabulary , | model_name : str , | image_feature_dim : int , | image_num_hidden_layers : int , | image_hidden_size : int , | image_num_attention_heads : int , | combined_hidden_size : int , | combined_num_attention_heads : int , | pooled_output_dim : int , | image_intermediate_size : int , | image_attention_dropout : float , | image_hidden_dropout : float , | image_biattention_id : List [ int ], | text_biattention_id : List [ int ], | text_fixed_layer : int , | image_fixed_layer : int , | fusion_method : str = \"sum\" | )","title":"from_huggingface_model_name"},{"location":"api/modules/backbones/vilbert_backbone/#forward","text":"class VilbertBackbone ( Backbone ): | ... | def forward ( | self , | box_features : torch . Tensor , | box_coordinates : torch . Tensor , | box_mask : torch . Tensor , | text : TextFieldTensors | ) -> Dict [ str , torch . Tensor ]","title":"forward"},{"location":"api/modules/matrix_attention/bilinear_matrix_attention/","text":"allennlp .modules .matrix_attention .bilinear_matrix_attention [SOURCE] BilinearMatrixAttention \u00b6 @MatrixAttention . register ( \"bilinear\" ) class BilinearMatrixAttention ( MatrixAttention ): | def __init__ ( | self , | matrix_1_dim : int , | matrix_2_dim : int , | activation : Activation = None , | use_input_biases : bool = False , | label_dim : int = 1 | ) -> None Computes attention between two matrices using a bilinear attention function. This function has a matrix of weights W and a bias b , and the similarity between the two matrices X and Y is computed as X W Y^T + b . Registered as a MatrixAttention with name \"bilinear\". Parameters \u00b6 matrix_1_dim : int The dimension of the matrix X , described above. This is X.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build the weight matrix correctly. matrix_2_dim : int The dimension of the matrix Y , described above. This is Y.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build the weight matrix correctly. activation : Activation , optional (default = linear ) An activation function applied after the X W Y^T + b calculation. Default is linear, i.e. no activation. use_input_biases : bool , optional (default = False ) If True, we add biases to the inputs such that the final computation is equivalent to the original bilinear matrix multiplication plus a projection of both inputs. label_dim : int , optional (default = 1 ) The number of output classes. Typically in an attention setting this will be one, but this parameter allows this class to function as an equivalent to torch.nn.Bilinear for matrices, rather than vectors. reset_parameters \u00b6 class BilinearMatrixAttention ( MatrixAttention ): | ... | def reset_parameters ( self ) forward \u00b6 class BilinearMatrixAttention ( MatrixAttention ): | ... | def forward ( | self , | matrix_1 : torch . Tensor , | matrix_2 : torch . Tensor | ) -> torch . Tensor","title":"bilinear_matrix_attention"},{"location":"api/modules/matrix_attention/bilinear_matrix_attention/#bilinearmatrixattention","text":"@MatrixAttention . register ( \"bilinear\" ) class BilinearMatrixAttention ( MatrixAttention ): | def __init__ ( | self , | matrix_1_dim : int , | matrix_2_dim : int , | activation : Activation = None , | use_input_biases : bool = False , | label_dim : int = 1 | ) -> None Computes attention between two matrices using a bilinear attention function. This function has a matrix of weights W and a bias b , and the similarity between the two matrices X and Y is computed as X W Y^T + b . Registered as a MatrixAttention with name \"bilinear\".","title":"BilinearMatrixAttention"},{"location":"api/modules/matrix_attention/bilinear_matrix_attention/#reset_parameters","text":"class BilinearMatrixAttention ( MatrixAttention ): | ... | def reset_parameters ( self )","title":"reset_parameters"},{"location":"api/modules/matrix_attention/bilinear_matrix_attention/#forward","text":"class BilinearMatrixAttention ( MatrixAttention ): | ... | def forward ( | self , | matrix_1 : torch . Tensor , | matrix_2 : torch . Tensor | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/matrix_attention/cosine_matrix_attention/","text":"allennlp .modules .matrix_attention .cosine_matrix_attention [SOURCE] CosineMatrixAttention \u00b6 @MatrixAttention . register ( \"cosine\" ) class CosineMatrixAttention ( MatrixAttention ) Computes attention between every entry in matrix_1 with every entry in matrix_2 using cosine similarity. Registered as a MatrixAttention with name \"cosine\". forward \u00b6 class CosineMatrixAttention ( MatrixAttention ): | ... | def forward ( | self , | matrix_1 : torch . Tensor , | matrix_2 : torch . Tensor | ) -> torch . Tensor","title":"cosine_matrix_attention"},{"location":"api/modules/matrix_attention/cosine_matrix_attention/#cosinematrixattention","text":"@MatrixAttention . register ( \"cosine\" ) class CosineMatrixAttention ( MatrixAttention ) Computes attention between every entry in matrix_1 with every entry in matrix_2 using cosine similarity. Registered as a MatrixAttention with name \"cosine\".","title":"CosineMatrixAttention"},{"location":"api/modules/matrix_attention/cosine_matrix_attention/#forward","text":"class CosineMatrixAttention ( MatrixAttention ): | ... | def forward ( | self , | matrix_1 : torch . Tensor , | matrix_2 : torch . Tensor | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/matrix_attention/dot_product_matrix_attention/","text":"allennlp .modules .matrix_attention .dot_product_matrix_attention [SOURCE] DotProductMatrixAttention \u00b6 @MatrixAttention . register ( \"dot_product\" ) class DotProductMatrixAttention ( MatrixAttention ) Computes attention between every entry in matrix_1 with every entry in matrix_2 using a dot product. Registered as a MatrixAttention with name \"dot_product\". forward \u00b6 class DotProductMatrixAttention ( MatrixAttention ): | ... | def forward ( | self , | matrix_1 : torch . Tensor , | matrix_2 : torch . Tensor | ) -> torch . Tensor","title":"dot_product_matrix_attention"},{"location":"api/modules/matrix_attention/dot_product_matrix_attention/#dotproductmatrixattention","text":"@MatrixAttention . register ( \"dot_product\" ) class DotProductMatrixAttention ( MatrixAttention ) Computes attention between every entry in matrix_1 with every entry in matrix_2 using a dot product. Registered as a MatrixAttention with name \"dot_product\".","title":"DotProductMatrixAttention"},{"location":"api/modules/matrix_attention/dot_product_matrix_attention/#forward","text":"class DotProductMatrixAttention ( MatrixAttention ): | ... | def forward ( | self , | matrix_1 : torch . Tensor , | matrix_2 : torch . Tensor | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/matrix_attention/linear_matrix_attention/","text":"allennlp .modules .matrix_attention .linear_matrix_attention [SOURCE] LinearMatrixAttention \u00b6 @MatrixAttention . register ( \"linear\" ) class LinearMatrixAttention ( MatrixAttention ): | def __init__ ( | self , | tensor_1_dim : int , | tensor_2_dim : int , | combination : str = \"x,y\" , | activation : Activation = None | ) -> None This MatrixAttention takes two matrices as input and returns a matrix of attentions by performing a dot product between a vector of weights and some combination of the two input matrices, followed by an (optional) activation function. The combination used is configurable. If the two vectors are x and y , we allow the following kinds of combinations : x , y , x*y , x+y , x-y , x/y , where each of those binary operations is performed elementwise. You can list as many combinations as you want, comma separated. For example, you might give x,y,x*y as the combination parameter to this class. The computed similarity function would then be w^T [x; y; x*y] + b , where w is a vector of weights, b is a bias parameter, and [;] is vector concatenation. Note that if you want a bilinear similarity function with a diagonal weight matrix W, where the similarity function is computed as x * w * y + b (with w the diagonal of W ), you can accomplish that with this class by using \"x*y\" for combination . Registered as a MatrixAttention with name \"linear\". Parameters \u00b6 tensor_1_dim : int The dimension of the first tensor, x , described above. This is x.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build weight vectors correctly. tensor_2_dim : int The dimension of the second tensor, y , described above. This is y.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build weight vectors correctly. combination : str , optional (default = \"x,y\" ) Described above. activation : Activation , optional (default = linear ) An activation function applied after the w^T * [x;y] + b calculation. Default is linear, i.e. no activation. reset_parameters \u00b6 class LinearMatrixAttention ( MatrixAttention ): | ... | def reset_parameters ( self ) forward \u00b6 class LinearMatrixAttention ( MatrixAttention ): | ... | def forward ( | self , | matrix_1 : torch . Tensor , | matrix_2 : torch . Tensor | ) -> torch . Tensor","title":"linear_matrix_attention"},{"location":"api/modules/matrix_attention/linear_matrix_attention/#linearmatrixattention","text":"@MatrixAttention . register ( \"linear\" ) class LinearMatrixAttention ( MatrixAttention ): | def __init__ ( | self , | tensor_1_dim : int , | tensor_2_dim : int , | combination : str = \"x,y\" , | activation : Activation = None | ) -> None This MatrixAttention takes two matrices as input and returns a matrix of attentions by performing a dot product between a vector of weights and some combination of the two input matrices, followed by an (optional) activation function. The combination used is configurable. If the two vectors are x and y , we allow the following kinds of combinations : x , y , x*y , x+y , x-y , x/y , where each of those binary operations is performed elementwise. You can list as many combinations as you want, comma separated. For example, you might give x,y,x*y as the combination parameter to this class. The computed similarity function would then be w^T [x; y; x*y] + b , where w is a vector of weights, b is a bias parameter, and [;] is vector concatenation. Note that if you want a bilinear similarity function with a diagonal weight matrix W, where the similarity function is computed as x * w * y + b (with w the diagonal of W ), you can accomplish that with this class by using \"x*y\" for combination . Registered as a MatrixAttention with name \"linear\".","title":"LinearMatrixAttention"},{"location":"api/modules/matrix_attention/linear_matrix_attention/#reset_parameters","text":"class LinearMatrixAttention ( MatrixAttention ): | ... | def reset_parameters ( self )","title":"reset_parameters"},{"location":"api/modules/matrix_attention/linear_matrix_attention/#forward","text":"class LinearMatrixAttention ( MatrixAttention ): | ... | def forward ( | self , | matrix_1 : torch . Tensor , | matrix_2 : torch . Tensor | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/matrix_attention/matrix_attention/","text":"allennlp .modules .matrix_attention .matrix_attention [SOURCE] MatrixAttention \u00b6 class MatrixAttention ( torch . nn . Module , Registrable ) MatrixAttention takes two matrices as input and returns a matrix of attentions. We compute the similarity between each row in each matrix and return unnormalized similarity scores. Because these scores are unnormalized, we don't take a mask as input; it's up to the caller to deal with masking properly when this output is used. Input: - matrix_1 : (batch_size, num_rows_1, embedding_dim_1) - matrix_2 : (batch_size, num_rows_2, embedding_dim_2) Output: - (batch_size, num_rows_1, num_rows_2) forward \u00b6 class MatrixAttention ( torch . nn . Module , Registrable ): | ... | def forward ( | self , | matrix_1 : torch . Tensor , | matrix_2 : torch . Tensor | ) -> torch . Tensor","title":"matrix_attention"},{"location":"api/modules/matrix_attention/matrix_attention/#matrixattention","text":"class MatrixAttention ( torch . nn . Module , Registrable ) MatrixAttention takes two matrices as input and returns a matrix of attentions. We compute the similarity between each row in each matrix and return unnormalized similarity scores. Because these scores are unnormalized, we don't take a mask as input; it's up to the caller to deal with masking properly when this output is used. Input: - matrix_1 : (batch_size, num_rows_1, embedding_dim_1) - matrix_2 : (batch_size, num_rows_2, embedding_dim_2) Output: - (batch_size, num_rows_1, num_rows_2)","title":"MatrixAttention"},{"location":"api/modules/matrix_attention/matrix_attention/#forward","text":"class MatrixAttention ( torch . nn . Module , Registrable ): | ... | def forward ( | self , | matrix_1 : torch . Tensor , | matrix_2 : torch . Tensor | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/matrix_attention/scaled_dot_product_matrix_attention/","text":"allennlp .modules .matrix_attention .scaled_dot_product_matrix_attention [SOURCE] ScaledDotProductMatrixAttention \u00b6 @MatrixAttention . register ( \"scaled_dot_product\" ) class ScaledDotProductMatrixAttention ( DotProductMatrixAttention ) Computes attention between every entry in matrix_1 with every entry in matrix_2 using a dot product. Scales the result by the size of the embeddings. Registered as a MatrixAttention with name \"scaled_dot_product\". forward \u00b6 class ScaledDotProductMatrixAttention ( DotProductMatrixAttention ): | ... | def forward ( | self , | matrix_1 : torch . Tensor , | matrix_2 : torch . Tensor | ) -> torch . Tensor","title":"scaled_dot_product_matrix_attention"},{"location":"api/modules/matrix_attention/scaled_dot_product_matrix_attention/#scaleddotproductmatrixattention","text":"@MatrixAttention . register ( \"scaled_dot_product\" ) class ScaledDotProductMatrixAttention ( DotProductMatrixAttention ) Computes attention between every entry in matrix_1 with every entry in matrix_2 using a dot product. Scales the result by the size of the embeddings. Registered as a MatrixAttention with name \"scaled_dot_product\".","title":"ScaledDotProductMatrixAttention"},{"location":"api/modules/matrix_attention/scaled_dot_product_matrix_attention/#forward","text":"class ScaledDotProductMatrixAttention ( DotProductMatrixAttention ): | ... | def forward ( | self , | matrix_1 : torch . Tensor , | matrix_2 : torch . Tensor | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/seq2seq_encoders/compose_encoder/","text":"allennlp .modules .seq2seq_encoders .compose_encoder [SOURCE] ComposeEncoder \u00b6 @Seq2SeqEncoder . register ( \"compose\" ) class ComposeEncoder ( Seq2SeqEncoder ): | def __init__ ( self , encoders : List [ Seq2SeqEncoder ]) This class can be used to compose several encoders in sequence. Among other things, this can be used to add a \"pre-contextualizer\" before a Seq2SeqEncoder. Registered as a Seq2SeqEncoder with name \"compose\". Parameters \u00b6 encoders : List[Seq2SeqEncoder] A non-empty list of encoders to compose. The encoders must match in bidirectionality. forward \u00b6 class ComposeEncoder ( Seq2SeqEncoder ): | ... | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor = None | ) -> torch . Tensor Parameters \u00b6 inputs : torch.Tensor A tensor of shape (batch_size, timesteps, input_dim) mask : torch.BoolTensor , optional (default = None ) A tensor of shape (batch_size, timesteps). Returns \u00b6 A tensor computed by composing the sequence of encoders. get_input_dim \u00b6 class ComposeEncoder ( Seq2SeqEncoder ): | ... | def get_input_dim ( self ) -> int get_output_dim \u00b6 class ComposeEncoder ( Seq2SeqEncoder ): | ... | def get_output_dim ( self ) -> int is_bidirectional \u00b6 class ComposeEncoder ( Seq2SeqEncoder ): | ... | def is_bidirectional ( self ) -> bool","title":"compose_encoder"},{"location":"api/modules/seq2seq_encoders/compose_encoder/#composeencoder","text":"@Seq2SeqEncoder . register ( \"compose\" ) class ComposeEncoder ( Seq2SeqEncoder ): | def __init__ ( self , encoders : List [ Seq2SeqEncoder ]) This class can be used to compose several encoders in sequence. Among other things, this can be used to add a \"pre-contextualizer\" before a Seq2SeqEncoder. Registered as a Seq2SeqEncoder with name \"compose\".","title":"ComposeEncoder"},{"location":"api/modules/seq2seq_encoders/compose_encoder/#forward","text":"class ComposeEncoder ( Seq2SeqEncoder ): | ... | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor = None | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/seq2seq_encoders/compose_encoder/#get_input_dim","text":"class ComposeEncoder ( Seq2SeqEncoder ): | ... | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"api/modules/seq2seq_encoders/compose_encoder/#get_output_dim","text":"class ComposeEncoder ( Seq2SeqEncoder ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/seq2seq_encoders/compose_encoder/#is_bidirectional","text":"class ComposeEncoder ( Seq2SeqEncoder ): | ... | def is_bidirectional ( self ) -> bool","title":"is_bidirectional"},{"location":"api/modules/seq2seq_encoders/feedforward_encoder/","text":"allennlp .modules .seq2seq_encoders .feedforward_encoder [SOURCE] FeedForwardEncoder \u00b6 @Seq2SeqEncoder . register ( \"feedforward\" ) class FeedForwardEncoder ( Seq2SeqEncoder ): | def __init__ ( self , feedforward : FeedForward ) -> None This class applies the FeedForward to each item in sequences. Registered as a Seq2SeqEncoder with name \"feedforward\". get_input_dim \u00b6 class FeedForwardEncoder ( Seq2SeqEncoder ): | ... | def get_input_dim ( self ) -> int get_output_dim \u00b6 class FeedForwardEncoder ( Seq2SeqEncoder ): | ... | def get_output_dim ( self ) -> int is_bidirectional \u00b6 class FeedForwardEncoder ( Seq2SeqEncoder ): | ... | def is_bidirectional ( self ) -> bool forward \u00b6 class FeedForwardEncoder ( Seq2SeqEncoder ): | ... | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor = None | ) -> torch . Tensor Parameters \u00b6 inputs : torch.Tensor A tensor of shape (batch_size, timesteps, input_dim) mask : torch.BoolTensor , optional (default = None ) A tensor of shape (batch_size, timesteps). Returns \u00b6 A tensor of shape (batch_size, timesteps, output_dim).","title":"feedforward_encoder"},{"location":"api/modules/seq2seq_encoders/feedforward_encoder/#feedforwardencoder","text":"@Seq2SeqEncoder . register ( \"feedforward\" ) class FeedForwardEncoder ( Seq2SeqEncoder ): | def __init__ ( self , feedforward : FeedForward ) -> None This class applies the FeedForward to each item in sequences. Registered as a Seq2SeqEncoder with name \"feedforward\".","title":"FeedForwardEncoder"},{"location":"api/modules/seq2seq_encoders/feedforward_encoder/#get_input_dim","text":"class FeedForwardEncoder ( Seq2SeqEncoder ): | ... | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"api/modules/seq2seq_encoders/feedforward_encoder/#get_output_dim","text":"class FeedForwardEncoder ( Seq2SeqEncoder ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/seq2seq_encoders/feedforward_encoder/#is_bidirectional","text":"class FeedForwardEncoder ( Seq2SeqEncoder ): | ... | def is_bidirectional ( self ) -> bool","title":"is_bidirectional"},{"location":"api/modules/seq2seq_encoders/feedforward_encoder/#forward","text":"class FeedForwardEncoder ( Seq2SeqEncoder ): | ... | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor = None | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/seq2seq_encoders/gated_cnn_encoder/","text":"allennlp .modules .seq2seq_encoders .gated_cnn_encoder [SOURCE] ResidualBlock \u00b6 class ResidualBlock ( torch . nn . Module ): | def __init__ ( | self , | input_dim : int , | layers : Sequence [ Sequence [ int ]], | direction : str , | do_weight_norm : bool = True , | dropout : float = 0.0 | ) -> None forward \u00b6 class ResidualBlock ( torch . nn . Module ): | ... | def forward ( self , x : torch . Tensor ) -> torch . Tensor GatedCnnEncoder \u00b6 @Seq2SeqEncoder . register ( \"gated-cnn-encoder\" ) class GatedCnnEncoder ( Seq2SeqEncoder ): | def __init__ ( | self , | input_dim : int , | layers : Sequence [ Sequence [ Sequence [ int ]]], | dropout : float = 0.0 , | return_all_layers : bool = False | ) -> None This is work-in-progress and has not been fully tested yet. Use at your own risk! A Seq2SeqEncoder that uses a Gated CNN. see Language Modeling with Gated Convolutional Networks, Yann N. Dauphin et al, ICML 2017 https://arxiv.org/abs/1612.08083 Convolutional Sequence to Sequence Learning, Jonas Gehring et al, ICML 2017 https://arxiv.org/abs/1705.03122 Some possibilities: Each element of the list is wrapped in a residual block: input_dim = 512 layers = [ [[4, 512]], [[4, 512], [4, 512]], [[4, 512], [4, 512]], [[4, 512], [4, 512]] dropout = 0.05 A \"bottleneck architecture\" input_dim = 512 layers = [ [[4, 512]], [[1, 128], [5, 128], [1, 512]], ... ] An architecture with dilated convolutions input_dim = 512 layers = [ [[2, 512, 1]], [[2, 512, 2]], [[2, 512, 4]], [[2, 512, 8]], # receptive field == 16 [[2, 512, 1]], [[2, 512, 2]], [[2, 512, 4]], [[2, 512, 8]], # receptive field == 31 [[2, 512, 1]], [[2, 512, 2]], [[2, 512, 4]], [[2, 512, 8]], # receptive field == 46 [[2, 512, 1]], [[2, 512, 2]], [[2, 512, 4]], [[2, 512, 8]], # receptive field == 57 ] Registered as a Seq2SeqEncoder with name \"gated-cnn-encoder\". Parameters \u00b6 input_dim : int The dimension of the inputs. layers : Sequence[Sequence[Sequence[int]]] The layer dimensions for each ResidualBlock . dropout : float , optional (default = 0.0 ) The dropout for each ResidualBlock . return_all_layers : bool , optional (default = False ) Whether to return all layers or just the last layer. forward \u00b6 class GatedCnnEncoder ( Seq2SeqEncoder ): | ... | def forward ( | self , | token_embeddings : torch . Tensor , | mask : torch . BoolTensor | ) get_input_dim \u00b6 class GatedCnnEncoder ( Seq2SeqEncoder ): | ... | def get_input_dim ( self ) -> int get_output_dim \u00b6 class GatedCnnEncoder ( Seq2SeqEncoder ): | ... | def get_output_dim ( self ) -> int is_bidirectional \u00b6 class GatedCnnEncoder ( Seq2SeqEncoder ): | ... | def is_bidirectional ( self ) -> bool","title":"gated_cnn_encoder"},{"location":"api/modules/seq2seq_encoders/gated_cnn_encoder/#residualblock","text":"class ResidualBlock ( torch . nn . Module ): | def __init__ ( | self , | input_dim : int , | layers : Sequence [ Sequence [ int ]], | direction : str , | do_weight_norm : bool = True , | dropout : float = 0.0 | ) -> None","title":"ResidualBlock"},{"location":"api/modules/seq2seq_encoders/gated_cnn_encoder/#forward","text":"class ResidualBlock ( torch . nn . Module ): | ... | def forward ( self , x : torch . Tensor ) -> torch . Tensor","title":"forward"},{"location":"api/modules/seq2seq_encoders/gated_cnn_encoder/#gatedcnnencoder","text":"@Seq2SeqEncoder . register ( \"gated-cnn-encoder\" ) class GatedCnnEncoder ( Seq2SeqEncoder ): | def __init__ ( | self , | input_dim : int , | layers : Sequence [ Sequence [ Sequence [ int ]]], | dropout : float = 0.0 , | return_all_layers : bool = False | ) -> None This is work-in-progress and has not been fully tested yet. Use at your own risk! A Seq2SeqEncoder that uses a Gated CNN. see Language Modeling with Gated Convolutional Networks, Yann N. Dauphin et al, ICML 2017 https://arxiv.org/abs/1612.08083 Convolutional Sequence to Sequence Learning, Jonas Gehring et al, ICML 2017 https://arxiv.org/abs/1705.03122 Some possibilities: Each element of the list is wrapped in a residual block: input_dim = 512 layers = [ [[4, 512]], [[4, 512], [4, 512]], [[4, 512], [4, 512]], [[4, 512], [4, 512]] dropout = 0.05 A \"bottleneck architecture\" input_dim = 512 layers = [ [[4, 512]], [[1, 128], [5, 128], [1, 512]], ... ] An architecture with dilated convolutions input_dim = 512 layers = [ [[2, 512, 1]], [[2, 512, 2]], [[2, 512, 4]], [[2, 512, 8]], # receptive field == 16 [[2, 512, 1]], [[2, 512, 2]], [[2, 512, 4]], [[2, 512, 8]], # receptive field == 31 [[2, 512, 1]], [[2, 512, 2]], [[2, 512, 4]], [[2, 512, 8]], # receptive field == 46 [[2, 512, 1]], [[2, 512, 2]], [[2, 512, 4]], [[2, 512, 8]], # receptive field == 57 ] Registered as a Seq2SeqEncoder with name \"gated-cnn-encoder\".","title":"GatedCnnEncoder"},{"location":"api/modules/seq2seq_encoders/gated_cnn_encoder/#forward_1","text":"class GatedCnnEncoder ( Seq2SeqEncoder ): | ... | def forward ( | self , | token_embeddings : torch . Tensor , | mask : torch . BoolTensor | )","title":"forward"},{"location":"api/modules/seq2seq_encoders/gated_cnn_encoder/#get_input_dim","text":"class GatedCnnEncoder ( Seq2SeqEncoder ): | ... | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"api/modules/seq2seq_encoders/gated_cnn_encoder/#get_output_dim","text":"class GatedCnnEncoder ( Seq2SeqEncoder ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/seq2seq_encoders/gated_cnn_encoder/#is_bidirectional","text":"class GatedCnnEncoder ( Seq2SeqEncoder ): | ... | def is_bidirectional ( self ) -> bool","title":"is_bidirectional"},{"location":"api/modules/seq2seq_encoders/pass_through_encoder/","text":"allennlp .modules .seq2seq_encoders .pass_through_encoder [SOURCE] PassThroughEncoder \u00b6 @Seq2SeqEncoder . register ( \"pass_through\" ) class PassThroughEncoder ( Seq2SeqEncoder ): | def __init__ ( self , input_dim : int ) -> None This class allows you to specify skipping a Seq2SeqEncoder just by changing a configuration file. This is useful for ablations and measuring the impact of different elements of your model. Registered as a Seq2SeqEncoder with name \"pass_through\". get_input_dim \u00b6 class PassThroughEncoder ( Seq2SeqEncoder ): | ... | def get_input_dim ( self ) -> int get_output_dim \u00b6 class PassThroughEncoder ( Seq2SeqEncoder ): | ... | def get_output_dim ( self ) -> int is_bidirectional \u00b6 class PassThroughEncoder ( Seq2SeqEncoder ): | ... | def is_bidirectional ( self ) forward \u00b6 class PassThroughEncoder ( Seq2SeqEncoder ): | ... | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor = None | ) -> torch . Tensor Parameters \u00b6 inputs : torch.Tensor A tensor of shape (batch_size, timesteps, input_dim) mask : torch.BoolTensor , optional (default = None ) A tensor of shape (batch_size, timesteps). Returns \u00b6 A tensor of shape (batch_size, timesteps, output_dim), where output_dim = input_dim.","title":"pass_through_encoder"},{"location":"api/modules/seq2seq_encoders/pass_through_encoder/#passthroughencoder","text":"@Seq2SeqEncoder . register ( \"pass_through\" ) class PassThroughEncoder ( Seq2SeqEncoder ): | def __init__ ( self , input_dim : int ) -> None This class allows you to specify skipping a Seq2SeqEncoder just by changing a configuration file. This is useful for ablations and measuring the impact of different elements of your model. Registered as a Seq2SeqEncoder with name \"pass_through\".","title":"PassThroughEncoder"},{"location":"api/modules/seq2seq_encoders/pass_through_encoder/#get_input_dim","text":"class PassThroughEncoder ( Seq2SeqEncoder ): | ... | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"api/modules/seq2seq_encoders/pass_through_encoder/#get_output_dim","text":"class PassThroughEncoder ( Seq2SeqEncoder ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/seq2seq_encoders/pass_through_encoder/#is_bidirectional","text":"class PassThroughEncoder ( Seq2SeqEncoder ): | ... | def is_bidirectional ( self )","title":"is_bidirectional"},{"location":"api/modules/seq2seq_encoders/pass_through_encoder/#forward","text":"class PassThroughEncoder ( Seq2SeqEncoder ): | ... | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor = None | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/seq2seq_encoders/pytorch_seq2seq_wrapper/","text":"allennlp .modules .seq2seq_encoders .pytorch_seq2seq_wrapper [SOURCE] PytorchSeq2SeqWrapper \u00b6 class PytorchSeq2SeqWrapper ( Seq2SeqEncoder ): | def __init__ ( | self , | module : torch . nn . Module , | stateful : bool = False | ) -> None Pytorch's RNNs have two outputs: the hidden state for every time step, and the hidden state at the last time step for every layer. We just want the first one as a single output. This wrapper pulls out that output, and adds a get_output_dim method, which is useful if you want to, e.g., define a linear + softmax layer on top of this to get some distribution over a set of labels. The linear layer needs to know its input dimension before it is called, and you can get that from get_output_dim . In order to be wrapped with this wrapper, a class must have the following members: - ` self . input_size : int ` - ` self . hidden_size : int ` - ` def forward ( inputs : PackedSequence , hidden_state : torch . Tensor ) -> Tuple [ PackedSequence , torch . Tensor ] ` . - ` self . bidirectional : bool ` ( optional ) This is what pytorch's RNN's look like - just make sure your class looks like those, and it should work. Note that we require you to pass a binary mask of shape (batch_size, sequence_length) when you call this module, to avoid subtle bugs around masking. If you already have a PackedSequence you can pass None as the second parameter. We support stateful RNNs where the final state from each batch is used as the initial state for the subsequent batch by passing stateful=True to the constructor. get_input_dim \u00b6 class PytorchSeq2SeqWrapper ( Seq2SeqEncoder ): | ... | def get_input_dim ( self ) -> int get_output_dim \u00b6 class PytorchSeq2SeqWrapper ( Seq2SeqEncoder ): | ... | def get_output_dim ( self ) -> int is_bidirectional \u00b6 class PytorchSeq2SeqWrapper ( Seq2SeqEncoder ): | ... | def is_bidirectional ( self ) -> bool forward \u00b6 class PytorchSeq2SeqWrapper ( Seq2SeqEncoder ): | ... | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor , | hidden_state : torch . Tensor = None | ) -> torch . Tensor GruSeq2SeqEncoder \u00b6 @Seq2SeqEncoder . register ( \"gru\" ) class GruSeq2SeqEncoder ( PytorchSeq2SeqWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int = 1 , | bias : bool = True , | dropout : float = 0.0 , | bidirectional : bool = False , | stateful : bool = False | ) Registered as a Seq2SeqEncoder with name \"gru\". LstmSeq2SeqEncoder \u00b6 @Seq2SeqEncoder . register ( \"lstm\" ) class LstmSeq2SeqEncoder ( PytorchSeq2SeqWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int = 1 , | bias : bool = True , | dropout : float = 0.0 , | bidirectional : bool = False , | stateful : bool = False | ) Registered as a Seq2SeqEncoder with name \"lstm\". RnnSeq2SeqEncoder \u00b6 @Seq2SeqEncoder . register ( \"rnn\" ) class RnnSeq2SeqEncoder ( PytorchSeq2SeqWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int = 1 , | nonlinearity : str = \"tanh\" , | bias : bool = True , | dropout : float = 0.0 , | bidirectional : bool = False , | stateful : bool = False | ) Registered as a Seq2SeqEncoder with name \"rnn\". AugmentedLstmSeq2SeqEncoder \u00b6 @Seq2SeqEncoder . register ( \"augmented_lstm\" ) class AugmentedLstmSeq2SeqEncoder ( PytorchSeq2SeqWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | go_forward : bool = True , | recurrent_dropout_probability : float = 0.0 , | use_highway : bool = True , | use_input_projection_bias : bool = True , | stateful : bool = False | ) -> None Registered as a Seq2SeqEncoder with name \"augmented_lstm\". StackedAlternatingLstmSeq2SeqEncoder \u00b6 @Seq2SeqEncoder . register ( \"alternating_lstm\" ) class StackedAlternatingLstmSeq2SeqEncoder ( PytorchSeq2SeqWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int , | recurrent_dropout_probability : float = 0.0 , | use_highway : bool = True , | use_input_projection_bias : bool = True , | stateful : bool = False | ) -> None Registered as a Seq2SeqEncoder with name \"alternating_lstm\". StackedBidirectionalLstmSeq2SeqEncoder \u00b6 @Seq2SeqEncoder . register ( \"stacked_bidirectional_lstm\" ) class StackedBidirectionalLstmSeq2SeqEncoder ( PytorchSeq2SeqWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int , | recurrent_dropout_probability : float = 0.0 , | layer_dropout_probability : float = 0.0 , | use_highway : bool = True , | stateful : bool = False | ) -> None Registered as a Seq2SeqEncoder with name \"stacked_bidirectional_lstm\".","title":"pytorch_seq2seq_wrapper"},{"location":"api/modules/seq2seq_encoders/pytorch_seq2seq_wrapper/#pytorchseq2seqwrapper","text":"class PytorchSeq2SeqWrapper ( Seq2SeqEncoder ): | def __init__ ( | self , | module : torch . nn . Module , | stateful : bool = False | ) -> None Pytorch's RNNs have two outputs: the hidden state for every time step, and the hidden state at the last time step for every layer. We just want the first one as a single output. This wrapper pulls out that output, and adds a get_output_dim method, which is useful if you want to, e.g., define a linear + softmax layer on top of this to get some distribution over a set of labels. The linear layer needs to know its input dimension before it is called, and you can get that from get_output_dim . In order to be wrapped with this wrapper, a class must have the following members: - ` self . input_size : int ` - ` self . hidden_size : int ` - ` def forward ( inputs : PackedSequence , hidden_state : torch . Tensor ) -> Tuple [ PackedSequence , torch . Tensor ] ` . - ` self . bidirectional : bool ` ( optional ) This is what pytorch's RNN's look like - just make sure your class looks like those, and it should work. Note that we require you to pass a binary mask of shape (batch_size, sequence_length) when you call this module, to avoid subtle bugs around masking. If you already have a PackedSequence you can pass None as the second parameter. We support stateful RNNs where the final state from each batch is used as the initial state for the subsequent batch by passing stateful=True to the constructor.","title":"PytorchSeq2SeqWrapper"},{"location":"api/modules/seq2seq_encoders/pytorch_seq2seq_wrapper/#get_input_dim","text":"class PytorchSeq2SeqWrapper ( Seq2SeqEncoder ): | ... | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"api/modules/seq2seq_encoders/pytorch_seq2seq_wrapper/#get_output_dim","text":"class PytorchSeq2SeqWrapper ( Seq2SeqEncoder ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/seq2seq_encoders/pytorch_seq2seq_wrapper/#is_bidirectional","text":"class PytorchSeq2SeqWrapper ( Seq2SeqEncoder ): | ... | def is_bidirectional ( self ) -> bool","title":"is_bidirectional"},{"location":"api/modules/seq2seq_encoders/pytorch_seq2seq_wrapper/#forward","text":"class PytorchSeq2SeqWrapper ( Seq2SeqEncoder ): | ... | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor , | hidden_state : torch . Tensor = None | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/seq2seq_encoders/pytorch_seq2seq_wrapper/#gruseq2seqencoder","text":"@Seq2SeqEncoder . register ( \"gru\" ) class GruSeq2SeqEncoder ( PytorchSeq2SeqWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int = 1 , | bias : bool = True , | dropout : float = 0.0 , | bidirectional : bool = False , | stateful : bool = False | ) Registered as a Seq2SeqEncoder with name \"gru\".","title":"GruSeq2SeqEncoder"},{"location":"api/modules/seq2seq_encoders/pytorch_seq2seq_wrapper/#lstmseq2seqencoder","text":"@Seq2SeqEncoder . register ( \"lstm\" ) class LstmSeq2SeqEncoder ( PytorchSeq2SeqWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int = 1 , | bias : bool = True , | dropout : float = 0.0 , | bidirectional : bool = False , | stateful : bool = False | ) Registered as a Seq2SeqEncoder with name \"lstm\".","title":"LstmSeq2SeqEncoder"},{"location":"api/modules/seq2seq_encoders/pytorch_seq2seq_wrapper/#rnnseq2seqencoder","text":"@Seq2SeqEncoder . register ( \"rnn\" ) class RnnSeq2SeqEncoder ( PytorchSeq2SeqWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int = 1 , | nonlinearity : str = \"tanh\" , | bias : bool = True , | dropout : float = 0.0 , | bidirectional : bool = False , | stateful : bool = False | ) Registered as a Seq2SeqEncoder with name \"rnn\".","title":"RnnSeq2SeqEncoder"},{"location":"api/modules/seq2seq_encoders/pytorch_seq2seq_wrapper/#augmentedlstmseq2seqencoder","text":"@Seq2SeqEncoder . register ( \"augmented_lstm\" ) class AugmentedLstmSeq2SeqEncoder ( PytorchSeq2SeqWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | go_forward : bool = True , | recurrent_dropout_probability : float = 0.0 , | use_highway : bool = True , | use_input_projection_bias : bool = True , | stateful : bool = False | ) -> None Registered as a Seq2SeqEncoder with name \"augmented_lstm\".","title":"AugmentedLstmSeq2SeqEncoder"},{"location":"api/modules/seq2seq_encoders/pytorch_seq2seq_wrapper/#stackedalternatinglstmseq2seqencoder","text":"@Seq2SeqEncoder . register ( \"alternating_lstm\" ) class StackedAlternatingLstmSeq2SeqEncoder ( PytorchSeq2SeqWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int , | recurrent_dropout_probability : float = 0.0 , | use_highway : bool = True , | use_input_projection_bias : bool = True , | stateful : bool = False | ) -> None Registered as a Seq2SeqEncoder with name \"alternating_lstm\".","title":"StackedAlternatingLstmSeq2SeqEncoder"},{"location":"api/modules/seq2seq_encoders/pytorch_seq2seq_wrapper/#stackedbidirectionallstmseq2seqencoder","text":"@Seq2SeqEncoder . register ( \"stacked_bidirectional_lstm\" ) class StackedBidirectionalLstmSeq2SeqEncoder ( PytorchSeq2SeqWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int , | recurrent_dropout_probability : float = 0.0 , | layer_dropout_probability : float = 0.0 , | use_highway : bool = True , | stateful : bool = False | ) -> None Registered as a Seq2SeqEncoder with name \"stacked_bidirectional_lstm\".","title":"StackedBidirectionalLstmSeq2SeqEncoder"},{"location":"api/modules/seq2seq_encoders/pytorch_transformer_wrapper/","text":"allennlp .modules .seq2seq_encoders .pytorch_transformer_wrapper [SOURCE] PytorchTransformer \u00b6 @Seq2SeqEncoder . register ( \"pytorch_transformer\" ) class PytorchTransformer ( Seq2SeqEncoder ): | def __init__ ( | self , | input_dim : int , | num_layers : int , | feedforward_hidden_dim : int = 2048 , | num_attention_heads : int = 8 , | positional_encoding : Optional [ str ] = None , | positional_embedding_size : int = 512 , | dropout_prob : float = 0.1 , | activation : str = \"relu\" | ) -> None Implements a stacked self-attention encoder similar to the Transformer architecture in Attention is all you Need . This class adapts the Transformer from torch.nn for use in AllenNLP. Optionally, it adds positional encodings. Registered as a Seq2SeqEncoder with name \"pytorch_transformer\". Parameters \u00b6 input_dim : int The input dimension of the encoder. num_layers : int The number of stacked self attention -> feedforward -> layer normalisation blocks. feedforward_hidden_dim : int The middle dimension of the FeedForward network. The input and output dimensions are fixed to ensure sizes match up for the self attention layers. num_attention_heads : int The number of attention heads to use per layer. positional_encoding : str , optional (default = None ) Specifies the type of positional encodings to use. Your options are None to have no positional encodings. \"sinusoidal\" to have sinusoidal encodings, as described in https://api.semanticscholar.org/CorpusID:13756489. \"embedding\" to treat positional encodings as learnable parameters Without positional encoding, the self attention layers have no idea of absolute or relative position (as they are just computing pairwise similarity between vectors of elements), which can be important features for many tasks. positional_embedding_size : int , optional (default = 512 ) The number of positional embeddings. dropout_prob : float , optional (default = 0.1 ) The dropout probability for the feedforward network. activation : str , optional (default = \"relu\" ) The activation function of intermediate layers. Must be either \"relu\" or \"gelu\" . get_input_dim \u00b6 class PytorchTransformer ( Seq2SeqEncoder ): | ... | def get_input_dim ( self ) -> int get_output_dim \u00b6 class PytorchTransformer ( Seq2SeqEncoder ): | ... | def get_output_dim ( self ) -> int is_bidirectional \u00b6 class PytorchTransformer ( Seq2SeqEncoder ): | ... | def is_bidirectional ( self ) forward \u00b6 class PytorchTransformer ( Seq2SeqEncoder ): | ... | def forward ( self , inputs : torch . Tensor , mask : torch . BoolTensor )","title":"pytorch_transformer_wrapper"},{"location":"api/modules/seq2seq_encoders/pytorch_transformer_wrapper/#pytorchtransformer","text":"@Seq2SeqEncoder . register ( \"pytorch_transformer\" ) class PytorchTransformer ( Seq2SeqEncoder ): | def __init__ ( | self , | input_dim : int , | num_layers : int , | feedforward_hidden_dim : int = 2048 , | num_attention_heads : int = 8 , | positional_encoding : Optional [ str ] = None , | positional_embedding_size : int = 512 , | dropout_prob : float = 0.1 , | activation : str = \"relu\" | ) -> None Implements a stacked self-attention encoder similar to the Transformer architecture in Attention is all you Need . This class adapts the Transformer from torch.nn for use in AllenNLP. Optionally, it adds positional encodings. Registered as a Seq2SeqEncoder with name \"pytorch_transformer\".","title":"PytorchTransformer"},{"location":"api/modules/seq2seq_encoders/pytorch_transformer_wrapper/#get_input_dim","text":"class PytorchTransformer ( Seq2SeqEncoder ): | ... | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"api/modules/seq2seq_encoders/pytorch_transformer_wrapper/#get_output_dim","text":"class PytorchTransformer ( Seq2SeqEncoder ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/seq2seq_encoders/pytorch_transformer_wrapper/#is_bidirectional","text":"class PytorchTransformer ( Seq2SeqEncoder ): | ... | def is_bidirectional ( self )","title":"is_bidirectional"},{"location":"api/modules/seq2seq_encoders/pytorch_transformer_wrapper/#forward","text":"class PytorchTransformer ( Seq2SeqEncoder ): | ... | def forward ( self , inputs : torch . Tensor , mask : torch . BoolTensor )","title":"forward"},{"location":"api/modules/seq2seq_encoders/seq2seq_encoder/","text":"allennlp .modules .seq2seq_encoders .seq2seq_encoder [SOURCE] Seq2SeqEncoder \u00b6 class Seq2SeqEncoder ( _EncoderBase , Registrable ) A Seq2SeqEncoder is a Module that takes as input a sequence of vectors and returns a modified sequence of vectors. Input shape : (batch_size, sequence_length, input_dim) ; output shape : (batch_size, sequence_length, output_dim) . We add two methods to the basic Module API: get_input_dim() and get_output_dim() . You might need this if you want to construct a Linear layer using the output of this encoder, or to raise sensible errors for mis-matching input dimensions. get_input_dim \u00b6 class Seq2SeqEncoder ( _EncoderBase , Registrable ): | ... | def get_input_dim ( self ) -> int Returns the dimension of the vector input for each element in the sequence input to a Seq2SeqEncoder . This is not the shape of the input tensor, but the last element of that shape. get_output_dim \u00b6 class Seq2SeqEncoder ( _EncoderBase , Registrable ): | ... | def get_output_dim ( self ) -> int Returns the dimension of each vector in the sequence output by this Seq2SeqEncoder . This is not the shape of the returned tensor, but the last element of that shape. is_bidirectional \u00b6 class Seq2SeqEncoder ( _EncoderBase , Registrable ): | ... | def is_bidirectional ( self ) -> bool Returns True if this encoder is bidirectional. If so, we assume the forward direction of the encoder is the first half of the final dimension, and the backward direction is the second half.","title":"seq2seq_encoder"},{"location":"api/modules/seq2seq_encoders/seq2seq_encoder/#seq2seqencoder","text":"class Seq2SeqEncoder ( _EncoderBase , Registrable ) A Seq2SeqEncoder is a Module that takes as input a sequence of vectors and returns a modified sequence of vectors. Input shape : (batch_size, sequence_length, input_dim) ; output shape : (batch_size, sequence_length, output_dim) . We add two methods to the basic Module API: get_input_dim() and get_output_dim() . You might need this if you want to construct a Linear layer using the output of this encoder, or to raise sensible errors for mis-matching input dimensions.","title":"Seq2SeqEncoder"},{"location":"api/modules/seq2seq_encoders/seq2seq_encoder/#get_input_dim","text":"class Seq2SeqEncoder ( _EncoderBase , Registrable ): | ... | def get_input_dim ( self ) -> int Returns the dimension of the vector input for each element in the sequence input to a Seq2SeqEncoder . This is not the shape of the input tensor, but the last element of that shape.","title":"get_input_dim"},{"location":"api/modules/seq2seq_encoders/seq2seq_encoder/#get_output_dim","text":"class Seq2SeqEncoder ( _EncoderBase , Registrable ): | ... | def get_output_dim ( self ) -> int Returns the dimension of each vector in the sequence output by this Seq2SeqEncoder . This is not the shape of the returned tensor, but the last element of that shape.","title":"get_output_dim"},{"location":"api/modules/seq2seq_encoders/seq2seq_encoder/#is_bidirectional","text":"class Seq2SeqEncoder ( _EncoderBase , Registrable ): | ... | def is_bidirectional ( self ) -> bool Returns True if this encoder is bidirectional. If so, we assume the forward direction of the encoder is the first half of the final dimension, and the backward direction is the second half.","title":"is_bidirectional"},{"location":"api/modules/seq2vec_encoders/bert_pooler/","text":"allennlp .modules .seq2vec_encoders .bert_pooler [SOURCE] BertPooler \u00b6 @Seq2VecEncoder . register ( \"bert_pooler\" ) class BertPooler ( Seq2VecEncoder ): | def __init__ ( | self , | pretrained_model : str , | * , override_weights_file : Optional [ str ] = None , | * , override_weights_strip_prefix : Optional [ str ] = None , | * , load_weights : bool = True , | * , requires_grad : bool = True , | * , dropout : float = 0.0 , | * , transformer_kwargs : Optional [ Dict [ str , Any ]] = None | ) -> None The pooling layer at the end of the BERT model. This returns an embedding for the [CLS] token, after passing it through a non-linear tanh activation; the non-linear layer is also part of the BERT model. If you want to use the pretrained BERT model to build a classifier and you want to use the AllenNLP token-indexer -> token-embedder -> seq2vec encoder setup, this is the Seq2VecEncoder to use. (For example, if you want to experiment with other embedding / encoding combinations.) Registered as a Seq2VecEncoder with name \"bert_pooler\". Parameters \u00b6 pretrained_model : Union[str, BertModel] The pretrained BERT model to use. If this is a string, we will call transformers.AutoModel.from_pretrained(pretrained_model) and use that. override_weights_file : Optional[str] , optional (default = None ) If set, this specifies a file from which to load alternate weights that override the weights from huggingface. The file is expected to contain a PyTorch state_dict , created with torch.save() . override_weights_strip_prefix : Optional[str] , optional (default = None ) If set, strip the given prefix from the state dict when loading it. load_weights : bool , optional (default = True ) Whether to load the pretraiend weights. requires_grad : bool , optional (default = True ) If True, the weights of the pooler will be updated during training. Otherwise they will not. dropout : float , optional (default = 0.0 ) Amount of dropout to apply after pooling transformer_kwargs : Dict[str, Any] , optional (default = None ) Dictionary with additional arguments for AutoModel.from_pretrained . get_input_dim \u00b6 class BertPooler ( Seq2VecEncoder ): | ... | def get_input_dim ( self ) -> int get_output_dim \u00b6 class BertPooler ( Seq2VecEncoder ): | ... | def get_output_dim ( self ) -> int forward \u00b6 class BertPooler ( Seq2VecEncoder ): | ... | def forward ( | self , | tokens : torch . Tensor , | mask : torch . BoolTensor = None , | num_wrapping_dims : int = 0 | )","title":"bert_pooler"},{"location":"api/modules/seq2vec_encoders/bert_pooler/#bertpooler","text":"@Seq2VecEncoder . register ( \"bert_pooler\" ) class BertPooler ( Seq2VecEncoder ): | def __init__ ( | self , | pretrained_model : str , | * , override_weights_file : Optional [ str ] = None , | * , override_weights_strip_prefix : Optional [ str ] = None , | * , load_weights : bool = True , | * , requires_grad : bool = True , | * , dropout : float = 0.0 , | * , transformer_kwargs : Optional [ Dict [ str , Any ]] = None | ) -> None The pooling layer at the end of the BERT model. This returns an embedding for the [CLS] token, after passing it through a non-linear tanh activation; the non-linear layer is also part of the BERT model. If you want to use the pretrained BERT model to build a classifier and you want to use the AllenNLP token-indexer -> token-embedder -> seq2vec encoder setup, this is the Seq2VecEncoder to use. (For example, if you want to experiment with other embedding / encoding combinations.) Registered as a Seq2VecEncoder with name \"bert_pooler\".","title":"BertPooler"},{"location":"api/modules/seq2vec_encoders/bert_pooler/#get_input_dim","text":"class BertPooler ( Seq2VecEncoder ): | ... | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"api/modules/seq2vec_encoders/bert_pooler/#get_output_dim","text":"class BertPooler ( Seq2VecEncoder ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/seq2vec_encoders/bert_pooler/#forward","text":"class BertPooler ( Seq2VecEncoder ): | ... | def forward ( | self , | tokens : torch . Tensor , | mask : torch . BoolTensor = None , | num_wrapping_dims : int = 0 | )","title":"forward"},{"location":"api/modules/seq2vec_encoders/boe_encoder/","text":"allennlp .modules .seq2vec_encoders .boe_encoder [SOURCE] BagOfEmbeddingsEncoder \u00b6 @Seq2VecEncoder . register ( \"boe\" ) @Seq2VecEncoder . register ( \"bag_of_embeddings\" ) class BagOfEmbeddingsEncoder ( Seq2VecEncoder ): | def __init__ ( self , embedding_dim : int , averaged : bool = False ) -> None A BagOfEmbeddingsEncoder is a simple Seq2VecEncoder which simply sums the embeddings of a sequence across the time dimension. The input to this module is of shape (batch_size, num_tokens, embedding_dim) , and the output is of shape (batch_size, embedding_dim) . Registered as a Seq2VecEncoder with name \"bag_of_embeddings\" and \"boe\". Parameters \u00b6 embedding_dim : int This is the input dimension to the encoder. averaged : bool , optional (default = False ) If True , this module will average the embeddings across time, rather than simply summing (ie. we will divide the summed embeddings by the length of the sentence). get_input_dim \u00b6 class BagOfEmbeddingsEncoder ( Seq2VecEncoder ): | ... | def get_input_dim ( self ) -> int get_output_dim \u00b6 class BagOfEmbeddingsEncoder ( Seq2VecEncoder ): | ... | def get_output_dim ( self ) -> int forward \u00b6 class BagOfEmbeddingsEncoder ( Seq2VecEncoder ): | ... | def forward ( | self , | tokens : torch . Tensor , | mask : torch . BoolTensor = None | )","title":"boe_encoder"},{"location":"api/modules/seq2vec_encoders/boe_encoder/#bagofembeddingsencoder","text":"@Seq2VecEncoder . register ( \"boe\" ) @Seq2VecEncoder . register ( \"bag_of_embeddings\" ) class BagOfEmbeddingsEncoder ( Seq2VecEncoder ): | def __init__ ( self , embedding_dim : int , averaged : bool = False ) -> None A BagOfEmbeddingsEncoder is a simple Seq2VecEncoder which simply sums the embeddings of a sequence across the time dimension. The input to this module is of shape (batch_size, num_tokens, embedding_dim) , and the output is of shape (batch_size, embedding_dim) . Registered as a Seq2VecEncoder with name \"bag_of_embeddings\" and \"boe\".","title":"BagOfEmbeddingsEncoder"},{"location":"api/modules/seq2vec_encoders/boe_encoder/#get_input_dim","text":"class BagOfEmbeddingsEncoder ( Seq2VecEncoder ): | ... | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"api/modules/seq2vec_encoders/boe_encoder/#get_output_dim","text":"class BagOfEmbeddingsEncoder ( Seq2VecEncoder ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/seq2vec_encoders/boe_encoder/#forward","text":"class BagOfEmbeddingsEncoder ( Seq2VecEncoder ): | ... | def forward ( | self , | tokens : torch . Tensor , | mask : torch . BoolTensor = None | )","title":"forward"},{"location":"api/modules/seq2vec_encoders/cls_pooler/","text":"allennlp .modules .seq2vec_encoders .cls_pooler [SOURCE] ClsPooler \u00b6 @Seq2VecEncoder . register ( \"cls_pooler\" ) class ClsPooler ( Seq2VecEncoder ): | def __init__ ( | self , | embedding_dim : int , | cls_is_last_token : bool = False | ) Just takes the first vector from a list of vectors (which in a transformer is typically the [CLS] token) and returns it. For BERT, it's recommended to use BertPooler instead. Registered as a Seq2VecEncoder with name \"cls_pooler\". Parameters \u00b6 embedding_dim : int This isn't needed for any computation that we do, but we sometimes rely on get_input_dim and get_output_dim to check parameter settings, or to instantiate final linear layers. In order to give the right values there, we need to know the embedding dimension. If you're using this with a transformer from the transformers library, this can often be found with model.config.hidden_size , if you're not sure. cls_is_last_token : bool , optional The [CLS] token is the first token for most of the pretrained transformer models. For some models such as XLNet, however, it is the last token, and we therefore need to select at the end. get_input_dim \u00b6 class ClsPooler ( Seq2VecEncoder ): | ... | def get_input_dim ( self ) -> int get_output_dim \u00b6 class ClsPooler ( Seq2VecEncoder ): | ... | def get_output_dim ( self ) -> int forward \u00b6 class ClsPooler ( Seq2VecEncoder ): | ... | def forward ( | self , | tokens : torch . Tensor , | mask : torch . BoolTensor = None | )","title":"cls_pooler"},{"location":"api/modules/seq2vec_encoders/cls_pooler/#clspooler","text":"@Seq2VecEncoder . register ( \"cls_pooler\" ) class ClsPooler ( Seq2VecEncoder ): | def __init__ ( | self , | embedding_dim : int , | cls_is_last_token : bool = False | ) Just takes the first vector from a list of vectors (which in a transformer is typically the [CLS] token) and returns it. For BERT, it's recommended to use BertPooler instead. Registered as a Seq2VecEncoder with name \"cls_pooler\".","title":"ClsPooler"},{"location":"api/modules/seq2vec_encoders/cls_pooler/#get_input_dim","text":"class ClsPooler ( Seq2VecEncoder ): | ... | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"api/modules/seq2vec_encoders/cls_pooler/#get_output_dim","text":"class ClsPooler ( Seq2VecEncoder ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/seq2vec_encoders/cls_pooler/#forward","text":"class ClsPooler ( Seq2VecEncoder ): | ... | def forward ( | self , | tokens : torch . Tensor , | mask : torch . BoolTensor = None | )","title":"forward"},{"location":"api/modules/seq2vec_encoders/cnn_encoder/","text":"allennlp .modules .seq2vec_encoders .cnn_encoder [SOURCE] CnnEncoder \u00b6 @Seq2VecEncoder . register ( \"cnn\" ) class CnnEncoder ( Seq2VecEncoder ): | def __init__ ( | self , | embedding_dim : int , | num_filters : int , | ngram_filter_sizes : Tuple [ int , ... ] = ( 2 , 3 , 4 , 5 ), | conv_layer_activation : Activation = None , | output_dim : Optional [ int ] = None | ) -> None A CnnEncoder is a combination of multiple convolution layers and max pooling layers. As a Seq2VecEncoder , the input to this module is of shape (batch_size, num_tokens, input_dim) , and the output is of shape (batch_size, output_dim) . The CNN has one convolution layer for each ngram filter size. Each convolution operation gives out a vector of size num_filters. The number of times a convolution layer will be used is num_tokens - ngram_size + 1 . The corresponding maxpooling layer aggregates all these outputs from the convolution layer and outputs the max. This operation is repeated for every ngram size passed, and consequently the dimensionality of the output after maxpooling is len(ngram_filter_sizes) * num_filters . This then gets (optionally) projected down to a lower dimensional output, specified by output_dim . We then use a fully connected layer to project in back to the desired output_dim. For more details, refer to \"A Sensitivity Analysis of (and Practitioners\u2019 Guide to) Convolutional Neural Networks for Sentence Classification\", Zhang and Wallace 2016, particularly Figure 1. Registered as a Seq2VecEncoder with name \"cnn\". Parameters \u00b6 embedding_dim : int This is the input dimension to the encoder. We need this because we can't do shape inference in pytorch, and we need to know what size filters to construct in the CNN. num_filters : int This is the output dim for each convolutional layer, which is the number of \"filters\" learned by that layer. ngram_filter_sizes : Tuple[int] , optional (default = (2, 3, 4, 5) ) This specifies both the number of convolutional layers we will create and their sizes. The default of (2, 3, 4, 5) will have four convolutional layers, corresponding to encoding ngrams of size 2 to 5 with some number of filters. conv_layer_activation : Activation , optional (default = torch.nn.ReLU ) Activation to use after the convolution layers. output_dim : Optional[int] , optional (default = None ) After doing convolutions and pooling, we'll project the collected features into a vector of this size. If this value is None , we will just return the result of the max pooling, giving an output of shape len(ngram_filter_sizes) * num_filters . get_input_dim \u00b6 class CnnEncoder ( Seq2VecEncoder ): | ... | def get_input_dim ( self ) -> int get_output_dim \u00b6 class CnnEncoder ( Seq2VecEncoder ): | ... | def get_output_dim ( self ) -> int forward \u00b6 class CnnEncoder ( Seq2VecEncoder ): | ... | def forward ( self , tokens : torch . Tensor , mask : torch . BoolTensor )","title":"cnn_encoder"},{"location":"api/modules/seq2vec_encoders/cnn_encoder/#cnnencoder","text":"@Seq2VecEncoder . register ( \"cnn\" ) class CnnEncoder ( Seq2VecEncoder ): | def __init__ ( | self , | embedding_dim : int , | num_filters : int , | ngram_filter_sizes : Tuple [ int , ... ] = ( 2 , 3 , 4 , 5 ), | conv_layer_activation : Activation = None , | output_dim : Optional [ int ] = None | ) -> None A CnnEncoder is a combination of multiple convolution layers and max pooling layers. As a Seq2VecEncoder , the input to this module is of shape (batch_size, num_tokens, input_dim) , and the output is of shape (batch_size, output_dim) . The CNN has one convolution layer for each ngram filter size. Each convolution operation gives out a vector of size num_filters. The number of times a convolution layer will be used is num_tokens - ngram_size + 1 . The corresponding maxpooling layer aggregates all these outputs from the convolution layer and outputs the max. This operation is repeated for every ngram size passed, and consequently the dimensionality of the output after maxpooling is len(ngram_filter_sizes) * num_filters . This then gets (optionally) projected down to a lower dimensional output, specified by output_dim . We then use a fully connected layer to project in back to the desired output_dim. For more details, refer to \"A Sensitivity Analysis of (and Practitioners\u2019 Guide to) Convolutional Neural Networks for Sentence Classification\", Zhang and Wallace 2016, particularly Figure 1. Registered as a Seq2VecEncoder with name \"cnn\".","title":"CnnEncoder"},{"location":"api/modules/seq2vec_encoders/cnn_encoder/#get_input_dim","text":"class CnnEncoder ( Seq2VecEncoder ): | ... | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"api/modules/seq2vec_encoders/cnn_encoder/#get_output_dim","text":"class CnnEncoder ( Seq2VecEncoder ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/seq2vec_encoders/cnn_encoder/#forward","text":"class CnnEncoder ( Seq2VecEncoder ): | ... | def forward ( self , tokens : torch . Tensor , mask : torch . BoolTensor )","title":"forward"},{"location":"api/modules/seq2vec_encoders/cnn_highway_encoder/","text":"allennlp .modules .seq2vec_encoders .cnn_highway_encoder [SOURCE] CnnHighwayEncoder \u00b6 @Seq2VecEncoder . register ( \"cnn-highway\" ) class CnnHighwayEncoder ( Seq2VecEncoder ): | def __init__ ( | self , | embedding_dim : int , | filters : Sequence [ Sequence [ int ]], | num_highway : int , | projection_dim : int , | activation : str = \"relu\" , | projection_location : str = \"after_highway\" , | do_layer_norm : bool = False | ) -> None The character CNN + highway encoder from Kim et al \"Character aware neural language models\" with an optional projection. Registered as a Seq2VecEncoder with name \"cnn-highway\". Parameters \u00b6 embedding_dim : int The dimension of the initial character embedding. filters : Sequence[Sequence[int]] A sequence of pairs (filter_width, num_filters). num_highway : int The number of highway layers. projection_dim : int The output dimension of the projection layer. activation : str , optional (default = 'relu' ) The activation function for the convolutional layers. projection_location : str , optional (default = 'after_highway' ) Where to apply the projection layer. Valid values are 'after_highway', 'after_cnn', and None. forward \u00b6 class CnnHighwayEncoder ( Seq2VecEncoder ): | ... | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor | ) -> Dict [ str , torch . Tensor ] Compute context insensitive token embeddings for ELMo representations. Parameters \u00b6 inputs : torch.Tensor Shape (batch_size, num_characters, embedding_dim) Character embeddings representing the current batch. mask : torch.BoolTensor Shape (batch_size, num_characters) Currently unused. The mask for characters is implicit. See TokenCharactersEncoder.forward. Returns \u00b6 encoding : Shape (batch_size, projection_dim) tensor with context-insensitive token representations. get_input_dim \u00b6 class CnnHighwayEncoder ( Seq2VecEncoder ): | ... | def get_input_dim ( self ) -> int get_output_dim \u00b6 class CnnHighwayEncoder ( Seq2VecEncoder ): | ... | def get_output_dim ( self ) -> int","title":"cnn_highway_encoder"},{"location":"api/modules/seq2vec_encoders/cnn_highway_encoder/#cnnhighwayencoder","text":"@Seq2VecEncoder . register ( \"cnn-highway\" ) class CnnHighwayEncoder ( Seq2VecEncoder ): | def __init__ ( | self , | embedding_dim : int , | filters : Sequence [ Sequence [ int ]], | num_highway : int , | projection_dim : int , | activation : str = \"relu\" , | projection_location : str = \"after_highway\" , | do_layer_norm : bool = False | ) -> None The character CNN + highway encoder from Kim et al \"Character aware neural language models\" with an optional projection. Registered as a Seq2VecEncoder with name \"cnn-highway\".","title":"CnnHighwayEncoder"},{"location":"api/modules/seq2vec_encoders/cnn_highway_encoder/#forward","text":"class CnnHighwayEncoder ( Seq2VecEncoder ): | ... | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor | ) -> Dict [ str , torch . Tensor ] Compute context insensitive token embeddings for ELMo representations.","title":"forward"},{"location":"api/modules/seq2vec_encoders/cnn_highway_encoder/#get_input_dim","text":"class CnnHighwayEncoder ( Seq2VecEncoder ): | ... | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"api/modules/seq2vec_encoders/cnn_highway_encoder/#get_output_dim","text":"class CnnHighwayEncoder ( Seq2VecEncoder ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/seq2vec_encoders/pytorch_seq2vec_wrapper/","text":"allennlp .modules .seq2vec_encoders .pytorch_seq2vec_wrapper [SOURCE] PytorchSeq2VecWrapper \u00b6 class PytorchSeq2VecWrapper ( Seq2VecEncoder ): | def __init__ ( self , module : torch . nn . modules . RNNBase ) -> None Pytorch's RNNs have two outputs: the final hidden state for every time step, and the hidden state at the last time step for every layer. We just want the final hidden state of the last time step. This wrapper pulls out that output, and adds a get_output_dim method, which is useful if you want to, e.g., define a linear + softmax layer on top of this to get some distribution over a set of labels. The linear layer needs to know its input dimension before it is called, and you can get that from get_output_dim . Also, there are lots of ways you could imagine going from an RNN hidden state at every timestep to a single vector - you could take the last vector at all layers in the stack, do some kind of pooling, take the last vector of the top layer in a stack, or many other options. We just take the final hidden state vector, or in the case of a bidirectional RNN cell, we concatenate the forward and backward final states together. TODO(mattg): allow for other ways of wrapping RNNs. In order to be wrapped with this wrapper, a class must have the following members: - ` self . input_size : int ` - ` self . hidden_size : int ` - ` def forward ( inputs : PackedSequence , hidden_state : torch . tensor ) -> Tuple [ PackedSequence , torch . Tensor ] ` . - ` self . bidirectional : bool ` ( optional ) This is what pytorch's RNN's look like - just make sure your class looks like those, and it should work. Note that we require you to pass a binary mask of shape (batch_size, sequence_length) when you call this module, to avoid subtle bugs around masking. If you already have a PackedSequence you can pass None as the second parameter. get_input_dim \u00b6 class PytorchSeq2VecWrapper ( Seq2VecEncoder ): | ... | def get_input_dim ( self ) -> int get_output_dim \u00b6 class PytorchSeq2VecWrapper ( Seq2VecEncoder ): | ... | def get_output_dim ( self ) -> int forward \u00b6 class PytorchSeq2VecWrapper ( Seq2VecEncoder ): | ... | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor , | hidden_state : torch . Tensor = None | ) -> torch . Tensor GruSeq2VecEncoder \u00b6 @Seq2VecEncoder . register ( \"gru\" ) class GruSeq2VecEncoder ( PytorchSeq2VecWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int = 1 , | bias : bool = True , | dropout : float = 0.0 , | bidirectional : bool = False | ) Registered as a Seq2VecEncoder with name \"gru\". LstmSeq2VecEncoder \u00b6 @Seq2VecEncoder . register ( \"lstm\" ) class LstmSeq2VecEncoder ( PytorchSeq2VecWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int = 1 , | bias : bool = True , | dropout : float = 0.0 , | bidirectional : bool = False | ) Registered as a Seq2VecEncoder with name \"lstm\". RnnSeq2VecEncoder \u00b6 @Seq2VecEncoder . register ( \"rnn\" ) class RnnSeq2VecEncoder ( PytorchSeq2VecWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int = 1 , | nonlinearity : str = \"tanh\" , | bias : bool = True , | dropout : float = 0.0 , | bidirectional : bool = False | ) Registered as a Seq2VecEncoder with name \"rnn\". AugmentedLstmSeq2VecEncoder \u00b6 @Seq2VecEncoder . register ( \"augmented_lstm\" ) class AugmentedLstmSeq2VecEncoder ( PytorchSeq2VecWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | go_forward : bool = True , | recurrent_dropout_probability : float = 0.0 , | use_highway : bool = True , | use_input_projection_bias : bool = True | ) -> None Registered as a Seq2VecEncoder with name \"augmented_lstm\". StackedAlternatingLstmSeq2VecEncoder \u00b6 @Seq2VecEncoder . register ( \"alternating_lstm\" ) class StackedAlternatingLstmSeq2VecEncoder ( PytorchSeq2VecWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int , | recurrent_dropout_probability : float = 0.0 , | use_highway : bool = True , | use_input_projection_bias : bool = True | ) -> None Registered as a Seq2VecEncoder with name \"alternating_lstm\". StackedBidirectionalLstmSeq2VecEncoder \u00b6 @Seq2VecEncoder . register ( \"stacked_bidirectional_lstm\" ) class StackedBidirectionalLstmSeq2VecEncoder ( PytorchSeq2VecWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int , | recurrent_dropout_probability : float = 0.0 , | layer_dropout_probability : float = 0.0 , | use_highway : bool = True | ) -> None Registered as a Seq2VecEncoder with name \"stacked_bidirectional_lstm\".","title":"pytorch_seq2vec_wrapper"},{"location":"api/modules/seq2vec_encoders/pytorch_seq2vec_wrapper/#pytorchseq2vecwrapper","text":"class PytorchSeq2VecWrapper ( Seq2VecEncoder ): | def __init__ ( self , module : torch . nn . modules . RNNBase ) -> None Pytorch's RNNs have two outputs: the final hidden state for every time step, and the hidden state at the last time step for every layer. We just want the final hidden state of the last time step. This wrapper pulls out that output, and adds a get_output_dim method, which is useful if you want to, e.g., define a linear + softmax layer on top of this to get some distribution over a set of labels. The linear layer needs to know its input dimension before it is called, and you can get that from get_output_dim . Also, there are lots of ways you could imagine going from an RNN hidden state at every timestep to a single vector - you could take the last vector at all layers in the stack, do some kind of pooling, take the last vector of the top layer in a stack, or many other options. We just take the final hidden state vector, or in the case of a bidirectional RNN cell, we concatenate the forward and backward final states together. TODO(mattg): allow for other ways of wrapping RNNs. In order to be wrapped with this wrapper, a class must have the following members: - ` self . input_size : int ` - ` self . hidden_size : int ` - ` def forward ( inputs : PackedSequence , hidden_state : torch . tensor ) -> Tuple [ PackedSequence , torch . Tensor ] ` . - ` self . bidirectional : bool ` ( optional ) This is what pytorch's RNN's look like - just make sure your class looks like those, and it should work. Note that we require you to pass a binary mask of shape (batch_size, sequence_length) when you call this module, to avoid subtle bugs around masking. If you already have a PackedSequence you can pass None as the second parameter.","title":"PytorchSeq2VecWrapper"},{"location":"api/modules/seq2vec_encoders/pytorch_seq2vec_wrapper/#get_input_dim","text":"class PytorchSeq2VecWrapper ( Seq2VecEncoder ): | ... | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"api/modules/seq2vec_encoders/pytorch_seq2vec_wrapper/#get_output_dim","text":"class PytorchSeq2VecWrapper ( Seq2VecEncoder ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/seq2vec_encoders/pytorch_seq2vec_wrapper/#forward","text":"class PytorchSeq2VecWrapper ( Seq2VecEncoder ): | ... | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor , | hidden_state : torch . Tensor = None | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/seq2vec_encoders/pytorch_seq2vec_wrapper/#gruseq2vecencoder","text":"@Seq2VecEncoder . register ( \"gru\" ) class GruSeq2VecEncoder ( PytorchSeq2VecWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int = 1 , | bias : bool = True , | dropout : float = 0.0 , | bidirectional : bool = False | ) Registered as a Seq2VecEncoder with name \"gru\".","title":"GruSeq2VecEncoder"},{"location":"api/modules/seq2vec_encoders/pytorch_seq2vec_wrapper/#lstmseq2vecencoder","text":"@Seq2VecEncoder . register ( \"lstm\" ) class LstmSeq2VecEncoder ( PytorchSeq2VecWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int = 1 , | bias : bool = True , | dropout : float = 0.0 , | bidirectional : bool = False | ) Registered as a Seq2VecEncoder with name \"lstm\".","title":"LstmSeq2VecEncoder"},{"location":"api/modules/seq2vec_encoders/pytorch_seq2vec_wrapper/#rnnseq2vecencoder","text":"@Seq2VecEncoder . register ( \"rnn\" ) class RnnSeq2VecEncoder ( PytorchSeq2VecWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int = 1 , | nonlinearity : str = \"tanh\" , | bias : bool = True , | dropout : float = 0.0 , | bidirectional : bool = False | ) Registered as a Seq2VecEncoder with name \"rnn\".","title":"RnnSeq2VecEncoder"},{"location":"api/modules/seq2vec_encoders/pytorch_seq2vec_wrapper/#augmentedlstmseq2vecencoder","text":"@Seq2VecEncoder . register ( \"augmented_lstm\" ) class AugmentedLstmSeq2VecEncoder ( PytorchSeq2VecWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | go_forward : bool = True , | recurrent_dropout_probability : float = 0.0 , | use_highway : bool = True , | use_input_projection_bias : bool = True | ) -> None Registered as a Seq2VecEncoder with name \"augmented_lstm\".","title":"AugmentedLstmSeq2VecEncoder"},{"location":"api/modules/seq2vec_encoders/pytorch_seq2vec_wrapper/#stackedalternatinglstmseq2vecencoder","text":"@Seq2VecEncoder . register ( \"alternating_lstm\" ) class StackedAlternatingLstmSeq2VecEncoder ( PytorchSeq2VecWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int , | recurrent_dropout_probability : float = 0.0 , | use_highway : bool = True , | use_input_projection_bias : bool = True | ) -> None Registered as a Seq2VecEncoder with name \"alternating_lstm\".","title":"StackedAlternatingLstmSeq2VecEncoder"},{"location":"api/modules/seq2vec_encoders/pytorch_seq2vec_wrapper/#stackedbidirectionallstmseq2vecencoder","text":"@Seq2VecEncoder . register ( \"stacked_bidirectional_lstm\" ) class StackedBidirectionalLstmSeq2VecEncoder ( PytorchSeq2VecWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int , | recurrent_dropout_probability : float = 0.0 , | layer_dropout_probability : float = 0.0 , | use_highway : bool = True | ) -> None Registered as a Seq2VecEncoder with name \"stacked_bidirectional_lstm\".","title":"StackedBidirectionalLstmSeq2VecEncoder"},{"location":"api/modules/seq2vec_encoders/seq2vec_encoder/","text":"allennlp .modules .seq2vec_encoders .seq2vec_encoder [SOURCE] Seq2VecEncoder \u00b6 class Seq2VecEncoder ( _EncoderBase , Registrable ) A Seq2VecEncoder is a Module that takes as input a sequence of vectors and returns a single vector. Input shape : (batch_size, sequence_length, input_dim) ; output shape: (batch_size, output_dim) . We add two methods to the basic Module API: get_input_dim() and get_output_dim() . You might need this if you want to construct a Linear layer using the output of this encoder, or to raise sensible errors for mis-matching input dimensions. get_input_dim \u00b6 class Seq2VecEncoder ( _EncoderBase , Registrable ): | ... | def get_input_dim ( self ) -> int Returns the dimension of the vector input for each element in the sequence input to a Seq2VecEncoder . This is not the shape of the input tensor, but the last element of that shape. get_output_dim \u00b6 class Seq2VecEncoder ( _EncoderBase , Registrable ): | ... | def get_output_dim ( self ) -> int Returns the dimension of the final vector output by this Seq2VecEncoder . This is not the shape of the returned tensor, but the last element of that shape.","title":"seq2vec_encoder"},{"location":"api/modules/seq2vec_encoders/seq2vec_encoder/#seq2vecencoder","text":"class Seq2VecEncoder ( _EncoderBase , Registrable ) A Seq2VecEncoder is a Module that takes as input a sequence of vectors and returns a single vector. Input shape : (batch_size, sequence_length, input_dim) ; output shape: (batch_size, output_dim) . We add two methods to the basic Module API: get_input_dim() and get_output_dim() . You might need this if you want to construct a Linear layer using the output of this encoder, or to raise sensible errors for mis-matching input dimensions.","title":"Seq2VecEncoder"},{"location":"api/modules/seq2vec_encoders/seq2vec_encoder/#get_input_dim","text":"class Seq2VecEncoder ( _EncoderBase , Registrable ): | ... | def get_input_dim ( self ) -> int Returns the dimension of the vector input for each element in the sequence input to a Seq2VecEncoder . This is not the shape of the input tensor, but the last element of that shape.","title":"get_input_dim"},{"location":"api/modules/seq2vec_encoders/seq2vec_encoder/#get_output_dim","text":"class Seq2VecEncoder ( _EncoderBase , Registrable ): | ... | def get_output_dim ( self ) -> int Returns the dimension of the final vector output by this Seq2VecEncoder . This is not the shape of the returned tensor, but the last element of that shape.","title":"get_output_dim"},{"location":"api/modules/span_extractors/bidirectional_endpoint_span_extractor/","text":"allennlp .modules .span_extractors .bidirectional_endpoint_span_extractor [SOURCE] BidirectionalEndpointSpanExtractor \u00b6 @SpanExtractor . register ( \"bidirectional_endpoint\" ) class BidirectionalEndpointSpanExtractor ( SpanExtractorWithSpanWidthEmbedding ): | def __init__ ( | self , | input_dim : int , | forward_combination : str = \"y-x\" , | backward_combination : str = \"x-y\" , | num_width_embeddings : int = None , | span_width_embedding_dim : int = None , | bucket_widths : bool = False , | use_sentinels : bool = True | ) -> None Represents spans from a bidirectional encoder as a concatenation of two different representations of the span endpoints, one for the forward direction of the encoder and one from the backward direction. This type of representation encodes some subtlety, because when you consider the forward and backward directions separately, the end index of the span for the backward direction's representation is actually the start index. By default, this SpanExtractor represents spans as sequence_tensor[inclusive_span_end] - sequence_tensor[exclusive_span_start] meaning that the representation is the difference between the the last word in the span and the word before the span started. Note that the start and end indices are with respect to the direction that the RNN is going in, so for the backward direction, the start/end indices are reversed. Additionally, the width of the spans can be embedded and concatenated on to the final combination. The following other types of representation are supported for both the forward and backward directions, assuming that x = span_start_embeddings and y = span_end_embeddings . x , y , x*y , x+y , x-y , x/y , where each of those binary operations is performed elementwise. You can list as many combinations as you want, comma separated. For example, you might give x,y,x*y as the combination parameter to this class. The computed similarity function would then be [x; y; x*y] , which can then be optionally concatenated with an embedded representation of the width of the span. Registered as a SpanExtractor with name \"bidirectional_endpoint\". Parameters \u00b6 input_dim : int The final dimension of the sequence_tensor . forward_combination : str , optional (default = \"y-x\" ) The method used to combine the forward_start_embeddings and forward_end_embeddings for the forward direction of the bidirectional representation. See above for a full description. backward_combination : str , optional (default = \"x-y\" ) The method used to combine the backward_start_embeddings and backward_end_embeddings for the backward direction of the bidirectional representation. See above for a full description. num_width_embeddings : int , optional (default = None ) Specifies the number of buckets to use when representing span width features. span_width_embedding_dim : int , optional (default = None ) The embedding size for the span_width features. bucket_widths : bool , optional (default = False ) Whether to bucket the span widths into log-space buckets. If False , the raw span widths are used. use_sentinels : bool , optional (default = True ) If True , sentinels are used to represent exclusive span indices for the elements in the first and last positions in the sequence (as the exclusive indices for these elements are outside of the the sequence boundary). This is not strictly necessary, as you may know that your exclusive start and end indices are always within your sequence representation, such as if you have appended/prepended and tokens to your sequence. get_output_dim \u00b6 class BidirectionalEndpointSpanExtractor ( SpanExtractorWithSpanWidthEmbedding ): | ... | def get_output_dim ( self ) -> int","title":"bidirectional_endpoint_span_extractor"},{"location":"api/modules/span_extractors/bidirectional_endpoint_span_extractor/#bidirectionalendpointspanextractor","text":"@SpanExtractor . register ( \"bidirectional_endpoint\" ) class BidirectionalEndpointSpanExtractor ( SpanExtractorWithSpanWidthEmbedding ): | def __init__ ( | self , | input_dim : int , | forward_combination : str = \"y-x\" , | backward_combination : str = \"x-y\" , | num_width_embeddings : int = None , | span_width_embedding_dim : int = None , | bucket_widths : bool = False , | use_sentinels : bool = True | ) -> None Represents spans from a bidirectional encoder as a concatenation of two different representations of the span endpoints, one for the forward direction of the encoder and one from the backward direction. This type of representation encodes some subtlety, because when you consider the forward and backward directions separately, the end index of the span for the backward direction's representation is actually the start index. By default, this SpanExtractor represents spans as sequence_tensor[inclusive_span_end] - sequence_tensor[exclusive_span_start] meaning that the representation is the difference between the the last word in the span and the word before the span started. Note that the start and end indices are with respect to the direction that the RNN is going in, so for the backward direction, the start/end indices are reversed. Additionally, the width of the spans can be embedded and concatenated on to the final combination. The following other types of representation are supported for both the forward and backward directions, assuming that x = span_start_embeddings and y = span_end_embeddings . x , y , x*y , x+y , x-y , x/y , where each of those binary operations is performed elementwise. You can list as many combinations as you want, comma separated. For example, you might give x,y,x*y as the combination parameter to this class. The computed similarity function would then be [x; y; x*y] , which can then be optionally concatenated with an embedded representation of the width of the span. Registered as a SpanExtractor with name \"bidirectional_endpoint\".","title":"BidirectionalEndpointSpanExtractor"},{"location":"api/modules/span_extractors/bidirectional_endpoint_span_extractor/#get_output_dim","text":"class BidirectionalEndpointSpanExtractor ( SpanExtractorWithSpanWidthEmbedding ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/span_extractors/endpoint_span_extractor/","text":"allennlp .modules .span_extractors .endpoint_span_extractor [SOURCE] EndpointSpanExtractor \u00b6 @SpanExtractor . register ( \"endpoint\" ) class EndpointSpanExtractor ( SpanExtractorWithSpanWidthEmbedding ): | def __init__ ( | self , | input_dim : int , | combination : str = \"x,y\" , | num_width_embeddings : int = None , | span_width_embedding_dim : int = None , | bucket_widths : bool = False , | use_exclusive_start_indices : bool = False | ) -> None Represents spans as a combination of the embeddings of their endpoints. Additionally, the width of the spans can be embedded and concatenated on to the final combination. The following types of representation are supported, assuming that x = span_start_embeddings and y = span_end_embeddings . x , y , x*y , x+y , x-y , x/y , where each of those binary operations is performed elementwise. You can list as many combinations as you want, comma separated. For example, you might give x,y,x*y as the combination parameter to this class. The computed similarity function would then be [x; y; x*y] , which can then be optionally concatenated with an embedded representation of the width of the span. Registered as a SpanExtractor with name \"endpoint\". Parameters \u00b6 input_dim : int The final dimension of the sequence_tensor . combination : str , optional (default = \"x,y\" ) The method used to combine the start_embedding and end_embedding representations. See above for a full description. num_width_embeddings : int , optional (default = None ) Specifies the number of buckets to use when representing span width features. span_width_embedding_dim : int , optional (default = None ) The embedding size for the span_width features. bucket_widths : bool , optional (default = False ) Whether to bucket the span widths into log-space buckets. If False , the raw span widths are used. use_exclusive_start_indices : bool , optional (default = False ) If True , the start indices extracted are converted to exclusive indices. Sentinels are used to represent exclusive span indices for the elements in the first position in the sequence (as the exclusive indices for these elements are outside of the the sequence boundary) so that start indices can be exclusive. NOTE: This option can be helpful to avoid the pathological case in which you want span differences for length 1 spans - if you use inclusive indices, you will end up with an x - x operation for length 1 spans, which is not good. get_output_dim \u00b6 class EndpointSpanExtractor ( SpanExtractorWithSpanWidthEmbedding ): | ... | def get_output_dim ( self ) -> int","title":"endpoint_span_extractor"},{"location":"api/modules/span_extractors/endpoint_span_extractor/#endpointspanextractor","text":"@SpanExtractor . register ( \"endpoint\" ) class EndpointSpanExtractor ( SpanExtractorWithSpanWidthEmbedding ): | def __init__ ( | self , | input_dim : int , | combination : str = \"x,y\" , | num_width_embeddings : int = None , | span_width_embedding_dim : int = None , | bucket_widths : bool = False , | use_exclusive_start_indices : bool = False | ) -> None Represents spans as a combination of the embeddings of their endpoints. Additionally, the width of the spans can be embedded and concatenated on to the final combination. The following types of representation are supported, assuming that x = span_start_embeddings and y = span_end_embeddings . x , y , x*y , x+y , x-y , x/y , where each of those binary operations is performed elementwise. You can list as many combinations as you want, comma separated. For example, you might give x,y,x*y as the combination parameter to this class. The computed similarity function would then be [x; y; x*y] , which can then be optionally concatenated with an embedded representation of the width of the span. Registered as a SpanExtractor with name \"endpoint\".","title":"EndpointSpanExtractor"},{"location":"api/modules/span_extractors/endpoint_span_extractor/#get_output_dim","text":"class EndpointSpanExtractor ( SpanExtractorWithSpanWidthEmbedding ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/span_extractors/max_pooling_span_extractor/","text":"allennlp .modules .span_extractors .max_pooling_span_extractor [SOURCE] MaxPoolingSpanExtractor \u00b6 @SpanExtractor . register ( \"max_pooling\" ) class MaxPoolingSpanExtractor ( SpanExtractorWithSpanWidthEmbedding ): | def __init__ ( | self , | input_dim : int , | num_width_embeddings : int = None , | span_width_embedding_dim : int = None , | bucket_widths : bool = False | ) -> None Represents spans through the application of a dimension-wise max-pooling operation. Given a span x_i, ..., x_j with i,j as span_start and span_end, each dimension d of the resulting span s is computed via s_d = max(x_id, ..., x_jd). Elements masked-out by sequence_mask are ignored when max-pooling is computed. Span representations of masked out span_indices by span_mask are set to '0.' Registered as a SpanExtractor with name \"max_pooling\". Parameters \u00b6 input_dim : int The final dimension of the sequence_tensor . num_width_embeddings : int , optional (default = None ) Specifies the number of buckets to use when representing span width features. span_width_embedding_dim : int , optional (default = None ) The embedding size for the span_width features. bucket_widths : bool , optional (default = False ) Whether to bucket the span widths into log-space buckets. If False , the raw span widths are used. Returns \u00b6 max_pooling_text_embeddings : torch.FloatTensor . A tensor of shape (batch_size, num_spans, input_dim), which each span representation is the result of a max-pooling operation. get_output_dim \u00b6 class MaxPoolingSpanExtractor ( SpanExtractorWithSpanWidthEmbedding ): | ... | def get_output_dim ( self ) -> int","title":"max_pooling_span_extractor"},{"location":"api/modules/span_extractors/max_pooling_span_extractor/#maxpoolingspanextractor","text":"@SpanExtractor . register ( \"max_pooling\" ) class MaxPoolingSpanExtractor ( SpanExtractorWithSpanWidthEmbedding ): | def __init__ ( | self , | input_dim : int , | num_width_embeddings : int = None , | span_width_embedding_dim : int = None , | bucket_widths : bool = False | ) -> None Represents spans through the application of a dimension-wise max-pooling operation. Given a span x_i, ..., x_j with i,j as span_start and span_end, each dimension d of the resulting span s is computed via s_d = max(x_id, ..., x_jd). Elements masked-out by sequence_mask are ignored when max-pooling is computed. Span representations of masked out span_indices by span_mask are set to '0.' Registered as a SpanExtractor with name \"max_pooling\".","title":"MaxPoolingSpanExtractor"},{"location":"api/modules/span_extractors/max_pooling_span_extractor/#get_output_dim","text":"class MaxPoolingSpanExtractor ( SpanExtractorWithSpanWidthEmbedding ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/span_extractors/self_attentive_span_extractor/","text":"allennlp .modules .span_extractors .self_attentive_span_extractor [SOURCE] SelfAttentiveSpanExtractor \u00b6 @SpanExtractor . register ( \"self_attentive\" ) class SelfAttentiveSpanExtractor ( SpanExtractorWithSpanWidthEmbedding ): | def __init__ ( | self , | input_dim : int , | num_width_embeddings : int = None , | span_width_embedding_dim : int = None , | bucket_widths : bool = False | ) -> None Computes span representations by generating an unnormalized attention score for each word in the document. Spans representations are computed with respect to these scores by normalising the attention scores for words inside the span. Given these attention distributions over every span, this module weights the corresponding vector representations of the words in the span by this distribution, returning a weighted representation of each span. Registered as a SpanExtractor with name \"self_attentive\". Parameters \u00b6 input_dim : int The final dimension of the sequence_tensor . num_width_embeddings : int , optional (default = None ) Specifies the number of buckets to use when representing span width features. span_width_embedding_dim : int , optional (default = None ) The embedding size for the span_width features. bucket_widths : bool , optional (default = False ) Whether to bucket the span widths into log-space buckets. If False , the raw span widths are used. Returns \u00b6 attended_text_embeddings : torch.FloatTensor . A tensor of shape (batch_size, num_spans, input_dim), which each span representation is formed by locally normalising a global attention over the sequence. The only way in which the attention distribution differs over different spans is in the set of words over which they are normalized. get_output_dim \u00b6 class SelfAttentiveSpanExtractor ( SpanExtractorWithSpanWidthEmbedding ): | ... | def get_output_dim ( self ) -> int","title":"self_attentive_span_extractor"},{"location":"api/modules/span_extractors/self_attentive_span_extractor/#selfattentivespanextractor","text":"@SpanExtractor . register ( \"self_attentive\" ) class SelfAttentiveSpanExtractor ( SpanExtractorWithSpanWidthEmbedding ): | def __init__ ( | self , | input_dim : int , | num_width_embeddings : int = None , | span_width_embedding_dim : int = None , | bucket_widths : bool = False | ) -> None Computes span representations by generating an unnormalized attention score for each word in the document. Spans representations are computed with respect to these scores by normalising the attention scores for words inside the span. Given these attention distributions over every span, this module weights the corresponding vector representations of the words in the span by this distribution, returning a weighted representation of each span. Registered as a SpanExtractor with name \"self_attentive\".","title":"SelfAttentiveSpanExtractor"},{"location":"api/modules/span_extractors/self_attentive_span_extractor/#get_output_dim","text":"class SelfAttentiveSpanExtractor ( SpanExtractorWithSpanWidthEmbedding ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/span_extractors/span_extractor/","text":"allennlp .modules .span_extractors .span_extractor [SOURCE] SpanExtractor \u00b6 class SpanExtractor ( torch . nn . Module , Registrable ) Many NLP models deal with representations of spans inside a sentence. SpanExtractors define methods for extracting and representing spans from a sentence. SpanExtractors take a sequence tensor of shape (batch_size, timesteps, embedding_dim) and indices of shape (batch_size, num_spans, 2) and return a tensor of shape (batch_size, num_spans, ...), forming some representation of the spans. forward \u00b6 class SpanExtractor ( torch . nn . Module , Registrable ): | ... | def forward ( | self , | sequence_tensor : torch . FloatTensor , | span_indices : torch . LongTensor , | sequence_mask : torch . BoolTensor = None , | span_indices_mask : torch . BoolTensor = None | ) Given a sequence tensor, extract spans and return representations of them. Span representation can be computed in many different ways, such as concatenation of the start and end spans, attention over the vectors contained inside the span, etc. Parameters \u00b6 sequence_tensor : torch.FloatTensor A tensor of shape (batch_size, sequence_length, embedding_size) representing an embedded sequence of words. span_indices : torch.LongTensor A tensor of shape (batch_size, num_spans, 2) , where the last dimension represents the inclusive start and end indices of the span to be extracted from the sequence_tensor . sequence_mask : torch.BoolTensor , optional (default = None ) A tensor of shape (batch_size, sequence_length) representing padded elements of the sequence. span_indices_mask : torch.BoolTensor , optional (default = None ) A tensor of shape (batch_size, num_spans) representing the valid spans in the indices tensor. This mask is optional because sometimes it's easier to worry about masking after calling this function, rather than passing a mask directly. Returns \u00b6 A tensor of shape (batch_size, num_spans, embedded_span_size) , where embedded_span_size depends on the way spans are represented. get_input_dim \u00b6 class SpanExtractor ( torch . nn . Module , Registrable ): | ... | def get_input_dim ( self ) -> int Returns the expected final dimension of the sequence_tensor . get_output_dim \u00b6 class SpanExtractor ( torch . nn . Module , Registrable ): | ... | def get_output_dim ( self ) -> int Returns the expected final dimension of the returned span representation.","title":"span_extractor"},{"location":"api/modules/span_extractors/span_extractor/#spanextractor","text":"class SpanExtractor ( torch . nn . Module , Registrable ) Many NLP models deal with representations of spans inside a sentence. SpanExtractors define methods for extracting and representing spans from a sentence. SpanExtractors take a sequence tensor of shape (batch_size, timesteps, embedding_dim) and indices of shape (batch_size, num_spans, 2) and return a tensor of shape (batch_size, num_spans, ...), forming some representation of the spans.","title":"SpanExtractor"},{"location":"api/modules/span_extractors/span_extractor/#forward","text":"class SpanExtractor ( torch . nn . Module , Registrable ): | ... | def forward ( | self , | sequence_tensor : torch . FloatTensor , | span_indices : torch . LongTensor , | sequence_mask : torch . BoolTensor = None , | span_indices_mask : torch . BoolTensor = None | ) Given a sequence tensor, extract spans and return representations of them. Span representation can be computed in many different ways, such as concatenation of the start and end spans, attention over the vectors contained inside the span, etc.","title":"forward"},{"location":"api/modules/span_extractors/span_extractor/#get_input_dim","text":"class SpanExtractor ( torch . nn . Module , Registrable ): | ... | def get_input_dim ( self ) -> int Returns the expected final dimension of the sequence_tensor .","title":"get_input_dim"},{"location":"api/modules/span_extractors/span_extractor/#get_output_dim","text":"class SpanExtractor ( torch . nn . Module , Registrable ): | ... | def get_output_dim ( self ) -> int Returns the expected final dimension of the returned span representation.","title":"get_output_dim"},{"location":"api/modules/span_extractors/span_extractor_with_span_width_embedding/","text":"allennlp .modules .span_extractors .span_extractor_with_span_width_embedding [SOURCE] SpanExtractorWithSpanWidthEmbedding \u00b6 class SpanExtractorWithSpanWidthEmbedding ( SpanExtractor ): | def __init__ ( | self , | input_dim : int , | num_width_embeddings : int = None , | span_width_embedding_dim : int = None , | bucket_widths : bool = False | ) -> None SpanExtractorWithSpanWidthEmbedding implements some common code for span extractors which will need to embed span width. Specifically, we initiate the span width embedding matrix and other attributes in __init__ , leave an _embed_spans method that can be implemented to compute span embeddings in different ways, and in forward we concatenate span embeddings returned by _embed_spans with span width embeddings to form the final span representations. We keep SpanExtractor as a purely abstract base class, just in case someone wants to build a totally different span extractor. Parameters \u00b6 input_dim : int The final dimension of the sequence_tensor . num_width_embeddings : int , optional (default = None ) Specifies the number of buckets to use when representing span width features. span_width_embedding_dim : int , optional (default = None ) The embedding size for the span_width features. bucket_widths : bool , optional (default = False ) Whether to bucket the span widths into log-space buckets. If False , the raw span widths are used. Returns \u00b6 span_embeddings : torch.FloatTensor . A tensor of shape (batch_size, num_spans, embedded_span_size) , where embedded_span_size depends on the way spans are represented. forward \u00b6 class SpanExtractorWithSpanWidthEmbedding ( SpanExtractor ): | ... | def forward ( | self , | sequence_tensor : torch . FloatTensor , | span_indices : torch . LongTensor , | sequence_mask : torch . BoolTensor = None , | span_indices_mask : torch . BoolTensor = None | ) Given a sequence tensor, extract spans, concatenate width embeddings when need and return representations of them. Parameters \u00b6 sequence_tensor : torch.FloatTensor A tensor of shape (batch_size, sequence_length, embedding_size) representing an embedded sequence of words. span_indices : torch.LongTensor A tensor of shape (batch_size, num_spans, 2) , where the last dimension represents the inclusive start and end indices of the span to be extracted from the sequence_tensor . sequence_mask : torch.BoolTensor , optional (default = None ) A tensor of shape (batch_size, sequence_length) representing padded elements of the sequence. span_indices_mask : torch.BoolTensor , optional (default = None ) A tensor of shape (batch_size, num_spans) representing the valid spans in the indices tensor. This mask is optional because sometimes it's easier to worry about masking after calling this function, rather than passing a mask directly. Returns \u00b6 A tensor of shape (batch_size, num_spans, embedded_span_size) , where embedded_span_size depends on the way spans are represented. get_input_dim \u00b6 class SpanExtractorWithSpanWidthEmbedding ( SpanExtractor ): | ... | def get_input_dim ( self ) -> int","title":"span_extractor_with_span_width_embedding"},{"location":"api/modules/span_extractors/span_extractor_with_span_width_embedding/#spanextractorwithspanwidthembedding","text":"class SpanExtractorWithSpanWidthEmbedding ( SpanExtractor ): | def __init__ ( | self , | input_dim : int , | num_width_embeddings : int = None , | span_width_embedding_dim : int = None , | bucket_widths : bool = False | ) -> None SpanExtractorWithSpanWidthEmbedding implements some common code for span extractors which will need to embed span width. Specifically, we initiate the span width embedding matrix and other attributes in __init__ , leave an _embed_spans method that can be implemented to compute span embeddings in different ways, and in forward we concatenate span embeddings returned by _embed_spans with span width embeddings to form the final span representations. We keep SpanExtractor as a purely abstract base class, just in case someone wants to build a totally different span extractor.","title":"SpanExtractorWithSpanWidthEmbedding"},{"location":"api/modules/span_extractors/span_extractor_with_span_width_embedding/#forward","text":"class SpanExtractorWithSpanWidthEmbedding ( SpanExtractor ): | ... | def forward ( | self , | sequence_tensor : torch . FloatTensor , | span_indices : torch . LongTensor , | sequence_mask : torch . BoolTensor = None , | span_indices_mask : torch . BoolTensor = None | ) Given a sequence tensor, extract spans, concatenate width embeddings when need and return representations of them.","title":"forward"},{"location":"api/modules/span_extractors/span_extractor_with_span_width_embedding/#get_input_dim","text":"class SpanExtractorWithSpanWidthEmbedding ( SpanExtractor ): | ... | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"api/modules/text_field_embedders/basic_text_field_embedder/","text":"allennlp .modules .text_field_embedders .basic_text_field_embedder [SOURCE] BasicTextFieldEmbedder \u00b6 @TextFieldEmbedder . register ( \"basic\" ) class BasicTextFieldEmbedder ( TextFieldEmbedder ): | def __init__ ( self , token_embedders : Dict [ str , TokenEmbedder ]) -> None This is a TextFieldEmbedder that wraps a collection of TokenEmbedder objects. Each TokenEmbedder embeds or encodes the representation output from one allennlp.data.TokenIndexer . As the data produced by a allennlp.data.fields.TextField is a dictionary mapping names to these representations, we take TokenEmbedders with corresponding names. Each TokenEmbedders embeds its input, and the result is concatenated in an arbitrary (but consistent) order. Registered as a TextFieldEmbedder with name \"basic\", which is also the default. Parameters \u00b6 token_embedders : Dict[str, TokenEmbedder] A dictionary mapping token embedder names to implementations. These names should match the corresponding indexer used to generate the tensor passed to the TokenEmbedder. get_output_dim \u00b6 class BasicTextFieldEmbedder ( TextFieldEmbedder ): | ... | def get_output_dim ( self ) -> int forward \u00b6 class BasicTextFieldEmbedder ( TextFieldEmbedder ): | ... | def forward ( | self , | text_field_input : TextFieldTensors , | num_wrapping_dims : int = 0 , | ** kwargs | ) -> torch . Tensor","title":"basic_text_field_embedder"},{"location":"api/modules/text_field_embedders/basic_text_field_embedder/#basictextfieldembedder","text":"@TextFieldEmbedder . register ( \"basic\" ) class BasicTextFieldEmbedder ( TextFieldEmbedder ): | def __init__ ( self , token_embedders : Dict [ str , TokenEmbedder ]) -> None This is a TextFieldEmbedder that wraps a collection of TokenEmbedder objects. Each TokenEmbedder embeds or encodes the representation output from one allennlp.data.TokenIndexer . As the data produced by a allennlp.data.fields.TextField is a dictionary mapping names to these representations, we take TokenEmbedders with corresponding names. Each TokenEmbedders embeds its input, and the result is concatenated in an arbitrary (but consistent) order. Registered as a TextFieldEmbedder with name \"basic\", which is also the default.","title":"BasicTextFieldEmbedder"},{"location":"api/modules/text_field_embedders/basic_text_field_embedder/#get_output_dim","text":"class BasicTextFieldEmbedder ( TextFieldEmbedder ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/text_field_embedders/basic_text_field_embedder/#forward","text":"class BasicTextFieldEmbedder ( TextFieldEmbedder ): | ... | def forward ( | self , | text_field_input : TextFieldTensors , | num_wrapping_dims : int = 0 , | ** kwargs | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/text_field_embedders/text_field_embedder/","text":"allennlp .modules .text_field_embedders .text_field_embedder [SOURCE] TextFieldEmbedder \u00b6 class TextFieldEmbedder ( torch . nn . Module , Registrable ) A TextFieldEmbedder is a Module that takes as input the DataArray produced by a TextField and returns as output an embedded representation of the tokens in that field. The DataArrays produced by TextFields are dictionaries with named representations, like \"words\" and \"characters\". When you create a TextField , you pass in a dictionary of TokenIndexer objects, telling the field how exactly the tokens in the field should be represented. This class changes the type signature of Module.forward , restricting TextFieldEmbedders to take inputs corresponding to a single TextField , which is a dictionary of tensors with the same names as were passed to the TextField . We also add a method to the basic Module API: get_output_dim() . You might need this if you want to construct a Linear layer using the output of this embedder, for instance. default_implementation \u00b6 class TextFieldEmbedder ( torch . nn . Module , Registrable ): | ... | default_implementation = \"basic\" forward \u00b6 class TextFieldEmbedder ( torch . nn . Module , Registrable ): | ... | def forward ( | self , | text_field_input : TextFieldTensors , | num_wrapping_dims : int = 0 , | ** kwargs | ) -> torch . Tensor Parameters \u00b6 text_field_input : TextFieldTensors A dictionary that was the output of a call to TextField.as_tensor . Each tensor in here is assumed to have a shape roughly similar to (batch_size, sequence_length) (perhaps with an extra trailing dimension for the characters in each token). num_wrapping_dims : int , optional (default = 0 ) If you have a ListField[TextField] that created the text_field_input , you'll end up with tensors of shape (batch_size, wrapping_dim1, wrapping_dim2, ..., sequence_length) . This parameter tells us how many wrapping dimensions there are, so that we can correctly TimeDistribute the embedding of each named representation. get_output_dim \u00b6 class TextFieldEmbedder ( torch . nn . Module , Registrable ): | ... | def get_output_dim ( self ) -> int Returns the dimension of the vector representing each token in the output of this TextFieldEmbedder . This is not the shape of the returned tensor, but the last element of that shape.","title":"text_field_embedder"},{"location":"api/modules/text_field_embedders/text_field_embedder/#textfieldembedder","text":"class TextFieldEmbedder ( torch . nn . Module , Registrable ) A TextFieldEmbedder is a Module that takes as input the DataArray produced by a TextField and returns as output an embedded representation of the tokens in that field. The DataArrays produced by TextFields are dictionaries with named representations, like \"words\" and \"characters\". When you create a TextField , you pass in a dictionary of TokenIndexer objects, telling the field how exactly the tokens in the field should be represented. This class changes the type signature of Module.forward , restricting TextFieldEmbedders to take inputs corresponding to a single TextField , which is a dictionary of tensors with the same names as were passed to the TextField . We also add a method to the basic Module API: get_output_dim() . You might need this if you want to construct a Linear layer using the output of this embedder, for instance.","title":"TextFieldEmbedder"},{"location":"api/modules/text_field_embedders/text_field_embedder/#default_implementation","text":"class TextFieldEmbedder ( torch . nn . Module , Registrable ): | ... | default_implementation = \"basic\"","title":"default_implementation"},{"location":"api/modules/text_field_embedders/text_field_embedder/#forward","text":"class TextFieldEmbedder ( torch . nn . Module , Registrable ): | ... | def forward ( | self , | text_field_input : TextFieldTensors , | num_wrapping_dims : int = 0 , | ** kwargs | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/text_field_embedders/text_field_embedder/#get_output_dim","text":"class TextFieldEmbedder ( torch . nn . Module , Registrable ): | ... | def get_output_dim ( self ) -> int Returns the dimension of the vector representing each token in the output of this TextFieldEmbedder . This is not the shape of the returned tensor, but the last element of that shape.","title":"get_output_dim"},{"location":"api/modules/token_embedders/bag_of_word_counts_token_embedder/","text":"allennlp .modules .token_embedders .bag_of_word_counts_token_embedder [SOURCE] BagOfWordCountsTokenEmbedder \u00b6 @TokenEmbedder . register ( \"bag_of_word_counts\" ) class BagOfWordCountsTokenEmbedder ( TokenEmbedder ): | def __init__ ( | self , | vocab : Vocabulary , | vocab_namespace : str = \"tokens\" , | projection_dim : int = None , | ignore_oov : bool = False | ) -> None Represents a sequence of tokens as a bag of (discrete) word ids, as it was done in the pre-neural days. Each sequence gets a vector of length vocabulary size, where the i'th entry in the vector corresponds to number of times the i'th token in the vocabulary appears in the sequence. By default, we ignore padding tokens. Registered as a TokenEmbedder with name \"bag_of_word_counts\". Parameters \u00b6 vocab : Vocabulary vocab_namespace : str , optional (default = \"tokens\" ) namespace of vocabulary to embed projection_dim : int , optional (default = None ) if specified, will project the resulting bag of words representation to specified dimension. ignore_oov : bool , optional (default = False ) If true, we ignore the OOV token. get_output_dim \u00b6 class BagOfWordCountsTokenEmbedder ( TokenEmbedder ): | ... | def get_output_dim ( self ) forward \u00b6 class BagOfWordCountsTokenEmbedder ( TokenEmbedder ): | ... | def forward ( self , inputs : torch . Tensor ) -> torch . Tensor Parameters \u00b6 inputs : torch.Tensor Shape (batch_size, timesteps, sequence_length) of word ids representing the current batch. Returns \u00b6 torch.Tensor The bag-of-words representations for the input sequence, shape (batch_size, vocab_size)","title":"bag_of_word_counts_token_embedder"},{"location":"api/modules/token_embedders/bag_of_word_counts_token_embedder/#bagofwordcountstokenembedder","text":"@TokenEmbedder . register ( \"bag_of_word_counts\" ) class BagOfWordCountsTokenEmbedder ( TokenEmbedder ): | def __init__ ( | self , | vocab : Vocabulary , | vocab_namespace : str = \"tokens\" , | projection_dim : int = None , | ignore_oov : bool = False | ) -> None Represents a sequence of tokens as a bag of (discrete) word ids, as it was done in the pre-neural days. Each sequence gets a vector of length vocabulary size, where the i'th entry in the vector corresponds to number of times the i'th token in the vocabulary appears in the sequence. By default, we ignore padding tokens. Registered as a TokenEmbedder with name \"bag_of_word_counts\".","title":"BagOfWordCountsTokenEmbedder"},{"location":"api/modules/token_embedders/bag_of_word_counts_token_embedder/#get_output_dim","text":"class BagOfWordCountsTokenEmbedder ( TokenEmbedder ): | ... | def get_output_dim ( self )","title":"get_output_dim"},{"location":"api/modules/token_embedders/bag_of_word_counts_token_embedder/#forward","text":"class BagOfWordCountsTokenEmbedder ( TokenEmbedder ): | ... | def forward ( self , inputs : torch . Tensor ) -> torch . Tensor","title":"forward"},{"location":"api/modules/token_embedders/elmo_token_embedder/","text":"allennlp .modules .token_embedders .elmo_token_embedder [SOURCE] ElmoTokenEmbedder \u00b6 @TokenEmbedder . register ( \"elmo_token_embedder\" ) class ElmoTokenEmbedder ( TokenEmbedder ): | def __init__ ( | self , | options_file : str = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/\" | + \"elmo_2x4096_512_2048cnn_2xhighway_options.json\" , | weight_file : str = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/\" | + \"elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\" , | do_layer_norm : bool = False , | dropout : float = 0.5 , | requires_grad : bool = False , | projection_dim : int = None , | vocab_to_cache : List [ str ] = None , | scalar_mix_parameters : List [ float ] = None | ) -> None Compute a single layer of ELMo representations. This class serves as a convenience when you only want to use one layer of ELMo representations at the input of your network. It's essentially a wrapper around Elmo(num_output_representations=1, ...) Registered as a TokenEmbedder with name \"elmo_token_embedder\". Parameters \u00b6 options_file : str An ELMo JSON options file. weight_file : str An ELMo hdf5 weight file. do_layer_norm : bool , optional Should we apply layer normalization (passed to ScalarMix )? dropout : float , optional (default = 0.5 ) The dropout value to be applied to the ELMo representations. requires_grad : bool , optional If True, compute gradient of ELMo parameters for fine tuning. projection_dim : int , optional If given, we will project the ELMo embedding down to this dimension. We recommend that you try using ELMo with a lot of dropout and no projection first, but we have found a few cases where projection helps (particularly where there is very limited training data). vocab_to_cache : List[str] , optional A list of words to pre-compute and cache character convolutions for. If you use this option, the ElmoTokenEmbedder expects that you pass word indices of shape (batch_size, timesteps) to forward, instead of character indices. If you use this option and pass a word which wasn't pre-cached, this will break. scalar_mix_parameters : List[int] , optional (default = None ) If not None , use these scalar mix parameters to weight the representations produced by different layers. These mixing weights are not updated during training. The mixing weights here should be the unnormalized (i.e., pre-softmax) weights. So, if you wanted to use only the 1st layer of a 2-layer ELMo, you can set this to [-9e10, 1, -9e10 ]. get_output_dim \u00b6 class ElmoTokenEmbedder ( TokenEmbedder ): | ... | def get_output_dim ( self ) -> int forward \u00b6 class ElmoTokenEmbedder ( TokenEmbedder ): | ... | def forward ( | self , | elmo_tokens : torch . Tensor , | word_inputs : torch . Tensor = None | ) -> torch . Tensor Parameters \u00b6 elmo_tokens : torch.Tensor Shape (batch_size, timesteps, 50) of character ids representing the current batch. word_inputs : torch.Tensor , optional If you passed a cached vocab, you can in addition pass a tensor of shape (batch_size, timesteps) , which represent word ids which have been pre-cached. Returns \u00b6 torch.Tensor The ELMo representations for the input sequence, shape (batch_size, timesteps, embedding_dim)","title":"elmo_token_embedder"},{"location":"api/modules/token_embedders/elmo_token_embedder/#elmotokenembedder","text":"@TokenEmbedder . register ( \"elmo_token_embedder\" ) class ElmoTokenEmbedder ( TokenEmbedder ): | def __init__ ( | self , | options_file : str = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/\" | + \"elmo_2x4096_512_2048cnn_2xhighway_options.json\" , | weight_file : str = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/\" | + \"elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\" , | do_layer_norm : bool = False , | dropout : float = 0.5 , | requires_grad : bool = False , | projection_dim : int = None , | vocab_to_cache : List [ str ] = None , | scalar_mix_parameters : List [ float ] = None | ) -> None Compute a single layer of ELMo representations. This class serves as a convenience when you only want to use one layer of ELMo representations at the input of your network. It's essentially a wrapper around Elmo(num_output_representations=1, ...) Registered as a TokenEmbedder with name \"elmo_token_embedder\".","title":"ElmoTokenEmbedder"},{"location":"api/modules/token_embedders/elmo_token_embedder/#get_output_dim","text":"class ElmoTokenEmbedder ( TokenEmbedder ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/token_embedders/elmo_token_embedder/#forward","text":"class ElmoTokenEmbedder ( TokenEmbedder ): | ... | def forward ( | self , | elmo_tokens : torch . Tensor , | word_inputs : torch . Tensor = None | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/token_embedders/embedding/","text":"allennlp .modules .token_embedders .embedding [SOURCE] Embedding \u00b6 @TokenEmbedder . register ( \"embedding\" ) class Embedding ( TokenEmbedder ): | def __init__ ( | self , | embedding_dim : int , | num_embeddings : int = None , | projection_dim : int = None , | weight : torch . FloatTensor = None , | padding_index : int = None , | trainable : bool = True , | max_norm : float = None , | norm_type : float = 2.0 , | scale_grad_by_freq : bool = False , | sparse : bool = False , | vocab_namespace : str = \"tokens\" , | pretrained_file : str = None , | vocab : Vocabulary = None | ) -> None A more featureful embedding module than the default in Pytorch. Adds the ability to: 1. embed higher - or der input s 2. pre - specify the weight matrix 3. use a non - trainable embedding 4. project the resultant embeddings to some other dim ension ( which on ly makes sense with non - trainable embeddings ) . Note that if you are using our data API and are trying to embed a TextField , you should use a TextFieldEmbedder instead of using this directly. Registered as a TokenEmbedder with name \"embedding\". Parameters \u00b6 num_embeddings : int Size of the dictionary of embeddings (vocabulary size). embedding_dim : int The size of each embedding vector. projection_dim : int , optional (default = None ) If given, we add a projection layer after the embedding layer. This really only makes sense if trainable is False . weight : torch.FloatTensor , optional (default = None ) A pre-initialised weight matrix for the embedding lookup, allowing the use of pretrained vectors. padding_index : int , optional (default = None ) If given, pads the output with zeros whenever it encounters the index. trainable : bool , optional (default = True ) Whether or not to optimize the embedding parameters. max_norm : float , optional (default = None ) If given, will renormalize the embeddings to always have a norm lesser than this norm_type : float , optional (default = 2 ) The p of the p-norm to compute for the max_norm option scale_grad_by_freq : bool , optional (default = False ) If given, this will scale gradients by the frequency of the words in the mini-batch. sparse : bool , optional (default = False ) Whether or not the Pytorch backend should use a sparse representation of the embedding weight. vocab_namespace : str , optional (default = None ) In case of fine-tuning/transfer learning, the model's embedding matrix needs to be extended according to the size of extended-vocabulary. To be able to know how much to extend the embedding-matrix, it's necessary to know which vocab_namspace was used to construct it in the original training. We store vocab_namespace used during the original training as an attribute, so that it can be retrieved during fine-tuning. pretrained_file : str , optional (default = None ) Path to a file of word vectors to initialize the embedding matrix. It can be the path to a local file or a URL of a (cached) remote file. Two formats are supported: * hdf5 file - containing an embedding matrix in the form of a torch.Tensor; * text file - an utf-8 encoded text file with space separated fields. vocab : Vocabulary , optional (default = None ) Used to construct an embedding from a pretrained file. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"embedding\", it gets specified as a top-level parameter, then is passed in to this module separately. Returns \u00b6 An Embedding module. get_output_dim \u00b6 class Embedding ( TokenEmbedder ): | ... | def get_output_dim ( self ) -> int forward \u00b6 class Embedding ( TokenEmbedder ): | ... | def forward ( self , tokens : torch . Tensor ) -> torch . Tensor extend_vocab \u00b6 class Embedding ( TokenEmbedder ): | ... | def extend_vocab ( | self , | extended_vocab : Vocabulary , | vocab_namespace : str = None , | extension_pretrained_file : str = None , | model_path : str = None | ) Extends the embedding matrix according to the extended vocabulary. If extension_pretrained_file is available, it will be used for initializing the new words embeddings in the extended vocabulary; otherwise we will check if _pretrained_file attribute is already available. If none is available, they will be initialized with xavier uniform. Parameters \u00b6 extended_vocab : Vocabulary Vocabulary extended from original vocabulary used to construct this Embedding . vocab_namespace : str , optional (default = None ) In case you know what vocab_namespace should be used for extension, you can pass it. If not passed, it will check if vocab_namespace used at the time of Embedding construction is available. If so, this namespace will be used or else extend_vocab will be a no-op. extension_pretrained_file : str , optional (default = None ) A file containing pretrained embeddings can be specified here. It can be the path to a local file or an URL of a (cached) remote file. Check format details in from_params of Embedding class. model_path : str , optional (default = None ) Path traversing the model attributes upto this embedding module. Eg. \"_text_field_embedder.token_embedder_tokens\". This is only useful to give a helpful error message when extend_vocab is implicitly called by train or any other command. format_embeddings_file_uri \u00b6 def format_embeddings_file_uri ( main_file_path_or_url : str , path_inside_archive : Optional [ str ] = None ) -> str EmbeddingsFileURI \u00b6 class EmbeddingsFileURI ( NamedTuple ) main_file_uri \u00b6 class EmbeddingsFileURI ( NamedTuple ): | ... | main_file_uri : str = None path_inside_archive \u00b6 class EmbeddingsFileURI ( NamedTuple ): | ... | path_inside_archive : Optional [ str ] = None parse_embeddings_file_uri \u00b6 def parse_embeddings_file_uri ( uri : str ) -> \"EmbeddingsFileURI\" EmbeddingsTextFile \u00b6 class EmbeddingsTextFile ( Iterator [ str ]): | def __init__ ( | self , | file_uri : str , | encoding : str = DEFAULT_ENCODING , | cache_dir : str = None | ) -> None Utility class for opening embeddings text files. Handles various compression formats, as well as context management. Parameters \u00b6 file_uri : str It can be: a file system path or a URL of an eventually compressed text file or a zip/tar archive containing a single file. URI of the type (archive_path_or_url)#file_path_inside_archive if the text file is contained in a multi-file archive. encoding : str cache_dir : str DEFAULT_ENCODING \u00b6 class EmbeddingsTextFile ( Iterator [ str ]): | ... | DEFAULT_ENCODING = \"utf-8\" read \u00b6 class EmbeddingsTextFile ( Iterator [ str ]): | ... | def read ( self ) -> str readline \u00b6 class EmbeddingsTextFile ( Iterator [ str ]): | ... | def readline ( self ) -> str close \u00b6 class EmbeddingsTextFile ( Iterator [ str ]): | ... | def close ( self ) -> None __iter__ \u00b6 class EmbeddingsTextFile ( Iterator [ str ]): | ... | def __iter__ ( self ) -> \"EmbeddingsTextFile\"","title":"embedding"},{"location":"api/modules/token_embedders/embedding/#embedding","text":"@TokenEmbedder . register ( \"embedding\" ) class Embedding ( TokenEmbedder ): | def __init__ ( | self , | embedding_dim : int , | num_embeddings : int = None , | projection_dim : int = None , | weight : torch . FloatTensor = None , | padding_index : int = None , | trainable : bool = True , | max_norm : float = None , | norm_type : float = 2.0 , | scale_grad_by_freq : bool = False , | sparse : bool = False , | vocab_namespace : str = \"tokens\" , | pretrained_file : str = None , | vocab : Vocabulary = None | ) -> None A more featureful embedding module than the default in Pytorch. Adds the ability to: 1. embed higher - or der input s 2. pre - specify the weight matrix 3. use a non - trainable embedding 4. project the resultant embeddings to some other dim ension ( which on ly makes sense with non - trainable embeddings ) . Note that if you are using our data API and are trying to embed a TextField , you should use a TextFieldEmbedder instead of using this directly. Registered as a TokenEmbedder with name \"embedding\".","title":"Embedding"},{"location":"api/modules/token_embedders/embedding/#get_output_dim","text":"class Embedding ( TokenEmbedder ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/token_embedders/embedding/#forward","text":"class Embedding ( TokenEmbedder ): | ... | def forward ( self , tokens : torch . Tensor ) -> torch . Tensor","title":"forward"},{"location":"api/modules/token_embedders/embedding/#extend_vocab","text":"class Embedding ( TokenEmbedder ): | ... | def extend_vocab ( | self , | extended_vocab : Vocabulary , | vocab_namespace : str = None , | extension_pretrained_file : str = None , | model_path : str = None | ) Extends the embedding matrix according to the extended vocabulary. If extension_pretrained_file is available, it will be used for initializing the new words embeddings in the extended vocabulary; otherwise we will check if _pretrained_file attribute is already available. If none is available, they will be initialized with xavier uniform.","title":"extend_vocab"},{"location":"api/modules/token_embedders/embedding/#format_embeddings_file_uri","text":"def format_embeddings_file_uri ( main_file_path_or_url : str , path_inside_archive : Optional [ str ] = None ) -> str","title":"format_embeddings_file_uri"},{"location":"api/modules/token_embedders/embedding/#embeddingsfileuri","text":"class EmbeddingsFileURI ( NamedTuple )","title":"EmbeddingsFileURI"},{"location":"api/modules/token_embedders/embedding/#main_file_uri","text":"class EmbeddingsFileURI ( NamedTuple ): | ... | main_file_uri : str = None","title":"main_file_uri"},{"location":"api/modules/token_embedders/embedding/#path_inside_archive","text":"class EmbeddingsFileURI ( NamedTuple ): | ... | path_inside_archive : Optional [ str ] = None","title":"path_inside_archive"},{"location":"api/modules/token_embedders/embedding/#parse_embeddings_file_uri","text":"def parse_embeddings_file_uri ( uri : str ) -> \"EmbeddingsFileURI\"","title":"parse_embeddings_file_uri"},{"location":"api/modules/token_embedders/embedding/#embeddingstextfile","text":"class EmbeddingsTextFile ( Iterator [ str ]): | def __init__ ( | self , | file_uri : str , | encoding : str = DEFAULT_ENCODING , | cache_dir : str = None | ) -> None Utility class for opening embeddings text files. Handles various compression formats, as well as context management.","title":"EmbeddingsTextFile"},{"location":"api/modules/token_embedders/embedding/#default_encoding","text":"class EmbeddingsTextFile ( Iterator [ str ]): | ... | DEFAULT_ENCODING = \"utf-8\"","title":"DEFAULT_ENCODING"},{"location":"api/modules/token_embedders/embedding/#read","text":"class EmbeddingsTextFile ( Iterator [ str ]): | ... | def read ( self ) -> str","title":"read"},{"location":"api/modules/token_embedders/embedding/#readline","text":"class EmbeddingsTextFile ( Iterator [ str ]): | ... | def readline ( self ) -> str","title":"readline"},{"location":"api/modules/token_embedders/embedding/#close","text":"class EmbeddingsTextFile ( Iterator [ str ]): | ... | def close ( self ) -> None","title":"close"},{"location":"api/modules/token_embedders/embedding/#__iter__","text":"class EmbeddingsTextFile ( Iterator [ str ]): | ... | def __iter__ ( self ) -> \"EmbeddingsTextFile\"","title":"__iter__"},{"location":"api/modules/token_embedders/empty_embedder/","text":"allennlp .modules .token_embedders .empty_embedder [SOURCE] EmptyEmbedder \u00b6 @TokenEmbedder . register ( \"empty\" ) class EmptyEmbedder ( TokenEmbedder ): | def __init__ ( self ) -> None Assumes you want to completely ignore the output of a TokenIndexer for some reason, and does not return anything when asked to embed it. You should almost never need to use this; normally you would just not use a particular TokenIndexer . It's only in very rare cases, like simplicity in data processing for language modeling (where we use just one TextField to handle input embedding and computing target ids), where you might want to use this. Registered as a TokenEmbedder with name \"empty\". get_output_dim \u00b6 class EmptyEmbedder ( TokenEmbedder ): | ... | def get_output_dim ( self ) forward \u00b6 class EmptyEmbedder ( TokenEmbedder ): | ... | def forward ( self , * inputs , ** kwargs ) -> torch . Tensor","title":"empty_embedder"},{"location":"api/modules/token_embedders/empty_embedder/#emptyembedder","text":"@TokenEmbedder . register ( \"empty\" ) class EmptyEmbedder ( TokenEmbedder ): | def __init__ ( self ) -> None Assumes you want to completely ignore the output of a TokenIndexer for some reason, and does not return anything when asked to embed it. You should almost never need to use this; normally you would just not use a particular TokenIndexer . It's only in very rare cases, like simplicity in data processing for language modeling (where we use just one TextField to handle input embedding and computing target ids), where you might want to use this. Registered as a TokenEmbedder with name \"empty\".","title":"EmptyEmbedder"},{"location":"api/modules/token_embedders/empty_embedder/#get_output_dim","text":"class EmptyEmbedder ( TokenEmbedder ): | ... | def get_output_dim ( self )","title":"get_output_dim"},{"location":"api/modules/token_embedders/empty_embedder/#forward","text":"class EmptyEmbedder ( TokenEmbedder ): | ... | def forward ( self , * inputs , ** kwargs ) -> torch . Tensor","title":"forward"},{"location":"api/modules/token_embedders/pass_through_token_embedder/","text":"allennlp .modules .token_embedders .pass_through_token_embedder [SOURCE] PassThroughTokenEmbedder \u00b6 @TokenEmbedder . register ( \"pass_through\" ) class PassThroughTokenEmbedder ( TokenEmbedder ): | def __init__ ( self , hidden_dim : int ) -> None Assumes that the input is already vectorized in some way, and just returns it. Registered as a TokenEmbedder with name \"pass_through\". Parameters \u00b6 hidden_dim : int get_output_dim \u00b6 class PassThroughTokenEmbedder ( TokenEmbedder ): | ... | def get_output_dim ( self ) forward \u00b6 class PassThroughTokenEmbedder ( TokenEmbedder ): | ... | def forward ( self , tokens : torch . Tensor ) -> torch . Tensor","title":"pass_through_token_embedder"},{"location":"api/modules/token_embedders/pass_through_token_embedder/#passthroughtokenembedder","text":"@TokenEmbedder . register ( \"pass_through\" ) class PassThroughTokenEmbedder ( TokenEmbedder ): | def __init__ ( self , hidden_dim : int ) -> None Assumes that the input is already vectorized in some way, and just returns it. Registered as a TokenEmbedder with name \"pass_through\".","title":"PassThroughTokenEmbedder"},{"location":"api/modules/token_embedders/pass_through_token_embedder/#get_output_dim","text":"class PassThroughTokenEmbedder ( TokenEmbedder ): | ... | def get_output_dim ( self )","title":"get_output_dim"},{"location":"api/modules/token_embedders/pass_through_token_embedder/#forward","text":"class PassThroughTokenEmbedder ( TokenEmbedder ): | ... | def forward ( self , tokens : torch . Tensor ) -> torch . Tensor","title":"forward"},{"location":"api/modules/token_embedders/pretrained_transformer_embedder/","text":"allennlp .modules .token_embedders .pretrained_transformer_embedder [SOURCE] PretrainedTransformerEmbedder \u00b6 @TokenEmbedder . register ( \"pretrained_transformer\" ) class PretrainedTransformerEmbedder ( TokenEmbedder ): | def __init__ ( | self , | model_name : str , | * , max_length : int = None , | * , sub_module : str = None , | * , train_parameters : bool = True , | * , eval_mode : bool = False , | * , last_layer_only : bool = True , | * , override_weights_file : Optional [ str ] = None , | * , override_weights_strip_prefix : Optional [ str ] = None , | * , reinit_modules : Optional [ Union [ int , Tuple [ int , ... ], Tuple [ str , ... ]]] = None , | * , load_weights : bool = True , | * , gradient_checkpointing : Optional [ bool ] = None , | * , tokenizer_kwargs : Optional [ Dict [ str , Any ]] = None , | * , transformer_kwargs : Optional [ Dict [ str , Any ]] = None | ) -> None Uses a pretrained model from transformers as a TokenEmbedder . Registered as a TokenEmbedder with name \"pretrained_transformer\". Parameters \u00b6 model_name : str The name of the transformers model to use. Should be the same as the corresponding PretrainedTransformerIndexer . max_length : int , optional (default = None ) If positive, folds input token IDs into multiple segments of this length, pass them through the transformer model independently, and concatenate the final representations. Should be set to the same value as the max_length option on the PretrainedTransformerIndexer . sub_module : str , optional (default = None ) The name of a submodule of the transformer to be used as the embedder. Some transformers naturally act as embedders such as BERT. However, other models consist of encoder and decoder, in which case we just want to use the encoder. train_parameters : bool , optional (default = True ) If this is True , the transformer weights get updated during training. If this is False , the transformer weights are not updated during training. eval_mode : bool , optional (default = False ) If this is True , the model is always set to evaluation mode (e.g., the dropout is disabled and the batch normalization layer statistics are not updated). If this is False , such dropout and batch normalization layers are only set to evaluation mode when when the model is evaluating on development or test data. last_layer_only : bool , optional (default = True ) When True (the default), only the final layer of the pretrained transformer is taken for the embeddings. But if set to False , a scalar mix of all of the layers is used. override_weights_file : Optional[str] , optional (default = None ) If set, this specifies a file from which to load alternate weights that override the weights from huggingface. The file is expected to contain a PyTorch state_dict , created with torch.save() . override_weights_strip_prefix : Optional[str] , optional (default = None ) If set, strip the given prefix from the state dict when loading it. reinit_modules : Optional[Union[int, Tuple[int, ...], Tuple[str, ...]]] , optional (default = None ) If this is an integer, the last reinit_modules layers of the transformer will be re-initialized. If this is a tuple of integers, the layers indexed by reinit_modules will be re-initialized. Note, because the module structure of the transformer model_name can differ, we cannot guarantee that providing an integer or tuple of integers will work. If this fails, you can instead provide a tuple of strings, which will be treated as regexes and any module with a name matching the regex will be re-initialized. Re-initializing the last few layers of a pretrained transformer can reduce the instability of fine-tuning on small datasets and may improve performance (https://arxiv.org/abs/2006.05987v3). Has no effect if load_weights is False or override_weights_file is not None . load_weights : bool , optional (default = True ) Whether to load the pretrained weights. If you're loading your model/predictor from an AllenNLP archive it usually makes sense to set this to False (via the overrides parameter) to avoid unnecessarily caching and loading the original pretrained weights, since the archive will already contain all of the weights needed. gradient_checkpointing : bool , optional (default = None ) Enable or disable gradient checkpointing. tokenizer_kwargs : Dict[str, Any] , optional (default = None ) Dictionary with additional arguments for AutoTokenizer.from_pretrained . transformer_kwargs : Dict[str, Any] , optional (default = None ) Dictionary with additional arguments for AutoModel.from_pretrained . authorized_missing_keys \u00b6 class PretrainedTransformerEmbedder ( TokenEmbedder ): | ... | authorized_missing_keys = [ r \"position_ids$\" ] train \u00b6 class PretrainedTransformerEmbedder ( TokenEmbedder ): | ... | def train ( self , mode : bool = True ) get_output_dim \u00b6 class PretrainedTransformerEmbedder ( TokenEmbedder ): | ... | def get_output_dim ( self ) forward \u00b6 class PretrainedTransformerEmbedder ( TokenEmbedder ): | ... | def forward ( | self , | token_ids : torch . LongTensor , | mask : torch . BoolTensor , | type_ids : Optional [ torch . LongTensor ] = None , | segment_concat_mask : Optional [ torch . BoolTensor ] = None | ) -> torch . Tensor Parameters \u00b6 token_ids : torch.LongTensor Shape: [batch_size, num_wordpieces if max_length is None else num_segment_concat_wordpieces] . num_segment_concat_wordpieces is num_wordpieces plus special tokens inserted in the middle, e.g. the length of: \"[CLS] A B C [SEP] [CLS] D E F [SEP]\" (see indexer logic). mask : torch.BoolTensor Shape: [batch_size, num_wordpieces]. type_ids : Optional[torch.LongTensor] Shape: [batch_size, num_wordpieces if max_length is None else num_segment_concat_wordpieces] . segment_concat_mask : Optional[torch.BoolTensor] Shape: [batch_size, num_segment_concat_wordpieces] . Returns \u00b6 torch.Tensor Shape: [batch_size, num_wordpieces, embedding_size] .","title":"pretrained_transformer_embedder"},{"location":"api/modules/token_embedders/pretrained_transformer_embedder/#pretrainedtransformerembedder","text":"@TokenEmbedder . register ( \"pretrained_transformer\" ) class PretrainedTransformerEmbedder ( TokenEmbedder ): | def __init__ ( | self , | model_name : str , | * , max_length : int = None , | * , sub_module : str = None , | * , train_parameters : bool = True , | * , eval_mode : bool = False , | * , last_layer_only : bool = True , | * , override_weights_file : Optional [ str ] = None , | * , override_weights_strip_prefix : Optional [ str ] = None , | * , reinit_modules : Optional [ Union [ int , Tuple [ int , ... ], Tuple [ str , ... ]]] = None , | * , load_weights : bool = True , | * , gradient_checkpointing : Optional [ bool ] = None , | * , tokenizer_kwargs : Optional [ Dict [ str , Any ]] = None , | * , transformer_kwargs : Optional [ Dict [ str , Any ]] = None | ) -> None Uses a pretrained model from transformers as a TokenEmbedder . Registered as a TokenEmbedder with name \"pretrained_transformer\".","title":"PretrainedTransformerEmbedder"},{"location":"api/modules/token_embedders/pretrained_transformer_embedder/#authorized_missing_keys","text":"class PretrainedTransformerEmbedder ( TokenEmbedder ): | ... | authorized_missing_keys = [ r \"position_ids$\" ]","title":"authorized_missing_keys"},{"location":"api/modules/token_embedders/pretrained_transformer_embedder/#train","text":"class PretrainedTransformerEmbedder ( TokenEmbedder ): | ... | def train ( self , mode : bool = True )","title":"train"},{"location":"api/modules/token_embedders/pretrained_transformer_embedder/#get_output_dim","text":"class PretrainedTransformerEmbedder ( TokenEmbedder ): | ... | def get_output_dim ( self )","title":"get_output_dim"},{"location":"api/modules/token_embedders/pretrained_transformer_embedder/#forward","text":"class PretrainedTransformerEmbedder ( TokenEmbedder ): | ... | def forward ( | self , | token_ids : torch . LongTensor , | mask : torch . BoolTensor , | type_ids : Optional [ torch . LongTensor ] = None , | segment_concat_mask : Optional [ torch . BoolTensor ] = None | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/token_embedders/pretrained_transformer_mismatched_embedder/","text":"allennlp .modules .token_embedders .pretrained_transformer_mismatched_embedder [SOURCE] PretrainedTransformerMismatchedEmbedder \u00b6 @TokenEmbedder . register ( \"pretrained_transformer_mismatched\" ) class PretrainedTransformerMismatchedEmbedder ( TokenEmbedder ): | def __init__ ( | self , | model_name : str , | max_length : int = None , | sub_module : str = None , | train_parameters : bool = True , | last_layer_only : bool = True , | override_weights_file : Optional [ str ] = None , | override_weights_strip_prefix : Optional [ str ] = None , | load_weights : bool = True , | gradient_checkpointing : Optional [ bool ] = None , | tokenizer_kwargs : Optional [ Dict [ str , Any ]] = None , | transformer_kwargs : Optional [ Dict [ str , Any ]] = None , | sub_token_mode : Optional [ str ] = \"avg\" | ) -> None Use this embedder to embed wordpieces given by PretrainedTransformerMismatchedIndexer and to get word-level representations. Registered as a TokenEmbedder with name \"pretrained_transformer_mismatched\". Parameters \u00b6 model_name : str The name of the transformers model to use. Should be the same as the corresponding PretrainedTransformerMismatchedIndexer . max_length : int , optional (default = None ) If positive, folds input token IDs into multiple segments of this length, pass them through the transformer model independently, and concatenate the final representations. Should be set to the same value as the max_length option on the PretrainedTransformerMismatchedIndexer . sub_module : str , optional (default = None ) The name of a submodule of the transformer to be used as the embedder. Some transformers naturally act as embedders such as BERT. However, other models consist of encoder and decoder, in which case we just want to use the encoder. train_parameters : bool , optional (default = True ) If this is True , the transformer weights get updated during training. last_layer_only : bool , optional (default = True ) When True (the default), only the final layer of the pretrained transformer is taken for the embeddings. But if set to False , a scalar mix of all of the layers is used. override_weights_file : Optional[str] , optional (default = None ) If set, this specifies a file from which to load alternate weights that override the weights from huggingface. The file is expected to contain a PyTorch state_dict , created with torch.save() . override_weights_strip_prefix : Optional[str] , optional (default = None ) If set, strip the given prefix from the state dict when loading it. load_weights : bool , optional (default = True ) Whether to load the pretrained weights. If you're loading your model/predictor from an AllenNLP archive it usually makes sense to set this to False (via the overrides parameter) to avoid unnecessarily caching and loading the original pretrained weights, since the archive will already contain all of the weights needed. gradient_checkpointing : bool , optional (default = None ) Enable or disable gradient checkpointing. tokenizer_kwargs : Dict[str, Any] , optional (default = None ) Dictionary with additional arguments for AutoTokenizer.from_pretrained . transformer_kwargs : Dict[str, Any] , optional (default = None ) Dictionary with additional arguments for AutoModel.from_pretrained . sub_token_mode : Optional[str] , optional (default = avg ) If sub_token_mode is set to first , return first sub-token representation as word-level representation If sub_token_mode is set to avg , return average of all the sub-tokens representation as word-level representation If sub_token_mode is not specified it defaults to avg If invalid sub_token_mode is provided, throw ConfigurationError get_output_dim \u00b6 class PretrainedTransformerMismatchedEmbedder ( TokenEmbedder ): | ... | def get_output_dim ( self ) forward \u00b6 class PretrainedTransformerMismatchedEmbedder ( TokenEmbedder ): | ... | def forward ( | self , | token_ids : torch . LongTensor , | mask : torch . BoolTensor , | offsets : torch . LongTensor , | wordpiece_mask : torch . BoolTensor , | type_ids : Optional [ torch . LongTensor ] = None , | segment_concat_mask : Optional [ torch . BoolTensor ] = None | ) -> torch . Tensor Parameters \u00b6 token_ids : torch.LongTensor Shape: [batch_size, num_wordpieces] (for exception see PretrainedTransformerEmbedder ). mask : torch.BoolTensor Shape: [batch_size, num_orig_tokens]. offsets : torch.LongTensor Shape: [batch_size, num_orig_tokens, 2]. Maps indices for the original tokens, i.e. those given as input to the indexer, to a span in token_ids. token_ids[i][offsets[i][j][0]:offsets[i][j][1] + 1] corresponds to the original j-th token from the i-th batch. wordpiece_mask : torch.BoolTensor Shape: [batch_size, num_wordpieces]. type_ids : Optional[torch.LongTensor] Shape: [batch_size, num_wordpieces]. segment_concat_mask : Optional[torch.BoolTensor] See PretrainedTransformerEmbedder . Returns \u00b6 torch.Tensor Shape: [batch_size, num_orig_tokens, embedding_size].","title":"pretrained_transformer_mismatched_embedder"},{"location":"api/modules/token_embedders/pretrained_transformer_mismatched_embedder/#pretrainedtransformermismatchedembedder","text":"@TokenEmbedder . register ( \"pretrained_transformer_mismatched\" ) class PretrainedTransformerMismatchedEmbedder ( TokenEmbedder ): | def __init__ ( | self , | model_name : str , | max_length : int = None , | sub_module : str = None , | train_parameters : bool = True , | last_layer_only : bool = True , | override_weights_file : Optional [ str ] = None , | override_weights_strip_prefix : Optional [ str ] = None , | load_weights : bool = True , | gradient_checkpointing : Optional [ bool ] = None , | tokenizer_kwargs : Optional [ Dict [ str , Any ]] = None , | transformer_kwargs : Optional [ Dict [ str , Any ]] = None , | sub_token_mode : Optional [ str ] = \"avg\" | ) -> None Use this embedder to embed wordpieces given by PretrainedTransformerMismatchedIndexer and to get word-level representations. Registered as a TokenEmbedder with name \"pretrained_transformer_mismatched\".","title":"PretrainedTransformerMismatchedEmbedder"},{"location":"api/modules/token_embedders/pretrained_transformer_mismatched_embedder/#get_output_dim","text":"class PretrainedTransformerMismatchedEmbedder ( TokenEmbedder ): | ... | def get_output_dim ( self )","title":"get_output_dim"},{"location":"api/modules/token_embedders/pretrained_transformer_mismatched_embedder/#forward","text":"class PretrainedTransformerMismatchedEmbedder ( TokenEmbedder ): | ... | def forward ( | self , | token_ids : torch . LongTensor , | mask : torch . BoolTensor , | offsets : torch . LongTensor , | wordpiece_mask : torch . BoolTensor , | type_ids : Optional [ torch . LongTensor ] = None , | segment_concat_mask : Optional [ torch . BoolTensor ] = None | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/token_embedders/token_characters_encoder/","text":"allennlp .modules .token_embedders .token_characters_encoder [SOURCE] TokenCharactersEncoder \u00b6 @TokenEmbedder . register ( \"character_encoding\" ) class TokenCharactersEncoder ( TokenEmbedder ): | def __init__ ( | self , | embedding : Embedding , | encoder : Seq2VecEncoder , | dropout : float = 0.0 | ) -> None A TokenCharactersEncoder takes the output of a TokenCharactersIndexer , which is a tensor of shape (batch_size, num_tokens, num_characters), embeds the characters, runs a token-level encoder, and returns the result, which is a tensor of shape (batch_size, num_tokens, encoding_dim). We also optionally apply dropout after the token-level encoder. We take the embedding and encoding modules as input, so this class is itself quite simple. Registered as a TokenEmbedder with name \"character_encoding\". get_output_dim \u00b6 class TokenCharactersEncoder ( TokenEmbedder ): | ... | def get_output_dim ( self ) -> int forward \u00b6 class TokenCharactersEncoder ( TokenEmbedder ): | ... | def forward ( self , token_characters : torch . Tensor ) -> torch . Tensor","title":"token_characters_encoder"},{"location":"api/modules/token_embedders/token_characters_encoder/#tokencharactersencoder","text":"@TokenEmbedder . register ( \"character_encoding\" ) class TokenCharactersEncoder ( TokenEmbedder ): | def __init__ ( | self , | embedding : Embedding , | encoder : Seq2VecEncoder , | dropout : float = 0.0 | ) -> None A TokenCharactersEncoder takes the output of a TokenCharactersIndexer , which is a tensor of shape (batch_size, num_tokens, num_characters), embeds the characters, runs a token-level encoder, and returns the result, which is a tensor of shape (batch_size, num_tokens, encoding_dim). We also optionally apply dropout after the token-level encoder. We take the embedding and encoding modules as input, so this class is itself quite simple. Registered as a TokenEmbedder with name \"character_encoding\".","title":"TokenCharactersEncoder"},{"location":"api/modules/token_embedders/token_characters_encoder/#get_output_dim","text":"class TokenCharactersEncoder ( TokenEmbedder ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/token_embedders/token_characters_encoder/#forward","text":"class TokenCharactersEncoder ( TokenEmbedder ): | ... | def forward ( self , token_characters : torch . Tensor ) -> torch . Tensor","title":"forward"},{"location":"api/modules/token_embedders/token_embedder/","text":"allennlp .modules .token_embedders .token_embedder [SOURCE] TokenEmbedder \u00b6 class TokenEmbedder ( torch . nn . Module , Registrable ) A TokenEmbedder is a Module that takes as input a tensor with integer ids that have been output from a TokenIndexer and outputs a vector per token in the input. The input typically has shape (batch_size, num_tokens) or (batch_size, num_tokens, num_characters) , and the output is of shape (batch_size, num_tokens, output_dim) . The simplest TokenEmbedder is just an embedding layer, but for character-level input, it could also be some kind of character encoder. We add a single method to the basic Module API: get_output_dim() . This lets us more easily compute output dimensions for the TextFieldEmbedder , which we might need when defining model parameters such as LSTMs or linear layers, which need to know their input dimension before the layers are called. default_implementation \u00b6 class TokenEmbedder ( torch . nn . Module , Registrable ): | ... | default_implementation = \"embedding\" get_output_dim \u00b6 class TokenEmbedder ( torch . nn . Module , Registrable ): | ... | def get_output_dim ( self ) -> int Returns the final output dimension that this TokenEmbedder uses to represent each token. This is not the shape of the returned tensor, but the last element of that shape.","title":"token_embedder"},{"location":"api/modules/token_embedders/token_embedder/#tokenembedder","text":"class TokenEmbedder ( torch . nn . Module , Registrable ) A TokenEmbedder is a Module that takes as input a tensor with integer ids that have been output from a TokenIndexer and outputs a vector per token in the input. The input typically has shape (batch_size, num_tokens) or (batch_size, num_tokens, num_characters) , and the output is of shape (batch_size, num_tokens, output_dim) . The simplest TokenEmbedder is just an embedding layer, but for character-level input, it could also be some kind of character encoder. We add a single method to the basic Module API: get_output_dim() . This lets us more easily compute output dimensions for the TextFieldEmbedder , which we might need when defining model parameters such as LSTMs or linear layers, which need to know their input dimension before the layers are called.","title":"TokenEmbedder"},{"location":"api/modules/token_embedders/token_embedder/#default_implementation","text":"class TokenEmbedder ( torch . nn . Module , Registrable ): | ... | default_implementation = \"embedding\"","title":"default_implementation"},{"location":"api/modules/token_embedders/token_embedder/#get_output_dim","text":"class TokenEmbedder ( torch . nn . Module , Registrable ): | ... | def get_output_dim ( self ) -> int Returns the final output dimension that this TokenEmbedder uses to represent each token. This is not the shape of the returned tensor, but the last element of that shape.","title":"get_output_dim"},{"location":"api/modules/transformer/activation_layer/","text":"allennlp .modules .transformer .activation_layer [SOURCE] ActivationLayer \u00b6 class ActivationLayer ( TransformerModule , FromParams ): | def __init__ ( | self , | hidden_size : int , | intermediate_size : int , | activation : Union [ str , torch . nn . Module ], | pool : bool = False | ) get_output_dim \u00b6 class ActivationLayer ( TransformerModule , FromParams ): | ... | def get_output_dim ( self ) -> int forward \u00b6 class ActivationLayer ( TransformerModule , FromParams ): | ... | def forward ( self , hidden_states )","title":"activation_layer"},{"location":"api/modules/transformer/activation_layer/#activationlayer","text":"class ActivationLayer ( TransformerModule , FromParams ): | def __init__ ( | self , | hidden_size : int , | intermediate_size : int , | activation : Union [ str , torch . nn . Module ], | pool : bool = False | )","title":"ActivationLayer"},{"location":"api/modules/transformer/activation_layer/#get_output_dim","text":"class ActivationLayer ( TransformerModule , FromParams ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/transformer/activation_layer/#forward","text":"class ActivationLayer ( TransformerModule , FromParams ): | ... | def forward ( self , hidden_states )","title":"forward"},{"location":"api/modules/transformer/attention_module/","text":"allennlp .modules .transformer .attention_module [SOURCE] AttentionOutput \u00b6 @dataclass class AttentionOutput Encapsulates the outputs of the Attention module. hidden_states \u00b6 class AttentionOutput : | ... | hidden_states : FloatT = None key_value_state \u00b6 class AttentionOutput : | ... | key_value_state : Optional [ Tuple [ FloatT , FloatT ]] = None position_bias \u00b6 class AttentionOutput : | ... | position_bias : Optional [ FloatT ] = None attention_probs \u00b6 class AttentionOutput : | ... | attention_probs : Optional [ FloatT ] = None AttentionModule \u00b6 class AttentionModule ( TransformerModule , FromParams ): | def __init__ ( | self , | hidden_size : int = 512 , | attention_head_size : int = 64 , | num_attention_heads : int = 8 , | scoring_func : str = \"scaled_dot_product\" , | output_linear : bool = False , | dropout : float = 0.0 , | bias : bool = True , | normalize_weights : bool = False , | is_decoder : bool = False , | is_cross_attention : bool = False , | relative_attention_num_buckets : Optional [ int ] = None | ) This module computes self-attention (or cross-attention), similar to the architecture in BERT. Details in the paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin et al, 2019 Additionally, it has the following functionality: the attention scoring function can be specified. it can be used in encoders as well as decoders. position_bias can be used, which makes it suitable for T5-style attention as well. Parameters \u00b6 hidden_size : int , optional (default = 512 ) The size of the expected input tensor. attention_head_size : int , optional (default = 64 ) The size of a single attention head. num_attention_heads : int , optional (default = 8 ) The number of attention heads. scoring_func : str , optional (default = scaled_dot_product ) The name of the attention-calculating function to be used. Eg. additive , linear , etc. For a complete list, please check matrix_attention . output_linear : bool , optional (default = False ) Whether to add an additional output linear layer at the end. dropout : float , optional (default = 0.0 ) The dropout probability. bias : bool , optional (default = True ) Whether to include bias weights in query, key, value (and output) linear layers. normalize_weights : bool , optional (default = False ) Whether to normalize the initial weights. is_decoder : bool , optional (default = False ) Whether this module is being used in a decoder stack or not. is_cross_attention : bool , optional (default = False ) Whether this module is being used for cross-attention in a decoder stack or not. If is_cross_attention is True , then is_decoder must also be True . relative_attention_num_buckets : int , optional (default = None ) The number of buckets to use in relative attention; if None , relative attention will not be applied. forward \u00b6 class AttentionModule ( TransformerModule , FromParams ): | ... | def forward ( | self , | query_states : torch . Tensor , | past_key_states : Optional [ torch . Tensor ] = None , | past_value_states : Optional [ torch . Tensor ] = None , | attention_mask : Optional [ torch . BoolTensor ] = None , | source_states : Optional [ torch . Tensor ] = None , | source_attention_mask : Optional [ torch . BoolTensor ] = None , | head_mask : Optional [ torch . Tensor ] = None , | position_bias : Optional [ torch . Tensor ] = None , | output_attentions : bool = False , | use_cache : bool = False , | query_length : Optional [ int ] = None | ) Parameters \u00b6 query_states : torch.Tensor Shape batch_size x seq_len x hidden_dim past_key_states : torch.Tensor , optional Shape batch_size x seq_len x hidden_dim These are the key_states from the previous step of the decoder. past_value_states : torch.Tensor , optional Shape batch_size x seq_len x hidden_dim These are the value_states from the previous step of the decoder. attention_mask : torch.BoolTensor , optional Shape batch_size x seq_len source_states : torch.Tensor , optional Shape batch_size x source_seq_len x hidden_dim This is from the final state of attention over the source (encoder); it is passed when this module is being used for cross-attention. source_attention_mask : torch.BoolTensor , optional Shape batch_size x source_seq_len head_mask : torch.BoolTensor , optional position_bias : torch.Tensor , optional output_attentions : bool Whether to also return the attention probabilities, default = False Note source_states needs to be passed in case of cross-attention. compute_bias \u00b6 class AttentionModule ( TransformerModule , FromParams ): | ... | def compute_bias ( self , query_length : int , key_length : int ) -> FloatT Compute binned relative position bias T5Attention \u00b6 class T5Attention ( AttentionModule ): | def __init__ ( | self , | is_decoder : bool = False , | hidden_size : int = 512 , | key_value_proj_dim : int = 64 , | num_heads : int = 8 , | has_relative_attention_bias : bool = False , | relative_attention_num_buckets : int = 32 , | dropout : float = 0.1 , | normalize : bool = True , | is_cross_attention : bool = False | ) forward \u00b6 class T5Attention ( AttentionModule ): | ... | def forward ( | self , | hidden_states : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None , | key_value_states : Optional [ FloatT ] = None , | position_bias : Optional [ FloatT ] = None , | past_key_value : Optional [ | Tuple [ FloatT , FloatT ] | ] = None , | layer_head_mask : Optional [ BoolT ] = None , | query_length : Optional [ int ] = None , | use_cache : bool = False , | output_attentions : bool = False | ) -> AttentionOutput Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states). SelfAttention \u00b6 class SelfAttention ( AttentionModule ): | def __init__ ( | self , | hidden_size : int , | num_attention_heads : int , | dropout : float = 0.0 , | scoring_func : str = \"scaled_dot_product\" , | output_linear : bool = False , | is_decoder : bool = False , | is_cross_attention : bool = False | ) This module computes the self-attention, similar to the architecture in BERT. Additionally, the attention scoring function can be specified. Details in the paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin et al, 2019 Parameters \u00b6 hidden_size : int num_attention_heads : int dropout : float , optional (default = 0.0 ) scoring_func : str , optional (default = scaled_dot_product ) The name of the attention-calculating function to be used. Eg. additive , linear , etc. For a complete list, please check attention .","title":"attention_module"},{"location":"api/modules/transformer/attention_module/#attentionoutput","text":"@dataclass class AttentionOutput Encapsulates the outputs of the Attention module.","title":"AttentionOutput"},{"location":"api/modules/transformer/attention_module/#hidden_states","text":"class AttentionOutput : | ... | hidden_states : FloatT = None","title":"hidden_states"},{"location":"api/modules/transformer/attention_module/#key_value_state","text":"class AttentionOutput : | ... | key_value_state : Optional [ Tuple [ FloatT , FloatT ]] = None","title":"key_value_state"},{"location":"api/modules/transformer/attention_module/#position_bias","text":"class AttentionOutput : | ... | position_bias : Optional [ FloatT ] = None","title":"position_bias"},{"location":"api/modules/transformer/attention_module/#attention_probs","text":"class AttentionOutput : | ... | attention_probs : Optional [ FloatT ] = None","title":"attention_probs"},{"location":"api/modules/transformer/attention_module/#attentionmodule","text":"class AttentionModule ( TransformerModule , FromParams ): | def __init__ ( | self , | hidden_size : int = 512 , | attention_head_size : int = 64 , | num_attention_heads : int = 8 , | scoring_func : str = \"scaled_dot_product\" , | output_linear : bool = False , | dropout : float = 0.0 , | bias : bool = True , | normalize_weights : bool = False , | is_decoder : bool = False , | is_cross_attention : bool = False , | relative_attention_num_buckets : Optional [ int ] = None | ) This module computes self-attention (or cross-attention), similar to the architecture in BERT. Details in the paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin et al, 2019 Additionally, it has the following functionality: the attention scoring function can be specified. it can be used in encoders as well as decoders. position_bias can be used, which makes it suitable for T5-style attention as well.","title":"AttentionModule"},{"location":"api/modules/transformer/attention_module/#forward","text":"class AttentionModule ( TransformerModule , FromParams ): | ... | def forward ( | self , | query_states : torch . Tensor , | past_key_states : Optional [ torch . Tensor ] = None , | past_value_states : Optional [ torch . Tensor ] = None , | attention_mask : Optional [ torch . BoolTensor ] = None , | source_states : Optional [ torch . Tensor ] = None , | source_attention_mask : Optional [ torch . BoolTensor ] = None , | head_mask : Optional [ torch . Tensor ] = None , | position_bias : Optional [ torch . Tensor ] = None , | output_attentions : bool = False , | use_cache : bool = False , | query_length : Optional [ int ] = None | )","title":"forward"},{"location":"api/modules/transformer/attention_module/#compute_bias","text":"class AttentionModule ( TransformerModule , FromParams ): | ... | def compute_bias ( self , query_length : int , key_length : int ) -> FloatT Compute binned relative position bias","title":"compute_bias"},{"location":"api/modules/transformer/attention_module/#t5attention","text":"class T5Attention ( AttentionModule ): | def __init__ ( | self , | is_decoder : bool = False , | hidden_size : int = 512 , | key_value_proj_dim : int = 64 , | num_heads : int = 8 , | has_relative_attention_bias : bool = False , | relative_attention_num_buckets : int = 32 , | dropout : float = 0.1 , | normalize : bool = True , | is_cross_attention : bool = False | )","title":"T5Attention"},{"location":"api/modules/transformer/attention_module/#forward_1","text":"class T5Attention ( AttentionModule ): | ... | def forward ( | self , | hidden_states : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None , | key_value_states : Optional [ FloatT ] = None , | position_bias : Optional [ FloatT ] = None , | past_key_value : Optional [ | Tuple [ FloatT , FloatT ] | ] = None , | layer_head_mask : Optional [ BoolT ] = None , | query_length : Optional [ int ] = None , | use_cache : bool = False , | output_attentions : bool = False | ) -> AttentionOutput Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).","title":"forward"},{"location":"api/modules/transformer/attention_module/#selfattention","text":"class SelfAttention ( AttentionModule ): | def __init__ ( | self , | hidden_size : int , | num_attention_heads : int , | dropout : float = 0.0 , | scoring_func : str = \"scaled_dot_product\" , | output_linear : bool = False , | is_decoder : bool = False , | is_cross_attention : bool = False | ) This module computes the self-attention, similar to the architecture in BERT. Additionally, the attention scoring function can be specified. Details in the paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin et al, 2019","title":"SelfAttention"},{"location":"api/modules/transformer/bimodal_attention/","text":"allennlp .modules .transformer .bimodal_attention [SOURCE] BiModalAttention \u00b6 class BiModalAttention ( TransformerModule , FromParams ): | def __init__ ( | self , | hidden_size1 : int , | hidden_size2 : int , | combined_hidden_size : int , | num_attention_heads : int , | dropout1 : float = 0.0 , | dropout2 : float = 0.0 , | scoring_func1 : str = \"scaled_dot_product\" , | scoring_func2 : str = \"scaled_dot_product\" | ) Computes attention for two modalities, based on ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks (Lu et al, 2019) . From the paper: \"The keys and values from each modality are passed as input to the other modality\u2019s multi-headed attention block. Consequentially, the attention block produces attention-pooled features for each modality conditioned on the other.\" For example, considering the case when the first modality is image, and the second modality is language, the module performs \"image-conditioned language attention in the visual stream and language-conditioned image attention in the linguistic stream.\" Parameters \u00b6 hidden_size1 : int The input hidden dim for the first modality. hidden_size2 : int The input hidden dim for the second modality. combined_hidden_size : int The output hidden dim for both modalities; it should be a multiple of num_attention_heads . num_attention_heads : int The number of attention heads. dropout1 : float , optional (default = 0.0 ) The dropout probability for the first modality stream. dropout2 : float , optional (default = 0.0 ) The dropout probability for the second modality stream. scoring_func1 : str , optional (default = scaled_dot_product ) The name of the attention-calculating function to be used for the first modality. scoring_func2 : str , optional (default = scaled_dot_product ) The name of the attention-calculating function to be used for the second modality. Eg. dot_product , linear , etc. For a complete list, please check matrix_attention . forward \u00b6 class BiModalAttention ( TransformerModule , FromParams ): | ... | def forward ( | self , | input_tensor1 , | input_tensor2 , | attention_mask1 = None , | attention_mask2 = None , | co_attention_mask = None | ) Parameters \u00b6 input_tensor1 : torch.Tensor Shape batch_size x seq_len1 x hidden_dim1 where seq_len1 can be the sequence length when the modality is text, or the number of regions when the modality is image. input_tensor2 : torch.Tensor Shape batch_size x seq_len2 x hidden_dim2 where seq_len2 can be the sequence length when the modality is text, or the number of regions when the modality is image. attention_mask1 : torch.BoolTensor , optional Shape batch_size x seq_len1 attention_mask : torch.BoolTensor , optional Shape batch_size x seq_len2 co_attention_mask : torch.Tensor , optional Shape batch_size x seq_len1 x seq_len2 x all_head_size This mask is for cases when you already have some prior information about the interaction between the two modalities. For example, if you know which words correspond to which regions in the image, this mask can be applied to limit the attention given the bias.","title":"bimodal_attention"},{"location":"api/modules/transformer/bimodal_attention/#bimodalattention","text":"class BiModalAttention ( TransformerModule , FromParams ): | def __init__ ( | self , | hidden_size1 : int , | hidden_size2 : int , | combined_hidden_size : int , | num_attention_heads : int , | dropout1 : float = 0.0 , | dropout2 : float = 0.0 , | scoring_func1 : str = \"scaled_dot_product\" , | scoring_func2 : str = \"scaled_dot_product\" | ) Computes attention for two modalities, based on ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks (Lu et al, 2019) . From the paper: \"The keys and values from each modality are passed as input to the other modality\u2019s multi-headed attention block. Consequentially, the attention block produces attention-pooled features for each modality conditioned on the other.\" For example, considering the case when the first modality is image, and the second modality is language, the module performs \"image-conditioned language attention in the visual stream and language-conditioned image attention in the linguistic stream.\"","title":"BiModalAttention"},{"location":"api/modules/transformer/bimodal_attention/#forward","text":"class BiModalAttention ( TransformerModule , FromParams ): | ... | def forward ( | self , | input_tensor1 , | input_tensor2 , | attention_mask1 = None , | attention_mask2 = None , | co_attention_mask = None | )","title":"forward"},{"location":"api/modules/transformer/bimodal_connection_layer/","text":"allennlp .modules .transformer .bimodal_connection_layer [SOURCE] BiModalOutput \u00b6 class BiModalOutput ( TransformerModule , FromParams ): | def __init__ ( | self , | hidden_size1 : int , | hidden_size2 : int , | combined_hidden_size : int , | dropout1 : float , | dropout2 : float | ) forward \u00b6 class BiModalOutput ( TransformerModule , FromParams ): | ... | def forward ( | self , | hidden_states1 , | input_tensor1 , | hidden_states2 , | input_tensor2 | ) BiModalConnectionLayer \u00b6 class BiModalConnectionLayer ( TransformerModule , FromParams ): | def __init__ ( | self , | hidden_size1 : int , | hidden_size2 : int , | combined_hidden_size : int , | intermediate_size1 : int , | intermediate_size2 : int , | num_attention_heads : int , | dropout1 : float , | dropout2 : float , | activation : str | ) forward \u00b6 class BiModalConnectionLayer ( TransformerModule , FromParams ): | ... | def forward ( | self , | input_tensor1 , | attention_mask1 , | input_tensor2 , | attention_mask2 , | co_attention_mask = None | )","title":"bimodal_connection_layer"},{"location":"api/modules/transformer/bimodal_connection_layer/#bimodaloutput","text":"class BiModalOutput ( TransformerModule , FromParams ): | def __init__ ( | self , | hidden_size1 : int , | hidden_size2 : int , | combined_hidden_size : int , | dropout1 : float , | dropout2 : float | )","title":"BiModalOutput"},{"location":"api/modules/transformer/bimodal_connection_layer/#forward","text":"class BiModalOutput ( TransformerModule , FromParams ): | ... | def forward ( | self , | hidden_states1 , | input_tensor1 , | hidden_states2 , | input_tensor2 | )","title":"forward"},{"location":"api/modules/transformer/bimodal_connection_layer/#bimodalconnectionlayer","text":"class BiModalConnectionLayer ( TransformerModule , FromParams ): | def __init__ ( | self , | hidden_size1 : int , | hidden_size2 : int , | combined_hidden_size : int , | intermediate_size1 : int , | intermediate_size2 : int , | num_attention_heads : int , | dropout1 : float , | dropout2 : float , | activation : str | )","title":"BiModalConnectionLayer"},{"location":"api/modules/transformer/bimodal_connection_layer/#forward_1","text":"class BiModalConnectionLayer ( TransformerModule , FromParams ): | ... | def forward ( | self , | input_tensor1 , | attention_mask1 , | input_tensor2 , | attention_mask2 , | co_attention_mask = None | )","title":"forward"},{"location":"api/modules/transformer/bimodal_encoder/","text":"allennlp .modules .transformer .bimodal_encoder [SOURCE] BiModalEncoder \u00b6 class BiModalEncoder ( TransformerModule , FromParams ): | def __init__ ( | self , | num_hidden_layers1 : int = 12 , | num_hidden_layers2 : int = 12 , | hidden_size1 : int = 1024 , | hidden_size2 : int = 1024 , | combined_hidden_size : int = 1024 , | intermediate_size1 : int = 1024 , | intermediate_size2 : int = 1024 , | num_attention_heads1 : int = 8 , | num_attention_heads2 : int = 8 , | combined_num_attention_heads : int = 8 , | attention_dropout1 : float = 0.1 , | hidden_dropout1 : float = 0.1 , | attention_dropout2 : float = 0.1 , | hidden_dropout2 : float = 0.1 , | activation : str = \"relu\" , | biattention_id1 : Optional [ List [ int ]] = None , | biattention_id2 : Optional [ List [ int ]] = None , | fixed_layer1 : int = 0 , | fixed_layer2 : int = 0 , | fast_mode : bool = False , | with_coattention : bool = True , | in_batch_pairs : bool = False | ) This module encodes two modalities separately, and performs bi-directional attention using a connection layer. It is based on the modified BertEncoder in the paper: ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks Parameters \u00b6 num_hidden_layers1 : int , optional (default = 12 ) Number of hidden layers in the transformer block for the first modality. num_hidden_layers2 : int , optional (default = 12 ) Number of hidden layers in the transformer block for the second modality. hidden_size1 : int , optional (default = 1024 ) hidden_size2 : int , optional (default = 1024 ) combined_hidden_size : int , optional (default = 1024 ) Hidden size for the connection layer. intermediate_size1 : int , optional (default = 1024 ) intermediate_size2 : int , optional (default = 1024 ) num_attention_heads1 : int , optional (default = 8 ) num_attention_heads2 : int , optional (default = 8 ) combined_num_attention_heads : int , optional (default = 8 ) Number of attention heads in the connection layer. attention_dropout1 : float , optional (default = 0.1 ) hidden_dropout1 : float , optional (default = 0.1 ) attention_dropout2 : float , optional (default = 0.1 ) hidden_dropout2 : float , optional (default = 0.1 ) biattention_id1 : List , optional (default = [1] ) biattention_id2 : List , optional (default = [1] ) fixed_layer1 : int , optional (default = 0 ) fixed_layer2 : int , optional (default = 0 ) fast_mode : bool , optional (default = False ) with_coattention : bool , optional (default = True ) in_batch_pairs : bool , optional (default = False ) forward \u00b6 class BiModalEncoder ( TransformerModule , FromParams ): | ... | def forward ( | self , | embedding1 , | embedding2 , | attention_mask1 , | attention_mask2 , | co_attention_mask = None , | output_all_encoded_layers = True | )","title":"bimodal_encoder"},{"location":"api/modules/transformer/bimodal_encoder/#bimodalencoder","text":"class BiModalEncoder ( TransformerModule , FromParams ): | def __init__ ( | self , | num_hidden_layers1 : int = 12 , | num_hidden_layers2 : int = 12 , | hidden_size1 : int = 1024 , | hidden_size2 : int = 1024 , | combined_hidden_size : int = 1024 , | intermediate_size1 : int = 1024 , | intermediate_size2 : int = 1024 , | num_attention_heads1 : int = 8 , | num_attention_heads2 : int = 8 , | combined_num_attention_heads : int = 8 , | attention_dropout1 : float = 0.1 , | hidden_dropout1 : float = 0.1 , | attention_dropout2 : float = 0.1 , | hidden_dropout2 : float = 0.1 , | activation : str = \"relu\" , | biattention_id1 : Optional [ List [ int ]] = None , | biattention_id2 : Optional [ List [ int ]] = None , | fixed_layer1 : int = 0 , | fixed_layer2 : int = 0 , | fast_mode : bool = False , | with_coattention : bool = True , | in_batch_pairs : bool = False | ) This module encodes two modalities separately, and performs bi-directional attention using a connection layer. It is based on the modified BertEncoder in the paper: ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks","title":"BiModalEncoder"},{"location":"api/modules/transformer/bimodal_encoder/#forward","text":"class BiModalEncoder ( TransformerModule , FromParams ): | ... | def forward ( | self , | embedding1 , | embedding2 , | attention_mask1 , | attention_mask2 , | co_attention_mask = None , | output_all_encoded_layers = True | )","title":"forward"},{"location":"api/modules/transformer/layer_norm/","text":"allennlp .modules .transformer .layer_norm [SOURCE] LayerNorm \u00b6 class LayerNorm ( torch . nn . LayerNorm , TransformerModule )","title":"layer_norm"},{"location":"api/modules/transformer/layer_norm/#layernorm","text":"class LayerNorm ( torch . nn . LayerNorm , TransformerModule )","title":"LayerNorm"},{"location":"api/modules/transformer/output_layer/","text":"allennlp .modules .transformer .output_layer [SOURCE] OutputLayer \u00b6 class OutputLayer ( TransformerModule , FromParams ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | dropout : float | ) get_output_dim \u00b6 class OutputLayer ( TransformerModule , FromParams ): | ... | def get_output_dim ( self ) -> int forward \u00b6 class OutputLayer ( TransformerModule , FromParams ): | ... | def forward ( self , hidden_states , input_tensor )","title":"output_layer"},{"location":"api/modules/transformer/output_layer/#outputlayer","text":"class OutputLayer ( TransformerModule , FromParams ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | dropout : float | )","title":"OutputLayer"},{"location":"api/modules/transformer/output_layer/#get_output_dim","text":"class OutputLayer ( TransformerModule , FromParams ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/transformer/output_layer/#forward","text":"class OutputLayer ( TransformerModule , FromParams ): | ... | def forward ( self , hidden_states , input_tensor )","title":"forward"},{"location":"api/modules/transformer/positional_encoding/","text":"allennlp .modules .transformer .positional_encoding [SOURCE] SinusoidalPositionalEncoding \u00b6 class SinusoidalPositionalEncoding ( torch . nn . Module , FromParams ): | def __init__ ( | self , | min_timescale : float = 1.0 , | max_timescale : float = 1.0e4 | ) Implements the frequency-based positional encoding described in Attention is All you Need . Adds sinusoids of different frequencies to a Tensor . A sinusoid of a different frequency and phase is added to each dimension of the input Tensor . This allows the attention heads to use absolute and relative positions. The number of timescales is equal to hidden_dim / 2 within the range (min_timescale, max_timescale). For each timescale, the two sinusoidal signals sin(timestep / timescale) and cos(timestep / timescale) are generated and concatenated along the hidden_dim dimension. Parameters \u00b6 tensor : torch.Tensor a Tensor with shape (batch_size, timesteps, hidden_dim). min_timescale : float , optional (default = 1.0 ) The smallest timescale to use. max_timescale : float , optional (default = 1.0e4 ) The largest timescale to use. Returns \u00b6 torch.Tensor The input tensor augmented with the sinusoidal frequencies. forward \u00b6 class SinusoidalPositionalEncoding ( torch . nn . Module , FromParams ): | ... | def forward ( self , input_tensor : torch . Tensor ) Adds a positional encoding to input_tensor .","title":"positional_encoding"},{"location":"api/modules/transformer/positional_encoding/#sinusoidalpositionalencoding","text":"class SinusoidalPositionalEncoding ( torch . nn . Module , FromParams ): | def __init__ ( | self , | min_timescale : float = 1.0 , | max_timescale : float = 1.0e4 | ) Implements the frequency-based positional encoding described in Attention is All you Need . Adds sinusoids of different frequencies to a Tensor . A sinusoid of a different frequency and phase is added to each dimension of the input Tensor . This allows the attention heads to use absolute and relative positions. The number of timescales is equal to hidden_dim / 2 within the range (min_timescale, max_timescale). For each timescale, the two sinusoidal signals sin(timestep / timescale) and cos(timestep / timescale) are generated and concatenated along the hidden_dim dimension.","title":"SinusoidalPositionalEncoding"},{"location":"api/modules/transformer/positional_encoding/#forward","text":"class SinusoidalPositionalEncoding ( torch . nn . Module , FromParams ): | ... | def forward ( self , input_tensor : torch . Tensor ) Adds a positional encoding to input_tensor .","title":"forward"},{"location":"api/modules/transformer/t5/","text":"allennlp .modules .transformer .t5 [SOURCE] An implementation of T5 , adapted from HuggingFace . T5LayerNorm \u00b6 class T5LayerNorm ( TransformerModule , FromParams ): | def __init__ ( self , hidden_size : int = 512 , eps : float = 1e-6 ) T5-style layer norm does not have bias and does not subtract the mean. forward \u00b6 class T5LayerNorm ( TransformerModule , FromParams ): | ... | def forward ( self , hidden_states ) -> FloatT T5FeedForwardProjection \u00b6 class T5FeedForwardProjection ( TransformerModule , Registrable ) forward \u00b6 class T5FeedForwardProjection ( TransformerModule , Registrable ): | ... | def forward ( self , hidden_states ) -> FloatT T5DenseReluDense \u00b6 @T5FeedForwardProjection . register ( \"relu\" ) class T5DenseReluDense ( TransformerModule , FromParams ): | def __init__ ( | self , | hidden_size : int = 512 , | ff_size : int = 2048 , | dropout : float = 0.1 | ) forward \u00b6 class T5DenseReluDense ( TransformerModule , FromParams ): | ... | def forward ( self , hidden_states ) -> FloatT T5DenseGatedGeluDense \u00b6 @T5FeedForwardProjection . register ( \"gated-gelu\" ) class T5DenseGatedGeluDense ( TransformerModule , FromParams ): | def __init__ ( | self , | hidden_size : int = 512 , | ff_size : int = 2048 , | dropout : float = 0.1 | ) forward \u00b6 class T5DenseGatedGeluDense ( TransformerModule , FromParams ): | ... | def forward ( self , hidden_states ) -> FloatT T5LayerFF \u00b6 class T5LayerFF ( TransformerModule , FromParams ): | def __init__ ( | self , | ff_proj : Optional [ T5FeedForwardProjection ] = None , | layer_norm : Optional [ T5LayerNorm ] = None , | dropout : float = 0.1 | ) forward \u00b6 class T5LayerFF ( TransformerModule , FromParams ): | ... | def forward ( self , hidden_states ) -> FloatT T5LayerSelfAttentionOutput \u00b6 class T5LayerSelfAttentionOutput ( NamedTuple ) hidden_states \u00b6 class T5LayerSelfAttentionOutput ( NamedTuple ): | ... | hidden_states : FloatT = None attn_key_value_state \u00b6 class T5LayerSelfAttentionOutput ( NamedTuple ): | ... | attn_key_value_state : Optional [ Tuple [ FloatT , FloatT ]] = None attn_position_bias \u00b6 class T5LayerSelfAttentionOutput ( NamedTuple ): | ... | attn_position_bias : FloatT = None attn_weights \u00b6 class T5LayerSelfAttentionOutput ( NamedTuple ): | ... | attn_weights : Optional [ FloatT ] = None T5LayerSelfAttention \u00b6 class T5LayerSelfAttention ( TransformerModule , FromParams ): | def __init__ ( | self , | self_attention : Optional [ T5Attention ] = None , | layer_norm : Optional [ T5LayerNorm ] = None , | dropout : float = 0.1 , | has_relative_attention_bias : bool = False | ) hidden_size \u00b6 class T5LayerSelfAttention ( TransformerModule , FromParams ): | ... | @property | def hidden_size ( self ) -> int forward \u00b6 class T5LayerSelfAttention ( TransformerModule , FromParams ): | ... | def forward ( | self , | hidden_states : FloatT , | attention_mask : Optional [ torch . BoolTensor ] = None , | position_bias : Optional [ torch . Tensor ] = None , | layer_head_mask : Optional [ torch . BoolTensor ] = None , | past_key_value : Optional [ Tuple [ FloatT ]] = None , | use_cache : bool = False , | output_attentions : bool = False | ) -> T5LayerSelfAttentionOutput T5LayerCrossAttentionOutput \u00b6 class T5LayerCrossAttentionOutput ( NamedTuple ) hidden_states \u00b6 class T5LayerCrossAttentionOutput ( NamedTuple ): | ... | hidden_states : FloatT = None attn_key_value_state \u00b6 class T5LayerCrossAttentionOutput ( NamedTuple ): | ... | attn_key_value_state : Optional [ Tuple [ FloatT , FloatT ]] = None attn_position_bias \u00b6 class T5LayerCrossAttentionOutput ( NamedTuple ): | ... | attn_position_bias : FloatT = None attn_weights \u00b6 class T5LayerCrossAttentionOutput ( NamedTuple ): | ... | attn_weights : Optional [ FloatT ] = None T5LayerCrossAttention \u00b6 class T5LayerCrossAttention ( TransformerModule , FromParams ): | def __init__ ( | self , | enc_dec_attention : Optional [ T5Attention ] = None , | layer_norm : Optional [ T5LayerNorm ] = None , | dropout : float = 0.1 | ) forward \u00b6 class T5LayerCrossAttention ( TransformerModule , FromParams ): | ... | def forward ( | self , | hidden_states : FloatT , | key_value_states : Optional [ FloatT ], | attention_mask : Optional [ torch . BoolTensor ] = None , | position_bias : Optional [ FloatT ] = None , | layer_head_mask : Optional [ torch . BoolTensor ] = None , | past_key_value : Optional [ Tuple [ Tuple [ FloatT ]]] = None , | use_cache : bool = False , | query_length : int = None , | output_attentions : bool = False | ) -> T5LayerCrossAttentionOutput KeyValueStates \u00b6 KeyValueStates = Union [ Tuple [ FloatT , FloatT ], # without cross attention Tuple [ FloatT , FloatT , FloatT , Float ... T5BlockOutput \u00b6 class T5BlockOutput ( NamedTuple ) hidden_states \u00b6 class T5BlockOutput ( NamedTuple ): | ... | hidden_states : FloatT = None present_key_value_states \u00b6 class T5BlockOutput ( NamedTuple ): | ... | present_key_value_states : Optional [ KeyValueStates ] = None self_attn_weights \u00b6 class T5BlockOutput ( NamedTuple ): | ... | self_attn_weights : Optional [ FloatT ] = None self_attn_position_bias \u00b6 class T5BlockOutput ( NamedTuple ): | ... | self_attn_position_bias : Optional [ FloatT ] = None cross_attn_weights \u00b6 class T5BlockOutput ( NamedTuple ): | ... | cross_attn_weights : Optional [ FloatT ] = None cross_attn_position_bias \u00b6 class T5BlockOutput ( NamedTuple ): | ... | cross_attn_position_bias : Optional [ FloatT ] = None T5Block \u00b6 class T5Block ( TransformerModule , FromParams ): | def __init__ ( | self , | attention : Optional [ T5LayerSelfAttention ] = None , | cross_attention : Optional [ T5LayerCrossAttention ] = None , | ff : Optional [ T5LayerFF ] = None | ) hidden_size \u00b6 class T5Block ( TransformerModule , FromParams ): | ... | @property | def hidden_size ( self ) -> int forward \u00b6 class T5Block ( TransformerModule , FromParams ): | ... | def forward ( | self , | hidden_states : FloatT , | attention_mask : Optional [ torch . BoolTensor ] = None , | position_bias : Optional [ FloatT ] = None , | encoder_hidden_states : Optional [ FloatT ] = None , | encoder_attention_mask : Optional [ torch . BoolTensor ] = None , | encoder_decoder_position_bias : Optional [ FloatT ] = None , | layer_head_mask : Optional [ torch . BoolTensor ] = None , | encoder_layer_head_mask : Optional [ torch . BoolTensor ] = None , | past_key_value : Optional [ KeyValueStates ] = None , | use_cache : bool = False , | output_attentions : bool = False | ) -> T5BlockOutput T5StackOutput \u00b6 class T5StackOutput ( NamedTuple ) last_hidden_state \u00b6 class T5StackOutput ( NamedTuple ): | ... | last_hidden_state : FloatT = None past_key_values \u00b6 class T5StackOutput ( NamedTuple ): | ... | past_key_values : Optional [ List [ KeyValueStates ]] = None all_hidden_states \u00b6 class T5StackOutput ( NamedTuple ): | ... | all_hidden_states : Optional [ List [ FloatT ]] = None attentions \u00b6 class T5StackOutput ( NamedTuple ): | ... | attentions : Optional [ List [ FloatT ]] = None cross_attentions \u00b6 class T5StackOutput ( NamedTuple ): | ... | cross_attentions : Optional [ List [ FloatT ]] = None T5Stack \u00b6 class T5Stack ( TransformerModule , FromParams ): | def __init__ ( | self , | token_embeddings : nn . Embedding , | blocks : List [ T5Block ], | final_layer_norm : Optional [ T5LayerNorm ] = None , | dropout : float = 0.1 | ) num_blocks \u00b6 class T5Stack ( TransformerModule , FromParams ): | ... | @property | def num_blocks ( self ) -> int hidden_size \u00b6 class T5Stack ( TransformerModule , FromParams ): | ... | @property | def hidden_size ( self ) -> int get_head_mask \u00b6 class T5Stack ( TransformerModule , FromParams ): | ... | @staticmethod | def get_head_mask ( | head_mask : Optional [ torch . BoolTensor ], | num_hidden_layers : int | ) -> BoolT resize_token_embeddings \u00b6 class T5Stack ( TransformerModule , FromParams ): | ... | def resize_token_embeddings ( | self , | new_size : int , | * , init_fn : Callable = torch . nn . init . normal_ | ) -> None forward \u00b6 class T5Stack ( TransformerModule , FromParams ): | ... | def forward ( | self , | input_ids : Optional [ torch . IntTensor ] = None , | attention_mask : Optional [ torch . BoolTensor ] = None , | encoder_hidden_states : Optional [ FloatT ] = None , | encoder_attention_mask : Optional [ torch . BoolTensor ] = None , | inputs_embeds : Optional [ FloatT ] = None , | head_mask : Optional [ torch . BoolTensor ] = None , | encoder_head_mask : Optional [ torch . BoolTensor ] = None , | past_key_values : Optional [ KeyValueStates ] = None , | use_cache : bool = False , | output_attentions : bool = False , | output_all_hidden_states : bool = False | ) -> T5StackOutput T5EncoderStack \u00b6 class T5EncoderStack ( T5Stack , FromParams ): | def __init__ ( | self , | token_embeddings : nn . Embedding , | blocks : List [ T5Block ], | final_layer_norm : Optional [ T5LayerNorm ] = None , | dropout : float = 0.1 | ) basic_encoder \u00b6 class T5EncoderStack ( T5Stack , FromParams ): | ... | @classmethod | def basic_encoder ( | cls , | token_embeddings : nn . Embedding , | num_blocks : int = 6 , | block_self_attention : Lazy [ T5Attention ] = Lazy ( T5Attention ), | final_layer_norm : Optional [ T5LayerNorm ] = None , | block_ff : Lazy [ T5LayerFF ] = Lazy ( T5LayerFF ), | dropout : float = 0.1 , | ddp_accelerator : Optional [ DdpAccelerator ] = None , | checkpoint_wrapper : Optional [ CheckpointWrapper ] = None | ) -> \"T5EncoderStack\" T5DecoderStack \u00b6 class T5DecoderStack ( T5Stack , FromParams ): | def __init__ ( | self , | token_embeddings : nn . Embedding , | blocks : List [ T5Block ], | final_layer_norm : Optional [ T5LayerNorm ] = None , | dropout : float = 0.1 | ) basic_decoder \u00b6 class T5DecoderStack ( T5Stack , FromParams ): | ... | @classmethod | def basic_decoder ( | cls , | token_embeddings : nn . Embedding , | num_blocks : int = 6 , | block_self_attention : Lazy [ T5Attention ] = Lazy ( T5Attention ), | block_cross_attention : Lazy [ T5Attention ] = Lazy ( T5Attention ), | final_layer_norm : Optional [ T5LayerNorm ] = None , | block_ff : Lazy [ T5LayerFF ] = Lazy ( T5LayerFF ), | dropout : float = 0.1 , | ddp_accelerator : Optional [ DdpAccelerator ] = None , | checkpoint_wrapper : Optional [ CheckpointWrapper ] = None | ) -> \"T5DecoderStack\" T5Output \u00b6 class T5Output ( NamedTuple ) Defines the output from the T5 model. encoder_last_hidden_state \u00b6 class T5Output ( NamedTuple ): | ... | encoder_last_hidden_state : FloatT = None Final hidden states from the encoder. Shape: (batch_size, target_length, hidden_dim) encoder_all_hidden_states \u00b6 class T5Output ( NamedTuple ): | ... | encoder_all_hidden_states : Optional [ List [ FloatT ]] = None All hidden states from the encoder. Shape (each): (batch_size, target_length, hidden_dim) decoder_last_hidden_state \u00b6 class T5Output ( NamedTuple ): | ... | decoder_last_hidden_state : Optional [ FloatT ] = None Final hidden states from the decoder. Only present when labels is given. Shape: (batch_size, target_length, hidden_dim) decoder_all_hidden_states \u00b6 class T5Output ( NamedTuple ): | ... | decoder_all_hidden_states : Optional [ List [ FloatT ]] = None All hidden states from the decoder. Only present when labels is given and output_all_hidden_states is True . Shape (each): (batch_size, target_length, hidden_dim) encoder_attentions \u00b6 class T5Output ( NamedTuple ): | ... | encoder_attentions : Optional [ List [ FloatT ]] = None Attention values from the encoder. Only present when output_attentions is True . decoder_attentions \u00b6 class T5Output ( NamedTuple ): | ... | decoder_attentions : Optional [ List [ FloatT ]] = None Attention values from the decoder. Only present when labels is given and output_attentions is True . cross_attentions \u00b6 class T5Output ( NamedTuple ): | ... | cross_attentions : Optional [ List [ FloatT ]] = None Cross-attention values from the decoder. Only present when labels is given and output_attentions is True . loss \u00b6 class T5Output ( NamedTuple ): | ... | loss : Optional [ FloatT ] = None The loss calculating with respect to labels . logits \u00b6 class T5Output ( NamedTuple ): | ... | logits : Optional [ FloatT ] = None The logits that are used to calculate the loss with respect to labels . predictions \u00b6 class T5Output ( NamedTuple ): | ... | predictions : Optional [ IntT ] = None Predicted token IDs from beam search. Shape: (batch_size, beam_size, max_decoding_steps) . predicted_log_probs \u00b6 class T5Output ( NamedTuple ): | ... | predicted_log_probs : Optional [ FloatT ] = None Probabilities corresponding to predictions . Shape: (batch_size, beam_size,) . T5 \u00b6 class T5 ( TransformerModule , Registrable ): | def __init__ ( | self , | token_embeddings : Optional [ nn . Embedding ] = None , | encoder : Lazy [ T5EncoderStack ] = Lazy ( T5EncoderStack . basic_encoder ), | decoder : Lazy [ T5DecoderStack ] = Lazy ( T5DecoderStack . basic_decoder ), | decoder_start_token_id : int = 0 , | pad_token_id : int = 0 , | eos_token_id : int = 1 , | vocab_size : int = 32128 , | model_dim : int = 512 , | output_attentions : bool = False , | output_all_hidden_states : bool = False , | beam_search : Lazy [ BeamSearch ] = Lazy ( BeamSearch , beam_size = 3 , max_steps = 100 ), | ddp_accelerator : Optional [ DdpAccelerator ] = None , | checkpoint_wrapper : Optional [ CheckpointWrapper ] = None , | tie_word_embeddings : bool = True | ) default_implementation \u00b6 class T5 ( TransformerModule , Registrable ): | ... | default_implementation = \"default\" resize_token_embeddings \u00b6 class T5 ( TransformerModule , Registrable ): | ... | def resize_token_embeddings ( | self , | new_size : int , | * , init_fn : Callable = torch . nn . init . normal_ | ) -> None Resizes the token embeddings in the model. This takes care of the token embeddings for the encoder, the decoder, and the LM head. new_size : int The new size of the token embeddings init_fn : Callable The function to use to initialize new embeddings. This function will be called with a single argument, the tensor to initialize, and it is expected to initialize the tensor in place. Many of the functions from torch.nn.init fit. forward \u00b6 class T5 ( TransformerModule , Registrable ): | ... | def forward ( | self , | input_ids : IntT , | attention_mask : Optional [ BoolT ] = None , | labels : Optional [ IntT ] = None , | decoder_attention_mask : Optional [ BoolT ] = None | ) -> T5Output Run forward pass of the model. take_search_step \u00b6 class T5 ( TransformerModule , Registrable ): | ... | def take_search_step ( | self , | last_predictions : torch . Tensor , | state : Dict [ str , torch . Tensor ], | step : int | ) -> Tuple [ torch . Tensor , Dict [ str , torch . Tensor ]] Take step during beam search. This function is what gets passed to the BeamSearch.search method. It takes predictions from the last timestep and the current state and outputs the log probabilities assigned to tokens for the next timestep, as well as the updated state.","title":"t5"},{"location":"api/modules/transformer/t5/#t5layernorm","text":"class T5LayerNorm ( TransformerModule , FromParams ): | def __init__ ( self , hidden_size : int = 512 , eps : float = 1e-6 ) T5-style layer norm does not have bias and does not subtract the mean.","title":"T5LayerNorm"},{"location":"api/modules/transformer/t5/#forward","text":"class T5LayerNorm ( TransformerModule , FromParams ): | ... | def forward ( self , hidden_states ) -> FloatT","title":"forward"},{"location":"api/modules/transformer/t5/#t5feedforwardprojection","text":"class T5FeedForwardProjection ( TransformerModule , Registrable )","title":"T5FeedForwardProjection"},{"location":"api/modules/transformer/t5/#forward_1","text":"class T5FeedForwardProjection ( TransformerModule , Registrable ): | ... | def forward ( self , hidden_states ) -> FloatT","title":"forward"},{"location":"api/modules/transformer/t5/#t5densereludense","text":"@T5FeedForwardProjection . register ( \"relu\" ) class T5DenseReluDense ( TransformerModule , FromParams ): | def __init__ ( | self , | hidden_size : int = 512 , | ff_size : int = 2048 , | dropout : float = 0.1 | )","title":"T5DenseReluDense"},{"location":"api/modules/transformer/t5/#forward_2","text":"class T5DenseReluDense ( TransformerModule , FromParams ): | ... | def forward ( self , hidden_states ) -> FloatT","title":"forward"},{"location":"api/modules/transformer/t5/#t5densegatedgeludense","text":"@T5FeedForwardProjection . register ( \"gated-gelu\" ) class T5DenseGatedGeluDense ( TransformerModule , FromParams ): | def __init__ ( | self , | hidden_size : int = 512 , | ff_size : int = 2048 , | dropout : float = 0.1 | )","title":"T5DenseGatedGeluDense"},{"location":"api/modules/transformer/t5/#forward_3","text":"class T5DenseGatedGeluDense ( TransformerModule , FromParams ): | ... | def forward ( self , hidden_states ) -> FloatT","title":"forward"},{"location":"api/modules/transformer/t5/#t5layerff","text":"class T5LayerFF ( TransformerModule , FromParams ): | def __init__ ( | self , | ff_proj : Optional [ T5FeedForwardProjection ] = None , | layer_norm : Optional [ T5LayerNorm ] = None , | dropout : float = 0.1 | )","title":"T5LayerFF"},{"location":"api/modules/transformer/t5/#forward_4","text":"class T5LayerFF ( TransformerModule , FromParams ): | ... | def forward ( self , hidden_states ) -> FloatT","title":"forward"},{"location":"api/modules/transformer/t5/#t5layerselfattentionoutput","text":"class T5LayerSelfAttentionOutput ( NamedTuple )","title":"T5LayerSelfAttentionOutput"},{"location":"api/modules/transformer/t5/#hidden_states","text":"class T5LayerSelfAttentionOutput ( NamedTuple ): | ... | hidden_states : FloatT = None","title":"hidden_states"},{"location":"api/modules/transformer/t5/#attn_key_value_state","text":"class T5LayerSelfAttentionOutput ( NamedTuple ): | ... | attn_key_value_state : Optional [ Tuple [ FloatT , FloatT ]] = None","title":"attn_key_value_state"},{"location":"api/modules/transformer/t5/#attn_position_bias","text":"class T5LayerSelfAttentionOutput ( NamedTuple ): | ... | attn_position_bias : FloatT = None","title":"attn_position_bias"},{"location":"api/modules/transformer/t5/#attn_weights","text":"class T5LayerSelfAttentionOutput ( NamedTuple ): | ... | attn_weights : Optional [ FloatT ] = None","title":"attn_weights"},{"location":"api/modules/transformer/t5/#t5layerselfattention","text":"class T5LayerSelfAttention ( TransformerModule , FromParams ): | def __init__ ( | self , | self_attention : Optional [ T5Attention ] = None , | layer_norm : Optional [ T5LayerNorm ] = None , | dropout : float = 0.1 , | has_relative_attention_bias : bool = False | )","title":"T5LayerSelfAttention"},{"location":"api/modules/transformer/t5/#hidden_size","text":"class T5LayerSelfAttention ( TransformerModule , FromParams ): | ... | @property | def hidden_size ( self ) -> int","title":"hidden_size"},{"location":"api/modules/transformer/t5/#forward_5","text":"class T5LayerSelfAttention ( TransformerModule , FromParams ): | ... | def forward ( | self , | hidden_states : FloatT , | attention_mask : Optional [ torch . BoolTensor ] = None , | position_bias : Optional [ torch . Tensor ] = None , | layer_head_mask : Optional [ torch . BoolTensor ] = None , | past_key_value : Optional [ Tuple [ FloatT ]] = None , | use_cache : bool = False , | output_attentions : bool = False | ) -> T5LayerSelfAttentionOutput","title":"forward"},{"location":"api/modules/transformer/t5/#t5layercrossattentionoutput","text":"class T5LayerCrossAttentionOutput ( NamedTuple )","title":"T5LayerCrossAttentionOutput"},{"location":"api/modules/transformer/t5/#hidden_states_1","text":"class T5LayerCrossAttentionOutput ( NamedTuple ): | ... | hidden_states : FloatT = None","title":"hidden_states"},{"location":"api/modules/transformer/t5/#attn_key_value_state_1","text":"class T5LayerCrossAttentionOutput ( NamedTuple ): | ... | attn_key_value_state : Optional [ Tuple [ FloatT , FloatT ]] = None","title":"attn_key_value_state"},{"location":"api/modules/transformer/t5/#attn_position_bias_1","text":"class T5LayerCrossAttentionOutput ( NamedTuple ): | ... | attn_position_bias : FloatT = None","title":"attn_position_bias"},{"location":"api/modules/transformer/t5/#attn_weights_1","text":"class T5LayerCrossAttentionOutput ( NamedTuple ): | ... | attn_weights : Optional [ FloatT ] = None","title":"attn_weights"},{"location":"api/modules/transformer/t5/#t5layercrossattention","text":"class T5LayerCrossAttention ( TransformerModule , FromParams ): | def __init__ ( | self , | enc_dec_attention : Optional [ T5Attention ] = None , | layer_norm : Optional [ T5LayerNorm ] = None , | dropout : float = 0.1 | )","title":"T5LayerCrossAttention"},{"location":"api/modules/transformer/t5/#forward_6","text":"class T5LayerCrossAttention ( TransformerModule , FromParams ): | ... | def forward ( | self , | hidden_states : FloatT , | key_value_states : Optional [ FloatT ], | attention_mask : Optional [ torch . BoolTensor ] = None , | position_bias : Optional [ FloatT ] = None , | layer_head_mask : Optional [ torch . BoolTensor ] = None , | past_key_value : Optional [ Tuple [ Tuple [ FloatT ]]] = None , | use_cache : bool = False , | query_length : int = None , | output_attentions : bool = False | ) -> T5LayerCrossAttentionOutput","title":"forward"},{"location":"api/modules/transformer/t5/#keyvaluestates","text":"KeyValueStates = Union [ Tuple [ FloatT , FloatT ], # without cross attention Tuple [ FloatT , FloatT , FloatT , Float ...","title":"KeyValueStates"},{"location":"api/modules/transformer/t5/#t5blockoutput","text":"class T5BlockOutput ( NamedTuple )","title":"T5BlockOutput"},{"location":"api/modules/transformer/t5/#hidden_states_2","text":"class T5BlockOutput ( NamedTuple ): | ... | hidden_states : FloatT = None","title":"hidden_states"},{"location":"api/modules/transformer/t5/#present_key_value_states","text":"class T5BlockOutput ( NamedTuple ): | ... | present_key_value_states : Optional [ KeyValueStates ] = None","title":"present_key_value_states"},{"location":"api/modules/transformer/t5/#self_attn_weights","text":"class T5BlockOutput ( NamedTuple ): | ... | self_attn_weights : Optional [ FloatT ] = None","title":"self_attn_weights"},{"location":"api/modules/transformer/t5/#self_attn_position_bias","text":"class T5BlockOutput ( NamedTuple ): | ... | self_attn_position_bias : Optional [ FloatT ] = None","title":"self_attn_position_bias"},{"location":"api/modules/transformer/t5/#cross_attn_weights","text":"class T5BlockOutput ( NamedTuple ): | ... | cross_attn_weights : Optional [ FloatT ] = None","title":"cross_attn_weights"},{"location":"api/modules/transformer/t5/#cross_attn_position_bias","text":"class T5BlockOutput ( NamedTuple ): | ... | cross_attn_position_bias : Optional [ FloatT ] = None","title":"cross_attn_position_bias"},{"location":"api/modules/transformer/t5/#t5block","text":"class T5Block ( TransformerModule , FromParams ): | def __init__ ( | self , | attention : Optional [ T5LayerSelfAttention ] = None , | cross_attention : Optional [ T5LayerCrossAttention ] = None , | ff : Optional [ T5LayerFF ] = None | )","title":"T5Block"},{"location":"api/modules/transformer/t5/#hidden_size_1","text":"class T5Block ( TransformerModule , FromParams ): | ... | @property | def hidden_size ( self ) -> int","title":"hidden_size"},{"location":"api/modules/transformer/t5/#forward_7","text":"class T5Block ( TransformerModule , FromParams ): | ... | def forward ( | self , | hidden_states : FloatT , | attention_mask : Optional [ torch . BoolTensor ] = None , | position_bias : Optional [ FloatT ] = None , | encoder_hidden_states : Optional [ FloatT ] = None , | encoder_attention_mask : Optional [ torch . BoolTensor ] = None , | encoder_decoder_position_bias : Optional [ FloatT ] = None , | layer_head_mask : Optional [ torch . BoolTensor ] = None , | encoder_layer_head_mask : Optional [ torch . BoolTensor ] = None , | past_key_value : Optional [ KeyValueStates ] = None , | use_cache : bool = False , | output_attentions : bool = False | ) -> T5BlockOutput","title":"forward"},{"location":"api/modules/transformer/t5/#t5stackoutput","text":"class T5StackOutput ( NamedTuple )","title":"T5StackOutput"},{"location":"api/modules/transformer/t5/#last_hidden_state","text":"class T5StackOutput ( NamedTuple ): | ... | last_hidden_state : FloatT = None","title":"last_hidden_state"},{"location":"api/modules/transformer/t5/#past_key_values","text":"class T5StackOutput ( NamedTuple ): | ... | past_key_values : Optional [ List [ KeyValueStates ]] = None","title":"past_key_values"},{"location":"api/modules/transformer/t5/#all_hidden_states","text":"class T5StackOutput ( NamedTuple ): | ... | all_hidden_states : Optional [ List [ FloatT ]] = None","title":"all_hidden_states"},{"location":"api/modules/transformer/t5/#attentions","text":"class T5StackOutput ( NamedTuple ): | ... | attentions : Optional [ List [ FloatT ]] = None","title":"attentions"},{"location":"api/modules/transformer/t5/#cross_attentions","text":"class T5StackOutput ( NamedTuple ): | ... | cross_attentions : Optional [ List [ FloatT ]] = None","title":"cross_attentions"},{"location":"api/modules/transformer/t5/#t5stack","text":"class T5Stack ( TransformerModule , FromParams ): | def __init__ ( | self , | token_embeddings : nn . Embedding , | blocks : List [ T5Block ], | final_layer_norm : Optional [ T5LayerNorm ] = None , | dropout : float = 0.1 | )","title":"T5Stack"},{"location":"api/modules/transformer/t5/#num_blocks","text":"class T5Stack ( TransformerModule , FromParams ): | ... | @property | def num_blocks ( self ) -> int","title":"num_blocks"},{"location":"api/modules/transformer/t5/#hidden_size_2","text":"class T5Stack ( TransformerModule , FromParams ): | ... | @property | def hidden_size ( self ) -> int","title":"hidden_size"},{"location":"api/modules/transformer/t5/#get_head_mask","text":"class T5Stack ( TransformerModule , FromParams ): | ... | @staticmethod | def get_head_mask ( | head_mask : Optional [ torch . BoolTensor ], | num_hidden_layers : int | ) -> BoolT","title":"get_head_mask"},{"location":"api/modules/transformer/t5/#resize_token_embeddings","text":"class T5Stack ( TransformerModule , FromParams ): | ... | def resize_token_embeddings ( | self , | new_size : int , | * , init_fn : Callable = torch . nn . init . normal_ | ) -> None","title":"resize_token_embeddings"},{"location":"api/modules/transformer/t5/#forward_8","text":"class T5Stack ( TransformerModule , FromParams ): | ... | def forward ( | self , | input_ids : Optional [ torch . IntTensor ] = None , | attention_mask : Optional [ torch . BoolTensor ] = None , | encoder_hidden_states : Optional [ FloatT ] = None , | encoder_attention_mask : Optional [ torch . BoolTensor ] = None , | inputs_embeds : Optional [ FloatT ] = None , | head_mask : Optional [ torch . BoolTensor ] = None , | encoder_head_mask : Optional [ torch . BoolTensor ] = None , | past_key_values : Optional [ KeyValueStates ] = None , | use_cache : bool = False , | output_attentions : bool = False , | output_all_hidden_states : bool = False | ) -> T5StackOutput","title":"forward"},{"location":"api/modules/transformer/t5/#t5encoderstack","text":"class T5EncoderStack ( T5Stack , FromParams ): | def __init__ ( | self , | token_embeddings : nn . Embedding , | blocks : List [ T5Block ], | final_layer_norm : Optional [ T5LayerNorm ] = None , | dropout : float = 0.1 | )","title":"T5EncoderStack"},{"location":"api/modules/transformer/t5/#basic_encoder","text":"class T5EncoderStack ( T5Stack , FromParams ): | ... | @classmethod | def basic_encoder ( | cls , | token_embeddings : nn . Embedding , | num_blocks : int = 6 , | block_self_attention : Lazy [ T5Attention ] = Lazy ( T5Attention ), | final_layer_norm : Optional [ T5LayerNorm ] = None , | block_ff : Lazy [ T5LayerFF ] = Lazy ( T5LayerFF ), | dropout : float = 0.1 , | ddp_accelerator : Optional [ DdpAccelerator ] = None , | checkpoint_wrapper : Optional [ CheckpointWrapper ] = None | ) -> \"T5EncoderStack\"","title":"basic_encoder"},{"location":"api/modules/transformer/t5/#t5decoderstack","text":"class T5DecoderStack ( T5Stack , FromParams ): | def __init__ ( | self , | token_embeddings : nn . Embedding , | blocks : List [ T5Block ], | final_layer_norm : Optional [ T5LayerNorm ] = None , | dropout : float = 0.1 | )","title":"T5DecoderStack"},{"location":"api/modules/transformer/t5/#basic_decoder","text":"class T5DecoderStack ( T5Stack , FromParams ): | ... | @classmethod | def basic_decoder ( | cls , | token_embeddings : nn . Embedding , | num_blocks : int = 6 , | block_self_attention : Lazy [ T5Attention ] = Lazy ( T5Attention ), | block_cross_attention : Lazy [ T5Attention ] = Lazy ( T5Attention ), | final_layer_norm : Optional [ T5LayerNorm ] = None , | block_ff : Lazy [ T5LayerFF ] = Lazy ( T5LayerFF ), | dropout : float = 0.1 , | ddp_accelerator : Optional [ DdpAccelerator ] = None , | checkpoint_wrapper : Optional [ CheckpointWrapper ] = None | ) -> \"T5DecoderStack\"","title":"basic_decoder"},{"location":"api/modules/transformer/t5/#t5output","text":"class T5Output ( NamedTuple ) Defines the output from the T5 model.","title":"T5Output"},{"location":"api/modules/transformer/t5/#encoder_last_hidden_state","text":"class T5Output ( NamedTuple ): | ... | encoder_last_hidden_state : FloatT = None Final hidden states from the encoder. Shape: (batch_size, target_length, hidden_dim)","title":"encoder_last_hidden_state"},{"location":"api/modules/transformer/t5/#encoder_all_hidden_states","text":"class T5Output ( NamedTuple ): | ... | encoder_all_hidden_states : Optional [ List [ FloatT ]] = None All hidden states from the encoder. Shape (each): (batch_size, target_length, hidden_dim)","title":"encoder_all_hidden_states"},{"location":"api/modules/transformer/t5/#decoder_last_hidden_state","text":"class T5Output ( NamedTuple ): | ... | decoder_last_hidden_state : Optional [ FloatT ] = None Final hidden states from the decoder. Only present when labels is given. Shape: (batch_size, target_length, hidden_dim)","title":"decoder_last_hidden_state"},{"location":"api/modules/transformer/t5/#decoder_all_hidden_states","text":"class T5Output ( NamedTuple ): | ... | decoder_all_hidden_states : Optional [ List [ FloatT ]] = None All hidden states from the decoder. Only present when labels is given and output_all_hidden_states is True . Shape (each): (batch_size, target_length, hidden_dim)","title":"decoder_all_hidden_states"},{"location":"api/modules/transformer/t5/#encoder_attentions","text":"class T5Output ( NamedTuple ): | ... | encoder_attentions : Optional [ List [ FloatT ]] = None Attention values from the encoder. Only present when output_attentions is True .","title":"encoder_attentions"},{"location":"api/modules/transformer/t5/#decoder_attentions","text":"class T5Output ( NamedTuple ): | ... | decoder_attentions : Optional [ List [ FloatT ]] = None Attention values from the decoder. Only present when labels is given and output_attentions is True .","title":"decoder_attentions"},{"location":"api/modules/transformer/t5/#cross_attentions_1","text":"class T5Output ( NamedTuple ): | ... | cross_attentions : Optional [ List [ FloatT ]] = None Cross-attention values from the decoder. Only present when labels is given and output_attentions is True .","title":"cross_attentions"},{"location":"api/modules/transformer/t5/#loss","text":"class T5Output ( NamedTuple ): | ... | loss : Optional [ FloatT ] = None The loss calculating with respect to labels .","title":"loss"},{"location":"api/modules/transformer/t5/#logits","text":"class T5Output ( NamedTuple ): | ... | logits : Optional [ FloatT ] = None The logits that are used to calculate the loss with respect to labels .","title":"logits"},{"location":"api/modules/transformer/t5/#predictions","text":"class T5Output ( NamedTuple ): | ... | predictions : Optional [ IntT ] = None Predicted token IDs from beam search. Shape: (batch_size, beam_size, max_decoding_steps) .","title":"predictions"},{"location":"api/modules/transformer/t5/#predicted_log_probs","text":"class T5Output ( NamedTuple ): | ... | predicted_log_probs : Optional [ FloatT ] = None Probabilities corresponding to predictions . Shape: (batch_size, beam_size,) .","title":"predicted_log_probs"},{"location":"api/modules/transformer/t5/#t5","text":"class T5 ( TransformerModule , Registrable ): | def __init__ ( | self , | token_embeddings : Optional [ nn . Embedding ] = None , | encoder : Lazy [ T5EncoderStack ] = Lazy ( T5EncoderStack . basic_encoder ), | decoder : Lazy [ T5DecoderStack ] = Lazy ( T5DecoderStack . basic_decoder ), | decoder_start_token_id : int = 0 , | pad_token_id : int = 0 , | eos_token_id : int = 1 , | vocab_size : int = 32128 , | model_dim : int = 512 , | output_attentions : bool = False , | output_all_hidden_states : bool = False , | beam_search : Lazy [ BeamSearch ] = Lazy ( BeamSearch , beam_size = 3 , max_steps = 100 ), | ddp_accelerator : Optional [ DdpAccelerator ] = None , | checkpoint_wrapper : Optional [ CheckpointWrapper ] = None , | tie_word_embeddings : bool = True | )","title":"T5"},{"location":"api/modules/transformer/t5/#default_implementation","text":"class T5 ( TransformerModule , Registrable ): | ... | default_implementation = \"default\"","title":"default_implementation"},{"location":"api/modules/transformer/t5/#resize_token_embeddings_1","text":"class T5 ( TransformerModule , Registrable ): | ... | def resize_token_embeddings ( | self , | new_size : int , | * , init_fn : Callable = torch . nn . init . normal_ | ) -> None Resizes the token embeddings in the model. This takes care of the token embeddings for the encoder, the decoder, and the LM head. new_size : int The new size of the token embeddings init_fn : Callable The function to use to initialize new embeddings. This function will be called with a single argument, the tensor to initialize, and it is expected to initialize the tensor in place. Many of the functions from torch.nn.init fit.","title":"resize_token_embeddings"},{"location":"api/modules/transformer/t5/#forward_9","text":"class T5 ( TransformerModule , Registrable ): | ... | def forward ( | self , | input_ids : IntT , | attention_mask : Optional [ BoolT ] = None , | labels : Optional [ IntT ] = None , | decoder_attention_mask : Optional [ BoolT ] = None | ) -> T5Output Run forward pass of the model.","title":"forward"},{"location":"api/modules/transformer/t5/#take_search_step","text":"class T5 ( TransformerModule , Registrable ): | ... | def take_search_step ( | self , | last_predictions : torch . Tensor , | state : Dict [ str , torch . Tensor ], | step : int | ) -> Tuple [ torch . Tensor , Dict [ str , torch . Tensor ]] Take step during beam search. This function is what gets passed to the BeamSearch.search method. It takes predictions from the last timestep and the current state and outputs the log probabilities assigned to tokens for the next timestep, as well as the updated state.","title":"take_search_step"},{"location":"api/modules/transformer/transformer_embeddings/","text":"allennlp .modules .transformer .transformer_embeddings [SOURCE] Embeddings \u00b6 class Embeddings ( TransformerModule , FromParams ): | def __init__ ( | self , | embeddings : torch . nn . ModuleDict , | embedding_size : int , | dropout : float , | layer_norm_eps : float = 1e-12 | ) General class for embeddings for any modality. Parameters \u00b6 embeddings : torch.nn.ModuleDict Named embedding layers. Eg. \"word_embeddings\" , \"position_embeddings\" , etc. All the embedding layers are expected to have different inputs; the output of one will not be passed to the other. All the layers should have the same embedding_dim / out_features . embedding_size : int The embedding_dim of all the embedding layers. dropout : float The probability of an element to be zeroed. forward \u00b6 class Embeddings ( TransformerModule , FromParams ): | ... | def forward ( self , * inputs ) -> torch . Tensor ImageFeatureEmbeddings \u00b6 class ImageFeatureEmbeddings ( Embeddings ): | def __init__ ( | self , | feature_size : int , | embedding_size : int , | dropout : float = 0.0 | ) Embedding module for image features. Parameters \u00b6 feature_size : int Number of image features. embedding_size : int The embedding_dim of all the embedding layers. dropout : float , optional (default = 0.0 ) The probability of an element to be zeroed. TransformerEmbeddings \u00b6 class TransformerEmbeddings ( Embeddings ): | def __init__ ( | self , | vocab_size : int , | embedding_size : int , | pad_token_id : int = 0 , | max_position_embeddings : int = 512 , | position_pad_token_id : Optional [ int ] = None , | type_vocab_size : int = 2 , | dropout : float = 0.1 , | layer_norm_eps : float = 1e-12 , | output_size : Optional [ int ] = None | ) Construct the embeddings from word, position and token_type embeddings. Details in the paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin et al, 2019 Parameters \u00b6 vocab_size : int The size of the input vocab. embedding_size : int The embedding_dim of all the embedding layers. pad_token_id : int , optional (default = 0 ) The token id of the <pad> token. max_position_embeddings : int , optional (default = 512 ) The maximum number of positions. type_vocab_size : int , optional (default = 2 ) The size of the input token_type vocab. dropout : int , optional (default = 0.1 ) The probability of an element to be zeroed. output_size : int , optional (default = None ) Optionally apply a linear transform after the dropout, projecting to output_size . forward \u00b6 class TransformerEmbeddings ( Embeddings ): | ... | def forward ( | self , | input_ids : torch . Tensor , | token_type_ids : Optional [ torch . Tensor ] = None , | position_ids : Optional [ torch . Tensor ] = None , | attention_mask : Optional [ torch . Tensor ] = None | ) -> torch . Tensor Parameters \u00b6 input_ids : torch.Tensor Shape batch_size x seq_len attention_mask : torch.Tensor Shape batch_size x seq_len . This parameter is ignored, but it is here for compatibility. token_type_ids : torch.Tensor , optional Shape batch_size x seq_len position_ids : torch.Tensor , optional Shape batch_size x seq_len","title":"transformer_embeddings"},{"location":"api/modules/transformer/transformer_embeddings/#embeddings","text":"class Embeddings ( TransformerModule , FromParams ): | def __init__ ( | self , | embeddings : torch . nn . ModuleDict , | embedding_size : int , | dropout : float , | layer_norm_eps : float = 1e-12 | ) General class for embeddings for any modality.","title":"Embeddings"},{"location":"api/modules/transformer/transformer_embeddings/#forward","text":"class Embeddings ( TransformerModule , FromParams ): | ... | def forward ( self , * inputs ) -> torch . Tensor","title":"forward"},{"location":"api/modules/transformer/transformer_embeddings/#imagefeatureembeddings","text":"class ImageFeatureEmbeddings ( Embeddings ): | def __init__ ( | self , | feature_size : int , | embedding_size : int , | dropout : float = 0.0 | ) Embedding module for image features.","title":"ImageFeatureEmbeddings"},{"location":"api/modules/transformer/transformer_embeddings/#transformerembeddings","text":"class TransformerEmbeddings ( Embeddings ): | def __init__ ( | self , | vocab_size : int , | embedding_size : int , | pad_token_id : int = 0 , | max_position_embeddings : int = 512 , | position_pad_token_id : Optional [ int ] = None , | type_vocab_size : int = 2 , | dropout : float = 0.1 , | layer_norm_eps : float = 1e-12 , | output_size : Optional [ int ] = None | ) Construct the embeddings from word, position and token_type embeddings. Details in the paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin et al, 2019","title":"TransformerEmbeddings"},{"location":"api/modules/transformer/transformer_embeddings/#forward_1","text":"class TransformerEmbeddings ( Embeddings ): | ... | def forward ( | self , | input_ids : torch . Tensor , | token_type_ids : Optional [ torch . Tensor ] = None , | position_ids : Optional [ torch . Tensor ] = None , | attention_mask : Optional [ torch . Tensor ] = None | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/transformer/transformer_layer/","text":"allennlp .modules .transformer .transformer_layer [SOURCE] AttentionLayer \u00b6 class AttentionLayer ( TransformerModule , FromParams ): | def __init__ ( | self , | hidden_size : int , | num_attention_heads : int , | attention_dropout : float = 0.0 , | hidden_dropout : float = 0.0 , | is_cross_attention : bool = False , | is_decoder : bool = False | ) This module wraps the self-attention with the output-layer, similar to the architecture in BERT. Details in the paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin et al, 2019 Parameters \u00b6 hidden_size : int num_attention_heads : int attention_dropout : float , optional (default = 0.0 ) Dropout probability for the SelfAttention layer. hidden_dropout : float , optional (default = 0.0 ) Dropout probability for the OutputLayer . forward \u00b6 class AttentionLayer ( TransformerModule , FromParams ): | ... | def forward ( | self , | input_tensor : torch . Tensor , | attention_mask : torch . BoolTensor , | head_mask : Optional [ torch . Tensor ] = None , | encoder_hidden_states : Optional [ torch . Tensor ] = None , | encoder_attention_mask : Optional [ torch . BoolTensor ] = None , | output_attentions : bool = False | ) Parameters \u00b6 input_tensor : torch.Tensor Shape batch_size x seq_len x hidden_dim attention_mask : torch.BoolTensor , optional Shape batch_size x seq_len head_mask : torch.BoolTensor , optional output_attentions : bool Whether to also return the attention probabilities, default = False TransformerLayerOutput \u00b6 @dataclass class TransformerLayerOutput Encapsulates the outputs of the TransformerLayer module. hidden_states \u00b6 class TransformerLayerOutput : | ... | hidden_states : FloatT = None self_attention_probs \u00b6 class TransformerLayerOutput : | ... | self_attention_probs : Optional [ FloatT ] = None cross_attention_probs \u00b6 class TransformerLayerOutput : | ... | cross_attention_probs : Optional [ FloatT ] = None TransformerLayer \u00b6 class TransformerLayer ( TransformerModule , FromParams ): | def __init__ ( | self , | hidden_size : int , | intermediate_size : int , | num_attention_heads : int , | attention_dropout : float = 0.0 , | hidden_dropout : float = 0.0 , | activation : Union [ str , torch . nn . Module ] = \"relu\" , | add_cross_attention : bool = False | ) This module is a single transformer layer, mapping to BertLayer in the architecture in BERT. Details in the paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin et al, 2019 Parameters \u00b6 hidden_size : int intermediate_size : int num_attention_heads : int attention_dropout : float , optional (default = 0.0 ) Dropout probability for the SelfAttention layer. hidden_dropout : float , optional (default = 0.0 ) Dropout probability for the OutputLayer . activation : Union[str, torch.nn.Module] add_cross_attention : bool , optional (default = False ) If True, an extra AttentionLayer is added for cross-attention. This is helpful when using the layer in a decoder. get_output_dim \u00b6 class TransformerLayer ( TransformerModule , FromParams ): | ... | def get_output_dim ( self ) -> int forward \u00b6 class TransformerLayer ( TransformerModule , FromParams ): | ... | def forward ( | self , | hidden_states : torch . Tensor , | attention_mask : torch . Tensor , | head_mask : Optional [ torch . Tensor ] = None , | encoder_hidden_states : Optional [ torch . Tensor ] = None , | encoder_attention_mask : Optional [ torch . Tensor ] = None , | output_attentions : bool = False | ) -> TransformerLayerOutput Parameters \u00b6 hidden_states : torch.Tensor Shape batch_size x seq_len x hidden_dim attention_mask : torch.BoolTensor , optional Shape batch_size x seq_len head_mask : torch.BoolTensor , optional encoder_hidden_states : torch.Tensor , optional encoder_attention_mask : torch.Tensor , optional output_attentions : bool Whether to also return the attention probabilities, default = False","title":"transformer_layer"},{"location":"api/modules/transformer/transformer_layer/#attentionlayer","text":"class AttentionLayer ( TransformerModule , FromParams ): | def __init__ ( | self , | hidden_size : int , | num_attention_heads : int , | attention_dropout : float = 0.0 , | hidden_dropout : float = 0.0 , | is_cross_attention : bool = False , | is_decoder : bool = False | ) This module wraps the self-attention with the output-layer, similar to the architecture in BERT. Details in the paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin et al, 2019","title":"AttentionLayer"},{"location":"api/modules/transformer/transformer_layer/#forward","text":"class AttentionLayer ( TransformerModule , FromParams ): | ... | def forward ( | self , | input_tensor : torch . Tensor , | attention_mask : torch . BoolTensor , | head_mask : Optional [ torch . Tensor ] = None , | encoder_hidden_states : Optional [ torch . Tensor ] = None , | encoder_attention_mask : Optional [ torch . BoolTensor ] = None , | output_attentions : bool = False | )","title":"forward"},{"location":"api/modules/transformer/transformer_layer/#transformerlayeroutput","text":"@dataclass class TransformerLayerOutput Encapsulates the outputs of the TransformerLayer module.","title":"TransformerLayerOutput"},{"location":"api/modules/transformer/transformer_layer/#hidden_states","text":"class TransformerLayerOutput : | ... | hidden_states : FloatT = None","title":"hidden_states"},{"location":"api/modules/transformer/transformer_layer/#self_attention_probs","text":"class TransformerLayerOutput : | ... | self_attention_probs : Optional [ FloatT ] = None","title":"self_attention_probs"},{"location":"api/modules/transformer/transformer_layer/#cross_attention_probs","text":"class TransformerLayerOutput : | ... | cross_attention_probs : Optional [ FloatT ] = None","title":"cross_attention_probs"},{"location":"api/modules/transformer/transformer_layer/#transformerlayer","text":"class TransformerLayer ( TransformerModule , FromParams ): | def __init__ ( | self , | hidden_size : int , | intermediate_size : int , | num_attention_heads : int , | attention_dropout : float = 0.0 , | hidden_dropout : float = 0.0 , | activation : Union [ str , torch . nn . Module ] = \"relu\" , | add_cross_attention : bool = False | ) This module is a single transformer layer, mapping to BertLayer in the architecture in BERT. Details in the paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin et al, 2019","title":"TransformerLayer"},{"location":"api/modules/transformer/transformer_layer/#get_output_dim","text":"class TransformerLayer ( TransformerModule , FromParams ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/transformer/transformer_layer/#forward_1","text":"class TransformerLayer ( TransformerModule , FromParams ): | ... | def forward ( | self , | hidden_states : torch . Tensor , | attention_mask : torch . Tensor , | head_mask : Optional [ torch . Tensor ] = None , | encoder_hidden_states : Optional [ torch . Tensor ] = None , | encoder_attention_mask : Optional [ torch . Tensor ] = None , | output_attentions : bool = False | ) -> TransformerLayerOutput","title":"forward"},{"location":"api/modules/transformer/transformer_module/","text":"allennlp .modules .transformer .transformer_module [SOURCE] TransformerModule \u00b6 class TransformerModule ( Module ) Base class to help with generalized loading of pretrained weights. Subclasses should override _from_config() if you want to instantiate them with from_pretrained_module() . _pretrained_mapping \u00b6 class TransformerModule ( Module ): | ... | _pretrained_mapping : Dict [ str , str ] = {} An optional mapping for each class that determines any differences in the module names between the class modules and the HuggingFace model's modules. Keys correspond to HuggingFace submodule names, values correspond to submodules names of this module. _pretrained_relevant_module \u00b6 class TransformerModule ( Module ): | ... | _pretrained_relevant_module : Optional [ Union [ str , List [ str ]]] = None An optional string or list of strings which contains the expected name of the module in the HuggingFace pretrained model. It can be a list to account for different names in different models. The search is carried out in the order of the list. _pretrained_ignore \u00b6 class TransformerModule ( Module ): | ... | _pretrained_ignore : Optional [ List [ str ]] = None An optional list of regular expressions that define which weights to ignore from a pretrained state_dict. _pretrained_allow_missing \u00b6 class TransformerModule ( Module ): | ... | _pretrained_allow_missing : Optional [ List [ str ]] = None An optional list of regular expressions that specifies which weights are allowed to be missing from a pretrained state dictionary. _from_config \u00b6 class TransformerModule ( Module ): | ... | @classmethod | def _from_config ( | cls : Type [ _T ], | config : \"PretrainedConfig\" , | ** kwargs | ) -> _T Instantiate this module from a HuggingFace config. Subclasses should override this method if you want to be able to instantiate them with from_pretrained_module() . from_pretrained_module \u00b6 class TransformerModule ( Module ): | ... | @classmethod | def from_pretrained_module ( | cls : Type [ _T ], | model_name : str , | * , load_weights : bool = True , | * , weights_path : Optional [ Union [ str , PathLike ]] = None , | * , auto_config_kwargs : Optional [ Dict [ str , Any ]] = None , | * , mapping : Optional [ Dict [ str , str ]] = None , | * , relevant_module : Optional [ Union [ str , List [ str ]]] = None , | * , ignore : Optional [ List [ str ]] = None , | * , allow_missing : Optional [ List [ str ]] = None , | * , strict : bool = True , | ** kwargs , | * , , | ) -> _T Initialize this module from a corresponding model on HuggingFace. Note This method is only available for subclasses that implement _from_config() . Otherwise a NotImplementedError will be raised. Parameters \u00b6 model_name : str The model identifier or path. load_weights : bool , optional (default = True ) Whether to download and load the pretrained weights. If False , the weights are left uninitialized. weights_path : Optional[Union[str, PathLike]] , optional (default = None ) When load_weights is True , this can be set to override the weights file. Otherwise the default weights from the pretrained model are used. auto_config_kwargs : Optional[Dict[str, Any]] , optional (default = None ) Optional key-word arguments to pass to transformers.AutoConfig.from_pretrained() to load the pretrained model's configuration file. mapping : Optional[Dict[str, str]] , optional (default = None ) Optional mapping that determines any differences in the submodule names between this module and the pretrained model from HuggingFace. If not given, the class's default is used: cls._pretrained_mapping . relevant_module : Optional[str] , optional (default = None ) An optional submodule of the HuggingFace module to initialize weights from. This is only relevant when load_weights is True . If not given, the class's default is used: cls._pretrained_relevant_module . ignore : Optional[List[str]] , optional (default = None ) An optional list of regular expressions that define which weights to ignore from a pretrained state_dict. This is only relevant when load_weights is True . If not specified, the class's default is used: cls._pretrained_ignore . allow_missing : Optional[List[str]] , optional (default = None ) An optional list of regular expressions that specifies which weights are allowed to be missing from the pretrained state dictionary. This is only relevant when load_weights is True . If not specified, the class's default is used: cls._pretrained_allow_missing . strict : bool , optional (default = True ) Whether to load the state_dict in \"strict\" model. This only applies when load_weights is True . **kwargs : Any Key word arguments to pass to cls.from_config() when instantiating the module.","title":"transformer_module"},{"location":"api/modules/transformer/transformer_module/#transformermodule","text":"class TransformerModule ( Module ) Base class to help with generalized loading of pretrained weights. Subclasses should override _from_config() if you want to instantiate them with from_pretrained_module() .","title":"TransformerModule"},{"location":"api/modules/transformer/transformer_module/#_pretrained_mapping","text":"class TransformerModule ( Module ): | ... | _pretrained_mapping : Dict [ str , str ] = {} An optional mapping for each class that determines any differences in the module names between the class modules and the HuggingFace model's modules. Keys correspond to HuggingFace submodule names, values correspond to submodules names of this module.","title":"_pretrained_mapping"},{"location":"api/modules/transformer/transformer_module/#_pretrained_relevant_module","text":"class TransformerModule ( Module ): | ... | _pretrained_relevant_module : Optional [ Union [ str , List [ str ]]] = None An optional string or list of strings which contains the expected name of the module in the HuggingFace pretrained model. It can be a list to account for different names in different models. The search is carried out in the order of the list.","title":"_pretrained_relevant_module"},{"location":"api/modules/transformer/transformer_module/#_pretrained_ignore","text":"class TransformerModule ( Module ): | ... | _pretrained_ignore : Optional [ List [ str ]] = None An optional list of regular expressions that define which weights to ignore from a pretrained state_dict.","title":"_pretrained_ignore"},{"location":"api/modules/transformer/transformer_module/#_pretrained_allow_missing","text":"class TransformerModule ( Module ): | ... | _pretrained_allow_missing : Optional [ List [ str ]] = None An optional list of regular expressions that specifies which weights are allowed to be missing from a pretrained state dictionary.","title":"_pretrained_allow_missing"},{"location":"api/modules/transformer/transformer_module/#_from_config","text":"class TransformerModule ( Module ): | ... | @classmethod | def _from_config ( | cls : Type [ _T ], | config : \"PretrainedConfig\" , | ** kwargs | ) -> _T Instantiate this module from a HuggingFace config. Subclasses should override this method if you want to be able to instantiate them with from_pretrained_module() .","title":"_from_config"},{"location":"api/modules/transformer/transformer_module/#from_pretrained_module","text":"class TransformerModule ( Module ): | ... | @classmethod | def from_pretrained_module ( | cls : Type [ _T ], | model_name : str , | * , load_weights : bool = True , | * , weights_path : Optional [ Union [ str , PathLike ]] = None , | * , auto_config_kwargs : Optional [ Dict [ str , Any ]] = None , | * , mapping : Optional [ Dict [ str , str ]] = None , | * , relevant_module : Optional [ Union [ str , List [ str ]]] = None , | * , ignore : Optional [ List [ str ]] = None , | * , allow_missing : Optional [ List [ str ]] = None , | * , strict : bool = True , | ** kwargs , | * , , | ) -> _T Initialize this module from a corresponding model on HuggingFace. Note This method is only available for subclasses that implement _from_config() . Otherwise a NotImplementedError will be raised.","title":"from_pretrained_module"},{"location":"api/modules/transformer/transformer_pooler/","text":"allennlp .modules .transformer .transformer_pooler [SOURCE] TransformerPooler \u00b6 class TransformerPooler ( ActivationLayer , FromParams ): | def __init__ ( | self , | hidden_size : int , | intermediate_size : int , | activation : Union [ str , torch . nn . Module ] = \"relu\" | )","title":"transformer_pooler"},{"location":"api/modules/transformer/transformer_pooler/#transformerpooler","text":"class TransformerPooler ( ActivationLayer , FromParams ): | def __init__ ( | self , | hidden_size : int , | intermediate_size : int , | activation : Union [ str , torch . nn . Module ] = \"relu\" | )","title":"TransformerPooler"},{"location":"api/modules/transformer/transformer_stack/","text":"allennlp .modules .transformer .transformer_stack [SOURCE] TransformerStackOutput \u00b6 @dataclass class TransformerStackOutput Encapsulates the outputs of the TransformerStack module. final_hidden_states \u00b6 class TransformerStackOutput : | ... | final_hidden_states : FloatT = None all_hidden_states \u00b6 class TransformerStackOutput : | ... | all_hidden_states : Optional [ Tuple ] = None all_self_attentions \u00b6 class TransformerStackOutput : | ... | all_self_attentions : Optional [ Tuple ] = None all_cross_attentions \u00b6 class TransformerStackOutput : | ... | all_cross_attentions : Optional [ Tuple ] = None TransformerStack \u00b6 class TransformerStack ( TransformerModule , FromParams ): | def __init__ ( | self , | num_hidden_layers : int , | layer : Optional [ TransformerLayer ] = None , | hidden_size : Optional [ int ] = None , | intermediate_size : Optional [ int ] = None , | num_attention_heads : int = 8 , | attention_dropout : float = 0.1 , | hidden_dropout : float = 0.1 , | activation : Union [ str , torch . nn . Module ] = \"relu\" , | add_cross_attention : bool = False | ) This module is the basic transformer stack. Details in the paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin et al, 2019 Parameters \u00b6 num_hidden_layers : int layer : TransformerLayer , optional hidden_size : int , optional This needs to be provided if no layer argument is passed. intermediate_size : int , optional This needs to be provided if no layer argument is passed. num_attention_heads : int attention_dropout : float , optional (default = 0.0 ) Dropout probability for the SelfAttention layer. hidden_dropout : float , optional (default = 0.0 ) Dropout probability for the OutputLayer . activation : Union[str, torch.nn.Module] , optional (default = \"relu\" ) add_cross_attention : bool , optional (default = False ) If True, the TransformerLayer modules will have cross attention modules as well. This is helpful when using the TransformerStack as a decoder. get_output_dim \u00b6 class TransformerStack ( TransformerModule , FromParams ): | ... | def get_output_dim ( self ) -> int forward \u00b6 class TransformerStack ( TransformerModule , FromParams ): | ... | def forward ( | self , | hidden_states : torch . Tensor , | attention_mask : Optional [ torch . Tensor ] = None , | head_mask : Optional [ torch . Tensor ] = None , | encoder_hidden_states : Optional [ torch . Tensor ] = None , | encoder_attention_mask : Optional [ torch . Tensor ] = None , | output_attentions : bool = False , | output_hidden_states : bool = False | ) -> TransformerStackOutput Parameters \u00b6 hidden_states : torch.Tensor Shape batch_size x seq_len x hidden_dim attention_mask : torch.BoolTensor , optional Shape batch_size x seq_len head_mask : torch.BoolTensor , optional output_attentions : bool Whether to also return the attention probabilities, default = False output_hidden_states : bool Whether to return the hidden_states for all layers, default = False","title":"transformer_stack"},{"location":"api/modules/transformer/transformer_stack/#transformerstackoutput","text":"@dataclass class TransformerStackOutput Encapsulates the outputs of the TransformerStack module.","title":"TransformerStackOutput"},{"location":"api/modules/transformer/transformer_stack/#final_hidden_states","text":"class TransformerStackOutput : | ... | final_hidden_states : FloatT = None","title":"final_hidden_states"},{"location":"api/modules/transformer/transformer_stack/#all_hidden_states","text":"class TransformerStackOutput : | ... | all_hidden_states : Optional [ Tuple ] = None","title":"all_hidden_states"},{"location":"api/modules/transformer/transformer_stack/#all_self_attentions","text":"class TransformerStackOutput : | ... | all_self_attentions : Optional [ Tuple ] = None","title":"all_self_attentions"},{"location":"api/modules/transformer/transformer_stack/#all_cross_attentions","text":"class TransformerStackOutput : | ... | all_cross_attentions : Optional [ Tuple ] = None","title":"all_cross_attentions"},{"location":"api/modules/transformer/transformer_stack/#transformerstack","text":"class TransformerStack ( TransformerModule , FromParams ): | def __init__ ( | self , | num_hidden_layers : int , | layer : Optional [ TransformerLayer ] = None , | hidden_size : Optional [ int ] = None , | intermediate_size : Optional [ int ] = None , | num_attention_heads : int = 8 , | attention_dropout : float = 0.1 , | hidden_dropout : float = 0.1 , | activation : Union [ str , torch . nn . Module ] = \"relu\" , | add_cross_attention : bool = False | ) This module is the basic transformer stack. Details in the paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin et al, 2019","title":"TransformerStack"},{"location":"api/modules/transformer/transformer_stack/#get_output_dim","text":"class TransformerStack ( TransformerModule , FromParams ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/transformer/transformer_stack/#forward","text":"class TransformerStack ( TransformerModule , FromParams ): | ... | def forward ( | self , | hidden_states : torch . Tensor , | attention_mask : Optional [ torch . Tensor ] = None , | head_mask : Optional [ torch . Tensor ] = None , | encoder_hidden_states : Optional [ torch . Tensor ] = None , | encoder_attention_mask : Optional [ torch . Tensor ] = None , | output_attentions : bool = False , | output_hidden_states : bool = False | ) -> TransformerStackOutput","title":"forward"},{"location":"api/modules/transformer/util/","text":"allennlp .modules .transformer .util [SOURCE] FloatT \u00b6 FloatT = Union [ torch . FloatTensor ] IntT \u00b6 IntT = Union [ torch . IntTensor ] BoolT \u00b6 BoolT = Union [ torch . BoolTensor ] apply_mask \u00b6 def apply_mask ( values : torch . FloatTensor , mask : Union [ torch . BoolTensor , torch . IntTensor , torch . FloatTensor ] ) -> torch . FloatTensor Parameters \u00b6 values : torch.FloatTensor Shape batch_size x num_attention_heads x source_seq_len x target_seq_len mask : torch.BoolTensor Shape batch_size x target_seq_len OR batch_size x 1 x 1 x target_seq_len get_extended_attention_mask \u00b6 def get_extended_attention_mask ( attention_mask : torch . Tensor , input_shape : Tuple [ int , ... ], dtype : torch . dtype , is_decoder : bool = False ) -> torch . Tensor Makes broadcastable attention and causal masks so that future and masked tokens are ignored. Parameters \u00b6 attention_mask : torch.Tensor Mask with ones indicating tokens to attend to, zeros for tokens to ignore. input_shape : Tuple[int, ...] The shape of the input to the model. dtype : torch.dtype The datatype of the resulting mask. is_decoder : bool , optional (default = False ) If this is for a decoder stack. Returns \u00b6 torch.Tensor The extended attention mask, with a the same dtype as attention_mask.dtype .","title":"util"},{"location":"api/modules/transformer/util/#floatt","text":"FloatT = Union [ torch . FloatTensor ]","title":"FloatT"},{"location":"api/modules/transformer/util/#intt","text":"IntT = Union [ torch . IntTensor ]","title":"IntT"},{"location":"api/modules/transformer/util/#boolt","text":"BoolT = Union [ torch . BoolTensor ]","title":"BoolT"},{"location":"api/modules/transformer/util/#apply_mask","text":"def apply_mask ( values : torch . FloatTensor , mask : Union [ torch . BoolTensor , torch . IntTensor , torch . FloatTensor ] ) -> torch . FloatTensor","title":"apply_mask"},{"location":"api/modules/transformer/util/#get_extended_attention_mask","text":"def get_extended_attention_mask ( attention_mask : torch . Tensor , input_shape : Tuple [ int , ... ], dtype : torch . dtype , is_decoder : bool = False ) -> torch . Tensor Makes broadcastable attention and causal masks so that future and masked tokens are ignored.","title":"get_extended_attention_mask"},{"location":"api/modules/vision/grid_embedder/","text":"allennlp .modules .vision .grid_embedder [SOURCE] GridEmbedder \u00b6 class GridEmbedder ( nn . Module , Registrable ) A GridEmbedder takes a batch of images as a tensor with shape (batch_size, color_channels, height, width) , and returns an ordered dictionary of tensors with shape (batch_size, *) , each representing a specific feature. forward \u00b6 class GridEmbedder ( nn . Module , Registrable ): | ... | def forward ( | self , | images : FloatTensor , | sizes : IntTensor | ) -> \"OrderedDict[str, FloatTensor]\" get_feature_names \u00b6 class GridEmbedder ( nn . Module , Registrable ): | ... | def get_feature_names ( self ) -> Tuple [ str , ... ] Returns the feature names, in order, i.e. the keys of the ordered output dictionary from .forward() . NullGridEmbedder \u00b6 @GridEmbedder . register ( \"null\" ) class NullGridEmbedder ( GridEmbedder ) A GridEmbedder that returns the input image as given. forward \u00b6 class NullGridEmbedder ( GridEmbedder ): | ... | def forward ( | self , | images : FloatTensor , | sizes : IntTensor | ) -> \"OrderedDict[str, FloatTensor]\" get_feature_names \u00b6 class NullGridEmbedder ( GridEmbedder ): | ... | def get_feature_names ( self ) -> Tuple [ str , ... ] ResnetBackbone \u00b6 @GridEmbedder . register ( \"resnet_backbone\" ) class ResnetBackbone ( GridEmbedder ): | def __init__ ( self ) -> None Runs an image through ResNet , as implemented by torchvision . forward \u00b6 class ResnetBackbone ( GridEmbedder ): | ... | def forward ( | self , | images : FloatTensor , | sizes : IntTensor | ) -> \"OrderedDict[str, FloatTensor]\" get_feature_names \u00b6 class ResnetBackbone ( GridEmbedder ): | ... | def get_feature_names ( self ) -> Tuple [ str , ... ]","title":"grid_embedder"},{"location":"api/modules/vision/grid_embedder/#gridembedder","text":"class GridEmbedder ( nn . Module , Registrable ) A GridEmbedder takes a batch of images as a tensor with shape (batch_size, color_channels, height, width) , and returns an ordered dictionary of tensors with shape (batch_size, *) , each representing a specific feature.","title":"GridEmbedder"},{"location":"api/modules/vision/grid_embedder/#forward","text":"class GridEmbedder ( nn . Module , Registrable ): | ... | def forward ( | self , | images : FloatTensor , | sizes : IntTensor | ) -> \"OrderedDict[str, FloatTensor]\"","title":"forward"},{"location":"api/modules/vision/grid_embedder/#get_feature_names","text":"class GridEmbedder ( nn . Module , Registrable ): | ... | def get_feature_names ( self ) -> Tuple [ str , ... ] Returns the feature names, in order, i.e. the keys of the ordered output dictionary from .forward() .","title":"get_feature_names"},{"location":"api/modules/vision/grid_embedder/#nullgridembedder","text":"@GridEmbedder . register ( \"null\" ) class NullGridEmbedder ( GridEmbedder ) A GridEmbedder that returns the input image as given.","title":"NullGridEmbedder"},{"location":"api/modules/vision/grid_embedder/#forward_1","text":"class NullGridEmbedder ( GridEmbedder ): | ... | def forward ( | self , | images : FloatTensor , | sizes : IntTensor | ) -> \"OrderedDict[str, FloatTensor]\"","title":"forward"},{"location":"api/modules/vision/grid_embedder/#get_feature_names_1","text":"class NullGridEmbedder ( GridEmbedder ): | ... | def get_feature_names ( self ) -> Tuple [ str , ... ]","title":"get_feature_names"},{"location":"api/modules/vision/grid_embedder/#resnetbackbone","text":"@GridEmbedder . register ( \"resnet_backbone\" ) class ResnetBackbone ( GridEmbedder ): | def __init__ ( self ) -> None Runs an image through ResNet , as implemented by torchvision .","title":"ResnetBackbone"},{"location":"api/modules/vision/grid_embedder/#forward_2","text":"class ResnetBackbone ( GridEmbedder ): | ... | def forward ( | self , | images : FloatTensor , | sizes : IntTensor | ) -> \"OrderedDict[str, FloatTensor]\"","title":"forward"},{"location":"api/modules/vision/grid_embedder/#get_feature_names_2","text":"class ResnetBackbone ( GridEmbedder ): | ... | def get_feature_names ( self ) -> Tuple [ str , ... ]","title":"get_feature_names"},{"location":"api/modules/vision/image2image/","text":"allennlp .modules .vision .image2image [SOURCE] Image2ImageModule \u00b6 class Image2ImageModule ( nn . Module , Registrable ) An Image2ImageModule takes a batch of images as a tensor with the dimensions (batch_size, color_channels, height, width) , and returns a tensor in the same format, after applying some transformation on the images. forward \u00b6 class Image2ImageModule ( nn . Module , Registrable ): | ... | def forward ( self , images : FloatTensor , sizes : IntTensor ) NormalizeImage \u00b6 @Image2ImageModule . register ( \"normalize\" ) class NormalizeImage ( Image2ImageModule ): | def __init__ ( self , means : List [ float ], stds : List [ float ]) Normalizes an image by subtracting the mean and dividing by the standard deviation, separately for each channel. forward \u00b6 class NormalizeImage ( Image2ImageModule ): | ... | def forward ( self , images : FloatTensor , sizes : IntTensor )","title":"image2image"},{"location":"api/modules/vision/image2image/#image2imagemodule","text":"class Image2ImageModule ( nn . Module , Registrable ) An Image2ImageModule takes a batch of images as a tensor with the dimensions (batch_size, color_channels, height, width) , and returns a tensor in the same format, after applying some transformation on the images.","title":"Image2ImageModule"},{"location":"api/modules/vision/image2image/#forward","text":"class Image2ImageModule ( nn . Module , Registrable ): | ... | def forward ( self , images : FloatTensor , sizes : IntTensor )","title":"forward"},{"location":"api/modules/vision/image2image/#normalizeimage","text":"@Image2ImageModule . register ( \"normalize\" ) class NormalizeImage ( Image2ImageModule ): | def __init__ ( self , means : List [ float ], stds : List [ float ]) Normalizes an image by subtracting the mean and dividing by the standard deviation, separately for each channel.","title":"NormalizeImage"},{"location":"api/modules/vision/image2image/#forward_1","text":"class NormalizeImage ( Image2ImageModule ): | ... | def forward ( self , images : FloatTensor , sizes : IntTensor )","title":"forward"},{"location":"api/modules/vision/region_detector/","text":"allennlp .modules .vision .region_detector [SOURCE] RegionDetectorOutput \u00b6 class RegionDetectorOutput ( NamedTuple ) The output type from the forward pass of a RegionDetector . features \u00b6 class RegionDetectorOutput ( NamedTuple ): | ... | features : List [ Tensor ] = None A list of tensors, each with shape (num_boxes, feature_dim) . boxes \u00b6 class RegionDetectorOutput ( NamedTuple ): | ... | boxes : List [ Tensor ] = None A list of tensors containing the coordinates for each box. Each has shape (num_boxes, 4) . class_probs \u00b6 class RegionDetectorOutput ( NamedTuple ): | ... | class_probs : Optional [ List [ Tensor ]] = None An optional list of tensors. These tensors can have shape (num_boxes,) or (num_boxes, *) if probabilities for multiple classes are given. class_labels \u00b6 class RegionDetectorOutput ( NamedTuple ): | ... | class_labels : Optional [ List [ Tensor ]] = None An optional list of tensors that give the labels corresponding to the class_probs tensors. This should be non- None whenever class_probs is, and each tensor should have the same shape as the corresponding tensor from class_probs . RegionDetector \u00b6 class RegionDetector ( nn . Module , Registrable ) A RegionDetector takes a batch of images, their sizes, and an ordered dictionary of image features as input, and finds regions of interest (or \"boxes\") within those images. Those regions of interest are described by three values: features ( List[Tensor] ): A feature vector for each region, which is a tensor of shape (num_boxes, feature_dim) . boxes ( List[Tensor] ): The coordinates of each region within the original image, with shape (num_boxes, 4) . class_probs ( Optional[List[Tensor]] ): Class probabilities from some object detector that was used to find the regions of interest, with shape (num_boxes,) or (num_boxes, *) if probabilities for more than one class are given. class_labels ( Optional[List[Tensor]] ): The labels corresponding to class_probs . Each tensor in this list has the same shape as the corresponding tensor in class_probs . forward \u00b6 class RegionDetector ( nn . Module , Registrable ): | ... | def forward ( | self , | images : FloatTensor , | sizes : IntTensor , | image_features : \"OrderedDict[str, FloatTensor]\" | ) -> RegionDetectorOutput RandomRegionDetector \u00b6 @RegionDetector . register ( \"random\" ) class RandomRegionDetector ( RegionDetector ): | def __init__ ( self , seed : Optional [ int ] = None ) A RegionDetector that returns two proposals per image, for testing purposes. The features for the proposal are a random 10-dimensional vector, and the coordinates are the size of the image. forward \u00b6 class RandomRegionDetector ( RegionDetector ): | ... | def forward ( | self , | images : FloatTensor , | sizes : IntTensor , | image_features : \"OrderedDict[str, FloatTensor]\" | ) -> RegionDetectorOutput FasterRcnnRegionDetector \u00b6 @RegionDetector . register ( \"faster_rcnn\" ) class FasterRcnnRegionDetector ( RegionDetector ): | def __init__ ( | self , | * , box_score_thresh : float = 0.05 , | * , box_nms_thresh : float = 0.5 , | * , max_boxes_per_image : int = 100 | ) A Faster R-CNN pretrained region detector. Unless you really know what you're doing, this should be used with the image features created from the ResnetBackbone GridEmbedder and on images loaded using the TorchImageLoader with the default settings. Note This module does not have any trainable parameters by default. All pretrained weights are frozen. Parameters \u00b6 box_score_thresh : float , optional (default = 0.05 ) During inference, only proposal boxes / regions with a label classification score greater than box_score_thresh will be returned. box_nms_thresh : float , optional (default = 0.5 ) During inference, non-maximum suppression (NMS) will applied to groups of boxes that share a common label. NMS iteratively removes lower scoring boxes which have an intersection-over-union (IoU) greater than box_nms_thresh with another higher scoring box. max_boxes_per_image : int , optional (default = 100 ) During inference, at most max_boxes_per_image boxes will be returned. The number of boxes returned will vary by image and will often be lower than max_boxes_per_image depending on the values of box_score_thresh and box_nms_thresh . forward \u00b6 class FasterRcnnRegionDetector ( RegionDetector ): | ... | def forward ( | self , | images : FloatTensor , | sizes : IntTensor , | image_features : \"OrderedDict[str, FloatTensor]\" | ) -> RegionDetectorOutput Extract regions and region features from the given images. In most cases image_features should come directly from the ResnetBackbone GridEmbedder . The images themselves should be standardized and resized using the default settings for the TorchImageLoader .","title":"region_detector"},{"location":"api/modules/vision/region_detector/#regiondetectoroutput","text":"class RegionDetectorOutput ( NamedTuple ) The output type from the forward pass of a RegionDetector .","title":"RegionDetectorOutput"},{"location":"api/modules/vision/region_detector/#features","text":"class RegionDetectorOutput ( NamedTuple ): | ... | features : List [ Tensor ] = None A list of tensors, each with shape (num_boxes, feature_dim) .","title":"features"},{"location":"api/modules/vision/region_detector/#boxes","text":"class RegionDetectorOutput ( NamedTuple ): | ... | boxes : List [ Tensor ] = None A list of tensors containing the coordinates for each box. Each has shape (num_boxes, 4) .","title":"boxes"},{"location":"api/modules/vision/region_detector/#class_probs","text":"class RegionDetectorOutput ( NamedTuple ): | ... | class_probs : Optional [ List [ Tensor ]] = None An optional list of tensors. These tensors can have shape (num_boxes,) or (num_boxes, *) if probabilities for multiple classes are given.","title":"class_probs"},{"location":"api/modules/vision/region_detector/#class_labels","text":"class RegionDetectorOutput ( NamedTuple ): | ... | class_labels : Optional [ List [ Tensor ]] = None An optional list of tensors that give the labels corresponding to the class_probs tensors. This should be non- None whenever class_probs is, and each tensor should have the same shape as the corresponding tensor from class_probs .","title":"class_labels"},{"location":"api/modules/vision/region_detector/#regiondetector","text":"class RegionDetector ( nn . Module , Registrable ) A RegionDetector takes a batch of images, their sizes, and an ordered dictionary of image features as input, and finds regions of interest (or \"boxes\") within those images. Those regions of interest are described by three values: features ( List[Tensor] ): A feature vector for each region, which is a tensor of shape (num_boxes, feature_dim) . boxes ( List[Tensor] ): The coordinates of each region within the original image, with shape (num_boxes, 4) . class_probs ( Optional[List[Tensor]] ): Class probabilities from some object detector that was used to find the regions of interest, with shape (num_boxes,) or (num_boxes, *) if probabilities for more than one class are given. class_labels ( Optional[List[Tensor]] ): The labels corresponding to class_probs . Each tensor in this list has the same shape as the corresponding tensor in class_probs .","title":"RegionDetector"},{"location":"api/modules/vision/region_detector/#forward","text":"class RegionDetector ( nn . Module , Registrable ): | ... | def forward ( | self , | images : FloatTensor , | sizes : IntTensor , | image_features : \"OrderedDict[str, FloatTensor]\" | ) -> RegionDetectorOutput","title":"forward"},{"location":"api/modules/vision/region_detector/#randomregiondetector","text":"@RegionDetector . register ( \"random\" ) class RandomRegionDetector ( RegionDetector ): | def __init__ ( self , seed : Optional [ int ] = None ) A RegionDetector that returns two proposals per image, for testing purposes. The features for the proposal are a random 10-dimensional vector, and the coordinates are the size of the image.","title":"RandomRegionDetector"},{"location":"api/modules/vision/region_detector/#forward_1","text":"class RandomRegionDetector ( RegionDetector ): | ... | def forward ( | self , | images : FloatTensor , | sizes : IntTensor , | image_features : \"OrderedDict[str, FloatTensor]\" | ) -> RegionDetectorOutput","title":"forward"},{"location":"api/modules/vision/region_detector/#fasterrcnnregiondetector","text":"@RegionDetector . register ( \"faster_rcnn\" ) class FasterRcnnRegionDetector ( RegionDetector ): | def __init__ ( | self , | * , box_score_thresh : float = 0.05 , | * , box_nms_thresh : float = 0.5 , | * , max_boxes_per_image : int = 100 | ) A Faster R-CNN pretrained region detector. Unless you really know what you're doing, this should be used with the image features created from the ResnetBackbone GridEmbedder and on images loaded using the TorchImageLoader with the default settings. Note This module does not have any trainable parameters by default. All pretrained weights are frozen.","title":"FasterRcnnRegionDetector"},{"location":"api/modules/vision/region_detector/#forward_2","text":"class FasterRcnnRegionDetector ( RegionDetector ): | ... | def forward ( | self , | images : FloatTensor , | sizes : IntTensor , | image_features : \"OrderedDict[str, FloatTensor]\" | ) -> RegionDetectorOutput Extract regions and region features from the given images. In most cases image_features should come directly from the ResnetBackbone GridEmbedder . The images themselves should be standardized and resized using the default settings for the TorchImageLoader .","title":"forward"},{"location":"api/nn/activations/","text":"allennlp .nn .activations [SOURCE] An Activation is just a function that takes some parameters and returns an element-wise activation function. For the most part we just use PyTorch activations . Here we provide a thin wrapper to allow registering them and instantiating them from_params . The available activation functions include \"linear\" \"mish\" \"swish\" \"relu\" \"relu6\" \"elu\" \"prelu\" \"leaky_relu\" \"threshold\" \"hardtanh\" \"sigmoid\" \"tanh\" \"log_sigmoid\" \"softplus\" \"softshrink\" \"softsign\" \"tanhshrink\" \"selu\" Activation \u00b6 class Activation ( torch . nn . Module , Registrable ) Pytorch has a number of built-in activation functions. We group those here under a common type, just to make it easier to configure and instantiate them from_params using Registrable . Note that we're only including element-wise activation functions in this list. You really need to think about masking when you do a softmax or other similar activation function, so it requires a different API. forward \u00b6 class Activation ( torch . nn . Module , Registrable ): | ... | def forward ( self , x : torch . Tensor ) -> torch . Tensor Registrable._registry[Activation] \u00b6 Registrable . _registry [ Activation ] = { \"relu\" : ( torch . nn . ReLU , None ), \"relu6\" : ( torch . nn . ReLU6 , None ), \"elu\" : ( torch . nn . ELU , ... LinearActivation \u00b6 @Activation . register ( \"linear\" ) class LinearActivation ( Activation ) forward \u00b6 class LinearActivation ( Activation ): | ... | def forward ( self , x : torch . Tensor ) -> torch . Tensor MishActivation \u00b6 @Activation . register ( \"mish\" ) class MishActivation ( Activation ) forward \u00b6 class MishActivation ( Activation ): | ... | def forward ( self , x : torch . Tensor ) -> torch . Tensor SwishActivation \u00b6 @Activation . register ( \"swish\" ) class SwishActivation ( Activation ) forward \u00b6 class SwishActivation ( Activation ): | ... | def forward ( self , x : torch . Tensor ) -> torch . Tensor GeluNew \u00b6 @Activation . register ( \"gelu_new\" ) class GeluNew ( Activation ) Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415 forward \u00b6 class GeluNew ( Activation ): | ... | def forward ( self , x : torch . Tensor ) -> torch . Tensor GeluFast \u00b6 @Activation . register ( \"gelu_fast\" ) class GeluFast ( Activation ) forward \u00b6 class GeluFast ( Activation ): | ... | def forward ( self , x : torch . Tensor ) -> torch . Tensor","title":"activations"},{"location":"api/nn/activations/#activation","text":"class Activation ( torch . nn . Module , Registrable ) Pytorch has a number of built-in activation functions. We group those here under a common type, just to make it easier to configure and instantiate them from_params using Registrable . Note that we're only including element-wise activation functions in this list. You really need to think about masking when you do a softmax or other similar activation function, so it requires a different API.","title":"Activation"},{"location":"api/nn/activations/#forward","text":"class Activation ( torch . nn . Module , Registrable ): | ... | def forward ( self , x : torch . Tensor ) -> torch . Tensor","title":"forward"},{"location":"api/nn/activations/#registrable_registryactivation","text":"Registrable . _registry [ Activation ] = { \"relu\" : ( torch . nn . ReLU , None ), \"relu6\" : ( torch . nn . ReLU6 , None ), \"elu\" : ( torch . nn . ELU , ...","title":"Registrable._registry[Activation]"},{"location":"api/nn/activations/#linearactivation","text":"@Activation . register ( \"linear\" ) class LinearActivation ( Activation )","title":"LinearActivation"},{"location":"api/nn/activations/#forward_1","text":"class LinearActivation ( Activation ): | ... | def forward ( self , x : torch . Tensor ) -> torch . Tensor","title":"forward"},{"location":"api/nn/activations/#mishactivation","text":"@Activation . register ( \"mish\" ) class MishActivation ( Activation )","title":"MishActivation"},{"location":"api/nn/activations/#forward_2","text":"class MishActivation ( Activation ): | ... | def forward ( self , x : torch . Tensor ) -> torch . Tensor","title":"forward"},{"location":"api/nn/activations/#swishactivation","text":"@Activation . register ( \"swish\" ) class SwishActivation ( Activation )","title":"SwishActivation"},{"location":"api/nn/activations/#forward_3","text":"class SwishActivation ( Activation ): | ... | def forward ( self , x : torch . Tensor ) -> torch . Tensor","title":"forward"},{"location":"api/nn/activations/#gelunew","text":"@Activation . register ( \"gelu_new\" ) class GeluNew ( Activation ) Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415","title":"GeluNew"},{"location":"api/nn/activations/#forward_4","text":"class GeluNew ( Activation ): | ... | def forward ( self , x : torch . Tensor ) -> torch . Tensor","title":"forward"},{"location":"api/nn/activations/#gelufast","text":"@Activation . register ( \"gelu_fast\" ) class GeluFast ( Activation )","title":"GeluFast"},{"location":"api/nn/activations/#forward_5","text":"class GeluFast ( Activation ): | ... | def forward ( self , x : torch . Tensor ) -> torch . Tensor","title":"forward"},{"location":"api/nn/beam_search/","text":"allennlp .nn .beam_search [SOURCE] StateType \u00b6 StateType = Dict [ str , torch . Tensor ] StepFunctionTypeWithTimestep \u00b6 StepFunctionTypeWithTimestep = Callable [ [ torch . Tensor , StateType , int ], Tuple [ torch . Tensor , StateType ] ] StepFunctionTypeNoTimestep \u00b6 StepFunctionTypeNoTimestep = Callable [[ torch . Tensor , StateType ], Tuple [ torch . Tensor , StateType ]] StepFunctionType \u00b6 StepFunctionType = TypeVar ( \"StepFunctionType\" , StepFunctionTypeWithTimestep , StepFunctionTypeNoTimestep ) The type of step function that can be passed to BeamSearch.search . This can either be StepFunctionTypeWithTimestep or StepFunctionTypeNoTimestep . ConstraintStateType \u00b6 ConstraintStateType = List [ List [ Dict [ str , Any ]]] Sampler \u00b6 class Sampler ( Registrable ) An abstract class that can be used to sample candidates (either nodes or beams) within BeamSearch . A Sampler just has three methods, init_state() , sample_nodes() and sample_beams() . init_state() takes three arguments: a tensor of starting log probs with shape (batch_size,, num_classes) , the batch size, an int, and the number of classes, also an int. It returns a state dictionary with any state tensors needed for subsequent calls to sample_nodes() and sample_beams() . By default this method just returns an empty dictionary. Both sample_nodes() and sample_beams() should take three arguments: tensor of normalized log probabilities with shape (batch_size, num_examples) , an integer representing the number of samples to take for each example in the batch, and a state dictionary which could contain any tensors needed for the Sampler to keep track of state. For sample_nodes() , num_examples = num_classes , but for sample_beams , num_examples = beam_size * per_node_beam_size . The return value should be a tuple containing: a tensor of log probabilities of the sampled examples with shape (batch_size, num_samples) , a tensor of indices of the sampled examples with shape (batch_size, num_samples) , and the updated state dictionary. A default implementation of sample_beams is provided, which just deterministically picks the k examples with highest log probability. default_implementation \u00b6 class Sampler ( Registrable ): | ... | default_implementation = \"deterministic\" init_state \u00b6 class Sampler ( Registrable ): | ... | def init_state ( | self , | start_class_log_probabilities : torch . Tensor , | batch_size : int , | num_classes : int | ) -> StateType sample_nodes \u00b6 class Sampler ( Registrable ): | ... | def sample_nodes ( | self , | log_probs : torch . Tensor , | per_node_beam_size : int , | state : StateType | ) -> Tuple [ torch . Tensor , torch . Tensor , StateType ] sample_beams \u00b6 class Sampler ( Registrable ): | ... | def sample_beams ( | self , | log_probs : torch . Tensor , | beam_size : int , | state : StateType | ) -> Tuple [ torch . Tensor , torch . Tensor , StateType ] DeterministicSampler \u00b6 @Sampler . register ( \"deterministic\" ) class DeterministicSampler ( Sampler ) A Sampler that just deterministically returns the k nodes or beams with highest log probability. sample_nodes \u00b6 class DeterministicSampler ( Sampler ): | ... | def sample_nodes ( | self , | log_probs : torch . Tensor , | per_node_beam_size : int , | state : StateType | ) -> Tuple [ torch . Tensor , torch . Tensor , StateType ] MultinomialSampler \u00b6 @Sampler . register ( \"multinomial\" ) class MultinomialSampler ( Sampler ): | def __init__ ( | self , | temperature : float = 1.0 , | with_replacement : bool = False | ) -> None A Sampler which samples nodes from the given multinomial distribution. Beams are sampled in the default, non-deterministic way. Parameters \u00b6 temperature : float , optional (default = 1.0 ) A temperature below 1.0 produces a sharper probability distribution and a temperature above 1.0 produces a flatter probability distribution. with_replacement : bool , optional (default = False ) Whether to sample with replacement. sample_nodes \u00b6 class MultinomialSampler ( Sampler ): | ... | def sample_nodes ( | self , | log_probs : torch . Tensor , | per_node_beam_size : int , | state : StateType | ) -> Tuple [ torch . Tensor , torch . Tensor , StateType ] TopKSampler \u00b6 @Sampler . register ( \"top-k\" ) class TopKSampler ( Sampler ): | def __init__ ( | self , | k : int = 1 , | temperature : float = 1.0 , | with_replacement : bool = False | ) A Sampler which redistributes the probability mass function for nodes among the top k choices, then samples from that subset after re-normalizing the probabilities. Beams are sampled in the default, deterministic way. Parameters \u00b6 k : int , optional (default = 1 ) The number of top choices to be selected from. temperature : float , optional (default = 1.0 ) A temperature below 1.0 produces a sharper probability distribution and a temperature above 1.0 produces a flatter probability distribution. with_replacement : bool , optional (default = False ) If set to True , samples will be selected with replacement from the top k choices. sample_nodes \u00b6 class TopKSampler ( Sampler ): | ... | def sample_nodes ( | self , | log_probs : torch . Tensor , | per_node_beam_size : int , | state : StateType | ) -> Tuple [ torch . Tensor , torch . Tensor , StateType ] TopPSampler \u00b6 @Sampler . register ( \"top-p\" ) class TopPSampler ( Sampler ): | def __init__ ( | self , | p : float = 0.9 , | temperature : float = 1.0 , | with_replacement : bool = False | ) A Sampler which redistributes the probability mass function for nodes among the top choices with a cumulative probability of at least p , then samples from that subset after re-normalizing the probabilities. Beams are sampled in the default, deterministic way. Parameters \u00b6 p : float , optional (default = 0.9 ) The cumulative probability cutoff threshold. A higher value of p will result in more possible examples to sample from. If with_replacement is False and the number of possible samples is insufficient to sample without replacement from when calling sample_nodes , then the top per_node_beam_size examples will be chosen. temperature : float , optional (default = 1.0 ) A temperature below 1.0 produces a sharper probability distribution and a temperature above 1.0 produces a flatter probability distribution. with_replacement : bool , optional (default = False ) If set to True , samples will be selected with replacement from the top choices. sample_nodes \u00b6 class TopPSampler ( Sampler ): | ... | def sample_nodes ( | self , | log_probs : torch . Tensor , | per_node_beam_size : int , | state : StateType | ) -> Tuple [ torch . Tensor , torch . Tensor , StateType ] GumbelSampler \u00b6 @Sampler . register ( \"gumbel\" ) class GumbelSampler ( Sampler ): | def __init__ ( self , temperature : float = 1.0 ) A Sampler which uses the Gumbel-Top-K trick to sample without replacement. See Stochastic Beams and Where to Find Them: The Gumbel-Top-k Trick for Sampling Sequences Without Replacement , W Kool, H Van Hoof and M Welling, 2010 . Parameters \u00b6 temperature : float , optional (default = 1.0 ) A temperature below 1.0 produces a sharper probability distribution and a temperature above 1.0 produces a flatter probability distribution. init_state \u00b6 class GumbelSampler ( Sampler ): | ... | def init_state ( | self , | start_class_log_probabilities : torch . Tensor , | batch_size : int , | num_classes : int | ) -> StateType sample_nodes \u00b6 class GumbelSampler ( Sampler ): | ... | def sample_nodes ( | self , | log_probs : torch . Tensor , | per_node_beam_size : int , | state : StateType | ) -> Tuple [ torch . Tensor , torch . Tensor , StateType ] sample_beams \u00b6 class GumbelSampler ( Sampler ): | ... | def sample_beams ( | self , | log_probs : torch . Tensor , | beam_size : int , | state : StateType | ) -> Tuple [ torch . Tensor , torch . Tensor , StateType ] Returns the beams with the highest perturbed log probabilities. gumbel \u00b6 class GumbelSampler ( Sampler ): | ... | def gumbel ( self , phi ) -> torch . Tensor Sample Gumbel(phi) . phi should have shape (batch_size, num_classes) . gumbel_with_max \u00b6 class GumbelSampler ( Sampler ): | ... | def gumbel_with_max ( self , phi , T ) -> torch . Tensor Sample Gumbel(phi) conditioned on the maximum value being equal to T . phi should have shape (batch_size, num_classes) and T should have shape (batch_size, 1) . FinalSequenceScorer \u00b6 class FinalSequenceScorer ( Registrable ) An abstract class that can be used to score the final generated sequences found by beam search. Given the predicted sequences and the corresponding log probabilities of those sequences, the class calculates and returns the final score of the sequences. The default implementation scores the sequences using the sum of the log probabilities of the sequence, which is passed as input. default_implementation \u00b6 class FinalSequenceScorer ( Registrable ): | ... | default_implementation = \"sequence-log-prob\" score \u00b6 class FinalSequenceScorer ( Registrable ): | ... | def score ( | self , | predictions : torch . Tensor , | log_probabilities : torch . Tensor , | end_index : int | ) -> torch . Tensor Score the final predictions found by beam search. Parameters \u00b6 predictions : torch.Tensor A tensor containing the initial predictions with shape (batch_size, beam_size, max_steps) . log_probabilities : torch.Tensor A tensor containing the log probabilities of the sequence, defined as the sum of the log probabilities per token, with shape (batch_size, beam_size) . end_index : int The index of the end symbol. Returns \u00b6 torch.Tensor A tensor of the final sequence scores of shape (batch_size, beam_size) . SequenceLogProbabilityScorer \u00b6 @FinalSequenceScorer . register ( \"sequence-log-prob\" ) class SequenceLogProbabilityScorer ( FinalSequenceScorer ) A FinalSequenceScorer which scores the sequences by the sum of the log probabilities across the sequence's tokens. score \u00b6 class SequenceLogProbabilityScorer ( FinalSequenceScorer ): | ... | def score ( | self , | predictions : torch . Tensor , | log_probabilities : torch . Tensor , | end_index : int | ) -> torch . Tensor LengthNormalizedSequenceLogProbabilityScorer \u00b6 @FinalSequenceScorer . register ( \"length-normalized-sequence-log-prob\" ) class LengthNormalizedSequenceLogProbabilityScorer ( FinalSequenceScorer ): | def __init__ ( self , length_penalty : float = 1.0 ) A FinalSequenceScorer which scores the sequences by the average log probability of the tokens in the sequence. It optionally includes a length penalty which promotes or demotes sequences based on their lengths. The final score for a sequence will be (sequence_log_probability) / (sequence_length ** length_penalty) . The sequence length here includes the end token. Parameters \u00b6 length_penalty : float , optional (default = 1.0 ) The length penalty to use. A value of 1.0 means no length penalty is used. A value > 1.0 favors longer sequences, and < 1.0 favors shorter sequences. score \u00b6 class LengthNormalizedSequenceLogProbabilityScorer ( FinalSequenceScorer ): | ... | def score ( | self , | predictions : torch . Tensor , | log_probabilities : torch . Tensor , | end_index : int | ) -> torch . Tensor Constraint \u00b6 class Constraint ( Registrable ): | def __init__ ( self , vocab : Optional [ Vocabulary ] = None ) -> None An abstract class that can be used to enforce constraints on the output predictions by manipulating the class log probabilities during beam search. A Constraint just has three methods that need to be implemented by subclasses: init_state() , apply() and _update_state() . init_state() takes one argument: the batch size, an int It returns a constraint state, which is a nested list of dictionaries, with any state needed for subsequent calls to apply() and update_state() . The length of the outer list should be equal to batch_size . Each inner list should be of length 1. apply() takes two arguments: the constraint state, which is a nested list of dictionaries. The length of the outer list is batch_size and the length of each inner list is beam_size except on the first time apply() is called when it is 1. class_log_probabilities , a tensor of shape (batch_size, beam_size, num_classes) that contains the log probabilities for the classes during search. The first time apply() is called, beam_size = 1 . The apply() method should return new class_log_probabilities that enforce the constraint for this step of beam search. For instance, it may prevent a specific class from being selected by setting the corresponding log probability to a negligible value such as float(\"-inf\") or min_value_of_dtype(class_log_probabilities.dtype) . _update_state() takes two arguments: the copied parent constraint state, which is a nested list of dictionaries. state[i][j] contains the copied state for the parent of last_prediction[i, j] . It is unique to that batch and beam, so it can be directly edited in-place without affecting the others. last_prediction, a tensor of shape (batch_size, beam_size) containing the predictions from the last step of beam search. The _update_state() function should return a new constraint state, a nested list of dictionaries of length batch_size and inner list of length beam_size , one for each of the predictions in last_prediction . init_state \u00b6 class Constraint ( Registrable ): | ... | def init_state ( self , batch_size : int ) -> ConstraintStateType apply \u00b6 class Constraint ( Registrable ): | ... | def apply ( | self , | state : ConstraintStateType , | class_log_probabilities : torch . Tensor | ) -> torch . Tensor update_state \u00b6 class Constraint ( Registrable ): | ... | def update_state ( | self , | state : ConstraintStateType , | last_prediction : torch . Tensor , | last_backpointer : Optional [ torch . Tensor ] = None | ) -> ConstraintStateType _update_state \u00b6 class Constraint ( Registrable ): | ... | def _update_state ( | self , | state : ConstraintStateType , | last_prediction : torch . Tensor | ) -> ConstraintStateType RepeatedNGramBlockingConstraint \u00b6 @Constraint . register ( \"repeated-ngram-blocking\" ) class RepeatedNGramBlockingConstraint ( Constraint ): | def __init__ ( self , ngram_size : int , ** kwargs ) -> None init_state \u00b6 class RepeatedNGramBlockingConstraint ( Constraint ): | ... | def init_state ( self , batch_size : int ) -> ConstraintStateType apply \u00b6 class RepeatedNGramBlockingConstraint ( Constraint ): | ... | def apply ( | self , | state : ConstraintStateType , | class_log_probabilities : torch . Tensor | ) -> torch . Tensor BeamSearch \u00b6 class BeamSearch ( Registrable ): | def __init__ ( | self , | end_index : int , | max_steps : int = 50 , | beam_size : int = 10 , | per_node_beam_size : int = None , | sampler : Sampler = None , | min_steps : Optional [ int ] = None , | final_sequence_scorer : FinalSequenceScorer = None , | constraints : Optional [ List [ Lazy [ Constraint ]]] = None , | vocab : Optional [ Vocabulary ] = None | ) -> None Implements the beam search algorithm for decoding the most likely sequences. Parameters \u00b6 end_index : int The index of the \"stop\" or \"end\" token in the target vocabulary. max_steps : int , optional (default = 50 ) The maximum number of decoding steps to take, i.e. the maximum length of the predicted sequences. beam_size : int , optional (default = 10 ) The width of the beam used. per_node_beam_size : int , optional (default = beam_size ) The maximum number of candidates to consider per node, at each step in the search. If not given, this just defaults to beam_size . Setting this parameter to a number smaller than beam_size may give better results, as it can introduce more diversity into the search. See Beam Search Strategies for Neural Machine Translation , Freitag and Al-Onaizan, 2017 . sampler : Sampler , optional (default = None ) An optional Sampler which is used to pick next candidate nodes and beams. If not specified, DeterministicSampler will be used, which just takes the per_node_beam_size most likely nodes and the beam_size most likely beams. Using the GumbelSampler , on the other hand, will give you Stochastic Beam Search . min_steps : int , optional (default = None ) The minimum number of decoding steps to take, i.e. the minimum length of the predicted sequences. This does not include the start or end tokens. If None , no minimum is enforced. final_sequence_scorer : FinalSequenceScorer , optional (default = None ) An optional FinalSequenceScorer which is used to score the final generated sequences. The output from this module is what is returned by the search method. If not specified, SequenceLogProbabilityScorer will be used, which scores the sequences by the sum of the token log probabilities. constraints : List[Constraint] , optional (default = None ) An optional list of Constraint s which should be applied during beam search. If not provided, no constraints will be enforced. vocab : Vocabulary If constraints is not None , then Vocabulary will be passed to each constraint during its initialization. Having access to the vocabulary may be useful for certain contraints, e.g., to mask out invalid predictions during structured prediction. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"model\", it gets specified as a top-level parameter, then is passed in to the model separately. default_implementation \u00b6 class BeamSearch ( Registrable ): | ... | default_implementation = \"beam_search\" search \u00b6 class BeamSearch ( Registrable ): | ... | @torch . no_grad () | def search ( | self , | start_predictions : torch . Tensor , | start_state : StateType , | step : StepFunctionType | ) -> Tuple [ torch . Tensor , torch . Tensor ] Given a starting state and a step function, apply beam search to find the most likely target sequences. Note If your step function returns -inf for some log probabilities (like if you're using a masked log-softmax) then some of the \"best\" sequences returned may also have -inf log probability. Specifically this happens when the beam size is smaller than the number of actions with finite log probability (non-zero probability) returned by the step function. Therefore if you're using a mask you may want to check the results from search and potentially discard sequences with non-finite log probability. Parameters \u00b6 start_predictions : torch.Tensor A tensor containing the initial predictions with shape (batch_size,) . Usually the initial predictions are just the index of the \"start\" token in the target vocabulary. start_state : StateType The initial state passed to the step function. Each value of the state dict should be a tensor of shape (batch_size, *) , where * means any other number of dimensions. step : StepFunctionType A function that is responsible for computing the next most likely tokens, given the current state and the predictions from the last time step. The function should accept two or three arguments: a tensor of shape (group_size,) representing the index of the predicted tokens from the last time step, the current state, a StateType , and optionally, the timestep, an int . The group_size will be batch_size * beam_size , except in the initial step, for which it will just be batch_size . The function is expected to return a tuple, where the first element is a tensor of shape (group_size, target_vocab_size) containing the log probabilities of the tokens for the next step, and the second element is the updated state. The tensor in the state should have shape (group_size, *) , where * means any other number of dimensions. Returns \u00b6 Tuple[torch.Tensor, torch.Tensor] Tuple of (predictions, final_scores) , where predictions has shape (batch_size, beam_size, max_steps) and final_scores has shape (batch_size, beam_size) .","title":"beam_search"},{"location":"api/nn/beam_search/#statetype","text":"StateType = Dict [ str , torch . Tensor ]","title":"StateType"},{"location":"api/nn/beam_search/#stepfunctiontypewithtimestep","text":"StepFunctionTypeWithTimestep = Callable [ [ torch . Tensor , StateType , int ], Tuple [ torch . Tensor , StateType ] ]","title":"StepFunctionTypeWithTimestep"},{"location":"api/nn/beam_search/#stepfunctiontypenotimestep","text":"StepFunctionTypeNoTimestep = Callable [[ torch . Tensor , StateType ], Tuple [ torch . Tensor , StateType ]]","title":"StepFunctionTypeNoTimestep"},{"location":"api/nn/beam_search/#stepfunctiontype","text":"StepFunctionType = TypeVar ( \"StepFunctionType\" , StepFunctionTypeWithTimestep , StepFunctionTypeNoTimestep ) The type of step function that can be passed to BeamSearch.search . This can either be StepFunctionTypeWithTimestep or StepFunctionTypeNoTimestep .","title":"StepFunctionType"},{"location":"api/nn/beam_search/#constraintstatetype","text":"ConstraintStateType = List [ List [ Dict [ str , Any ]]]","title":"ConstraintStateType"},{"location":"api/nn/beam_search/#sampler","text":"class Sampler ( Registrable ) An abstract class that can be used to sample candidates (either nodes or beams) within BeamSearch . A Sampler just has three methods, init_state() , sample_nodes() and sample_beams() . init_state() takes three arguments: a tensor of starting log probs with shape (batch_size,, num_classes) , the batch size, an int, and the number of classes, also an int. It returns a state dictionary with any state tensors needed for subsequent calls to sample_nodes() and sample_beams() . By default this method just returns an empty dictionary. Both sample_nodes() and sample_beams() should take three arguments: tensor of normalized log probabilities with shape (batch_size, num_examples) , an integer representing the number of samples to take for each example in the batch, and a state dictionary which could contain any tensors needed for the Sampler to keep track of state. For sample_nodes() , num_examples = num_classes , but for sample_beams , num_examples = beam_size * per_node_beam_size . The return value should be a tuple containing: a tensor of log probabilities of the sampled examples with shape (batch_size, num_samples) , a tensor of indices of the sampled examples with shape (batch_size, num_samples) , and the updated state dictionary. A default implementation of sample_beams is provided, which just deterministically picks the k examples with highest log probability.","title":"Sampler"},{"location":"api/nn/beam_search/#default_implementation","text":"class Sampler ( Registrable ): | ... | default_implementation = \"deterministic\"","title":"default_implementation"},{"location":"api/nn/beam_search/#init_state","text":"class Sampler ( Registrable ): | ... | def init_state ( | self , | start_class_log_probabilities : torch . Tensor , | batch_size : int , | num_classes : int | ) -> StateType","title":"init_state"},{"location":"api/nn/beam_search/#sample_nodes","text":"class Sampler ( Registrable ): | ... | def sample_nodes ( | self , | log_probs : torch . Tensor , | per_node_beam_size : int , | state : StateType | ) -> Tuple [ torch . Tensor , torch . Tensor , StateType ]","title":"sample_nodes"},{"location":"api/nn/beam_search/#sample_beams","text":"class Sampler ( Registrable ): | ... | def sample_beams ( | self , | log_probs : torch . Tensor , | beam_size : int , | state : StateType | ) -> Tuple [ torch . Tensor , torch . Tensor , StateType ]","title":"sample_beams"},{"location":"api/nn/beam_search/#deterministicsampler","text":"@Sampler . register ( \"deterministic\" ) class DeterministicSampler ( Sampler ) A Sampler that just deterministically returns the k nodes or beams with highest log probability.","title":"DeterministicSampler"},{"location":"api/nn/beam_search/#sample_nodes_1","text":"class DeterministicSampler ( Sampler ): | ... | def sample_nodes ( | self , | log_probs : torch . Tensor , | per_node_beam_size : int , | state : StateType | ) -> Tuple [ torch . Tensor , torch . Tensor , StateType ]","title":"sample_nodes"},{"location":"api/nn/beam_search/#multinomialsampler","text":"@Sampler . register ( \"multinomial\" ) class MultinomialSampler ( Sampler ): | def __init__ ( | self , | temperature : float = 1.0 , | with_replacement : bool = False | ) -> None A Sampler which samples nodes from the given multinomial distribution. Beams are sampled in the default, non-deterministic way.","title":"MultinomialSampler"},{"location":"api/nn/beam_search/#sample_nodes_2","text":"class MultinomialSampler ( Sampler ): | ... | def sample_nodes ( | self , | log_probs : torch . Tensor , | per_node_beam_size : int , | state : StateType | ) -> Tuple [ torch . Tensor , torch . Tensor , StateType ]","title":"sample_nodes"},{"location":"api/nn/beam_search/#topksampler","text":"@Sampler . register ( \"top-k\" ) class TopKSampler ( Sampler ): | def __init__ ( | self , | k : int = 1 , | temperature : float = 1.0 , | with_replacement : bool = False | ) A Sampler which redistributes the probability mass function for nodes among the top k choices, then samples from that subset after re-normalizing the probabilities. Beams are sampled in the default, deterministic way.","title":"TopKSampler"},{"location":"api/nn/beam_search/#sample_nodes_3","text":"class TopKSampler ( Sampler ): | ... | def sample_nodes ( | self , | log_probs : torch . Tensor , | per_node_beam_size : int , | state : StateType | ) -> Tuple [ torch . Tensor , torch . Tensor , StateType ]","title":"sample_nodes"},{"location":"api/nn/beam_search/#toppsampler","text":"@Sampler . register ( \"top-p\" ) class TopPSampler ( Sampler ): | def __init__ ( | self , | p : float = 0.9 , | temperature : float = 1.0 , | with_replacement : bool = False | ) A Sampler which redistributes the probability mass function for nodes among the top choices with a cumulative probability of at least p , then samples from that subset after re-normalizing the probabilities. Beams are sampled in the default, deterministic way.","title":"TopPSampler"},{"location":"api/nn/beam_search/#sample_nodes_4","text":"class TopPSampler ( Sampler ): | ... | def sample_nodes ( | self , | log_probs : torch . Tensor , | per_node_beam_size : int , | state : StateType | ) -> Tuple [ torch . Tensor , torch . Tensor , StateType ]","title":"sample_nodes"},{"location":"api/nn/beam_search/#gumbelsampler","text":"@Sampler . register ( \"gumbel\" ) class GumbelSampler ( Sampler ): | def __init__ ( self , temperature : float = 1.0 ) A Sampler which uses the Gumbel-Top-K trick to sample without replacement. See Stochastic Beams and Where to Find Them: The Gumbel-Top-k Trick for Sampling Sequences Without Replacement , W Kool, H Van Hoof and M Welling, 2010 .","title":"GumbelSampler"},{"location":"api/nn/beam_search/#init_state_1","text":"class GumbelSampler ( Sampler ): | ... | def init_state ( | self , | start_class_log_probabilities : torch . Tensor , | batch_size : int , | num_classes : int | ) -> StateType","title":"init_state"},{"location":"api/nn/beam_search/#sample_nodes_5","text":"class GumbelSampler ( Sampler ): | ... | def sample_nodes ( | self , | log_probs : torch . Tensor , | per_node_beam_size : int , | state : StateType | ) -> Tuple [ torch . Tensor , torch . Tensor , StateType ]","title":"sample_nodes"},{"location":"api/nn/beam_search/#sample_beams_1","text":"class GumbelSampler ( Sampler ): | ... | def sample_beams ( | self , | log_probs : torch . Tensor , | beam_size : int , | state : StateType | ) -> Tuple [ torch . Tensor , torch . Tensor , StateType ] Returns the beams with the highest perturbed log probabilities.","title":"sample_beams"},{"location":"api/nn/beam_search/#gumbel","text":"class GumbelSampler ( Sampler ): | ... | def gumbel ( self , phi ) -> torch . Tensor Sample Gumbel(phi) . phi should have shape (batch_size, num_classes) .","title":"gumbel"},{"location":"api/nn/beam_search/#gumbel_with_max","text":"class GumbelSampler ( Sampler ): | ... | def gumbel_with_max ( self , phi , T ) -> torch . Tensor Sample Gumbel(phi) conditioned on the maximum value being equal to T . phi should have shape (batch_size, num_classes) and T should have shape (batch_size, 1) .","title":"gumbel_with_max"},{"location":"api/nn/beam_search/#finalsequencescorer","text":"class FinalSequenceScorer ( Registrable ) An abstract class that can be used to score the final generated sequences found by beam search. Given the predicted sequences and the corresponding log probabilities of those sequences, the class calculates and returns the final score of the sequences. The default implementation scores the sequences using the sum of the log probabilities of the sequence, which is passed as input.","title":"FinalSequenceScorer"},{"location":"api/nn/beam_search/#default_implementation_1","text":"class FinalSequenceScorer ( Registrable ): | ... | default_implementation = \"sequence-log-prob\"","title":"default_implementation"},{"location":"api/nn/beam_search/#score","text":"class FinalSequenceScorer ( Registrable ): | ... | def score ( | self , | predictions : torch . Tensor , | log_probabilities : torch . Tensor , | end_index : int | ) -> torch . Tensor Score the final predictions found by beam search.","title":"score"},{"location":"api/nn/beam_search/#sequencelogprobabilityscorer","text":"@FinalSequenceScorer . register ( \"sequence-log-prob\" ) class SequenceLogProbabilityScorer ( FinalSequenceScorer ) A FinalSequenceScorer which scores the sequences by the sum of the log probabilities across the sequence's tokens.","title":"SequenceLogProbabilityScorer"},{"location":"api/nn/beam_search/#score_1","text":"class SequenceLogProbabilityScorer ( FinalSequenceScorer ): | ... | def score ( | self , | predictions : torch . Tensor , | log_probabilities : torch . Tensor , | end_index : int | ) -> torch . Tensor","title":"score"},{"location":"api/nn/beam_search/#lengthnormalizedsequencelogprobabilityscorer","text":"@FinalSequenceScorer . register ( \"length-normalized-sequence-log-prob\" ) class LengthNormalizedSequenceLogProbabilityScorer ( FinalSequenceScorer ): | def __init__ ( self , length_penalty : float = 1.0 ) A FinalSequenceScorer which scores the sequences by the average log probability of the tokens in the sequence. It optionally includes a length penalty which promotes or demotes sequences based on their lengths. The final score for a sequence will be (sequence_log_probability) / (sequence_length ** length_penalty) . The sequence length here includes the end token.","title":"LengthNormalizedSequenceLogProbabilityScorer"},{"location":"api/nn/beam_search/#score_2","text":"class LengthNormalizedSequenceLogProbabilityScorer ( FinalSequenceScorer ): | ... | def score ( | self , | predictions : torch . Tensor , | log_probabilities : torch . Tensor , | end_index : int | ) -> torch . Tensor","title":"score"},{"location":"api/nn/beam_search/#constraint","text":"class Constraint ( Registrable ): | def __init__ ( self , vocab : Optional [ Vocabulary ] = None ) -> None An abstract class that can be used to enforce constraints on the output predictions by manipulating the class log probabilities during beam search. A Constraint just has three methods that need to be implemented by subclasses: init_state() , apply() and _update_state() . init_state() takes one argument: the batch size, an int It returns a constraint state, which is a nested list of dictionaries, with any state needed for subsequent calls to apply() and update_state() . The length of the outer list should be equal to batch_size . Each inner list should be of length 1. apply() takes two arguments: the constraint state, which is a nested list of dictionaries. The length of the outer list is batch_size and the length of each inner list is beam_size except on the first time apply() is called when it is 1. class_log_probabilities , a tensor of shape (batch_size, beam_size, num_classes) that contains the log probabilities for the classes during search. The first time apply() is called, beam_size = 1 . The apply() method should return new class_log_probabilities that enforce the constraint for this step of beam search. For instance, it may prevent a specific class from being selected by setting the corresponding log probability to a negligible value such as float(\"-inf\") or min_value_of_dtype(class_log_probabilities.dtype) . _update_state() takes two arguments: the copied parent constraint state, which is a nested list of dictionaries. state[i][j] contains the copied state for the parent of last_prediction[i, j] . It is unique to that batch and beam, so it can be directly edited in-place without affecting the others. last_prediction, a tensor of shape (batch_size, beam_size) containing the predictions from the last step of beam search. The _update_state() function should return a new constraint state, a nested list of dictionaries of length batch_size and inner list of length beam_size , one for each of the predictions in last_prediction .","title":"Constraint"},{"location":"api/nn/beam_search/#init_state_2","text":"class Constraint ( Registrable ): | ... | def init_state ( self , batch_size : int ) -> ConstraintStateType","title":"init_state"},{"location":"api/nn/beam_search/#apply","text":"class Constraint ( Registrable ): | ... | def apply ( | self , | state : ConstraintStateType , | class_log_probabilities : torch . Tensor | ) -> torch . Tensor","title":"apply"},{"location":"api/nn/beam_search/#update_state","text":"class Constraint ( Registrable ): | ... | def update_state ( | self , | state : ConstraintStateType , | last_prediction : torch . Tensor , | last_backpointer : Optional [ torch . Tensor ] = None | ) -> ConstraintStateType","title":"update_state"},{"location":"api/nn/beam_search/#_update_state","text":"class Constraint ( Registrable ): | ... | def _update_state ( | self , | state : ConstraintStateType , | last_prediction : torch . Tensor | ) -> ConstraintStateType","title":"_update_state"},{"location":"api/nn/beam_search/#repeatedngramblockingconstraint","text":"@Constraint . register ( \"repeated-ngram-blocking\" ) class RepeatedNGramBlockingConstraint ( Constraint ): | def __init__ ( self , ngram_size : int , ** kwargs ) -> None","title":"RepeatedNGramBlockingConstraint"},{"location":"api/nn/beam_search/#init_state_3","text":"class RepeatedNGramBlockingConstraint ( Constraint ): | ... | def init_state ( self , batch_size : int ) -> ConstraintStateType","title":"init_state"},{"location":"api/nn/beam_search/#apply_1","text":"class RepeatedNGramBlockingConstraint ( Constraint ): | ... | def apply ( | self , | state : ConstraintStateType , | class_log_probabilities : torch . Tensor | ) -> torch . Tensor","title":"apply"},{"location":"api/nn/beam_search/#beamsearch","text":"class BeamSearch ( Registrable ): | def __init__ ( | self , | end_index : int , | max_steps : int = 50 , | beam_size : int = 10 , | per_node_beam_size : int = None , | sampler : Sampler = None , | min_steps : Optional [ int ] = None , | final_sequence_scorer : FinalSequenceScorer = None , | constraints : Optional [ List [ Lazy [ Constraint ]]] = None , | vocab : Optional [ Vocabulary ] = None | ) -> None Implements the beam search algorithm for decoding the most likely sequences.","title":"BeamSearch"},{"location":"api/nn/beam_search/#default_implementation_2","text":"class BeamSearch ( Registrable ): | ... | default_implementation = \"beam_search\"","title":"default_implementation"},{"location":"api/nn/beam_search/#search","text":"class BeamSearch ( Registrable ): | ... | @torch . no_grad () | def search ( | self , | start_predictions : torch . Tensor , | start_state : StateType , | step : StepFunctionType | ) -> Tuple [ torch . Tensor , torch . Tensor ] Given a starting state and a step function, apply beam search to find the most likely target sequences. Note If your step function returns -inf for some log probabilities (like if you're using a masked log-softmax) then some of the \"best\" sequences returned may also have -inf log probability. Specifically this happens when the beam size is smaller than the number of actions with finite log probability (non-zero probability) returned by the step function. Therefore if you're using a mask you may want to check the results from search and potentially discard sequences with non-finite log probability.","title":"search"},{"location":"api/nn/chu_liu_edmonds/","text":"allennlp .nn .chu_liu_edmonds [SOURCE] decode_mst \u00b6 def decode_mst ( energy : numpy . ndarray , length : int , has_labels : bool = True ) -> Tuple [ numpy . ndarray , numpy . ndarray ] Note: Counter to typical intuition, this function decodes the maximum spanning tree. Decode the optimal MST tree with the Chu-Liu-Edmonds algorithm for maximum spanning arborescences on graphs. Parameters \u00b6 energy : numpy.ndarray A tensor with shape (num_labels, timesteps, timesteps) containing the energy of each edge. If has_labels is False , the tensor should have shape (timesteps, timesteps) instead. length : int The length of this sequence, as the energy may have come from a padded batch. has_labels : bool , optional (default = True ) Whether the graph has labels or not. chu_liu_edmonds \u00b6 def chu_liu_edmonds ( length : int , score_matrix : numpy . ndarray , current_nodes : List [ bool ], final_edges : Dict [ int , int ], old_input : numpy . ndarray , old_output : numpy . ndarray , representatives : List [ Set [ int ]] ) Applies the chu-liu-edmonds algorithm recursively to a graph with edge weights defined by score_matrix. Note that this function operates in place, so variables will be modified. Parameters \u00b6 length : int The number of nodes. score_matrix : numpy.ndarray The score matrix representing the scores for pairs of nodes. current_nodes : List[bool] The nodes which are representatives in the graph. A representative at it's most basic represents a node, but as the algorithm progresses, individual nodes will represent collapsed cycles in the graph. final_edges : Dict[int, int] An empty dictionary which will be populated with the nodes which are connected in the maximum spanning tree. old_input : numpy.ndarray old_output : numpy.ndarray representatives : List[Set[int]] A list containing the nodes that a particular node is representing at this iteration in the graph. Returns \u00b6 Nothing - all variables are modified in place.","title":"chu_liu_edmonds"},{"location":"api/nn/chu_liu_edmonds/#decode_mst","text":"def decode_mst ( energy : numpy . ndarray , length : int , has_labels : bool = True ) -> Tuple [ numpy . ndarray , numpy . ndarray ] Note: Counter to typical intuition, this function decodes the maximum spanning tree. Decode the optimal MST tree with the Chu-Liu-Edmonds algorithm for maximum spanning arborescences on graphs.","title":"decode_mst"},{"location":"api/nn/chu_liu_edmonds/#chu_liu_edmonds","text":"def chu_liu_edmonds ( length : int , score_matrix : numpy . ndarray , current_nodes : List [ bool ], final_edges : Dict [ int , int ], old_input : numpy . ndarray , old_output : numpy . ndarray , representatives : List [ Set [ int ]] ) Applies the chu-liu-edmonds algorithm recursively to a graph with edge weights defined by score_matrix. Note that this function operates in place, so variables will be modified.","title":"chu_liu_edmonds"},{"location":"api/nn/initializers/","text":"allennlp .nn .initializers [SOURCE] An initializer is just a PyTorch function. Here we implement a proxy class that allows us to register them and supply any additional function arguments (for example, the mean and std of a normal initializer) as named arguments to the constructor. The available initialization functions are \"normal\" \"uniform\" \"constant\" \"eye\" \"dirac\" \"xavier_uniform\" \"xavier_normal\" \"kaiming_uniform\" \"kaiming_normal\" \"orthogonal\" \"sparse\" \"block_orthogonal\" \"uniform_unit_scaling\" \"pretrained\" Initializer \u00b6 class Initializer ( Registrable ) An initializer is really just a bare pytorch function. This class is a proxy that allows us to implement Registrable for those functions. default_implementation \u00b6 class Initializer ( Registrable ): | ... | default_implementation = \"normal\" __call__ \u00b6 class Initializer ( Registrable ): | ... | def __call__ ( self , tensor : torch . Tensor , ** kwargs ) -> None This function is here just to make mypy happy. We expect initialization functions to follow this API; the builtin pytorch initialization functions follow this just fine, even though they don't subclass Initialization . We're just making it explicit here, so mypy knows that initializers are callable like this. uniform_unit_scaling \u00b6 def uniform_unit_scaling ( tensor : torch . Tensor , nonlinearity : str = \"linear\" ) An initaliser which preserves output variance for approximately gaussian distributed inputs. This boils down to initialising layers using a uniform distribution in the range (-sqrt(3/dim[0]) * scale, sqrt(3 / dim[0]) * scale) , where dim[0] is equal to the input dimension of the parameter and the scale is a constant scaling factor which depends on the non-linearity used. See Random Walk Initialisation for Training Very Deep Feedforward Networks <https://www.semanticscholar.org/paper/Random-Walk-Initialization-for-Training-Very-Deep-Sussillo-Abbott/be9728a0728b6acf7a485225b1e41592176eda0b> _ for more information. Parameters \u00b6 tensor : torch.Tensor The tensor to initialise. nonlinearity : str , optional (default = \"linear\" ) The non-linearity which is performed after the projection that this tensor is involved in. This must be the name of a function contained in the torch.nn.functional package. Returns \u00b6 The initialised tensor. block_orthogonal \u00b6 def block_orthogonal ( tensor : torch . Tensor , split_sizes : List [ int ], gain : float = 1.0 ) -> None An initializer which allows initializing model parameters in \"blocks\". This is helpful in the case of recurrent models which use multiple gates applied to linear projections, which can be computed efficiently if they are concatenated together. However, they are separate parameters which should be initialized independently. Parameters \u00b6 tensor : torch.Tensor A tensor to initialize. split_sizes : List[int] A list of length tensor.ndim() specifying the size of the blocks along that particular dimension. E.g. [10, 20] would result in the tensor being split into chunks of size 10 along the first dimension and 20 along the second. gain : float , optional (default = 1.0 ) The gain (scaling) applied to the orthogonal initialization. zero \u00b6 def zero ( tensor : torch . Tensor ) -> None lstm_hidden_bias \u00b6 def lstm_hidden_bias ( tensor : torch . Tensor ) -> None Initialize the biases of the forget gate to 1, and all other gates to 0, following Jozefowicz et al., An Empirical Exploration of Recurrent Network Architectures NormalInitializer \u00b6 @Initializer . register ( \"normal\" ) class NormalInitializer ( _InitializerWrapper ): | def __init__ ( self , mean : float = 0.0 , std : float = 0.1 ) Registered as an Initializer with name \"normal\". OrthogonalInitializer \u00b6 @Initializer . register ( \"orthogonal\" ) class OrthogonalInitializer ( _InitializerWrapper ): | def __init__ ( self , gain : float = 1.0 ) Registered as an Initializer with name \"orthogonal\". UniformInitializer \u00b6 @Initializer . register ( \"uniform\" ) class UniformInitializer ( _InitializerWrapper ): | def __init__ ( self , a : float = 0.0 , b : float = 1.0 ) Registered as an Initializer with name \"uniform\". ConstantInitializer \u00b6 @Initializer . register ( \"constant\" ) class ConstantInitializer ( _InitializerWrapper ): | def __init__ ( self , val : float ) Registered as an Initializer with name \"constant\". DiracInitializer \u00b6 @Initializer . register ( \"dirac\" ) class DiracInitializer ( _InitializerWrapper ): | def __init__ ( self ) Registered as an Initializer with name \"dirac\". XavierUniformInitializer \u00b6 @Initializer . register ( \"xavier_uniform\" ) class XavierUniformInitializer ( _InitializerWrapper ): | def __init__ ( self , gain : float = 1.0 ) Registered as an Initializer with name \"xavir_uniform\". XavierNormalInitializer \u00b6 @Initializer . register ( \"xavier_normal\" ) class XavierNormalInitializer ( _InitializerWrapper ): | def __init__ ( self , gain : float = 1.0 ) Registered as an Initializer with name \"xavier_normal\". KaimingUniformInitializer \u00b6 @Initializer . register ( \"kaiming_uniform\" ) class KaimingUniformInitializer ( _InitializerWrapper ): | def __init__ ( | self , | a : float = 0.0 , | mode : str = \"fan_in\" , | nonlinearity : str = \"leaky_relu\" | ) Registered as an Initializer with name \"kaiming_uniform\". KaimingNormalInitializer \u00b6 @Initializer . register ( \"kaiming_normal\" ) class KaimingNormalInitializer ( _InitializerWrapper ): | def __init__ ( | self , | a : float = 0.0 , | mode : str = \"fan_in\" , | nonlinearity : str = \"leaky_relu\" | ) Registered as an Initializer with name \"kaiming_normal\". SparseInitializer \u00b6 @Initializer . register ( \"sparse\" ) class SparseInitializer ( _InitializerWrapper ): | def __init__ ( self , sparsity : float , std : float = 0.01 ) Registered as an Initializer with name \"sparse\". EyeInitializer \u00b6 @Initializer . register ( \"eye\" ) class EyeInitializer ( _InitializerWrapper ): | def __init__ ( self ) Registered as an Initializer with name \"eye\". BlockOrthogonalInitializer \u00b6 @Initializer . register ( \"block_orthogonal\" ) class BlockOrthogonalInitializer ( _InitializerWrapper ): | def __init__ ( self , split_sizes : List [ int ], gain : float = 1.0 ) Registered as an Initializer with name \"block_orthogonal\". UniformUnitScalingInitializer \u00b6 @Initializer . register ( \"uniform_unit_scaling\" ) class UniformUnitScalingInitializer ( _InitializerWrapper ): | def __init__ ( self , nonlinearity : str = \"linear\" ) Registered as an Initializer with name \"uniform_unit_scaling\". ZeroInitializer \u00b6 @Initializer . register ( \"zero\" ) class ZeroInitializer ( _InitializerWrapper ): | def __init__ ( self ) Registered as an Initializer with name \"zero\". LstmHiddenBiasInitializer \u00b6 @Initializer . register ( \"lstm_hidden_bias\" ) class LstmHiddenBiasInitializer ( _InitializerWrapper ): | def __init__ ( self ) Registered as an Initializer with name \"lstm_hidden_bias\". PretrainedModelInitializer \u00b6 @Initializer . register ( \"pretrained\" ) class PretrainedModelInitializer ( Initializer ): | def __init__ ( | self , | weights_file_path : str , | parameter_name_overrides : Dict [ str , str ] = None | ) -> None An initializer which allows initializing parameters using a pretrained model. The initializer will load all of the weights from the weights_file_path and use the name of the new parameters to index into the pretrained parameters. Therefore, by default, the names of the new and pretrained parameters must be the same. However, this behavior can be overridden using the parameter_name_overrides , which remaps the name of the new parameter to the key which should be used to index into the pretrained parameters. The initializer will load all of the weights from the weights_file_path regardless of which parameters will actually be used to initialize the new model. So, if you need to initialize several parameters using a pretrained model, the most memory-efficient way to do this is to use one PretrainedModelInitializer per weights file and use a regex to match all of the new parameters which need to be initialized. If you are using a configuration file to instantiate this object, the below entry in the InitializerApplicator parameters will initialize linear_1.weight and linear_2.weight using a pretrained model. linear_1.weight will be initialized to the pretrained parameters called linear_1.weight , but linear_2.weight will be initialized to the pretrained parameters called linear_3.weight :: [\"linear_1.weight|linear_2.weight\", { \"type\": \"pretrained\", \"weights_file_path\": \"best.th\", \"parameter_name_overrides\": { \"linear_2.weight\": \"linear_3.weight\" } } ] To initialize weights for all the parameters from a pretrained model (assuming their names remain unchanged), use the following instead: [\".*\", { \"type\": \"pretrained\", \"weights_file_path\": \"best.th\", \"parameter_name_overrides\": {} } ] Registered as an Initializer with name \"pretrained\". Parameters \u00b6 weights_file_path : str The path to the weights file which has the pretrained model parameters. parameter_name_overrides : Dict[str, str] , optional (default = None ) The mapping from the new parameter name to the name which should be used to index into the pretrained model parameters. If a parameter name is not specified, the initializer will use the parameter's default name as the key. __call__ \u00b6 class PretrainedModelInitializer ( Initializer ): | ... | def __call__ ( | self , | tensor : torch . Tensor , | parameter_name : str , | ** kwargs | ) -> None InitializerApplicator \u00b6 class InitializerApplicator ( FromParams ): | def __init__ ( | self , | regexes : List [ Tuple [ str , Initializer ]] = None , | prevent_regexes : List [ str ] = None | ) -> None Applies initializers to the parameters of a Module based on regex matches. Any parameter not explicitly matching a regex will not be initialized, instead using whatever the default initialization was in the module's code. If you are instantiating this object from a config file, an example configuration is as follows: { \"regexes\" : [ [ \"parameter_regex_match1\" , { \"type\" : \"normal\" \"mean\" : 0.01 \"std\" : 0.1 } ], [ \"parameter_regex_match2\" , \"uniform\" ] ], \"prevent_regexes\" : [ \"prevent_init_regex\" ] } where the first item in each tuple under the regexes parameters is the regex that matches to parameters, and the second item specifies an Initializer. These values can either be strings, in which case they correspond to the names of initializers, or dictionaries, in which case they must contain the \"type\" key, corresponding to the name of an initializer. In addition, they may contain auxiliary named parameters which will be fed to the initializer itself. To determine valid auxiliary parameters, please refer to the torch.nn.init documentation. Parameters \u00b6 regexes : List[Tuple[str, Initializer]] , optional (default = [] ) A list mapping parameter regexes to initializers. We will check each parameter against each regex in turn, and apply the initializer paired with the first matching regex, if any. prevent_regexes : List[str] , optional (default = None ) Any parameter name matching one of these regexes will not be initialized, regardless of whether it matches one of the regexes passed in the regexes parameter. __call__ \u00b6 class InitializerApplicator ( FromParams ): | ... | def __call__ ( self , module : torch . nn . Module ) -> None Applies an initializer to all parameters in a module that match one of the regexes we were given in this object's constructor. Does nothing to parameters that do not match. Parameters \u00b6 module : torch.nn.Module The Pytorch module to apply the initializers to.","title":"initializers"},{"location":"api/nn/initializers/#initializer","text":"class Initializer ( Registrable ) An initializer is really just a bare pytorch function. This class is a proxy that allows us to implement Registrable for those functions.","title":"Initializer"},{"location":"api/nn/initializers/#default_implementation","text":"class Initializer ( Registrable ): | ... | default_implementation = \"normal\"","title":"default_implementation"},{"location":"api/nn/initializers/#__call__","text":"class Initializer ( Registrable ): | ... | def __call__ ( self , tensor : torch . Tensor , ** kwargs ) -> None This function is here just to make mypy happy. We expect initialization functions to follow this API; the builtin pytorch initialization functions follow this just fine, even though they don't subclass Initialization . We're just making it explicit here, so mypy knows that initializers are callable like this.","title":"__call__"},{"location":"api/nn/initializers/#uniform_unit_scaling","text":"def uniform_unit_scaling ( tensor : torch . Tensor , nonlinearity : str = \"linear\" ) An initaliser which preserves output variance for approximately gaussian distributed inputs. This boils down to initialising layers using a uniform distribution in the range (-sqrt(3/dim[0]) * scale, sqrt(3 / dim[0]) * scale) , where dim[0] is equal to the input dimension of the parameter and the scale is a constant scaling factor which depends on the non-linearity used. See Random Walk Initialisation for Training Very Deep Feedforward Networks <https://www.semanticscholar.org/paper/Random-Walk-Initialization-for-Training-Very-Deep-Sussillo-Abbott/be9728a0728b6acf7a485225b1e41592176eda0b> _ for more information.","title":"uniform_unit_scaling"},{"location":"api/nn/initializers/#block_orthogonal","text":"def block_orthogonal ( tensor : torch . Tensor , split_sizes : List [ int ], gain : float = 1.0 ) -> None An initializer which allows initializing model parameters in \"blocks\". This is helpful in the case of recurrent models which use multiple gates applied to linear projections, which can be computed efficiently if they are concatenated together. However, they are separate parameters which should be initialized independently.","title":"block_orthogonal"},{"location":"api/nn/initializers/#zero","text":"def zero ( tensor : torch . Tensor ) -> None","title":"zero"},{"location":"api/nn/initializers/#lstm_hidden_bias","text":"def lstm_hidden_bias ( tensor : torch . Tensor ) -> None Initialize the biases of the forget gate to 1, and all other gates to 0, following Jozefowicz et al., An Empirical Exploration of Recurrent Network Architectures","title":"lstm_hidden_bias"},{"location":"api/nn/initializers/#normalinitializer","text":"@Initializer . register ( \"normal\" ) class NormalInitializer ( _InitializerWrapper ): | def __init__ ( self , mean : float = 0.0 , std : float = 0.1 ) Registered as an Initializer with name \"normal\".","title":"NormalInitializer"},{"location":"api/nn/initializers/#orthogonalinitializer","text":"@Initializer . register ( \"orthogonal\" ) class OrthogonalInitializer ( _InitializerWrapper ): | def __init__ ( self , gain : float = 1.0 ) Registered as an Initializer with name \"orthogonal\".","title":"OrthogonalInitializer"},{"location":"api/nn/initializers/#uniforminitializer","text":"@Initializer . register ( \"uniform\" ) class UniformInitializer ( _InitializerWrapper ): | def __init__ ( self , a : float = 0.0 , b : float = 1.0 ) Registered as an Initializer with name \"uniform\".","title":"UniformInitializer"},{"location":"api/nn/initializers/#constantinitializer","text":"@Initializer . register ( \"constant\" ) class ConstantInitializer ( _InitializerWrapper ): | def __init__ ( self , val : float ) Registered as an Initializer with name \"constant\".","title":"ConstantInitializer"},{"location":"api/nn/initializers/#diracinitializer","text":"@Initializer . register ( \"dirac\" ) class DiracInitializer ( _InitializerWrapper ): | def __init__ ( self ) Registered as an Initializer with name \"dirac\".","title":"DiracInitializer"},{"location":"api/nn/initializers/#xavieruniforminitializer","text":"@Initializer . register ( \"xavier_uniform\" ) class XavierUniformInitializer ( _InitializerWrapper ): | def __init__ ( self , gain : float = 1.0 ) Registered as an Initializer with name \"xavir_uniform\".","title":"XavierUniformInitializer"},{"location":"api/nn/initializers/#xaviernormalinitializer","text":"@Initializer . register ( \"xavier_normal\" ) class XavierNormalInitializer ( _InitializerWrapper ): | def __init__ ( self , gain : float = 1.0 ) Registered as an Initializer with name \"xavier_normal\".","title":"XavierNormalInitializer"},{"location":"api/nn/initializers/#kaiminguniforminitializer","text":"@Initializer . register ( \"kaiming_uniform\" ) class KaimingUniformInitializer ( _InitializerWrapper ): | def __init__ ( | self , | a : float = 0.0 , | mode : str = \"fan_in\" , | nonlinearity : str = \"leaky_relu\" | ) Registered as an Initializer with name \"kaiming_uniform\".","title":"KaimingUniformInitializer"},{"location":"api/nn/initializers/#kaimingnormalinitializer","text":"@Initializer . register ( \"kaiming_normal\" ) class KaimingNormalInitializer ( _InitializerWrapper ): | def __init__ ( | self , | a : float = 0.0 , | mode : str = \"fan_in\" , | nonlinearity : str = \"leaky_relu\" | ) Registered as an Initializer with name \"kaiming_normal\".","title":"KaimingNormalInitializer"},{"location":"api/nn/initializers/#sparseinitializer","text":"@Initializer . register ( \"sparse\" ) class SparseInitializer ( _InitializerWrapper ): | def __init__ ( self , sparsity : float , std : float = 0.01 ) Registered as an Initializer with name \"sparse\".","title":"SparseInitializer"},{"location":"api/nn/initializers/#eyeinitializer","text":"@Initializer . register ( \"eye\" ) class EyeInitializer ( _InitializerWrapper ): | def __init__ ( self ) Registered as an Initializer with name \"eye\".","title":"EyeInitializer"},{"location":"api/nn/initializers/#blockorthogonalinitializer","text":"@Initializer . register ( \"block_orthogonal\" ) class BlockOrthogonalInitializer ( _InitializerWrapper ): | def __init__ ( self , split_sizes : List [ int ], gain : float = 1.0 ) Registered as an Initializer with name \"block_orthogonal\".","title":"BlockOrthogonalInitializer"},{"location":"api/nn/initializers/#uniformunitscalinginitializer","text":"@Initializer . register ( \"uniform_unit_scaling\" ) class UniformUnitScalingInitializer ( _InitializerWrapper ): | def __init__ ( self , nonlinearity : str = \"linear\" ) Registered as an Initializer with name \"uniform_unit_scaling\".","title":"UniformUnitScalingInitializer"},{"location":"api/nn/initializers/#zeroinitializer","text":"@Initializer . register ( \"zero\" ) class ZeroInitializer ( _InitializerWrapper ): | def __init__ ( self ) Registered as an Initializer with name \"zero\".","title":"ZeroInitializer"},{"location":"api/nn/initializers/#lstmhiddenbiasinitializer","text":"@Initializer . register ( \"lstm_hidden_bias\" ) class LstmHiddenBiasInitializer ( _InitializerWrapper ): | def __init__ ( self ) Registered as an Initializer with name \"lstm_hidden_bias\".","title":"LstmHiddenBiasInitializer"},{"location":"api/nn/initializers/#pretrainedmodelinitializer","text":"@Initializer . register ( \"pretrained\" ) class PretrainedModelInitializer ( Initializer ): | def __init__ ( | self , | weights_file_path : str , | parameter_name_overrides : Dict [ str , str ] = None | ) -> None An initializer which allows initializing parameters using a pretrained model. The initializer will load all of the weights from the weights_file_path and use the name of the new parameters to index into the pretrained parameters. Therefore, by default, the names of the new and pretrained parameters must be the same. However, this behavior can be overridden using the parameter_name_overrides , which remaps the name of the new parameter to the key which should be used to index into the pretrained parameters. The initializer will load all of the weights from the weights_file_path regardless of which parameters will actually be used to initialize the new model. So, if you need to initialize several parameters using a pretrained model, the most memory-efficient way to do this is to use one PretrainedModelInitializer per weights file and use a regex to match all of the new parameters which need to be initialized. If you are using a configuration file to instantiate this object, the below entry in the InitializerApplicator parameters will initialize linear_1.weight and linear_2.weight using a pretrained model. linear_1.weight will be initialized to the pretrained parameters called linear_1.weight , but linear_2.weight will be initialized to the pretrained parameters called linear_3.weight :: [\"linear_1.weight|linear_2.weight\", { \"type\": \"pretrained\", \"weights_file_path\": \"best.th\", \"parameter_name_overrides\": { \"linear_2.weight\": \"linear_3.weight\" } } ] To initialize weights for all the parameters from a pretrained model (assuming their names remain unchanged), use the following instead: [\".*\", { \"type\": \"pretrained\", \"weights_file_path\": \"best.th\", \"parameter_name_overrides\": {} } ] Registered as an Initializer with name \"pretrained\".","title":"PretrainedModelInitializer"},{"location":"api/nn/initializers/#__call___1","text":"class PretrainedModelInitializer ( Initializer ): | ... | def __call__ ( | self , | tensor : torch . Tensor , | parameter_name : str , | ** kwargs | ) -> None","title":"__call__"},{"location":"api/nn/initializers/#initializerapplicator","text":"class InitializerApplicator ( FromParams ): | def __init__ ( | self , | regexes : List [ Tuple [ str , Initializer ]] = None , | prevent_regexes : List [ str ] = None | ) -> None Applies initializers to the parameters of a Module based on regex matches. Any parameter not explicitly matching a regex will not be initialized, instead using whatever the default initialization was in the module's code. If you are instantiating this object from a config file, an example configuration is as follows: { \"regexes\" : [ [ \"parameter_regex_match1\" , { \"type\" : \"normal\" \"mean\" : 0.01 \"std\" : 0.1 } ], [ \"parameter_regex_match2\" , \"uniform\" ] ], \"prevent_regexes\" : [ \"prevent_init_regex\" ] } where the first item in each tuple under the regexes parameters is the regex that matches to parameters, and the second item specifies an Initializer. These values can either be strings, in which case they correspond to the names of initializers, or dictionaries, in which case they must contain the \"type\" key, corresponding to the name of an initializer. In addition, they may contain auxiliary named parameters which will be fed to the initializer itself. To determine valid auxiliary parameters, please refer to the torch.nn.init documentation.","title":"InitializerApplicator"},{"location":"api/nn/initializers/#__call___2","text":"class InitializerApplicator ( FromParams ): | ... | def __call__ ( self , module : torch . nn . Module ) -> None Applies an initializer to all parameters in a module that match one of the regexes we were given in this object's constructor. Does nothing to parameters that do not match.","title":"__call__"},{"location":"api/nn/module/","text":"allennlp .nn .module [SOURCE] Module \u00b6 class Module ( torch . nn . Module ) This is just torch.nn.Module with some extra functionality. _post_load_state_dict \u00b6 class Module ( torch . nn . Module ): | ... | def _post_load_state_dict ( | self , | missing_keys : List [ str ], | unexpected_keys : List [ str ] | ) -> Tuple [ List [ str ], List [ str ]] Subclasses can override this and potentially modify missing_keys or unexpected_keys . load_state_dict \u00b6 class Module ( torch . nn . Module ): | ... | def load_state_dict ( | self , | state_dict : StateDictType , | strict : bool = True | ) -> _IncompatibleKeys Same as torch.nn.Module.load_state_dict() except we also run the _post_load_state_dict method before returning, which can be implemented by subclasses to customize the behavior. load_state_dict_distributed \u00b6 class Module ( torch . nn . Module ): | ... | def load_state_dict_distributed ( | self , | state_dict : Optional [ StateDictType ], | strict : bool = True | ) -> _IncompatibleKeys","title":"module"},{"location":"api/nn/module/#module","text":"class Module ( torch . nn . Module ) This is just torch.nn.Module with some extra functionality.","title":"Module"},{"location":"api/nn/module/#_post_load_state_dict","text":"class Module ( torch . nn . Module ): | ... | def _post_load_state_dict ( | self , | missing_keys : List [ str ], | unexpected_keys : List [ str ] | ) -> Tuple [ List [ str ], List [ str ]] Subclasses can override this and potentially modify missing_keys or unexpected_keys .","title":"_post_load_state_dict"},{"location":"api/nn/module/#load_state_dict","text":"class Module ( torch . nn . Module ): | ... | def load_state_dict ( | self , | state_dict : StateDictType , | strict : bool = True | ) -> _IncompatibleKeys Same as torch.nn.Module.load_state_dict() except we also run the _post_load_state_dict method before returning, which can be implemented by subclasses to customize the behavior.","title":"load_state_dict"},{"location":"api/nn/module/#load_state_dict_distributed","text":"class Module ( torch . nn . Module ): | ... | def load_state_dict_distributed ( | self , | state_dict : Optional [ StateDictType ], | strict : bool = True | ) -> _IncompatibleKeys","title":"load_state_dict_distributed"},{"location":"api/nn/util/","text":"allennlp .nn .util [SOURCE] Assorted utilities for working with neural networks in AllenNLP. T \u00b6 T = TypeVar ( \"T\" ) StateDictType \u00b6 StateDictType = Union [ Dict [ str , torch . Tensor ], \"OrderedDict[str, torch.Tensor]\" ] move_to_device \u00b6 def move_to_device ( obj , device : Union [ torch . device , int ]) Given a structure (possibly) containing Tensors, move all the Tensors to the specified device (or do nothing, if they are already on the target device). clamp_tensor \u00b6 def clamp_tensor ( tensor , minimum , maximum ) Supports sparse and dense tensors. Returns a tensor with values clamped between the provided minimum and maximum, without modifying the original tensor. batch_tensor_dicts \u00b6 def batch_tensor_dicts ( tensor_dicts : List [ Dict [ str , torch . Tensor ]], remove_trailing_dimension : bool = False ) -> Dict [ str , torch . Tensor ] Takes a list of tensor dictionaries, where each dictionary is assumed to have matching keys, and returns a single dictionary with all tensors with the same key batched together. Parameters \u00b6 tensor_dicts : List[Dict[str, torch.Tensor]] The list of tensor dictionaries to batch. remove_trailing_dimension : bool If True , we will check for a trailing dimension of size 1 on the tensors that are being batched, and remove it if we find it. get_lengths_from_binary_sequence_mask \u00b6 def get_lengths_from_binary_sequence_mask ( mask : torch . BoolTensor ) -> torch . LongTensor Compute sequence lengths for each batch element in a tensor using a binary mask. Parameters \u00b6 mask : torch.BoolTensor A 2D binary mask of shape (batch_size, sequence_length) to calculate the per-batch sequence lengths from. Returns \u00b6 torch.LongTensor A torch.LongTensor of shape (batch_size,) representing the lengths of the sequences in the batch. get_mask_from_sequence_lengths \u00b6 def get_mask_from_sequence_lengths ( sequence_lengths : torch . Tensor , max_length : int ) -> torch . BoolTensor Given a variable of shape (batch_size,) that represents the sequence lengths of each batch element, this function returns a (batch_size, max_length) mask variable. For example, if our input was [2, 2, 3] , with a max_length of 4, we'd return [[1, 1, 0, 0], [1, 1, 0, 0], [1, 1, 1, 0]] . We require max_length here instead of just computing it from the input sequence_lengths because it lets us avoid finding the max, then copying that value from the GPU to the CPU so that we can use it to construct a new tensor. sort_batch_by_length \u00b6 def sort_batch_by_length ( tensor : torch . Tensor , sequence_lengths : torch . Tensor ) Sort a batch first tensor by some specified lengths. Parameters \u00b6 tensor : torch.FloatTensor A batch first Pytorch tensor. sequence_lengths : torch.LongTensor A tensor representing the lengths of some dimension of the tensor which we want to sort by. Returns \u00b6 sorted_tensor : torch.FloatTensor The original tensor sorted along the batch dimension with respect to sequence_lengths. sorted_sequence_lengths : torch.LongTensor The original sequence_lengths sorted by decreasing size. restoration_indices : torch.LongTensor Indices into the sorted_tensor such that sorted_tensor.index_select(0, restoration_indices) == original_tensor permutation_index : torch.LongTensor The indices used to sort the tensor. This is useful if you want to sort many tensors using the same ordering. get_final_encoder_states \u00b6 def get_final_encoder_states ( encoder_outputs : torch . Tensor , mask : torch . BoolTensor , bidirectional : bool = False ) -> torch . Tensor Given the output from a Seq2SeqEncoder , with shape (batch_size, sequence_length, encoding_dim) , this method returns the final hidden state for each element of the batch, giving a tensor of shape (batch_size, encoding_dim) . This is not as simple as encoder_outputs[:, -1] , because the sequences could have different lengths. We use the mask (which has shape (batch_size, sequence_length) ) to find the final state for each batch instance. Additionally, if bidirectional is True , we will split the final dimension of the encoder_outputs into two and assume that the first half is for the forward direction of the encoder and the second half is for the backward direction. We will concatenate the last state for each encoder dimension, giving encoder_outputs[:, -1, :encoding_dim/2] concatenated with encoder_outputs[:, 0, encoding_dim/2:] . get_dropout_mask \u00b6 def get_dropout_mask ( dropout_probability : float , tensor_for_masking : torch . Tensor ) Computes and returns an element-wise dropout mask for a given tensor, where each element in the mask is dropped out with probability dropout_probability. Note that the mask is NOT applied to the tensor - the tensor is passed to retain the correct CUDA tensor type for the mask. Parameters \u00b6 dropout_probability : float Probability of dropping a dimension of the input. tensor_for_masking : torch.Tensor Returns \u00b6 torch.FloatTensor A torch.FloatTensor consisting of the binary mask scaled by 1/ (1 - dropout_probability). This scaling ensures expected values and variances of the output of applying this mask and the original tensor are the same. masked_softmax \u00b6 def masked_softmax ( vector : torch . Tensor , mask : torch . BoolTensor , dim : int = - 1 , memory_efficient : bool = False ) -> torch . Tensor torch.nn.functional.softmax(vector) does not work if some elements of vector should be masked. This performs a softmax on just the non-masked portions of vector . Passing None in for the mask is also acceptable; you'll just get a regular softmax. vector can have an arbitrary number of dimensions; the only requirement is that mask is broadcastable to vector's shape. If mask has fewer dimensions than vector , we will unsqueeze on dimension 1 until they match. If you need a different unsqueezing of your mask, do it yourself before passing the mask into this function. If memory_efficient is set to true, we will simply use a very large negative number for those masked positions so that the probabilities of those positions would be approximately 0. This is not accurate in math, but works for most cases and consumes less memory. In the case that the input vector is completely masked and memory_efficient is false, this function returns an array of 0.0 . This behavior may cause NaN if this is used as the last layer of a model that uses categorical cross-entropy loss. Instead, if memory_efficient is true, this function will treat every element as equal, and do softmax over equal numbers. masked_log_softmax \u00b6 def masked_log_softmax ( vector : torch . Tensor , mask : torch . BoolTensor , dim : int = - 1 ) -> torch . Tensor torch.nn.functional.log_softmax(vector) does not work if some elements of vector should be masked. This performs a log_softmax on just the non-masked portions of vector . Passing None in for the mask is also acceptable; you'll just get a regular log_softmax. vector can have an arbitrary number of dimensions; the only requirement is that mask is broadcastable to vector's shape. If mask has fewer dimensions than vector , we will unsqueeze on dimension 1 until they match. If you need a different unsqueezing of your mask, do it yourself before passing the mask into this function. In the case that the input vector is completely masked, the return value of this function is arbitrary, but not nan . You should be masking the result of whatever computation comes out of this in that case, anyway, so the specific values returned shouldn't matter. Also, the way that we deal with this case relies on having single-precision floats; mixing half-precision floats with fully-masked vectors will likely give you nans . If your logits are all extremely negative (i.e., the max value in your logit vector is -50 or lower), the way we handle masking here could mess you up. But if you've got logit values that extreme, you've got bigger problems than this. masked_max \u00b6 def masked_max ( vector : torch . Tensor , mask : torch . BoolTensor , dim : int , keepdim : bool = False ) -> torch . Tensor To calculate max along certain dimensions on masked values Parameters \u00b6 vector : torch.Tensor The vector to calculate max, assume unmasked parts are already zeros mask : torch.BoolTensor The mask of the vector. It must be broadcastable with vector. dim : int The dimension to calculate max keepdim : bool Whether to keep dimension Returns \u00b6 torch.Tensor A torch.Tensor of including the maximum values. masked_mean \u00b6 def masked_mean ( vector : torch . Tensor , mask : torch . BoolTensor , dim : int , keepdim : bool = False ) -> torch . Tensor To calculate mean along certain dimensions on masked values Parameters \u00b6 vector : torch.Tensor The vector to calculate mean. mask : torch.BoolTensor The mask of the vector. It must be broadcastable with vector. dim : int The dimension to calculate mean keepdim : bool Whether to keep dimension Returns \u00b6 torch.Tensor A torch.Tensor of including the mean values. masked_flip \u00b6 def masked_flip ( padded_sequence : torch . Tensor , sequence_lengths : List [ int ] ) -> torch . Tensor Flips a padded tensor along the time dimension without affecting masked entries. Parameters \u00b6 padded_sequence : torch.Tensor The tensor to flip along the time dimension. Assumed to be of dimensions (batch size, num timesteps, ...) sequence_lengths : torch.Tensor A list containing the lengths of each unpadded sequence in the batch. Returns \u00b6 torch.Tensor A torch.Tensor of the same shape as padded_sequence. viterbi_decode \u00b6 def viterbi_decode ( tag_sequence : torch . Tensor , transition_matrix : torch . Tensor , tag_observations : Optional [ List [ int ]] = None , allowed_start_transitions : torch . Tensor = None , allowed_end_transitions : torch . Tensor = None , top_k : int = None ) Perform Viterbi decoding in log space over a sequence given a transition matrix specifying pairwise (transition) potentials between tags and a matrix of shape (sequence_length, num_tags) specifying unary potentials for possible tags per timestep. Parameters \u00b6 tag_sequence : torch.Tensor A tensor of shape (sequence_length, num_tags) representing scores for a set of tags over a given sequence. transition_matrix : torch.Tensor A tensor of shape (num_tags, num_tags) representing the binary potentials for transitioning between a given pair of tags. tag_observations : Optional[List[int]] , optional (default = None ) A list of length sequence_length containing the class ids of observed elements in the sequence, with unobserved elements being set to -1. Note that it is possible to provide evidence which results in degenerate labelings if the sequences of tags you provide as evidence cannot transition between each other, or those transitions are extremely unlikely. In this situation we log a warning, but the responsibility for providing self-consistent evidence ultimately lies with the user. allowed_start_transitions : torch.Tensor , optional (default = None ) An optional tensor of shape (num_tags,) describing which tags the START token may transition to . If provided, additional transition constraints will be used for determining the start element of the sequence. allowed_end_transitions : torch.Tensor , optional (default = None ) An optional tensor of shape (num_tags,) describing which tags may transition to the end tag. If provided, additional transition constraints will be used for determining the end element of the sequence. top_k : int , optional (default = None ) Optional integer specifying how many of the top paths to return. For top_k>=1, returns a tuple of two lists: top_k_paths, top_k_scores, For top_k==None, returns a flattened tuple with just the top path and its score (not in lists, for backwards compatibility). Returns \u00b6 viterbi_path : List[int] The tag indices of the maximum likelihood tag sequence. viterbi_score : torch.Tensor The score of the viterbi path. get_text_field_mask \u00b6 def get_text_field_mask ( text_field_tensors : Dict [ str , Dict [ str , torch . Tensor ]], num_wrapping_dims : int = 0 , padding_id : int = 0 ) -> torch . BoolTensor Takes the dictionary of tensors produced by a TextField and returns a mask with 0 where the tokens are padding, and 1 otherwise. padding_id specifies the id of padding tokens. We also handle TextFields wrapped by an arbitrary number of ListFields , where the number of wrapping ListFields is given by num_wrapping_dims . If num_wrapping_dims == 0 , the returned mask has shape (batch_size, num_tokens) . If num_wrapping_dims > 0 then the returned mask has num_wrapping_dims extra dimensions, so the shape will be (batch_size, ..., num_tokens) . There could be several entries in the tensor dictionary with different shapes (e.g., one for word ids, one for character ids). In order to get a token mask, we use the tensor in the dictionary with the lowest number of dimensions. After subtracting num_wrapping_dims , if this tensor has two dimensions we assume it has shape (batch_size, ..., num_tokens) , and use it for the mask. If instead it has three dimensions, we assume it has shape (batch_size, ..., num_tokens, num_features) , and sum over the last dimension to produce the mask. Most frequently this will be a character id tensor, but it could also be a featurized representation of each token, etc. If the input text_field_tensors contains the \"mask\" key, this is returned instead of inferring the mask. get_token_ids_from_text_field_tensors \u00b6 def get_token_ids_from_text_field_tensors ( text_field_tensors : Dict [ str , Dict [ str , torch . Tensor ]] ) -> torch . Tensor Our TextFieldTensors are complex output structures, because they try to handle a lot of potential variation. Sometimes, you just want to grab the token ids from this data structure, and that's not trivial without hard-coding assumptions about your data processing, which defeats the entire purpose of that generality. This method tries to let you get the token ids out of the data structure in your model without hard-coding any assumptions. weighted_sum \u00b6 def weighted_sum ( matrix : torch . Tensor , attention : torch . Tensor ) -> torch . Tensor Takes a matrix of vectors and a set of weights over the rows in the matrix (which we call an \"attention\" vector), and returns a weighted sum of the rows in the matrix. This is the typical computation performed after an attention mechanism. Note that while we call this a \"matrix\" of vectors and an attention \"vector\", we also handle higher-order tensors. We always sum over the second-to-last dimension of the \"matrix\", and we assume that all dimensions in the \"matrix\" prior to the last dimension are matched in the \"vector\". Non-matched dimensions in the \"vector\" must be directly after the batch dimension . For example, say I have a \"matrix\" with dimensions (batch_size, num_queries, num_words, embedding_dim) . The attention \"vector\" then must have at least those dimensions, and could have more. Both: - ` ( batch_size , num_queries , num_words ) ` ( distribution over words for each query ) - ` ( batch_size , num_documents , num_queries , num_words ) ` ( distribution over words in a query for each document ) are valid input \"vectors\", producing tensors of shape: (batch_size, num_queries, embedding_dim) and (batch_size, num_documents, num_queries, embedding_dim) respectively. sequence_cross_entropy_with_logits \u00b6 def sequence_cross_entropy_with_logits ( logits : torch . FloatTensor , targets : torch . LongTensor , weights : Union [ torch . FloatTensor , torch . BoolTensor ], average : str = \"batch\" , label_smoothing : float = None , gamma : float = None , alpha : Union [ float , List [ float ], torch . FloatTensor ] = None ) -> torch . FloatTensor Computes the cross entropy loss of a sequence, weighted with respect to some user provided weights. Note that the weighting here is not the same as in the torch.nn.CrossEntropyLoss() criterion, which is weighting classes; here we are weighting the loss contribution from particular elements in the sequence. This allows loss computations for models which use padding. Parameters \u00b6 logits : torch.FloatTensor A torch.FloatTensor of size (batch_size, sequence_length, num_classes) which contains the unnormalized probability for each class. targets : torch.LongTensor A torch.LongTensor of size (batch, sequence_length) which contains the index of the true class for each corresponding step. weights : Union[torch.FloatTensor, torch.BoolTensor] A torch.FloatTensor of size (batch, sequence_length) average : str , optional (default = \"batch\" ) If \"batch\", average the loss across the batches. If \"token\", average the loss across each item in the input. If None , return a vector of losses per batch element. label_smoothing : float , optional (default = None ) Whether or not to apply label smoothing to the cross-entropy loss. For example, with a label smoothing value of 0.2, a 4 class classification target would look like [0.05, 0.05, 0.85, 0.05] if the 3rd class was the correct label. gamma : float , optional (default = None ) Focal loss[*] focusing parameter gamma to reduces the relative loss for well-classified examples and put more focus on hard. The greater value gamma is, the more focus on hard examples. alpha : Union[float, List[float]] , optional (default = None ) Focal loss[ ] weighting factor alpha to balance between classes. Can be used independently with gamma . If a single float is provided, it is assumed binary case using alpha and 1 - alpha for positive and negative respectively. If a list of float is provided, with the same length as the number of classes, the weights will match the classes. [ ] T. Lin, P. Goyal, R. Girshick, K. He and P. Doll\u00e1r, \"Focal Loss for Dense Object Detection,\" 2017 IEEE International Conference on Computer Vision (ICCV), Venice, 2017, pp. 2999-3007. Returns \u00b6 torch.FloatTensor A torch.FloatTensor representing the cross entropy loss. If average==\"batch\" or average==\"token\" , the returned loss is a scalar. If average is None , the returned loss is a vector of shape (batch_size,). replace_masked_values \u00b6 def replace_masked_values ( tensor : torch . Tensor , mask : torch . BoolTensor , replace_with : float ) -> torch . Tensor Replaces all masked values in tensor with replace_with . mask must be broadcastable to the same shape as tensor . We require that tensor.dim() == mask.dim() , as otherwise we won't know which dimensions of the mask to unsqueeze. This just does tensor.masked_fill() , except the pytorch method fills in things with a mask value of 1, where we want the opposite. You can do this in your own code with tensor.masked_fill(~mask, replace_with) . tensors_equal \u00b6 def tensors_equal ( tensor1 : torch . Tensor , tensor2 : torch . Tensor , tolerance : float = 1e-12 ) -> bool A check for tensor equality (by value). We make sure that the tensors have the same shape, then check all of the entries in the tensor for equality. We additionally allow the input tensors to be lists or dictionaries, where we then do the above check on every position in the list / item in the dictionary. If we find objects that aren't tensors as we're doing that, we just defer to their equality check. This is kind of a catch-all method that's designed to make implementing __eq__ methods easier, in a way that's really only intended to be useful for tests. device_mapping \u00b6 def device_mapping ( cuda_device : int ) In order to torch.load() a GPU-trained model onto a CPU (or specific GPU), you have to supply a map_location function. Call this with the desired cuda_device to get the function that torch.load() needs. read_state_dict \u00b6 def read_state_dict ( path : Union [ PathLike , str ], strip_prefix : Optional [ str ] = None , ignore : Optional [ List [ str ]] = None , strict : bool = True , cuda_device : int = - 1 ) -> Dict [ str , torch . Tensor ] Read a PyTorch model state dictionary from a checkpoint at the given path . Parameters \u00b6 path : Union[PathLike, str] strip_prefix : Optional[str] , optional (default = None ) A prefix to remove from all of the state dict keys. ignore : Optional[List[str]] , optional (default = None ) Optional list of regular expressions. Keys that match any of these will be removed from the state dict. Note If strip_prefix is given, the regular expressions in ignore are matched before the prefix is stripped. strict : bool , optional (default = True ) If True (the default) and strip_prefix was never used or any of the regular expressions in ignore never matched, a ValueError will be raised. cuda_device : int , optional (default = -1 ) The device to load the parameters onto. Use -1 (the default) for CPU. Returns \u00b6 Dict[str, torch.Tensor] An ordered dictionary of the state. combine_tensors \u00b6 def combine_tensors ( combination : str , tensors : List [ torch . Tensor ] ) -> torch . Tensor Combines a list of tensors using element-wise operations and concatenation, specified by a combination string. The string refers to (1-indexed) positions in the input tensor list, and looks like \"1,2,1+2,3-1\" . We allow the following kinds of combinations : x , x*y , x+y , x-y , and x/y , where x and y are positive integers less than or equal to len(tensors) . Each of the binary operations is performed elementwise. You can give as many combinations as you want in the combination string. For example, for the input string \"1,2,1*2\" , the result would be [1;2;1*2] , as you would expect, where [;] is concatenation along the last dimension. If you have a fixed, known way to combine tensors that you use in a model, you should probably just use something like torch.cat([x_tensor, y_tensor, x_tensor * y_tensor]) . This function adds some complexity that is only necessary if you want the specific combination used to be configurable . If you want to do any element-wise operations, the tensors involved in each element-wise operation must have the same shape. This function also accepts x and y in place of 1 and 2 in the combination string. combine_tensors_and_multiply \u00b6 def combine_tensors_and_multiply ( combination : str , tensors : List [ torch . Tensor ], weights : torch . nn . Parameter ) -> torch . Tensor Like combine_tensors , but does a weighted (linear) multiplication while combining. This is a separate function from combine_tensors because we try to avoid instantiating large intermediate tensors during the combination, which is possible because we know that we're going to be multiplying by a weight vector in the end. Parameters \u00b6 combination : str Same as in combine_tensors tensors : List[torch.Tensor] A list of tensors to combine, where the integers in the combination are (1-indexed) positions in this list of tensors. These tensors are all expected to have either three or four dimensions, with the final dimension being an embedding. If there are four dimensions, one of them must have length 1. weights : torch.nn.Parameter A vector of weights to use for the combinations. This should have shape (combined_dim,), as calculated by get_combined_dim . get_combined_dim \u00b6 def get_combined_dim ( combination : str , tensor_dims : List [ int ]) -> int For use with combine_tensors . This function computes the resultant dimension when calling combine_tensors(combination, tensors) , when the tensor dimension is known. This is necessary for knowing the sizes of weight matrices when building models that use combine_tensors . Parameters \u00b6 combination : str A comma-separated list of combination pieces, like \"1,2,1*2\" , specified identically to combination in combine_tensors . tensor_dims : List[int] A list of tensor dimensions, where each dimension is from the last axis of the tensors that will be input to combine_tensors . logsumexp \u00b6 def logsumexp ( tensor : torch . Tensor , dim : int = - 1 , keepdim : bool = False ) -> torch . Tensor A numerically stable computation of logsumexp. This is mathematically equivalent to tensor.exp().sum(dim, keep=keepdim).log() . This function is typically used for summing log probabilities. Parameters \u00b6 tensor : torch.FloatTensor A tensor of arbitrary size. dim : int , optional (default = -1 ) The dimension of the tensor to apply the logsumexp to. keepdim : bool , optional (default = False ) Whether to retain a dimension of size one at the dimension we reduce over. get_device_of \u00b6 def get_device_of ( tensor : torch . Tensor ) -> int Returns the device of the tensor. flatten_and_batch_shift_indices \u00b6 def flatten_and_batch_shift_indices ( indices : torch . Tensor , sequence_length : int ) -> torch . Tensor This is a subroutine for batched_index_select . The given indices of size (batch_size, d_1, ..., d_n) indexes into dimension 2 of a target tensor, which has size (batch_size, sequence_length, embedding_size) . This function returns a vector that correctly indexes into the flattened target. The sequence length of the target must be provided to compute the appropriate offsets. indices = torch . ones ([ 2 , 3 ], dtype = torch . long ) # Sequence length of the target tensor. sequence_length = 10 shifted_indices = flatten_and_batch_shift_indices ( indices , sequence_length ) # Indices into the second element in the batch are correctly shifted # to take into account that the target tensor will be flattened before # the indices are applied. assert shifted_indices == [ 1 , 1 , 1 , 11 , 11 , 11 ] Parameters \u00b6 indices : torch.LongTensor sequence_length : int The length of the sequence the indices index into. This must be the second dimension of the tensor. Returns \u00b6 offset_indices : torch.LongTensor batched_index_select \u00b6 def batched_index_select ( target : torch . Tensor , indices : torch . LongTensor , flattened_indices : Optional [ torch . LongTensor ] = None ) -> torch . Tensor The given indices of size (batch_size, d_1, ..., d_n) indexes into the sequence dimension (dimension 2) of the target, which has size (batch_size, sequence_length, embedding_size) . This function returns selected values in the target with respect to the provided indices, which have size (batch_size, d_1, ..., d_n, embedding_size) . This can use the optionally precomputed flattened_indices with size (batch_size * d_1 * ... * d_n) if given. An example use case of this function is looking up the start and end indices of spans in a sequence tensor. This is used in the CoreferenceResolver model to select contextual word representations corresponding to the start and end indices of mentions. The key reason this can't be done with basic torch functions is that we want to be able to use look-up tensors with an arbitrary number of dimensions (for example, in the coref model, we don't know a-priori how many spans we are looking up). Parameters \u00b6 target : torch.Tensor A 3 dimensional tensor of shape (batch_size, sequence_length, embedding_size). This is the tensor to be indexed. indices : torch.LongTensor A tensor of shape (batch_size, ...), where each element is an index into the sequence_length dimension of the target tensor. flattened_indices : Optional[torch.Tensor] , optional (default = None ) An optional tensor representing the result of calling flatten_and_batch_shift_indices on indices . This is helpful in the case that the indices can be flattened once and cached for many batch lookups. Returns \u00b6 selected_targets : torch.Tensor A tensor with shape [indices.size(), target.size(-1)] representing the embedded indices extracted from the batch flattened target tensor. masked_index_fill \u00b6 def masked_index_fill ( target : torch . Tensor , indices : torch . LongTensor , mask : torch . BoolTensor , fill_value : int = 1 ) -> torch . Tensor The given indices in target will be will be filled with fill_value given a mask . Parameters \u00b6 target : torch.Tensor A 2 dimensional tensor of shape (batch_size, sequence_length). This is the tensor to be filled. indices : torch.LongTensor A 2 dimensional tensor of shape (batch_size, num_indices), These are the indices that will be filled in the original tensor. mask : torch.Tensor A 2 dimensional tensor of shape (batch_size, num_indices), mask.sum() == nonzero_indices . fill_value : int , optional (default = 1 ) The value we fill the tensor with. Returns \u00b6 filled_target : torch.Tensor A tensor with shape (batch_size, sequence_length) where 'indices' are filled with fill_value masked_index_replace \u00b6 def masked_index_replace ( target : torch . Tensor , indices : torch . LongTensor , mask : torch . BoolTensor , replace : torch . Tensor ) -> torch . Tensor The given indices in target will be will be replaced with corresponding index from the replace tensor given a mask . Parameters \u00b6 target : torch.Tensor A 3 dimensional tensor of shape (batch_size, sequence_length, embedding_dim). This is the tensor to be replaced into. indices : torch.LongTensor A 2 dimensional tensor of shape (batch_size, num_indices), These are the indices that will be replaced in the original tensor. mask : torch.Tensor A 2 dimensional tensor of shape (batch_size, num_indices), mask.sum() == nonzero_indices . replace : torch.Tensor A 3 dimensional tensor of shape (batch_size, num_indices, embedding_dim), The tensor to perform scatter from. Returns \u00b6 replaced_target : torch.Tensor A tensor with shape (batch_size, sequence_length, embedding_dim) where 'indices' are replaced with the corrosponding vector from replace batched_span_select \u00b6 def batched_span_select ( target : torch . Tensor , spans : torch . LongTensor ) -> torch . Tensor The given spans of size (batch_size, num_spans, 2) indexes into the sequence dimension (dimension 2) of the target, which has size (batch_size, sequence_length, embedding_size) . This function returns segmented spans in the target with respect to the provided span indices. Parameters \u00b6 target : torch.Tensor A 3 dimensional tensor of shape (batch_size, sequence_length, embedding_size). This is the tensor to be indexed. indices : torch.LongTensor A 3 dimensional tensor of shape (batch_size, num_spans, 2) representing start and end indices (both inclusive) into the sequence_length dimension of the target tensor. Returns \u00b6 span_embeddings : torch.Tensor A tensor with shape (batch_size, num_spans, max_batch_span_width, embedding_size] representing the embedded spans extracted from the batch flattened target tensor. span_mask : torch.BoolTensor A tensor with shape (batch_size, num_spans, max_batch_span_width) representing the mask on the returned span embeddings. flattened_index_select \u00b6 def flattened_index_select ( target : torch . Tensor , indices : torch . LongTensor ) -> torch . Tensor The given indices of size (set_size, subset_size) specifies subsets of the target that each of the set_size rows should select. The target has size (batch_size, sequence_length, embedding_size) , and the resulting selected tensor has size (batch_size, set_size, subset_size, embedding_size) . Parameters \u00b6 target : torch.Tensor A Tensor of shape (batch_size, sequence_length, embedding_size). indices : torch.LongTensor A LongTensor of shape (set_size, subset_size). All indices must be < sequence_length as this tensor is an index into the sequence_length dimension of the target. Returns \u00b6 selected : torch.Tensor , required. A Tensor of shape (batch_size, set_size, subset_size, embedding_size). get_range_vector \u00b6 def get_range_vector ( size : int , device : int ) -> torch . Tensor Returns a range vector with the desired size, starting at 0. The CUDA implementation is meant to avoid copy data from CPU to GPU. bucket_values \u00b6 def bucket_values ( distances : torch . Tensor , num_identity_buckets : int = 4 , num_total_buckets : int = 10 ) -> torch . Tensor Places the given values (designed for distances) into num_total_buckets semi-logscale buckets, with num_identity_buckets of these capturing single values. The default settings will bucket values into the following buckets: [0, 1, 2, 3, 4, 5-7, 8-15, 16-31, 32-63, 64+]. Parameters \u00b6 distances : torch.Tensor A Tensor of any size, to be bucketed. num_identity_buckets : int , optional (default = 4 ) The number of identity buckets (those only holding a single value). num_total_buckets : int , optional (default = 10 ) The total number of buckets to bucket values into. Returns \u00b6 torch.Tensor A tensor of the same shape as the input, containing the indices of the buckets the values were placed in. add_sentence_boundary_token_ids \u00b6 def add_sentence_boundary_token_ids ( tensor : torch . Tensor , mask : torch . BoolTensor , sentence_begin_token : Any , sentence_end_token : Any ) -> Tuple [ torch . Tensor , torch . BoolTensor ] Add begin/end of sentence tokens to the batch of sentences. Given a batch of sentences with size (batch_size, timesteps) or (batch_size, timesteps, dim) this returns a tensor of shape (batch_size, timesteps + 2) or (batch_size, timesteps + 2, dim) respectively. Returns both the new tensor and updated mask. Parameters \u00b6 tensor : torch.Tensor A tensor of shape (batch_size, timesteps) or (batch_size, timesteps, dim) mask : torch.BoolTensor A tensor of shape (batch_size, timesteps) sentence_begin_token : Any Can be anything that can be broadcast in torch for assignment. For 2D input, a scalar with the <S> id. For 3D input, a tensor with length dim. sentence_end_token : Any Can be anything that can be broadcast in torch for assignment. For 2D input, a scalar with the </S> id. For 3D input, a tensor with length dim. Returns \u00b6 tensor_with_boundary_tokens : torch.Tensor The tensor with the appended and prepended boundary tokens. If the input was 2D, it has shape (batch_size, timesteps + 2) and if the input was 3D, it has shape (batch_size, timesteps + 2, dim). new_mask : torch.BoolTensor The new mask for the tensor, taking into account the appended tokens marking the beginning and end of the sentence. remove_sentence_boundaries \u00b6 def remove_sentence_boundaries ( tensor : torch . Tensor , mask : torch . BoolTensor ) -> Tuple [ torch . Tensor , torch . Tensor ] Remove begin/end of sentence embeddings from the batch of sentences. Given a batch of sentences with size (batch_size, timesteps, dim) this returns a tensor of shape (batch_size, timesteps - 2, dim) after removing the beginning and end sentence markers. The sentences are assumed to be padded on the right, with the beginning of each sentence assumed to occur at index 0 (i.e., mask[:, 0] is assumed to be 1). Returns both the new tensor and updated mask. This function is the inverse of add_sentence_boundary_token_ids . Parameters \u00b6 tensor : torch.Tensor A tensor of shape (batch_size, timesteps, dim) mask : torch.BoolTensor A tensor of shape (batch_size, timesteps) Returns \u00b6 tensor_without_boundary_tokens : torch.Tensor The tensor after removing the boundary tokens of shape (batch_size, timesteps - 2, dim) new_mask : torch.BoolTensor The new mask for the tensor of shape (batch_size, timesteps - 2) . add_positional_features \u00b6 def add_positional_features ( tensor : torch . Tensor , min_timescale : float = 1.0 , max_timescale : float = 1.0e4 ) Implements the frequency-based positional encoding described in Attention is All you Need . Adds sinusoids of different frequencies to a Tensor . A sinusoid of a different frequency and phase is added to each dimension of the input Tensor . This allows the attention heads to use absolute and relative positions. The number of timescales is equal to hidden_dim / 2 within the range (min_timescale, max_timescale). For each timescale, the two sinusoidal signals sin(timestep / timescale) and cos(timestep / timescale) are generated and concatenated along the hidden_dim dimension. Parameters \u00b6 tensor : torch.Tensor a Tensor with shape (batch_size, timesteps, hidden_dim). min_timescale : float , optional (default = 1.0 ) The smallest timescale to use. max_timescale : float , optional (default = 1.0e4 ) The largest timescale to use. Returns \u00b6 torch.Tensor The input tensor augmented with the sinusoidal frequencies. clone \u00b6 def clone ( module : torch . nn . Module , num_copies : int ) -> torch . nn . ModuleList Produce N identical layers. combine_initial_dims \u00b6 def combine_initial_dims ( tensor : torch . Tensor ) -> torch . Tensor Given a (possibly higher order) tensor of ids with shape (d1, ..., dn, sequence_length) Return a view that's (d1 * ... * dn, sequence_length). If original tensor is 1-d or 2-d, return it as is. uncombine_initial_dims \u00b6 def uncombine_initial_dims ( tensor : torch . Tensor , original_size : torch . Size ) -> torch . Tensor Given a tensor of embeddings with shape (d1 * ... * dn, sequence_length, embedding_dim) and the original shape (d1, ..., dn, sequence_length), return the reshaped tensor of embeddings with shape (d1, ..., dn, sequence_length, embedding_dim). If original size is 1-d or 2-d, return it as is. inspect_parameters \u00b6 def inspect_parameters ( module : torch . nn . Module , quiet : bool = False ) -> Dict [ str , Any ] Inspects the model/module parameters and their tunability. The output is structured in a nested dict so that parameters in same sub-modules are grouped together. This can be helpful to setup module path based regex, for example in initializer. It prints it by default (optional) and returns the inspection dict. Eg. output:: { \"_text_field_embedder\": { \"token_embedder_tokens\": { \"_projection\": { \"bias\": \"tunable\", \"weight\": \"tunable\" }, \"weight\": \"frozen\" } } } find_text_field_embedder \u00b6 def find_text_field_embedder ( model : torch . nn . Module ) -> torch . nn . Module Takes a Model and returns the Module that is a TextFieldEmbedder . We return just the first one, as it's very rare to have more than one. If there isn't a TextFieldEmbedder in the given Model , we raise a ValueError . find_embedding_layer \u00b6 def find_embedding_layer ( model : torch . nn . Module ) -> torch . nn . Module Takes a model (typically an AllenNLP Model , but this works for any torch.nn.Module ) and makes a best guess about which module is the embedding layer. For typical AllenNLP models, this often is the TextFieldEmbedder , but if you're using a pre-trained contextualizer, we really want layer 0 of that contextualizer, not the output. So there are a bunch of hacks in here for specific pre-trained contextualizers. get_token_offsets_from_text_field_inputs \u00b6 def get_token_offsets_from_text_field_inputs ( text_field_inputs : List [ Any ] ) -> Optional [ torch . Tensor ] Given a list of inputs to a TextFieldEmbedder, tries to find token offsets from those inputs, if there are any. You will have token offsets if you are using a mismatched token embedder; if you're not, the return value from this function should be None. This function is intended to be called from a forward_hook attached to a TextFieldEmbedder , so the inputs are formatted just as a list. It's possible in theory that you could have multiple offsets as inputs to a single call to a TextFieldEmbedder , but that's an extremely rare use case (I can't really imagine anyone wanting to do that). In that case, we'll only return the first one. If you need different behavior for your model, open an issue on github describing what you're doing. extend_layer \u00b6 def extend_layer ( layer : torch . nn . Module , new_dim : int ) -> None masked_topk \u00b6 def masked_topk ( input_ : torch . FloatTensor , mask : torch . BoolTensor , k : Union [ int , torch . LongTensor ], dim : int = - 1 ) -> Tuple [ torch . LongTensor , torch . LongTensor , torch . FloatTensor ] Extracts the top-k items along a certain dimension. This is similar to torch.topk except: (1) we allow of a mask that makes the function not consider certain elements; (2) the returned top input, mask, and indices are sorted in their original order in the input; (3) May use the same k for all dimensions, or different k for each. Parameters \u00b6 input_ : torch.FloatTensor A tensor containing the items that we want to prune. mask : torch.BoolTensor A tensor with the same shape as input_ that makes the function not consider masked out (i.e. False) elements. k : Union[int, torch.LongTensor] If a tensor of shape as input_ except without dimension dim , specifies the number of items to keep for each dimension. If an int, keep the same number of items for all dimensions. Returns \u00b6 top_input : torch.FloatTensor The values of the top-k scoring items. Has the same shape as input_ except dimension dim has value k when it's an int or k.max() when it's a tensor. top_mask : torch.BoolTensor The corresponding mask for top_input . Has the shape as top_input . top_indices : torch.IntTensor The indices of the top-k scoring items into the original input_ tensor. This is returned because it can be useful to retain pointers to the original items, if each item is being scored by multiple distinct scorers, for instance. Has the shape as top_input . info_value_of_dtype \u00b6 def info_value_of_dtype ( dtype : torch . dtype ) Returns the finfo or iinfo object of a given PyTorch data type. Does not allow torch.bool. min_value_of_dtype \u00b6 def min_value_of_dtype ( dtype : torch . dtype ) Returns the minimum value of a given PyTorch data type. Does not allow torch.bool. max_value_of_dtype \u00b6 def max_value_of_dtype ( dtype : torch . dtype ) Returns the maximum value of a given PyTorch data type. Does not allow torch.bool. tiny_value_of_dtype \u00b6 def tiny_value_of_dtype ( dtype : torch . dtype ) Returns a moderately tiny value for a given PyTorch data type that is used to avoid numerical issues such as division by zero. This is different from info_value_of_dtype(dtype).tiny because it causes some NaN bugs. Only supports floating point dtypes. distributed_device \u00b6 def distributed_device () -> torch . device Get the correct torch.device of the current process to use for distributed point-to-point communication. dist_reduce \u00b6 def dist_reduce ( value : _V , reduce_op ) -> _V Reduces the given value across all distributed worker nodes according the given reduction operation. If called outside of a distributed context, it will just return value . Parameters \u00b6 value : _V The value to reduce across distributed nodes. reduce_op : torch.distributed.ReduceOp The reduction operation to use. **kwargs : Any Additional arguments used to construct the tensor that will wrap value . Returns \u00b6 _V The final value. dist_reduce_sum \u00b6 def dist_reduce_sum ( value : _V ) -> _V Sums the given value across distributed worker nodes. This is equivalent to calling dist_reduce(v, dist.ReduceOp.SUM) . load_state_dict_distributed \u00b6 def load_state_dict_distributed ( module : torch . nn . Module , state_dict : Optional [ StateDictType ], strict : bool = True , prefix : str = \"\" ) -> _IncompatibleKeys Load a state_dict to the module within a distributed process. Only the global primary process requires the state_dict to not be None . All other processes will have the state tensors broadcasted to them one-by-one. If strict is True , then the keys of state_dict must exactly match the keys returned by module.state_dict() . Note The returned missing_keys and unexpected_keys will only be accurate in the primary process. Returns \u00b6 A NamedTuple with missing_keys and unexpected_keys fields, both of which are lists of strings. Raises \u00b6 RuntimeError If strict is True and there are missing or unexpected keys.","title":"util"},{"location":"api/nn/util/#t","text":"T = TypeVar ( \"T\" )","title":"T"},{"location":"api/nn/util/#statedicttype","text":"StateDictType = Union [ Dict [ str , torch . Tensor ], \"OrderedDict[str, torch.Tensor]\" ]","title":"StateDictType"},{"location":"api/nn/util/#move_to_device","text":"def move_to_device ( obj , device : Union [ torch . device , int ]) Given a structure (possibly) containing Tensors, move all the Tensors to the specified device (or do nothing, if they are already on the target device).","title":"move_to_device"},{"location":"api/nn/util/#clamp_tensor","text":"def clamp_tensor ( tensor , minimum , maximum ) Supports sparse and dense tensors. Returns a tensor with values clamped between the provided minimum and maximum, without modifying the original tensor.","title":"clamp_tensor"},{"location":"api/nn/util/#batch_tensor_dicts","text":"def batch_tensor_dicts ( tensor_dicts : List [ Dict [ str , torch . Tensor ]], remove_trailing_dimension : bool = False ) -> Dict [ str , torch . Tensor ] Takes a list of tensor dictionaries, where each dictionary is assumed to have matching keys, and returns a single dictionary with all tensors with the same key batched together.","title":"batch_tensor_dicts"},{"location":"api/nn/util/#get_lengths_from_binary_sequence_mask","text":"def get_lengths_from_binary_sequence_mask ( mask : torch . BoolTensor ) -> torch . LongTensor Compute sequence lengths for each batch element in a tensor using a binary mask.","title":"get_lengths_from_binary_sequence_mask"},{"location":"api/nn/util/#get_mask_from_sequence_lengths","text":"def get_mask_from_sequence_lengths ( sequence_lengths : torch . Tensor , max_length : int ) -> torch . BoolTensor Given a variable of shape (batch_size,) that represents the sequence lengths of each batch element, this function returns a (batch_size, max_length) mask variable. For example, if our input was [2, 2, 3] , with a max_length of 4, we'd return [[1, 1, 0, 0], [1, 1, 0, 0], [1, 1, 1, 0]] . We require max_length here instead of just computing it from the input sequence_lengths because it lets us avoid finding the max, then copying that value from the GPU to the CPU so that we can use it to construct a new tensor.","title":"get_mask_from_sequence_lengths"},{"location":"api/nn/util/#sort_batch_by_length","text":"def sort_batch_by_length ( tensor : torch . Tensor , sequence_lengths : torch . Tensor ) Sort a batch first tensor by some specified lengths.","title":"sort_batch_by_length"},{"location":"api/nn/util/#get_final_encoder_states","text":"def get_final_encoder_states ( encoder_outputs : torch . Tensor , mask : torch . BoolTensor , bidirectional : bool = False ) -> torch . Tensor Given the output from a Seq2SeqEncoder , with shape (batch_size, sequence_length, encoding_dim) , this method returns the final hidden state for each element of the batch, giving a tensor of shape (batch_size, encoding_dim) . This is not as simple as encoder_outputs[:, -1] , because the sequences could have different lengths. We use the mask (which has shape (batch_size, sequence_length) ) to find the final state for each batch instance. Additionally, if bidirectional is True , we will split the final dimension of the encoder_outputs into two and assume that the first half is for the forward direction of the encoder and the second half is for the backward direction. We will concatenate the last state for each encoder dimension, giving encoder_outputs[:, -1, :encoding_dim/2] concatenated with encoder_outputs[:, 0, encoding_dim/2:] .","title":"get_final_encoder_states"},{"location":"api/nn/util/#get_dropout_mask","text":"def get_dropout_mask ( dropout_probability : float , tensor_for_masking : torch . Tensor ) Computes and returns an element-wise dropout mask for a given tensor, where each element in the mask is dropped out with probability dropout_probability. Note that the mask is NOT applied to the tensor - the tensor is passed to retain the correct CUDA tensor type for the mask.","title":"get_dropout_mask"},{"location":"api/nn/util/#masked_softmax","text":"def masked_softmax ( vector : torch . Tensor , mask : torch . BoolTensor , dim : int = - 1 , memory_efficient : bool = False ) -> torch . Tensor torch.nn.functional.softmax(vector) does not work if some elements of vector should be masked. This performs a softmax on just the non-masked portions of vector . Passing None in for the mask is also acceptable; you'll just get a regular softmax. vector can have an arbitrary number of dimensions; the only requirement is that mask is broadcastable to vector's shape. If mask has fewer dimensions than vector , we will unsqueeze on dimension 1 until they match. If you need a different unsqueezing of your mask, do it yourself before passing the mask into this function. If memory_efficient is set to true, we will simply use a very large negative number for those masked positions so that the probabilities of those positions would be approximately 0. This is not accurate in math, but works for most cases and consumes less memory. In the case that the input vector is completely masked and memory_efficient is false, this function returns an array of 0.0 . This behavior may cause NaN if this is used as the last layer of a model that uses categorical cross-entropy loss. Instead, if memory_efficient is true, this function will treat every element as equal, and do softmax over equal numbers.","title":"masked_softmax"},{"location":"api/nn/util/#masked_log_softmax","text":"def masked_log_softmax ( vector : torch . Tensor , mask : torch . BoolTensor , dim : int = - 1 ) -> torch . Tensor torch.nn.functional.log_softmax(vector) does not work if some elements of vector should be masked. This performs a log_softmax on just the non-masked portions of vector . Passing None in for the mask is also acceptable; you'll just get a regular log_softmax. vector can have an arbitrary number of dimensions; the only requirement is that mask is broadcastable to vector's shape. If mask has fewer dimensions than vector , we will unsqueeze on dimension 1 until they match. If you need a different unsqueezing of your mask, do it yourself before passing the mask into this function. In the case that the input vector is completely masked, the return value of this function is arbitrary, but not nan . You should be masking the result of whatever computation comes out of this in that case, anyway, so the specific values returned shouldn't matter. Also, the way that we deal with this case relies on having single-precision floats; mixing half-precision floats with fully-masked vectors will likely give you nans . If your logits are all extremely negative (i.e., the max value in your logit vector is -50 or lower), the way we handle masking here could mess you up. But if you've got logit values that extreme, you've got bigger problems than this.","title":"masked_log_softmax"},{"location":"api/nn/util/#masked_max","text":"def masked_max ( vector : torch . Tensor , mask : torch . BoolTensor , dim : int , keepdim : bool = False ) -> torch . Tensor To calculate max along certain dimensions on masked values","title":"masked_max"},{"location":"api/nn/util/#masked_mean","text":"def masked_mean ( vector : torch . Tensor , mask : torch . BoolTensor , dim : int , keepdim : bool = False ) -> torch . Tensor To calculate mean along certain dimensions on masked values","title":"masked_mean"},{"location":"api/nn/util/#masked_flip","text":"def masked_flip ( padded_sequence : torch . Tensor , sequence_lengths : List [ int ] ) -> torch . Tensor Flips a padded tensor along the time dimension without affecting masked entries.","title":"masked_flip"},{"location":"api/nn/util/#viterbi_decode","text":"def viterbi_decode ( tag_sequence : torch . Tensor , transition_matrix : torch . Tensor , tag_observations : Optional [ List [ int ]] = None , allowed_start_transitions : torch . Tensor = None , allowed_end_transitions : torch . Tensor = None , top_k : int = None ) Perform Viterbi decoding in log space over a sequence given a transition matrix specifying pairwise (transition) potentials between tags and a matrix of shape (sequence_length, num_tags) specifying unary potentials for possible tags per timestep.","title":"viterbi_decode"},{"location":"api/nn/util/#get_text_field_mask","text":"def get_text_field_mask ( text_field_tensors : Dict [ str , Dict [ str , torch . Tensor ]], num_wrapping_dims : int = 0 , padding_id : int = 0 ) -> torch . BoolTensor Takes the dictionary of tensors produced by a TextField and returns a mask with 0 where the tokens are padding, and 1 otherwise. padding_id specifies the id of padding tokens. We also handle TextFields wrapped by an arbitrary number of ListFields , where the number of wrapping ListFields is given by num_wrapping_dims . If num_wrapping_dims == 0 , the returned mask has shape (batch_size, num_tokens) . If num_wrapping_dims > 0 then the returned mask has num_wrapping_dims extra dimensions, so the shape will be (batch_size, ..., num_tokens) . There could be several entries in the tensor dictionary with different shapes (e.g., one for word ids, one for character ids). In order to get a token mask, we use the tensor in the dictionary with the lowest number of dimensions. After subtracting num_wrapping_dims , if this tensor has two dimensions we assume it has shape (batch_size, ..., num_tokens) , and use it for the mask. If instead it has three dimensions, we assume it has shape (batch_size, ..., num_tokens, num_features) , and sum over the last dimension to produce the mask. Most frequently this will be a character id tensor, but it could also be a featurized representation of each token, etc. If the input text_field_tensors contains the \"mask\" key, this is returned instead of inferring the mask.","title":"get_text_field_mask"},{"location":"api/nn/util/#get_token_ids_from_text_field_tensors","text":"def get_token_ids_from_text_field_tensors ( text_field_tensors : Dict [ str , Dict [ str , torch . Tensor ]] ) -> torch . Tensor Our TextFieldTensors are complex output structures, because they try to handle a lot of potential variation. Sometimes, you just want to grab the token ids from this data structure, and that's not trivial without hard-coding assumptions about your data processing, which defeats the entire purpose of that generality. This method tries to let you get the token ids out of the data structure in your model without hard-coding any assumptions.","title":"get_token_ids_from_text_field_tensors"},{"location":"api/nn/util/#weighted_sum","text":"def weighted_sum ( matrix : torch . Tensor , attention : torch . Tensor ) -> torch . Tensor Takes a matrix of vectors and a set of weights over the rows in the matrix (which we call an \"attention\" vector), and returns a weighted sum of the rows in the matrix. This is the typical computation performed after an attention mechanism. Note that while we call this a \"matrix\" of vectors and an attention \"vector\", we also handle higher-order tensors. We always sum over the second-to-last dimension of the \"matrix\", and we assume that all dimensions in the \"matrix\" prior to the last dimension are matched in the \"vector\". Non-matched dimensions in the \"vector\" must be directly after the batch dimension . For example, say I have a \"matrix\" with dimensions (batch_size, num_queries, num_words, embedding_dim) . The attention \"vector\" then must have at least those dimensions, and could have more. Both: - ` ( batch_size , num_queries , num_words ) ` ( distribution over words for each query ) - ` ( batch_size , num_documents , num_queries , num_words ) ` ( distribution over words in a query for each document ) are valid input \"vectors\", producing tensors of shape: (batch_size, num_queries, embedding_dim) and (batch_size, num_documents, num_queries, embedding_dim) respectively.","title":"weighted_sum"},{"location":"api/nn/util/#sequence_cross_entropy_with_logits","text":"def sequence_cross_entropy_with_logits ( logits : torch . FloatTensor , targets : torch . LongTensor , weights : Union [ torch . FloatTensor , torch . BoolTensor ], average : str = \"batch\" , label_smoothing : float = None , gamma : float = None , alpha : Union [ float , List [ float ], torch . FloatTensor ] = None ) -> torch . FloatTensor Computes the cross entropy loss of a sequence, weighted with respect to some user provided weights. Note that the weighting here is not the same as in the torch.nn.CrossEntropyLoss() criterion, which is weighting classes; here we are weighting the loss contribution from particular elements in the sequence. This allows loss computations for models which use padding.","title":"sequence_cross_entropy_with_logits"},{"location":"api/nn/util/#replace_masked_values","text":"def replace_masked_values ( tensor : torch . Tensor , mask : torch . BoolTensor , replace_with : float ) -> torch . Tensor Replaces all masked values in tensor with replace_with . mask must be broadcastable to the same shape as tensor . We require that tensor.dim() == mask.dim() , as otherwise we won't know which dimensions of the mask to unsqueeze. This just does tensor.masked_fill() , except the pytorch method fills in things with a mask value of 1, where we want the opposite. You can do this in your own code with tensor.masked_fill(~mask, replace_with) .","title":"replace_masked_values"},{"location":"api/nn/util/#tensors_equal","text":"def tensors_equal ( tensor1 : torch . Tensor , tensor2 : torch . Tensor , tolerance : float = 1e-12 ) -> bool A check for tensor equality (by value). We make sure that the tensors have the same shape, then check all of the entries in the tensor for equality. We additionally allow the input tensors to be lists or dictionaries, where we then do the above check on every position in the list / item in the dictionary. If we find objects that aren't tensors as we're doing that, we just defer to their equality check. This is kind of a catch-all method that's designed to make implementing __eq__ methods easier, in a way that's really only intended to be useful for tests.","title":"tensors_equal"},{"location":"api/nn/util/#device_mapping","text":"def device_mapping ( cuda_device : int ) In order to torch.load() a GPU-trained model onto a CPU (or specific GPU), you have to supply a map_location function. Call this with the desired cuda_device to get the function that torch.load() needs.","title":"device_mapping"},{"location":"api/nn/util/#read_state_dict","text":"def read_state_dict ( path : Union [ PathLike , str ], strip_prefix : Optional [ str ] = None , ignore : Optional [ List [ str ]] = None , strict : bool = True , cuda_device : int = - 1 ) -> Dict [ str , torch . Tensor ] Read a PyTorch model state dictionary from a checkpoint at the given path .","title":"read_state_dict"},{"location":"api/nn/util/#combine_tensors","text":"def combine_tensors ( combination : str , tensors : List [ torch . Tensor ] ) -> torch . Tensor Combines a list of tensors using element-wise operations and concatenation, specified by a combination string. The string refers to (1-indexed) positions in the input tensor list, and looks like \"1,2,1+2,3-1\" . We allow the following kinds of combinations : x , x*y , x+y , x-y , and x/y , where x and y are positive integers less than or equal to len(tensors) . Each of the binary operations is performed elementwise. You can give as many combinations as you want in the combination string. For example, for the input string \"1,2,1*2\" , the result would be [1;2;1*2] , as you would expect, where [;] is concatenation along the last dimension. If you have a fixed, known way to combine tensors that you use in a model, you should probably just use something like torch.cat([x_tensor, y_tensor, x_tensor * y_tensor]) . This function adds some complexity that is only necessary if you want the specific combination used to be configurable . If you want to do any element-wise operations, the tensors involved in each element-wise operation must have the same shape. This function also accepts x and y in place of 1 and 2 in the combination string.","title":"combine_tensors"},{"location":"api/nn/util/#combine_tensors_and_multiply","text":"def combine_tensors_and_multiply ( combination : str , tensors : List [ torch . Tensor ], weights : torch . nn . Parameter ) -> torch . Tensor Like combine_tensors , but does a weighted (linear) multiplication while combining. This is a separate function from combine_tensors because we try to avoid instantiating large intermediate tensors during the combination, which is possible because we know that we're going to be multiplying by a weight vector in the end.","title":"combine_tensors_and_multiply"},{"location":"api/nn/util/#get_combined_dim","text":"def get_combined_dim ( combination : str , tensor_dims : List [ int ]) -> int For use with combine_tensors . This function computes the resultant dimension when calling combine_tensors(combination, tensors) , when the tensor dimension is known. This is necessary for knowing the sizes of weight matrices when building models that use combine_tensors .","title":"get_combined_dim"},{"location":"api/nn/util/#logsumexp","text":"def logsumexp ( tensor : torch . Tensor , dim : int = - 1 , keepdim : bool = False ) -> torch . Tensor A numerically stable computation of logsumexp. This is mathematically equivalent to tensor.exp().sum(dim, keep=keepdim).log() . This function is typically used for summing log probabilities.","title":"logsumexp"},{"location":"api/nn/util/#get_device_of","text":"def get_device_of ( tensor : torch . Tensor ) -> int Returns the device of the tensor.","title":"get_device_of"},{"location":"api/nn/util/#flatten_and_batch_shift_indices","text":"def flatten_and_batch_shift_indices ( indices : torch . Tensor , sequence_length : int ) -> torch . Tensor This is a subroutine for batched_index_select . The given indices of size (batch_size, d_1, ..., d_n) indexes into dimension 2 of a target tensor, which has size (batch_size, sequence_length, embedding_size) . This function returns a vector that correctly indexes into the flattened target. The sequence length of the target must be provided to compute the appropriate offsets. indices = torch . ones ([ 2 , 3 ], dtype = torch . long ) # Sequence length of the target tensor. sequence_length = 10 shifted_indices = flatten_and_batch_shift_indices ( indices , sequence_length ) # Indices into the second element in the batch are correctly shifted # to take into account that the target tensor will be flattened before # the indices are applied. assert shifted_indices == [ 1 , 1 , 1 , 11 , 11 , 11 ]","title":"flatten_and_batch_shift_indices"},{"location":"api/nn/util/#batched_index_select","text":"def batched_index_select ( target : torch . Tensor , indices : torch . LongTensor , flattened_indices : Optional [ torch . LongTensor ] = None ) -> torch . Tensor The given indices of size (batch_size, d_1, ..., d_n) indexes into the sequence dimension (dimension 2) of the target, which has size (batch_size, sequence_length, embedding_size) . This function returns selected values in the target with respect to the provided indices, which have size (batch_size, d_1, ..., d_n, embedding_size) . This can use the optionally precomputed flattened_indices with size (batch_size * d_1 * ... * d_n) if given. An example use case of this function is looking up the start and end indices of spans in a sequence tensor. This is used in the CoreferenceResolver model to select contextual word representations corresponding to the start and end indices of mentions. The key reason this can't be done with basic torch functions is that we want to be able to use look-up tensors with an arbitrary number of dimensions (for example, in the coref model, we don't know a-priori how many spans we are looking up).","title":"batched_index_select"},{"location":"api/nn/util/#masked_index_fill","text":"def masked_index_fill ( target : torch . Tensor , indices : torch . LongTensor , mask : torch . BoolTensor , fill_value : int = 1 ) -> torch . Tensor The given indices in target will be will be filled with fill_value given a mask .","title":"masked_index_fill"},{"location":"api/nn/util/#masked_index_replace","text":"def masked_index_replace ( target : torch . Tensor , indices : torch . LongTensor , mask : torch . BoolTensor , replace : torch . Tensor ) -> torch . Tensor The given indices in target will be will be replaced with corresponding index from the replace tensor given a mask .","title":"masked_index_replace"},{"location":"api/nn/util/#batched_span_select","text":"def batched_span_select ( target : torch . Tensor , spans : torch . LongTensor ) -> torch . Tensor The given spans of size (batch_size, num_spans, 2) indexes into the sequence dimension (dimension 2) of the target, which has size (batch_size, sequence_length, embedding_size) . This function returns segmented spans in the target with respect to the provided span indices.","title":"batched_span_select"},{"location":"api/nn/util/#flattened_index_select","text":"def flattened_index_select ( target : torch . Tensor , indices : torch . LongTensor ) -> torch . Tensor The given indices of size (set_size, subset_size) specifies subsets of the target that each of the set_size rows should select. The target has size (batch_size, sequence_length, embedding_size) , and the resulting selected tensor has size (batch_size, set_size, subset_size, embedding_size) .","title":"flattened_index_select"},{"location":"api/nn/util/#get_range_vector","text":"def get_range_vector ( size : int , device : int ) -> torch . Tensor Returns a range vector with the desired size, starting at 0. The CUDA implementation is meant to avoid copy data from CPU to GPU.","title":"get_range_vector"},{"location":"api/nn/util/#bucket_values","text":"def bucket_values ( distances : torch . Tensor , num_identity_buckets : int = 4 , num_total_buckets : int = 10 ) -> torch . Tensor Places the given values (designed for distances) into num_total_buckets semi-logscale buckets, with num_identity_buckets of these capturing single values. The default settings will bucket values into the following buckets: [0, 1, 2, 3, 4, 5-7, 8-15, 16-31, 32-63, 64+].","title":"bucket_values"},{"location":"api/nn/util/#add_sentence_boundary_token_ids","text":"def add_sentence_boundary_token_ids ( tensor : torch . Tensor , mask : torch . BoolTensor , sentence_begin_token : Any , sentence_end_token : Any ) -> Tuple [ torch . Tensor , torch . BoolTensor ] Add begin/end of sentence tokens to the batch of sentences. Given a batch of sentences with size (batch_size, timesteps) or (batch_size, timesteps, dim) this returns a tensor of shape (batch_size, timesteps + 2) or (batch_size, timesteps + 2, dim) respectively. Returns both the new tensor and updated mask.","title":"add_sentence_boundary_token_ids"},{"location":"api/nn/util/#remove_sentence_boundaries","text":"def remove_sentence_boundaries ( tensor : torch . Tensor , mask : torch . BoolTensor ) -> Tuple [ torch . Tensor , torch . Tensor ] Remove begin/end of sentence embeddings from the batch of sentences. Given a batch of sentences with size (batch_size, timesteps, dim) this returns a tensor of shape (batch_size, timesteps - 2, dim) after removing the beginning and end sentence markers. The sentences are assumed to be padded on the right, with the beginning of each sentence assumed to occur at index 0 (i.e., mask[:, 0] is assumed to be 1). Returns both the new tensor and updated mask. This function is the inverse of add_sentence_boundary_token_ids .","title":"remove_sentence_boundaries"},{"location":"api/nn/util/#add_positional_features","text":"def add_positional_features ( tensor : torch . Tensor , min_timescale : float = 1.0 , max_timescale : float = 1.0e4 ) Implements the frequency-based positional encoding described in Attention is All you Need . Adds sinusoids of different frequencies to a Tensor . A sinusoid of a different frequency and phase is added to each dimension of the input Tensor . This allows the attention heads to use absolute and relative positions. The number of timescales is equal to hidden_dim / 2 within the range (min_timescale, max_timescale). For each timescale, the two sinusoidal signals sin(timestep / timescale) and cos(timestep / timescale) are generated and concatenated along the hidden_dim dimension.","title":"add_positional_features"},{"location":"api/nn/util/#clone","text":"def clone ( module : torch . nn . Module , num_copies : int ) -> torch . nn . ModuleList Produce N identical layers.","title":"clone"},{"location":"api/nn/util/#combine_initial_dims","text":"def combine_initial_dims ( tensor : torch . Tensor ) -> torch . Tensor Given a (possibly higher order) tensor of ids with shape (d1, ..., dn, sequence_length) Return a view that's (d1 * ... * dn, sequence_length). If original tensor is 1-d or 2-d, return it as is.","title":"combine_initial_dims"},{"location":"api/nn/util/#uncombine_initial_dims","text":"def uncombine_initial_dims ( tensor : torch . Tensor , original_size : torch . Size ) -> torch . Tensor Given a tensor of embeddings with shape (d1 * ... * dn, sequence_length, embedding_dim) and the original shape (d1, ..., dn, sequence_length), return the reshaped tensor of embeddings with shape (d1, ..., dn, sequence_length, embedding_dim). If original size is 1-d or 2-d, return it as is.","title":"uncombine_initial_dims"},{"location":"api/nn/util/#inspect_parameters","text":"def inspect_parameters ( module : torch . nn . Module , quiet : bool = False ) -> Dict [ str , Any ] Inspects the model/module parameters and their tunability. The output is structured in a nested dict so that parameters in same sub-modules are grouped together. This can be helpful to setup module path based regex, for example in initializer. It prints it by default (optional) and returns the inspection dict. Eg. output:: { \"_text_field_embedder\": { \"token_embedder_tokens\": { \"_projection\": { \"bias\": \"tunable\", \"weight\": \"tunable\" }, \"weight\": \"frozen\" } } }","title":"inspect_parameters"},{"location":"api/nn/util/#find_text_field_embedder","text":"def find_text_field_embedder ( model : torch . nn . Module ) -> torch . nn . Module Takes a Model and returns the Module that is a TextFieldEmbedder . We return just the first one, as it's very rare to have more than one. If there isn't a TextFieldEmbedder in the given Model , we raise a ValueError .","title":"find_text_field_embedder"},{"location":"api/nn/util/#find_embedding_layer","text":"def find_embedding_layer ( model : torch . nn . Module ) -> torch . nn . Module Takes a model (typically an AllenNLP Model , but this works for any torch.nn.Module ) and makes a best guess about which module is the embedding layer. For typical AllenNLP models, this often is the TextFieldEmbedder , but if you're using a pre-trained contextualizer, we really want layer 0 of that contextualizer, not the output. So there are a bunch of hacks in here for specific pre-trained contextualizers.","title":"find_embedding_layer"},{"location":"api/nn/util/#get_token_offsets_from_text_field_inputs","text":"def get_token_offsets_from_text_field_inputs ( text_field_inputs : List [ Any ] ) -> Optional [ torch . Tensor ] Given a list of inputs to a TextFieldEmbedder, tries to find token offsets from those inputs, if there are any. You will have token offsets if you are using a mismatched token embedder; if you're not, the return value from this function should be None. This function is intended to be called from a forward_hook attached to a TextFieldEmbedder , so the inputs are formatted just as a list. It's possible in theory that you could have multiple offsets as inputs to a single call to a TextFieldEmbedder , but that's an extremely rare use case (I can't really imagine anyone wanting to do that). In that case, we'll only return the first one. If you need different behavior for your model, open an issue on github describing what you're doing.","title":"get_token_offsets_from_text_field_inputs"},{"location":"api/nn/util/#extend_layer","text":"def extend_layer ( layer : torch . nn . Module , new_dim : int ) -> None","title":"extend_layer"},{"location":"api/nn/util/#masked_topk","text":"def masked_topk ( input_ : torch . FloatTensor , mask : torch . BoolTensor , k : Union [ int , torch . LongTensor ], dim : int = - 1 ) -> Tuple [ torch . LongTensor , torch . LongTensor , torch . FloatTensor ] Extracts the top-k items along a certain dimension. This is similar to torch.topk except: (1) we allow of a mask that makes the function not consider certain elements; (2) the returned top input, mask, and indices are sorted in their original order in the input; (3) May use the same k for all dimensions, or different k for each.","title":"masked_topk"},{"location":"api/nn/util/#info_value_of_dtype","text":"def info_value_of_dtype ( dtype : torch . dtype ) Returns the finfo or iinfo object of a given PyTorch data type. Does not allow torch.bool.","title":"info_value_of_dtype"},{"location":"api/nn/util/#min_value_of_dtype","text":"def min_value_of_dtype ( dtype : torch . dtype ) Returns the minimum value of a given PyTorch data type. Does not allow torch.bool.","title":"min_value_of_dtype"},{"location":"api/nn/util/#max_value_of_dtype","text":"def max_value_of_dtype ( dtype : torch . dtype ) Returns the maximum value of a given PyTorch data type. Does not allow torch.bool.","title":"max_value_of_dtype"},{"location":"api/nn/util/#tiny_value_of_dtype","text":"def tiny_value_of_dtype ( dtype : torch . dtype ) Returns a moderately tiny value for a given PyTorch data type that is used to avoid numerical issues such as division by zero. This is different from info_value_of_dtype(dtype).tiny because it causes some NaN bugs. Only supports floating point dtypes.","title":"tiny_value_of_dtype"},{"location":"api/nn/util/#distributed_device","text":"def distributed_device () -> torch . device Get the correct torch.device of the current process to use for distributed point-to-point communication.","title":"distributed_device"},{"location":"api/nn/util/#dist_reduce","text":"def dist_reduce ( value : _V , reduce_op ) -> _V Reduces the given value across all distributed worker nodes according the given reduction operation. If called outside of a distributed context, it will just return value .","title":"dist_reduce"},{"location":"api/nn/util/#dist_reduce_sum","text":"def dist_reduce_sum ( value : _V ) -> _V Sums the given value across distributed worker nodes. This is equivalent to calling dist_reduce(v, dist.ReduceOp.SUM) .","title":"dist_reduce_sum"},{"location":"api/nn/util/#load_state_dict_distributed","text":"def load_state_dict_distributed ( module : torch . nn . Module , state_dict : Optional [ StateDictType ], strict : bool = True , prefix : str = \"\" ) -> _IncompatibleKeys Load a state_dict to the module within a distributed process. Only the global primary process requires the state_dict to not be None . All other processes will have the state tensors broadcasted to them one-by-one. If strict is True , then the keys of state_dict must exactly match the keys returned by module.state_dict() . Note The returned missing_keys and unexpected_keys will only be accurate in the primary process.","title":"load_state_dict_distributed"},{"location":"api/nn/checkpoint/checkpoint_wrapper/","text":"allennlp .nn .checkpoint .checkpoint_wrapper [SOURCE] CheckpointWrapper \u00b6 class CheckpointWrapper ( Registrable ) A CheckpointWrapper is used to enable activation/gradient checkpointing on modules that you wrap via the .wrap_module() method. default_implementation \u00b6 class CheckpointWrapper ( Registrable ): | ... | default_implementation = \"torch\" wrap_module \u00b6 class CheckpointWrapper ( Registrable ): | ... | def wrap_module ( self , module : nn . Module , ** kwargs ) -> nn . Module TorchCheckpointWrapper \u00b6 @CheckpointWrapper . register ( \"torch\" ) class TorchCheckpointWrapper ( CheckpointWrapper ) wrap_module \u00b6 class TorchCheckpointWrapper ( CheckpointWrapper ): | ... | def wrap_module ( self , module : nn . Module , ** kwargs ) -> nn . Module Wrap a module so that the forward method uses PyTorch's checkpointing functionality . Note Currently this CheckpointWrapper implementation requires that the wrapped module is called with positional arguments only. We recommend you use the FairScaleCheckpointWrapper if you need more flexibility.","title":"checkpoint_wrapper"},{"location":"api/nn/checkpoint/checkpoint_wrapper/#checkpointwrapper","text":"class CheckpointWrapper ( Registrable ) A CheckpointWrapper is used to enable activation/gradient checkpointing on modules that you wrap via the .wrap_module() method.","title":"CheckpointWrapper"},{"location":"api/nn/checkpoint/checkpoint_wrapper/#default_implementation","text":"class CheckpointWrapper ( Registrable ): | ... | default_implementation = \"torch\"","title":"default_implementation"},{"location":"api/nn/checkpoint/checkpoint_wrapper/#wrap_module","text":"class CheckpointWrapper ( Registrable ): | ... | def wrap_module ( self , module : nn . Module , ** kwargs ) -> nn . Module","title":"wrap_module"},{"location":"api/nn/checkpoint/checkpoint_wrapper/#torchcheckpointwrapper","text":"@CheckpointWrapper . register ( \"torch\" ) class TorchCheckpointWrapper ( CheckpointWrapper )","title":"TorchCheckpointWrapper"},{"location":"api/nn/checkpoint/checkpoint_wrapper/#wrap_module_1","text":"class TorchCheckpointWrapper ( CheckpointWrapper ): | ... | def wrap_module ( self , module : nn . Module , ** kwargs ) -> nn . Module Wrap a module so that the forward method uses PyTorch's checkpointing functionality . Note Currently this CheckpointWrapper implementation requires that the wrapped module is called with positional arguments only. We recommend you use the FairScaleCheckpointWrapper if you need more flexibility.","title":"wrap_module"},{"location":"api/nn/checkpoint/fairscale_checkpoint_wrapper/","text":"allennlp .nn .checkpoint .fairscale_checkpoint_wrapper [SOURCE] FairScaleCheckpointWrapper \u00b6 @CheckpointWrapper . register ( \"fairscale\" ) class FairScaleCheckpointWrapper ( CheckpointWrapper ): | def __init__ ( self , offload_to_cpu : Optional [ bool ] = True ) -> None Provides FairScale 's activation/gradient checkpointing functionality. The parameters and their defaults are the same as they are in FairScale, and any of them can be overriden on a per-module basis by passing the corresponding parameter to .wrap_module() . This can also be used in conjunction with the FairScaleFsdpAccelerator . See the T5 implementation for an example of how to use the two together. wrap_module \u00b6 class FairScaleCheckpointWrapper ( CheckpointWrapper ): | ... | def wrap_module ( self , module : nn . Module , ** kwargs ,) -> nn . Module","title":"fairscale_checkpoint_wrapper"},{"location":"api/nn/checkpoint/fairscale_checkpoint_wrapper/#fairscalecheckpointwrapper","text":"@CheckpointWrapper . register ( \"fairscale\" ) class FairScaleCheckpointWrapper ( CheckpointWrapper ): | def __init__ ( self , offload_to_cpu : Optional [ bool ] = True ) -> None Provides FairScale 's activation/gradient checkpointing functionality. The parameters and their defaults are the same as they are in FairScale, and any of them can be overriden on a per-module basis by passing the corresponding parameter to .wrap_module() . This can also be used in conjunction with the FairScaleFsdpAccelerator . See the T5 implementation for an example of how to use the two together.","title":"FairScaleCheckpointWrapper"},{"location":"api/nn/checkpoint/fairscale_checkpoint_wrapper/#wrap_module","text":"class FairScaleCheckpointWrapper ( CheckpointWrapper ): | ... | def wrap_module ( self , module : nn . Module , ** kwargs ,) -> nn . Module","title":"wrap_module"},{"location":"api/nn/parallel/ddp_accelerator/","text":"allennlp .nn .parallel .ddp_accelerator [SOURCE] StateDictType \u00b6 StateDictType = Union [ Dict [ str , torch . Tensor ], OrderedDict [ str , torch . Tensor ]] LoadStateDictReturnType \u00b6 class LoadStateDictReturnType ( NamedTuple ) missing_keys \u00b6 class LoadStateDictReturnType ( NamedTuple ): | ... | missing_keys : List [ str ] = None unexpected_keys \u00b6 class LoadStateDictReturnType ( NamedTuple ): | ... | unexpected_keys : List [ str ] = None DdpWrappedModel \u00b6 class DdpWrappedModel : | def __init__ ( | self , | model : torch . nn . Module , | local_rank : Optional [ int ] = None , | world_size : Optional [ int ] = None | ) -> None The type of the wrapped model returned from DdpAccelerator.wrap_model . is_sharded \u00b6 class DdpWrappedModel : | ... | @property | def is_sharded ( self ) -> bool consolidate_sharded_state \u00b6 class DdpWrappedModel : | ... | @staticmethod | def consolidate_sharded_state ( | sharded_state_files : Sequence [ Union [ str , os . PathLike ]] | ) -> StateDictType load_state_dict \u00b6 class DdpWrappedModel : | ... | def load_state_dict ( | self , | state_dict : StateDictType , | strict : bool = True | ) -> LoadStateDictReturnType state_dict \u00b6 class DdpWrappedModel : | ... | def state_dict ( self , * args , ** kwargs ) -> StateDictType clip_grad_norm_ \u00b6 class DdpWrappedModel : | ... | def clip_grad_norm_ ( self , max_norm : Union [ float , int ]) -> torch . Tensor init_grad_scaler \u00b6 class DdpWrappedModel : | ... | def init_grad_scaler ( self ) -> amp . GradScaler DdpAccelerator \u00b6 class DdpAccelerator ( Registrable ): | def __init__ ( | self , | local_rank : Optional [ int ] = None , | world_size : Optional [ int ] = None , | cuda_device : Union [ torch . device , int ] = - 1 | ) -> None A DdpAccelerator is a generalization of PyTorch's DistributedDataParallel class. This is primarly used within the GradientDescentTrainer to allow for different DDP implementations, such as FairScale's FullyShardedDataParallel . In a typical AllenNLP configuration file, local_rank , world_size , and cuda_device should not be specified. Warning This API is experimental and may change in the future. default_implementation \u00b6 class DdpAccelerator ( Registrable ): | ... | default_implementation = \"torch\" wrap_model \u00b6 class DdpAccelerator ( Registrable ): | ... | def wrap_model ( | self , | model : \"Model\" | ) -> Tuple [ \"Model\" , DdpWrappedModel ] Wrap the AllenNLP Model , returning the original model (possibly on a different device) and the wrapper model . wrap_module \u00b6 class DdpAccelerator ( Registrable ): | ... | def wrap_module ( self , module : torch . nn . Module ) -> torch . nn . Module Wrap an individual module. By default this just returns the module, but some subclass implementations such as FairScaleFsdpAccelerator do more. TorchDdpAccelerator \u00b6 @DdpAccelerator . register ( \"torch\" ) class TorchDdpAccelerator ( DdpAccelerator ): | def __init__ ( | self , | * , find_unused_parameters : bool = False , | * , local_rank : Optional [ int ] = None , | * , world_size : Optional [ int ] = None , | * , cuda_device : Union [ torch . device , int ] = - 1 | ) -> None The default implementation of DdpAccelerator , which is just a thin wrapper around PyTorch's DistributedDataParallel . wrap_model \u00b6 class TorchDdpAccelerator ( DdpAccelerator ): | ... | def wrap_model ( | self , | model : \"Model\" | ) -> Tuple [ \"Model\" , DdpWrappedModel ]","title":"ddp_accelerator"},{"location":"api/nn/parallel/ddp_accelerator/#statedicttype","text":"StateDictType = Union [ Dict [ str , torch . Tensor ], OrderedDict [ str , torch . Tensor ]]","title":"StateDictType"},{"location":"api/nn/parallel/ddp_accelerator/#loadstatedictreturntype","text":"class LoadStateDictReturnType ( NamedTuple )","title":"LoadStateDictReturnType"},{"location":"api/nn/parallel/ddp_accelerator/#missing_keys","text":"class LoadStateDictReturnType ( NamedTuple ): | ... | missing_keys : List [ str ] = None","title":"missing_keys"},{"location":"api/nn/parallel/ddp_accelerator/#unexpected_keys","text":"class LoadStateDictReturnType ( NamedTuple ): | ... | unexpected_keys : List [ str ] = None","title":"unexpected_keys"},{"location":"api/nn/parallel/ddp_accelerator/#ddpwrappedmodel","text":"class DdpWrappedModel : | def __init__ ( | self , | model : torch . nn . Module , | local_rank : Optional [ int ] = None , | world_size : Optional [ int ] = None | ) -> None The type of the wrapped model returned from DdpAccelerator.wrap_model .","title":"DdpWrappedModel"},{"location":"api/nn/parallel/ddp_accelerator/#is_sharded","text":"class DdpWrappedModel : | ... | @property | def is_sharded ( self ) -> bool","title":"is_sharded"},{"location":"api/nn/parallel/ddp_accelerator/#consolidate_sharded_state","text":"class DdpWrappedModel : | ... | @staticmethod | def consolidate_sharded_state ( | sharded_state_files : Sequence [ Union [ str , os . PathLike ]] | ) -> StateDictType","title":"consolidate_sharded_state"},{"location":"api/nn/parallel/ddp_accelerator/#load_state_dict","text":"class DdpWrappedModel : | ... | def load_state_dict ( | self , | state_dict : StateDictType , | strict : bool = True | ) -> LoadStateDictReturnType","title":"load_state_dict"},{"location":"api/nn/parallel/ddp_accelerator/#state_dict","text":"class DdpWrappedModel : | ... | def state_dict ( self , * args , ** kwargs ) -> StateDictType","title":"state_dict"},{"location":"api/nn/parallel/ddp_accelerator/#clip_grad_norm_","text":"class DdpWrappedModel : | ... | def clip_grad_norm_ ( self , max_norm : Union [ float , int ]) -> torch . Tensor","title":"clip_grad_norm_"},{"location":"api/nn/parallel/ddp_accelerator/#init_grad_scaler","text":"class DdpWrappedModel : | ... | def init_grad_scaler ( self ) -> amp . GradScaler","title":"init_grad_scaler"},{"location":"api/nn/parallel/ddp_accelerator/#ddpaccelerator","text":"class DdpAccelerator ( Registrable ): | def __init__ ( | self , | local_rank : Optional [ int ] = None , | world_size : Optional [ int ] = None , | cuda_device : Union [ torch . device , int ] = - 1 | ) -> None A DdpAccelerator is a generalization of PyTorch's DistributedDataParallel class. This is primarly used within the GradientDescentTrainer to allow for different DDP implementations, such as FairScale's FullyShardedDataParallel . In a typical AllenNLP configuration file, local_rank , world_size , and cuda_device should not be specified. Warning This API is experimental and may change in the future.","title":"DdpAccelerator"},{"location":"api/nn/parallel/ddp_accelerator/#default_implementation","text":"class DdpAccelerator ( Registrable ): | ... | default_implementation = \"torch\"","title":"default_implementation"},{"location":"api/nn/parallel/ddp_accelerator/#wrap_model","text":"class DdpAccelerator ( Registrable ): | ... | def wrap_model ( | self , | model : \"Model\" | ) -> Tuple [ \"Model\" , DdpWrappedModel ] Wrap the AllenNLP Model , returning the original model (possibly on a different device) and the wrapper model .","title":"wrap_model"},{"location":"api/nn/parallel/ddp_accelerator/#wrap_module","text":"class DdpAccelerator ( Registrable ): | ... | def wrap_module ( self , module : torch . nn . Module ) -> torch . nn . Module Wrap an individual module. By default this just returns the module, but some subclass implementations such as FairScaleFsdpAccelerator do more.","title":"wrap_module"},{"location":"api/nn/parallel/ddp_accelerator/#torchddpaccelerator","text":"@DdpAccelerator . register ( \"torch\" ) class TorchDdpAccelerator ( DdpAccelerator ): | def __init__ ( | self , | * , find_unused_parameters : bool = False , | * , local_rank : Optional [ int ] = None , | * , world_size : Optional [ int ] = None , | * , cuda_device : Union [ torch . device , int ] = - 1 | ) -> None The default implementation of DdpAccelerator , which is just a thin wrapper around PyTorch's DistributedDataParallel .","title":"TorchDdpAccelerator"},{"location":"api/nn/parallel/ddp_accelerator/#wrap_model_1","text":"class TorchDdpAccelerator ( DdpAccelerator ): | ... | def wrap_model ( | self , | model : \"Model\" | ) -> Tuple [ \"Model\" , DdpWrappedModel ]","title":"wrap_model"},{"location":"api/nn/parallel/fairscale_fsdp_accelerator/","text":"allennlp .nn .parallel .fairscale_fsdp_accelerator [SOURCE] FairScaleFsdpWrappedModel \u00b6 class FairScaleFsdpWrappedModel ( DdpWrappedModel ) The wrapped model type returned from FairScaleFsdpWrappedModel.wrap_model . consolidate_sharded_state \u00b6 class FairScaleFsdpWrappedModel ( DdpWrappedModel ): | ... | @staticmethod | def consolidate_sharded_state ( | sharded_state_files : Sequence [ Union [ str , os . PathLike ]] | ) -> StateDictType load_state_dict \u00b6 class FairScaleFsdpWrappedModel ( DdpWrappedModel ): | ... | def load_state_dict ( | self , | state_dict : StateDictType , | strict : bool = True | ) -> LoadStateDictReturnType state_dict \u00b6 class FairScaleFsdpWrappedModel ( DdpWrappedModel ): | ... | def state_dict ( self , * args , ** kwargs ) -> StateDictType clip_grad_norm_ \u00b6 class FairScaleFsdpWrappedModel ( DdpWrappedModel ): | ... | def clip_grad_norm_ ( self , max_norm : Union [ float , int ]) -> torch . Tensor init_grad_scaler \u00b6 class FairScaleFsdpWrappedModel ( DdpWrappedModel ): | ... | def init_grad_scaler ( self ) -> amp . GradScaler FairScaleFsdpAccelerator \u00b6 @DdpAccelerator . register ( \"fairscale_fsdp\" ) class FairScaleFsdpAccelerator ( DdpAccelerator ): | def __init__ ( | self , | * , mixed_precision : bool = False , | * , reshard_after_forward : bool = True , | * , flatten_parameters : bool = True , | * , local_rank : Optional [ int ] = None , | * , world_size : Optional [ int ] = None , | * , cuda_device : Union [ torch . device , int ] = - 1 | ) -> None A DdpAccelerator for FairScale's FullyShardedDataParallel . To save memory while initializing a model, you should call .wrap_module() on submodules as they're created. See the T5 class for an example of how to use this. wrap_model \u00b6 class FairScaleFsdpAccelerator ( DdpAccelerator ): | ... | def wrap_model ( | self , | model : \"Model\" | ) -> Tuple [ \"Model\" , DdpWrappedModel ] wrap_module \u00b6 class FairScaleFsdpAccelerator ( DdpAccelerator ): | ... | def wrap_module ( self , module : torch . nn . Module ) -> torch . nn . Module","title":"fairscale_fsdp_accelerator"},{"location":"api/nn/parallel/fairscale_fsdp_accelerator/#fairscalefsdpwrappedmodel","text":"class FairScaleFsdpWrappedModel ( DdpWrappedModel ) The wrapped model type returned from FairScaleFsdpWrappedModel.wrap_model .","title":"FairScaleFsdpWrappedModel"},{"location":"api/nn/parallel/fairscale_fsdp_accelerator/#consolidate_sharded_state","text":"class FairScaleFsdpWrappedModel ( DdpWrappedModel ): | ... | @staticmethod | def consolidate_sharded_state ( | sharded_state_files : Sequence [ Union [ str , os . PathLike ]] | ) -> StateDictType","title":"consolidate_sharded_state"},{"location":"api/nn/parallel/fairscale_fsdp_accelerator/#load_state_dict","text":"class FairScaleFsdpWrappedModel ( DdpWrappedModel ): | ... | def load_state_dict ( | self , | state_dict : StateDictType , | strict : bool = True | ) -> LoadStateDictReturnType","title":"load_state_dict"},{"location":"api/nn/parallel/fairscale_fsdp_accelerator/#state_dict","text":"class FairScaleFsdpWrappedModel ( DdpWrappedModel ): | ... | def state_dict ( self , * args , ** kwargs ) -> StateDictType","title":"state_dict"},{"location":"api/nn/parallel/fairscale_fsdp_accelerator/#clip_grad_norm_","text":"class FairScaleFsdpWrappedModel ( DdpWrappedModel ): | ... | def clip_grad_norm_ ( self , max_norm : Union [ float , int ]) -> torch . Tensor","title":"clip_grad_norm_"},{"location":"api/nn/parallel/fairscale_fsdp_accelerator/#init_grad_scaler","text":"class FairScaleFsdpWrappedModel ( DdpWrappedModel ): | ... | def init_grad_scaler ( self ) -> amp . GradScaler","title":"init_grad_scaler"},{"location":"api/nn/parallel/fairscale_fsdp_accelerator/#fairscalefsdpaccelerator","text":"@DdpAccelerator . register ( \"fairscale_fsdp\" ) class FairScaleFsdpAccelerator ( DdpAccelerator ): | def __init__ ( | self , | * , mixed_precision : bool = False , | * , reshard_after_forward : bool = True , | * , flatten_parameters : bool = True , | * , local_rank : Optional [ int ] = None , | * , world_size : Optional [ int ] = None , | * , cuda_device : Union [ torch . device , int ] = - 1 | ) -> None A DdpAccelerator for FairScale's FullyShardedDataParallel . To save memory while initializing a model, you should call .wrap_module() on submodules as they're created. See the T5 class for an example of how to use this.","title":"FairScaleFsdpAccelerator"},{"location":"api/nn/parallel/fairscale_fsdp_accelerator/#wrap_model","text":"class FairScaleFsdpAccelerator ( DdpAccelerator ): | ... | def wrap_model ( | self , | model : \"Model\" | ) -> Tuple [ \"Model\" , DdpWrappedModel ]","title":"wrap_model"},{"location":"api/nn/parallel/fairscale_fsdp_accelerator/#wrap_module","text":"class FairScaleFsdpAccelerator ( DdpAccelerator ): | ... | def wrap_module ( self , module : torch . nn . Module ) -> torch . nn . Module","title":"wrap_module"},{"location":"api/nn/parallel/sharded_module_mixin/","text":"allennlp .nn .parallel .sharded_module_mixin [SOURCE] ShardedModuleMixin \u00b6 class ShardedModuleMixin Mixin class for sharded data parallel wrappers. Subclasses should implement get_original_module() which returns a reference the original inner wrapped module. get_original_module \u00b6 class ShardedModuleMixin : | ... | def get_original_module ( self ) -> torch . nn . Module Get the original","title":"sharded_module_mixin"},{"location":"api/nn/parallel/sharded_module_mixin/#shardedmodulemixin","text":"class ShardedModuleMixin Mixin class for sharded data parallel wrappers. Subclasses should implement get_original_module() which returns a reference the original inner wrapped module.","title":"ShardedModuleMixin"},{"location":"api/nn/parallel/sharded_module_mixin/#get_original_module","text":"class ShardedModuleMixin : | ... | def get_original_module ( self ) -> torch . nn . Module Get the original","title":"get_original_module"},{"location":"api/nn/regularizers/regularizer/","text":"allennlp .nn .regularizers .regularizer [SOURCE] Regularizer \u00b6 class Regularizer ( Registrable ) An abstract class representing a regularizer. It must implement call, returning a scalar tensor. default_implementation \u00b6 class Regularizer ( Registrable ): | ... | default_implementation = \"l2\" __call__ \u00b6 class Regularizer ( Registrable ): | ... | def __call__ ( self , parameter : torch . Tensor ) -> torch . Tensor","title":"regularizer"},{"location":"api/nn/regularizers/regularizer/#regularizer","text":"class Regularizer ( Registrable ) An abstract class representing a regularizer. It must implement call, returning a scalar tensor.","title":"Regularizer"},{"location":"api/nn/regularizers/regularizer/#default_implementation","text":"class Regularizer ( Registrable ): | ... | default_implementation = \"l2\"","title":"default_implementation"},{"location":"api/nn/regularizers/regularizer/#__call__","text":"class Regularizer ( Registrable ): | ... | def __call__ ( self , parameter : torch . Tensor ) -> torch . Tensor","title":"__call__"},{"location":"api/nn/regularizers/regularizer_applicator/","text":"allennlp .nn .regularizers .regularizer_applicator [SOURCE] RegularizerApplicator \u00b6 class RegularizerApplicator ( FromParams ): | def __init__ ( | self , | regexes : List [ Tuple [ str , Regularizer ]] = None | ) -> None Applies regularizers to the parameters of a Module based on regex matches. __call__ \u00b6 class RegularizerApplicator ( FromParams ): | ... | def __call__ ( self , module : torch . nn . Module ) -> torch . Tensor Parameters \u00b6 module : torch.nn.Module The module to regularize.","title":"regularizer_applicator"},{"location":"api/nn/regularizers/regularizer_applicator/#regularizerapplicator","text":"class RegularizerApplicator ( FromParams ): | def __init__ ( | self , | regexes : List [ Tuple [ str , Regularizer ]] = None | ) -> None Applies regularizers to the parameters of a Module based on regex matches.","title":"RegularizerApplicator"},{"location":"api/nn/regularizers/regularizer_applicator/#__call__","text":"class RegularizerApplicator ( FromParams ): | ... | def __call__ ( self , module : torch . nn . Module ) -> torch . Tensor","title":"__call__"},{"location":"api/nn/regularizers/regularizers/","text":"allennlp .nn .regularizers .regularizers [SOURCE] L1Regularizer \u00b6 @Regularizer . register ( \"l1\" ) class L1Regularizer ( Regularizer ): | def __init__ ( self , alpha : float = 0.01 ) -> None Represents a penalty proportional to the sum of the absolute values of the parameters Registered as a Regularizer with name \"l1\". __call__ \u00b6 class L1Regularizer ( Regularizer ): | ... | def __call__ ( self , parameter : torch . Tensor ) -> torch . Tensor L2Regularizer \u00b6 @Regularizer . register ( \"l2\" ) class L2Regularizer ( Regularizer ): | def __init__ ( self , alpha : float = 0.01 ) -> None Represents a penalty proportional to the sum of squared values of the parameters Registered as a Regularizer with name \"l2\". __call__ \u00b6 class L2Regularizer ( Regularizer ): | ... | def __call__ ( self , parameter : torch . Tensor ) -> torch . Tensor","title":"regularizers"},{"location":"api/nn/regularizers/regularizers/#l1regularizer","text":"@Regularizer . register ( \"l1\" ) class L1Regularizer ( Regularizer ): | def __init__ ( self , alpha : float = 0.01 ) -> None Represents a penalty proportional to the sum of the absolute values of the parameters Registered as a Regularizer with name \"l1\".","title":"L1Regularizer"},{"location":"api/nn/regularizers/regularizers/#__call__","text":"class L1Regularizer ( Regularizer ): | ... | def __call__ ( self , parameter : torch . Tensor ) -> torch . Tensor","title":"__call__"},{"location":"api/nn/regularizers/regularizers/#l2regularizer","text":"@Regularizer . register ( \"l2\" ) class L2Regularizer ( Regularizer ): | def __init__ ( self , alpha : float = 0.01 ) -> None Represents a penalty proportional to the sum of squared values of the parameters Registered as a Regularizer with name \"l2\".","title":"L2Regularizer"},{"location":"api/nn/regularizers/regularizers/#__call___1","text":"class L2Regularizer ( Regularizer ): | ... | def __call__ ( self , parameter : torch . Tensor ) -> torch . Tensor","title":"__call__"},{"location":"api/predictors/multitask/","text":"allennlp .predictors .multitask [SOURCE] MultiTaskPredictor \u00b6 @Predictor . register ( \"multitask\" ) class MultiTaskPredictor ( Predictor ): | def __init__ ( | self , | model : MultiTaskModel , | dataset_reader : MultiTaskDatasetReader | ) -> None Predictor for multitask models. Registered as a Predictor with name \"multitask\". This predictor is tightly coupled to MultiTaskDatasetReader and MultiTaskModel , and will not work if used with other readers or models. predict_instance \u00b6 class MultiTaskPredictor ( Predictor ): | ... | def predict_instance ( self , instance : Instance ) -> JsonDict predict_batch_instance \u00b6 class MultiTaskPredictor ( Predictor ): | ... | def predict_batch_instance ( | self , | instances : List [ Instance ] | ) -> List [ JsonDict ]","title":"multitask"},{"location":"api/predictors/multitask/#multitaskpredictor","text":"@Predictor . register ( \"multitask\" ) class MultiTaskPredictor ( Predictor ): | def __init__ ( | self , | model : MultiTaskModel , | dataset_reader : MultiTaskDatasetReader | ) -> None Predictor for multitask models. Registered as a Predictor with name \"multitask\". This predictor is tightly coupled to MultiTaskDatasetReader and MultiTaskModel , and will not work if used with other readers or models.","title":"MultiTaskPredictor"},{"location":"api/predictors/multitask/#predict_instance","text":"class MultiTaskPredictor ( Predictor ): | ... | def predict_instance ( self , instance : Instance ) -> JsonDict","title":"predict_instance"},{"location":"api/predictors/multitask/#predict_batch_instance","text":"class MultiTaskPredictor ( Predictor ): | ... | def predict_batch_instance ( | self , | instances : List [ Instance ] | ) -> List [ JsonDict ]","title":"predict_batch_instance"},{"location":"api/predictors/predictor/","text":"allennlp .predictors .predictor [SOURCE] Predictor \u00b6 class Predictor ( Registrable ): | def __init__ ( | self , | model : Model , | dataset_reader : DatasetReader , | frozen : bool = True | ) -> None a Predictor is a thin wrapper around an AllenNLP model that handles JSON -> JSON predictions that can be used for serving models through the web API or making predictions in bulk. load_line \u00b6 class Predictor ( Registrable ): | ... | def load_line ( self , line : str ) -> JsonDict If your inputs are not in JSON-lines format (e.g. you have a CSV) you can override this function to parse them correctly. dump_line \u00b6 class Predictor ( Registrable ): | ... | def dump_line ( self , outputs : JsonDict ) -> str If you don't want your outputs in JSON-lines format you can override this function to output them differently. predict_json \u00b6 class Predictor ( Registrable ): | ... | def predict_json ( self , inputs : JsonDict ) -> JsonDict json_to_labeled_instances \u00b6 class Predictor ( Registrable ): | ... | def json_to_labeled_instances ( | self , | inputs : JsonDict | ) -> List [ Instance ] Converts incoming json to a Instance , runs the model on the newly created instance, and adds labels to the Instance s given by the model's output. Returns \u00b6 List[instance] A list of Instance 's. get_gradients \u00b6 class Predictor ( Registrable ): | ... | def get_gradients ( | self , | instances : List [ Instance ] | ) -> Tuple [ Dict [ str , Any ], Dict [ str , Any ]] Gets the gradients of the loss with respect to the model inputs. Parameters \u00b6 instances : List[Instance] Returns \u00b6 Tuple[Dict[str, Any], Dict[str, Any]] The first item is a Dict of gradient entries for each input. The keys have the form {grad_input_1: ..., grad_input_2: ... } up to the number of inputs given. The second item is the model's output. Notes \u00b6 Takes a JsonDict representing the inputs of the model and converts them to Instances ), sends these through the model forward function after registering hooks on the embedding layer of the model. Calls backward on the loss and then removes the hooks. get_interpretable_layer \u00b6 class Predictor ( Registrable ): | ... | def get_interpretable_layer ( self ) -> torch . nn . Module Returns the input/embedding layer of the model. If the predictor wraps around a non-AllenNLP model, this function should be overridden to specify the correct input/embedding layer. For the cases where the input layer is an embedding layer, this should be the layer 0 of the embedder. get_interpretable_text_field_embedder \u00b6 class Predictor ( Registrable ): | ... | def get_interpretable_text_field_embedder ( self ) -> torch . nn . Module Returns the first TextFieldEmbedder of the model. If the predictor wraps around a non-AllenNLP model, this function should be overridden to specify the correct embedder. capture_model_internals \u00b6 class Predictor ( Registrable ): | ... | @contextmanager | def capture_model_internals ( | self , | module_regex : str = \".*\" | ) -> Iterator [ dict ] Context manager that captures the internal-module outputs of this predictor's model. The idea is that you could use it as follows: with predictor.capture_model_internals() as internals: outputs = predictor.predict_json(inputs) return {**outputs, \"model_internals\": internals} predict_instance \u00b6 class Predictor ( Registrable ): | ... | def predict_instance ( self , instance : Instance ) -> JsonDict predictions_to_labeled_instances \u00b6 class Predictor ( Registrable ): | ... | def predictions_to_labeled_instances ( | self , | instance : Instance , | outputs : Dict [ str , numpy . ndarray ] | ) -> List [ Instance ] This function takes a model's outputs for an Instance, and it labels that instance according to the outputs . This function is used to (1) compute gradients of what the model predicted; (2) label the instance for the attack. For example, (a) for the untargeted attack for classification this function labels the instance according to the class with the highest probability; (b) for targeted attack, it directly constructs fields from the given target. The return type is a list because in some tasks there are multiple predictions in the output (e.g., in NER a model predicts multiple spans). In this case, each instance in the returned list of Instances contains an individual entity prediction as the label. predict_batch_json \u00b6 class Predictor ( Registrable ): | ... | def predict_batch_json ( self , inputs : List [ JsonDict ]) -> List [ JsonDict ] predict_batch_instance \u00b6 class Predictor ( Registrable ): | ... | def predict_batch_instance ( | self , | instances : List [ Instance ] | ) -> List [ JsonDict ] from_path \u00b6 class Predictor ( Registrable ): | ... | @classmethod | def from_path ( | cls , | archive_path : Union [ str , PathLike ], | predictor_name : str = None , | cuda_device : int = - 1 , | dataset_reader_to_load : str = \"validation\" , | frozen : bool = True , | import_plugins : bool = True , | overrides : Union [ str , Dict [ str , Any ]] = \"\" , | ** kwargs | ) -> \"Predictor\" Instantiate a Predictor from an archive path. If you need more detailed configuration options, such as overrides, please use from_archive . Parameters \u00b6 archive_path : Union[str, PathLike] The path to the archive. predictor_name : str , optional (default = None ) Name that the predictor is registered as, or None to use the predictor associated with the model. cuda_device : int , optional (default = -1 ) If cuda_device is >= 0, the model will be loaded onto the corresponding GPU. Otherwise it will be loaded onto the CPU. dataset_reader_to_load : str , optional (default = \"validation\" ) Which dataset reader to load from the archive, either \"train\" or \"validation\". frozen : bool , optional (default = True ) If we should call model.eval() when building the predictor. import_plugins : bool , optional (default = True ) If True , we attempt to import plugins before loading the predictor. This comes with additional overhead, but means you don't need to explicitly import the modules that your predictor depends on as long as those modules can be found by allennlp.common.plugins.import_plugins() . overrides : Union[str, Dict[str, Any]] , optional (default = \"\" ) JSON overrides to apply to the unarchived Params object. **kwargs : Any Additional key-word arguments that will be passed to the Predictor 's __init__() method. Returns \u00b6 Predictor A Predictor instance. from_archive \u00b6 class Predictor ( Registrable ): | ... | @classmethod | def from_archive ( | cls , | archive : Archive , | predictor_name : str = None , | dataset_reader_to_load : str = \"validation\" , | frozen : bool = True , | extra_args : Optional [ Dict [ str , Any ]] = None | ) -> \"Predictor\" Instantiate a Predictor from an Archive ; that is, from the result of training a model. Optionally specify which Predictor subclass; otherwise, we try to find a corresponding predictor in DEFAULT_PREDICTORS , or if one is not found, the base class (i.e. Predictor ) will be used. Optionally specify which DatasetReader should be loaded; otherwise, the validation one will be used if it exists followed by the training dataset reader. Optionally specify if the loaded model should be frozen, meaning model.eval() will be called.","title":"predictor"},{"location":"api/predictors/predictor/#predictor","text":"class Predictor ( Registrable ): | def __init__ ( | self , | model : Model , | dataset_reader : DatasetReader , | frozen : bool = True | ) -> None a Predictor is a thin wrapper around an AllenNLP model that handles JSON -> JSON predictions that can be used for serving models through the web API or making predictions in bulk.","title":"Predictor"},{"location":"api/predictors/predictor/#load_line","text":"class Predictor ( Registrable ): | ... | def load_line ( self , line : str ) -> JsonDict If your inputs are not in JSON-lines format (e.g. you have a CSV) you can override this function to parse them correctly.","title":"load_line"},{"location":"api/predictors/predictor/#dump_line","text":"class Predictor ( Registrable ): | ... | def dump_line ( self , outputs : JsonDict ) -> str If you don't want your outputs in JSON-lines format you can override this function to output them differently.","title":"dump_line"},{"location":"api/predictors/predictor/#predict_json","text":"class Predictor ( Registrable ): | ... | def predict_json ( self , inputs : JsonDict ) -> JsonDict","title":"predict_json"},{"location":"api/predictors/predictor/#json_to_labeled_instances","text":"class Predictor ( Registrable ): | ... | def json_to_labeled_instances ( | self , | inputs : JsonDict | ) -> List [ Instance ] Converts incoming json to a Instance , runs the model on the newly created instance, and adds labels to the Instance s given by the model's output.","title":"json_to_labeled_instances"},{"location":"api/predictors/predictor/#get_gradients","text":"class Predictor ( Registrable ): | ... | def get_gradients ( | self , | instances : List [ Instance ] | ) -> Tuple [ Dict [ str , Any ], Dict [ str , Any ]] Gets the gradients of the loss with respect to the model inputs.","title":"get_gradients"},{"location":"api/predictors/predictor/#get_interpretable_layer","text":"class Predictor ( Registrable ): | ... | def get_interpretable_layer ( self ) -> torch . nn . Module Returns the input/embedding layer of the model. If the predictor wraps around a non-AllenNLP model, this function should be overridden to specify the correct input/embedding layer. For the cases where the input layer is an embedding layer, this should be the layer 0 of the embedder.","title":"get_interpretable_layer"},{"location":"api/predictors/predictor/#get_interpretable_text_field_embedder","text":"class Predictor ( Registrable ): | ... | def get_interpretable_text_field_embedder ( self ) -> torch . nn . Module Returns the first TextFieldEmbedder of the model. If the predictor wraps around a non-AllenNLP model, this function should be overridden to specify the correct embedder.","title":"get_interpretable_text_field_embedder"},{"location":"api/predictors/predictor/#capture_model_internals","text":"class Predictor ( Registrable ): | ... | @contextmanager | def capture_model_internals ( | self , | module_regex : str = \".*\" | ) -> Iterator [ dict ] Context manager that captures the internal-module outputs of this predictor's model. The idea is that you could use it as follows: with predictor.capture_model_internals() as internals: outputs = predictor.predict_json(inputs) return {**outputs, \"model_internals\": internals}","title":"capture_model_internals"},{"location":"api/predictors/predictor/#predict_instance","text":"class Predictor ( Registrable ): | ... | def predict_instance ( self , instance : Instance ) -> JsonDict","title":"predict_instance"},{"location":"api/predictors/predictor/#predictions_to_labeled_instances","text":"class Predictor ( Registrable ): | ... | def predictions_to_labeled_instances ( | self , | instance : Instance , | outputs : Dict [ str , numpy . ndarray ] | ) -> List [ Instance ] This function takes a model's outputs for an Instance, and it labels that instance according to the outputs . This function is used to (1) compute gradients of what the model predicted; (2) label the instance for the attack. For example, (a) for the untargeted attack for classification this function labels the instance according to the class with the highest probability; (b) for targeted attack, it directly constructs fields from the given target. The return type is a list because in some tasks there are multiple predictions in the output (e.g., in NER a model predicts multiple spans). In this case, each instance in the returned list of Instances contains an individual entity prediction as the label.","title":"predictions_to_labeled_instances"},{"location":"api/predictors/predictor/#predict_batch_json","text":"class Predictor ( Registrable ): | ... | def predict_batch_json ( self , inputs : List [ JsonDict ]) -> List [ JsonDict ]","title":"predict_batch_json"},{"location":"api/predictors/predictor/#predict_batch_instance","text":"class Predictor ( Registrable ): | ... | def predict_batch_instance ( | self , | instances : List [ Instance ] | ) -> List [ JsonDict ]","title":"predict_batch_instance"},{"location":"api/predictors/predictor/#from_path","text":"class Predictor ( Registrable ): | ... | @classmethod | def from_path ( | cls , | archive_path : Union [ str , PathLike ], | predictor_name : str = None , | cuda_device : int = - 1 , | dataset_reader_to_load : str = \"validation\" , | frozen : bool = True , | import_plugins : bool = True , | overrides : Union [ str , Dict [ str , Any ]] = \"\" , | ** kwargs | ) -> \"Predictor\" Instantiate a Predictor from an archive path. If you need more detailed configuration options, such as overrides, please use from_archive .","title":"from_path"},{"location":"api/predictors/predictor/#from_archive","text":"class Predictor ( Registrable ): | ... | @classmethod | def from_archive ( | cls , | archive : Archive , | predictor_name : str = None , | dataset_reader_to_load : str = \"validation\" , | frozen : bool = True , | extra_args : Optional [ Dict [ str , Any ]] = None | ) -> \"Predictor\" Instantiate a Predictor from an Archive ; that is, from the result of training a model. Optionally specify which Predictor subclass; otherwise, we try to find a corresponding predictor in DEFAULT_PREDICTORS , or if one is not found, the base class (i.e. Predictor ) will be used. Optionally specify which DatasetReader should be loaded; otherwise, the validation one will be used if it exists followed by the training dataset reader. Optionally specify if the loaded model should be frozen, meaning model.eval() will be called.","title":"from_archive"},{"location":"api/predictors/sentence_tagger/","text":"allennlp .predictors .sentence_tagger [SOURCE] SentenceTaggerPredictor \u00b6 @Predictor . register ( \"sentence_tagger\" ) class SentenceTaggerPredictor ( Predictor ): | def __init__ ( | self , | model : Model , | dataset_reader : DatasetReader , | language : str = \"en_core_web_sm\" | ) -> None Predictor for any model that takes in a sentence and returns a single set of tags for it. In particular, it can be used with the CrfTagger model and also the SimpleTagger model. Registered as a Predictor with name \"sentence_tagger\". predict \u00b6 class SentenceTaggerPredictor ( Predictor ): | ... | def predict ( self , sentence : str ) -> JsonDict predictions_to_labeled_instances \u00b6 class SentenceTaggerPredictor ( Predictor ): | ... | def predictions_to_labeled_instances ( | self , | instance : Instance , | outputs : Dict [ str , numpy . ndarray ] | ) -> List [ Instance ] This function currently only handles BIOUL tags. Imagine an NER model predicts three named entities (each one with potentially multiple tokens). For each individual entity, we create a new Instance that has the label set to only that entity and the rest of the tokens are labeled as outside. We then return a list of those Instances. For example: Mary went to Seattle to visit Microsoft Research U-Per O O U-Loc O O B-Org L-Org We create three instances. Mary went to Seattle to visit Microsoft Research U-Per O O O O O O O Mary went to Seattle to visit Microsoft Research O O O U-LOC O O O O Mary went to Seattle to visit Microsoft Research O O O O O O B-Org L-Org We additionally add a flag to these instances to tell the model to only compute loss on non-O tags, so that we get gradients that are specific to the particular span prediction that each instance represents.","title":"sentence_tagger"},{"location":"api/predictors/sentence_tagger/#sentencetaggerpredictor","text":"@Predictor . register ( \"sentence_tagger\" ) class SentenceTaggerPredictor ( Predictor ): | def __init__ ( | self , | model : Model , | dataset_reader : DatasetReader , | language : str = \"en_core_web_sm\" | ) -> None Predictor for any model that takes in a sentence and returns a single set of tags for it. In particular, it can be used with the CrfTagger model and also the SimpleTagger model. Registered as a Predictor with name \"sentence_tagger\".","title":"SentenceTaggerPredictor"},{"location":"api/predictors/sentence_tagger/#predict","text":"class SentenceTaggerPredictor ( Predictor ): | ... | def predict ( self , sentence : str ) -> JsonDict","title":"predict"},{"location":"api/predictors/sentence_tagger/#predictions_to_labeled_instances","text":"class SentenceTaggerPredictor ( Predictor ): | ... | def predictions_to_labeled_instances ( | self , | instance : Instance , | outputs : Dict [ str , numpy . ndarray ] | ) -> List [ Instance ] This function currently only handles BIOUL tags. Imagine an NER model predicts three named entities (each one with potentially multiple tokens). For each individual entity, we create a new Instance that has the label set to only that entity and the rest of the tokens are labeled as outside. We then return a list of those Instances. For example: Mary went to Seattle to visit Microsoft Research U-Per O O U-Loc O O B-Org L-Org We create three instances. Mary went to Seattle to visit Microsoft Research U-Per O O O O O O O Mary went to Seattle to visit Microsoft Research O O O U-LOC O O O O Mary went to Seattle to visit Microsoft Research O O O O O O B-Org L-Org We additionally add a flag to these instances to tell the model to only compute loss on non-O tags, so that we get gradients that are specific to the particular span prediction that each instance represents.","title":"predictions_to_labeled_instances"},{"location":"api/predictors/text_classifier/","text":"allennlp .predictors .text_classifier [SOURCE] TextClassifierPredictor \u00b6 @Predictor . register ( \"text_classifier\" ) class TextClassifierPredictor ( Predictor ) Predictor for any model that takes in a sentence and returns a single class for it. In particular, it can be used with the BasicClassifier model. Registered as a Predictor with name \"text_classifier\". predict \u00b6 class TextClassifierPredictor ( Predictor ): | ... | def predict ( self , sentence : str ) -> JsonDict predictions_to_labeled_instances \u00b6 class TextClassifierPredictor ( Predictor ): | ... | def predictions_to_labeled_instances ( | self , | instance : Instance , | outputs : Dict [ str , numpy . ndarray ] | ) -> List [ Instance ]","title":"text_classifier"},{"location":"api/predictors/text_classifier/#textclassifierpredictor","text":"@Predictor . register ( \"text_classifier\" ) class TextClassifierPredictor ( Predictor ) Predictor for any model that takes in a sentence and returns a single class for it. In particular, it can be used with the BasicClassifier model. Registered as a Predictor with name \"text_classifier\".","title":"TextClassifierPredictor"},{"location":"api/predictors/text_classifier/#predict","text":"class TextClassifierPredictor ( Predictor ): | ... | def predict ( self , sentence : str ) -> JsonDict","title":"predict"},{"location":"api/predictors/text_classifier/#predictions_to_labeled_instances","text":"class TextClassifierPredictor ( Predictor ): | ... | def predictions_to_labeled_instances ( | self , | instance : Instance , | outputs : Dict [ str , numpy . ndarray ] | ) -> List [ Instance ]","title":"predictions_to_labeled_instances"},{"location":"api/tools/archive_surgery/","text":"allennlp .tools .archive_surgery [SOURCE] Helper script for modifying config.json files that are locked inside model.tar.gz archives. This is useful if you need to rename things or add or remove values, usually because of changes to the library. This script will untar the archive to a temp directory, launch an editor to modify the config.json, and then re-tar everything to a new archive. If your $EDITOR environment variable is not set, you'll have to explicitly specify which editor to use. main \u00b6 def main ()","title":"archive_surgery"},{"location":"api/tools/archive_surgery/#main","text":"def main ()","title":"main"},{"location":"api/tools/create_elmo_embeddings_from_vocab/","text":"allennlp .tools .create_elmo_embeddings_from_vocab [SOURCE] main \u00b6 def main ( vocab_path : str , elmo_config_path : str , elmo_weights_path : str , output_dir : str , batch_size : int , device : int , use_custom_oov_token : bool = False ) Creates ELMo word representations from a vocabulary file. These word representations are independent - they are the result of running the CNN and Highway layers of the ELMo model, but not the Bidirectional LSTM. ELMo requires 2 additional tokens: and . The first token in this file is assumed to be an unknown token. This script produces two artifacts: A new vocabulary file with the and tokens inserted and a glove formatted embedding file containing word : vector pairs, one per line, with all values separated by a space.","title":"create_elmo_embeddings_from_vocab"},{"location":"api/tools/create_elmo_embeddings_from_vocab/#main","text":"def main ( vocab_path : str , elmo_config_path : str , elmo_weights_path : str , output_dir : str , batch_size : int , device : int , use_custom_oov_token : bool = False ) Creates ELMo word representations from a vocabulary file. These word representations are independent - they are the result of running the CNN and Highway layers of the ELMo model, but not the Bidirectional LSTM. ELMo requires 2 additional tokens: and . The first token in this file is assumed to be an unknown token. This script produces two artifacts: A new vocabulary file with the and tokens inserted and a glove formatted embedding file containing word : vector pairs, one per line, with all values separated by a space.","title":"main"},{"location":"api/tools/inspect_cache/","text":"allennlp .tools .inspect_cache [SOURCE] main \u00b6 def main ()","title":"inspect_cache"},{"location":"api/tools/inspect_cache/#main","text":"def main ()","title":"main"},{"location":"api/training/checkpointer/","text":"allennlp .training .checkpointer [SOURCE] Checkpointer \u00b6 class Checkpointer ( Registrable ): | def __init__ ( | self , | serialization_dir : Union [ str , os . PathLike ], | save_completed_epochs : bool = True , | save_every_num_seconds : Optional [ float ] = None , | save_every_num_batches : Optional [ int ] = None , | keep_most_recent_by_count : Optional [ int ] = 2 , | keep_most_recent_by_age : Optional [ int ] = None | ) -> None This class implements the functionality for checkpointing your model and trainer state during training. It is agnostic as to what those states look like (they are typed as Dict[str, Any] ), but they will be fed to torch.save so they should be serializable in that sense. They will also be restored as Dict[str, Any] , which means the calling code is responsible for knowing what to do with them. Parameters \u00b6 save_completed_epochs : bool , optional (default = True ) Saves model and trainer state at the end of each completed epoch. save_every_num_seconds : int , optional (default = None ) If set, makes sure we never go longer than this number of seconds between saving a model. save_every_num_batches : int , optional (default = None ) If set, makes sure we never go longer than this number of batches between saving a model. keep_most_recent_by_count : int , optional (default = 2 ) Sets the number of model checkpoints to keep on disk. If both keep_most_recent_by_count and keep_most_recent_by_age are set, we'll keep checkpoints that satisfy either criterion. If both are None , we keep all checkpoints. keep_most_recent_by_age : int , optional (default = None ) Sets the number of seconds we'll keep a checkpoint before deleting it. If both keep_most_recent_by_count and keep_most_recent_by_age are set, we'll keep checkpoints that satisfy either criterion. If both are None , we keep all checkpoints. default_implementation \u00b6 class Checkpointer ( Registrable ): | ... | default_implementation = \"default\" maybe_save_checkpoint \u00b6 class Checkpointer ( Registrable ): | ... | def maybe_save_checkpoint ( | self , | trainer : Trainer , | num_epochs_completed : int , | num_batches_in_epoch_completed : int | ) -> bool Figures out whether we need to save a checkpoint, and does so if necessary. save_checkpoint \u00b6 class Checkpointer ( Registrable ): | ... | def save_checkpoint ( self , trainer : Trainer ) -> None find_latest_checkpoint \u00b6 class Checkpointer ( Registrable ): | ... | def find_latest_checkpoint ( self ) -> Optional [ Tuple [ str , str ]] Return the location of the latest model and training state files. If there isn't a valid checkpoint then return None. load_checkpoint \u00b6 class Checkpointer ( Registrable ): | ... | def load_checkpoint ( self ) -> Optional [ TrainerCheckpoint ] Loads model state from a serialization_dir corresponding to the last saved checkpoint. This includes a training state, which is serialized separately from model parameters. This function should only be used to continue training - if you wish to load a model for inference/load parts of a model into a new computation graph, you should use the native Pytorch functions: model.load_state_dict(torch.load(\"/path/to/model/weights.th\")) If self._serialization_dir does not exist or does not contain any checkpointed weights, this function will do nothing and return empty dicts. Returns \u00b6 states : Tuple[Dict[str, Any], Dict[str, Any]] The model state and the training state.","title":"checkpointer"},{"location":"api/training/checkpointer/#checkpointer","text":"class Checkpointer ( Registrable ): | def __init__ ( | self , | serialization_dir : Union [ str , os . PathLike ], | save_completed_epochs : bool = True , | save_every_num_seconds : Optional [ float ] = None , | save_every_num_batches : Optional [ int ] = None , | keep_most_recent_by_count : Optional [ int ] = 2 , | keep_most_recent_by_age : Optional [ int ] = None | ) -> None This class implements the functionality for checkpointing your model and trainer state during training. It is agnostic as to what those states look like (they are typed as Dict[str, Any] ), but they will be fed to torch.save so they should be serializable in that sense. They will also be restored as Dict[str, Any] , which means the calling code is responsible for knowing what to do with them.","title":"Checkpointer"},{"location":"api/training/checkpointer/#default_implementation","text":"class Checkpointer ( Registrable ): | ... | default_implementation = \"default\"","title":"default_implementation"},{"location":"api/training/checkpointer/#maybe_save_checkpoint","text":"class Checkpointer ( Registrable ): | ... | def maybe_save_checkpoint ( | self , | trainer : Trainer , | num_epochs_completed : int , | num_batches_in_epoch_completed : int | ) -> bool Figures out whether we need to save a checkpoint, and does so if necessary.","title":"maybe_save_checkpoint"},{"location":"api/training/checkpointer/#save_checkpoint","text":"class Checkpointer ( Registrable ): | ... | def save_checkpoint ( self , trainer : Trainer ) -> None","title":"save_checkpoint"},{"location":"api/training/checkpointer/#find_latest_checkpoint","text":"class Checkpointer ( Registrable ): | ... | def find_latest_checkpoint ( self ) -> Optional [ Tuple [ str , str ]] Return the location of the latest model and training state files. If there isn't a valid checkpoint then return None.","title":"find_latest_checkpoint"},{"location":"api/training/checkpointer/#load_checkpoint","text":"class Checkpointer ( Registrable ): | ... | def load_checkpoint ( self ) -> Optional [ TrainerCheckpoint ] Loads model state from a serialization_dir corresponding to the last saved checkpoint. This includes a training state, which is serialized separately from model parameters. This function should only be used to continue training - if you wish to load a model for inference/load parts of a model into a new computation graph, you should use the native Pytorch functions: model.load_state_dict(torch.load(\"/path/to/model/weights.th\")) If self._serialization_dir does not exist or does not contain any checkpointed weights, this function will do nothing and return empty dicts.","title":"load_checkpoint"},{"location":"api/training/gradient_descent_trainer/","text":"allennlp .training .gradient_descent_trainer [SOURCE] GradientDescentTrainer \u00b6 @Trainer . register ( \"gradient_descent\" , constructor = \"from_partial_objects\" ) class GradientDescentTrainer ( Trainer ): | def __init__ ( | self , | model : Model , | optimizer : torch . optim . Optimizer , | data_loader : DataLoader , | patience : Optional [ int ] = None , | validation_metric : Union [ str , List [ str ]] = \"-loss\" , | validation_data_loader : DataLoader = None , | num_epochs : int = 20 , | serialization_dir : Optional [ Union [ str , os . PathLike ]] = None , | checkpointer : Optional [ Checkpointer ] = None , | cuda_device : Optional [ Union [ int , torch . device ]] = None , | grad_norm : Union [ float , bool ] = False , | grad_clipping : Optional [ float ] = None , | learning_rate_scheduler : Optional [ LearningRateScheduler ] = None , | momentum_scheduler : Optional [ MomentumScheduler ] = None , | moving_average : Optional [ MovingAverage ] = None , | callbacks : List [ TrainerCallback ] = None , | distributed : bool = False , | local_rank : int = 0 , | world_size : int = 1 , | num_gradient_accumulation_steps : int = 1 , | use_amp : bool = False , | enable_default_callbacks : bool = True , | run_confidence_checks : bool = True , | grad_scaling : bool = True , | ddp_wrapped_model : Optional [ DdpWrappedModel ] = None , | ** kwargs | ) -> None A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset and a DataLoader , and uses the supplied Optimizer to learn the weights for your model over some fixed number of epochs. You can also pass in a validation data_loader and enable early stopping. There are many other bells and whistles as well. Registered as a Trainer with the name \"gradient_descent\" (and is also the default Trainer ). The constructor that is registered is from_partial_objects - see the arguments to that function for the exact keys that should be used, if you are using a configuration file. They largely match the arguments to __init__ , and we don't repeat their docstrings in from_partial_objects . Parameters \u00b6 model : Model An AllenNLP model to be optimized. Pytorch Modules can also be optimized if their forward method returns a dictionary with a \"loss\" key, containing a scalar tensor representing the loss function to be optimized. If you are training your model using GPUs, your model should already be on the correct device. (If you are using our train command this will be handled for you.) In a typical AllenNLP configuration file, this parameter does not get an entry under the \"trainer\", it gets constructed separately. optimizer : torch.nn.Optimizer An instance of a Pytorch Optimizer, instantiated with the parameters of the model to be optimized. data_loader : DataLoader A DataLoader containing your Dataset , yielding padded indexed batches. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"trainer\", it gets constructed separately. patience : Optional[int] > 0 , optional (default = None ) Number of epochs to be patient before early stopping: the training is stopped after patience epochs with no improvement. If given, it must be > 0 . If None, early stopping is disabled. validation_metric : Union[str, List[str]] , optional (default = \"-loss\" ) Validation metric to measure for whether to stop training using patience and whether to serialize an is_best model each epoch. The metric name must be prepended with either \"+\" or \"-\", which specifies whether the metric is an increasing or decreasing function. If you specify more than one metric, the metrics will be summed to make the is_best decision. validation_data_loader : DataLoader , optional (default = None ) A DataLoader to use for the validation set. If None , then use the training DataLoader with the validation data. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"trainer\", it gets constructed separately. num_epochs : int , optional (default = 20 ) Number of training epochs. serialization_dir : str , optional (default = None ) Path to directory for saving and loading model files. Models will not be saved if this parameter is not passed. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"trainer\", it gets constructed separately. checkpointer : Checkpointer , optional (default = None ) A Checkpointer is responsible for periodically saving model weights. If none is given here, we will construct one with default parameters. cuda_device : Optional[Union[int, torch.device]] , optional (default = None ) An integer or torch.device specifying the CUDA device to use for this process. If -1, the CPU is used. If None and you have a GPU available, that GPU will be used. Note If you don't intend to use a GPU, but you have one available, you'll need to explicitly set cuda_device=-1 . Note If you intend to use a GPU, your model already needs to be on the correct device, which you can do with model = model.cuda() . Note Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU. grad_norm : Union[float, bool] , optional (default = False ) If a float, gradient norms will be rescaled to have a maximum of this value. If True , the gradient norms will be calculated and passed through to any TrainerCallbacks , but won't be rescaled. If False , gradient norms will not be calculated or rescaled. grad_clipping : float , optional (default = None ) If provided, gradients will be clipped during the backward pass to have an (absolute) maximum of this value. If you are getting NaNs in your gradients during training that are not solved by using grad_norm , you may need this. learning_rate_scheduler : LearningRateScheduler , optional (default = None ) If specified, the learning rate will be decayed with respect to this schedule at the end of each epoch (or batch, if the scheduler implements the step_batch method). If you use torch.optim.lr_scheduler.ReduceLROnPlateau , this will use the validation_metric provided to determine if learning has plateaued. To support updating the learning rate on every batch, this can optionally implement step_batch(batch_num_total) which updates the learning rate given the batch number. momentum_scheduler : MomentumScheduler , optional (default = None ) If specified, the momentum will be updated at the end of each batch or epoch according to the schedule. moving_average : MovingAverage , optional (default = None ) If provided, we will maintain moving averages for all parameters. During training, we employ a shadow variable for each parameter, which maintains the moving average. During evaluation, we backup the original parameters and assign the moving averages to corresponding parameters. Be careful that when saving the checkpoint, we will save the moving averages of parameters. This is necessary because we want the saved model to perform as well as the validated model if we load it later. But this may cause problems if you restart the training from checkpoint. callbacks : List[TrainerCallback] , optional (default = None ) A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start and end of training, etc. distributed : bool , optional (default = False ) If set, PyTorch's DistributedDataParallel is used to train the model in multiple GPUs. This also requires world_size to be greater than 1. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to the \"trainer\" entry, that specifies a list of \"cuda_devices\"). local_rank : int , optional (default = 0 ) This is the unique identifier of the Trainer in a distributed process group. The GPU device id is used as the rank. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"trainer\", it gets constructed separately. world_size : int , optional (default = 1 ) The number of Trainer workers participating in the distributed training. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"trainer\", it gets constructed separately. num_gradient_accumulation_steps : int , optional (default = 1 ) Gradients are accumulated for the given number of steps before doing an optimizer step. This can be useful to accommodate batches that are larger than the RAM size. Refer Thomas Wolf's post for details on Gradient Accumulation. use_amp : bool , optional (default = False ) If True , we'll train using Automatic Mixed Precision . enable_default_callbacks : bool , optional (default = True ) When True , the DEFAULT_CALLBACKS will be used in addition to any other callbacks listed in the callbacks parameter. When set to False , DEFAULT_CALLBACKS are not used. run_confidence_checks : bool , optional (default = True ) Determines whether model confidence checks, such as NormalizationBiasVerification , are run. run_sanity_checks : bool , optional (default = True ) This parameter is deprecated. Please use run_confidence_checks instead. grad_scaling : bool , optional (default = True ) When use_amp is True , this determines whether or not to use a GradScaler . Note This parameter is ignored when use_amp is False . ddp_wrapped_model : Optional[DdpWrappedModel] , optional (default = None ) The model wrapped with a DdpAccelerator for distributed training. Note This is required for distributed training. clip_gradient \u00b6 class GradientDescentTrainer ( Trainer ): | ... | def clip_gradient ( self ) Performs gradient clipping. If the model is in mixed precision training, we would first unscale the gradient. rescale_gradients \u00b6 class GradientDescentTrainer ( Trainer ): | ... | def rescale_gradients ( self ) -> Optional [ float ] Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled. Returns the norm of the gradients if grad_norm is True or a float , otherwise returns None . batch_outputs \u00b6 class GradientDescentTrainer ( Trainer ): | ... | def batch_outputs ( | self , | batch : TensorDict , | for_training : bool | ) -> Dict [ str , torch . Tensor ] Does a forward pass on the given batch and returns the output dictionary that the model returns, after adding any specified regularization penalty to the loss (if training). train \u00b6 class GradientDescentTrainer ( Trainer ): | ... | def train ( self ) -> Dict [ str , Any ] Trains the supplied model with the supplied parameters. get_checkpoint_state \u00b6 class GradientDescentTrainer ( Trainer ): | ... | def get_checkpoint_state ( self ) -> Optional [ TrainerCheckpoint ] from_partial_objects \u00b6 class GradientDescentTrainer ( Trainer ): | ... | @classmethod | def from_partial_objects ( | cls , | model : Model , | serialization_dir : str , | data_loader : DataLoader , | validation_data_loader : DataLoader = None , | local_rank : int = 0 , | patience : int = None , | validation_metric : Union [ str , List [ str ]] = \"-loss\" , | num_epochs : int = 20 , | cuda_device : Optional [ Union [ int , torch . device ]] = None , | grad_norm : Union [ float , bool ] = False , | grad_clipping : float = None , | distributed : bool = False , | world_size : int = 1 , | num_gradient_accumulation_steps : int = 1 , | use_amp : bool = False , | no_grad : List [ str ] = None , | optimizer : Lazy [ Optimizer ] = Lazy ( Optimizer . default ), | learning_rate_scheduler : Lazy [ LearningRateScheduler ] = None , | momentum_scheduler : Lazy [ MomentumScheduler ] = None , | moving_average : Lazy [ MovingAverage ] = None , | checkpointer : Optional [ Lazy [ Checkpointer ]] = Lazy ( Checkpointer ), | callbacks : List [ Lazy [ TrainerCallback ]] = None , | enable_default_callbacks : bool = True , | run_confidence_checks : bool = True , | grad_scaling : bool = True , | ddp_accelerator : Optional [ DdpAccelerator ] = None , | ** kwargs | ) -> Trainer This method exists so that we can have a documented method to construct this class using FromParams . If you are not using FromParams or config files, you can safely ignore this method. The reason we can't just use __init__ with FromParams here is because there are sequential dependencies to this class's arguments. Anything that has a Lazy[] type annotation needs something from one of the non- Lazy arguments. The Optimizer needs to have the parameters from the Model before it's constructed, and the Schedulers need to have the Optimizer . Because of this, the typical way we construct things FromParams doesn't work, so we use Lazy to allow for constructing the objects sequentially. If you're not using FromParams , you can just construct these arguments in the right order yourself in your code and call the constructor directly. get_best_weights_path \u00b6 class GradientDescentTrainer ( Trainer ): | ... | def get_best_weights_path ( self ) -> Optional [ str ] DEFAULT_CALLBACKS \u00b6 DEFAULT_CALLBACKS : Tuple [ Type [ TrainerCallback ]] = ( ConsoleLoggerCallback ,) The default callbacks used by GradientDescentTrainer .","title":"gradient_descent_trainer"},{"location":"api/training/gradient_descent_trainer/#gradientdescenttrainer","text":"@Trainer . register ( \"gradient_descent\" , constructor = \"from_partial_objects\" ) class GradientDescentTrainer ( Trainer ): | def __init__ ( | self , | model : Model , | optimizer : torch . optim . Optimizer , | data_loader : DataLoader , | patience : Optional [ int ] = None , | validation_metric : Union [ str , List [ str ]] = \"-loss\" , | validation_data_loader : DataLoader = None , | num_epochs : int = 20 , | serialization_dir : Optional [ Union [ str , os . PathLike ]] = None , | checkpointer : Optional [ Checkpointer ] = None , | cuda_device : Optional [ Union [ int , torch . device ]] = None , | grad_norm : Union [ float , bool ] = False , | grad_clipping : Optional [ float ] = None , | learning_rate_scheduler : Optional [ LearningRateScheduler ] = None , | momentum_scheduler : Optional [ MomentumScheduler ] = None , | moving_average : Optional [ MovingAverage ] = None , | callbacks : List [ TrainerCallback ] = None , | distributed : bool = False , | local_rank : int = 0 , | world_size : int = 1 , | num_gradient_accumulation_steps : int = 1 , | use_amp : bool = False , | enable_default_callbacks : bool = True , | run_confidence_checks : bool = True , | grad_scaling : bool = True , | ddp_wrapped_model : Optional [ DdpWrappedModel ] = None , | ** kwargs | ) -> None A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset and a DataLoader , and uses the supplied Optimizer to learn the weights for your model over some fixed number of epochs. You can also pass in a validation data_loader and enable early stopping. There are many other bells and whistles as well. Registered as a Trainer with the name \"gradient_descent\" (and is also the default Trainer ). The constructor that is registered is from_partial_objects - see the arguments to that function for the exact keys that should be used, if you are using a configuration file. They largely match the arguments to __init__ , and we don't repeat their docstrings in from_partial_objects .","title":"GradientDescentTrainer"},{"location":"api/training/gradient_descent_trainer/#clip_gradient","text":"class GradientDescentTrainer ( Trainer ): | ... | def clip_gradient ( self ) Performs gradient clipping. If the model is in mixed precision training, we would first unscale the gradient.","title":"clip_gradient"},{"location":"api/training/gradient_descent_trainer/#rescale_gradients","text":"class GradientDescentTrainer ( Trainer ): | ... | def rescale_gradients ( self ) -> Optional [ float ] Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled. Returns the norm of the gradients if grad_norm is True or a float , otherwise returns None .","title":"rescale_gradients"},{"location":"api/training/gradient_descent_trainer/#batch_outputs","text":"class GradientDescentTrainer ( Trainer ): | ... | def batch_outputs ( | self , | batch : TensorDict , | for_training : bool | ) -> Dict [ str , torch . Tensor ] Does a forward pass on the given batch and returns the output dictionary that the model returns, after adding any specified regularization penalty to the loss (if training).","title":"batch_outputs"},{"location":"api/training/gradient_descent_trainer/#train","text":"class GradientDescentTrainer ( Trainer ): | ... | def train ( self ) -> Dict [ str , Any ] Trains the supplied model with the supplied parameters.","title":"train"},{"location":"api/training/gradient_descent_trainer/#get_checkpoint_state","text":"class GradientDescentTrainer ( Trainer ): | ... | def get_checkpoint_state ( self ) -> Optional [ TrainerCheckpoint ]","title":"get_checkpoint_state"},{"location":"api/training/gradient_descent_trainer/#from_partial_objects","text":"class GradientDescentTrainer ( Trainer ): | ... | @classmethod | def from_partial_objects ( | cls , | model : Model , | serialization_dir : str , | data_loader : DataLoader , | validation_data_loader : DataLoader = None , | local_rank : int = 0 , | patience : int = None , | validation_metric : Union [ str , List [ str ]] = \"-loss\" , | num_epochs : int = 20 , | cuda_device : Optional [ Union [ int , torch . device ]] = None , | grad_norm : Union [ float , bool ] = False , | grad_clipping : float = None , | distributed : bool = False , | world_size : int = 1 , | num_gradient_accumulation_steps : int = 1 , | use_amp : bool = False , | no_grad : List [ str ] = None , | optimizer : Lazy [ Optimizer ] = Lazy ( Optimizer . default ), | learning_rate_scheduler : Lazy [ LearningRateScheduler ] = None , | momentum_scheduler : Lazy [ MomentumScheduler ] = None , | moving_average : Lazy [ MovingAverage ] = None , | checkpointer : Optional [ Lazy [ Checkpointer ]] = Lazy ( Checkpointer ), | callbacks : List [ Lazy [ TrainerCallback ]] = None , | enable_default_callbacks : bool = True , | run_confidence_checks : bool = True , | grad_scaling : bool = True , | ddp_accelerator : Optional [ DdpAccelerator ] = None , | ** kwargs | ) -> Trainer This method exists so that we can have a documented method to construct this class using FromParams . If you are not using FromParams or config files, you can safely ignore this method. The reason we can't just use __init__ with FromParams here is because there are sequential dependencies to this class's arguments. Anything that has a Lazy[] type annotation needs something from one of the non- Lazy arguments. The Optimizer needs to have the parameters from the Model before it's constructed, and the Schedulers need to have the Optimizer . Because of this, the typical way we construct things FromParams doesn't work, so we use Lazy to allow for constructing the objects sequentially. If you're not using FromParams , you can just construct these arguments in the right order yourself in your code and call the constructor directly.","title":"from_partial_objects"},{"location":"api/training/gradient_descent_trainer/#get_best_weights_path","text":"class GradientDescentTrainer ( Trainer ): | ... | def get_best_weights_path ( self ) -> Optional [ str ]","title":"get_best_weights_path"},{"location":"api/training/gradient_descent_trainer/#default_callbacks","text":"DEFAULT_CALLBACKS : Tuple [ Type [ TrainerCallback ]] = ( ConsoleLoggerCallback ,) The default callbacks used by GradientDescentTrainer .","title":"DEFAULT_CALLBACKS"},{"location":"api/training/metric_tracker/","text":"allennlp .training .metric_tracker [SOURCE] MetricTracker \u00b6 class MetricTracker : | def __init__ ( | self , | metric_name : Union [ str , List [ str ]], | patience : Optional [ int ] = None | ) -> None This class tracks a metric during training for the dual purposes of early stopping and for knowing whether the current value is the best so far. It mimics the PyTorch state_dict / load_state_dict interface, so that it can be checkpointed along with your model and optimizer. Some metrics improve by increasing; others by decreasing. You can provide a metric_name that starts with \"+\" to indicate an increasing metric, or \"-\" to indicate a decreasing metric. Parameters \u00b6 metric_name : Union[str, List[str]] Specifies the metric or metrics to track. Metric names have to start with \"+\" for increasing metrics or \"-\" for decreasing ones. If you specify more than one, it tracks the sum of the increasing metrics metrics minus the sum of the decreasing metrics. patience : int , optional (default = None ) If provided, then should_stop_early() returns True if we go this many epochs without seeing a new best value. clear \u00b6 class MetricTracker : | ... | def clear ( self ) -> None Clears out the tracked metrics, but keeps the patience state_dict \u00b6 class MetricTracker : | ... | def state_dict ( self ) -> Dict [ str , Any ] A Trainer can use this to serialize the state of the metric tracker. load_state_dict \u00b6 class MetricTracker : | ... | def load_state_dict ( self , state_dict : Dict [ str , Any ]) -> None A Trainer can use this to hydrate a metric tracker from a serialized state. add_metrics \u00b6 class MetricTracker : | ... | def add_metrics ( self , metrics : Dict [ str , float ]) -> None Record a new value of the metric and update the various things that depend on it. is_best_so_far \u00b6 class MetricTracker : | ... | def is_best_so_far ( self ) -> bool Returns true if the most recent value of the metric is the best so far. should_stop_early \u00b6 class MetricTracker : | ... | def should_stop_early ( self ) -> bool Returns true if improvement has stopped for long enough. combined_score \u00b6 class MetricTracker : | ... | def combined_score ( self , metrics : Dict [ str , float ]) -> float","title":"metric_tracker"},{"location":"api/training/metric_tracker/#metrictracker","text":"class MetricTracker : | def __init__ ( | self , | metric_name : Union [ str , List [ str ]], | patience : Optional [ int ] = None | ) -> None This class tracks a metric during training for the dual purposes of early stopping and for knowing whether the current value is the best so far. It mimics the PyTorch state_dict / load_state_dict interface, so that it can be checkpointed along with your model and optimizer. Some metrics improve by increasing; others by decreasing. You can provide a metric_name that starts with \"+\" to indicate an increasing metric, or \"-\" to indicate a decreasing metric.","title":"MetricTracker"},{"location":"api/training/metric_tracker/#clear","text":"class MetricTracker : | ... | def clear ( self ) -> None Clears out the tracked metrics, but keeps the patience","title":"clear"},{"location":"api/training/metric_tracker/#state_dict","text":"class MetricTracker : | ... | def state_dict ( self ) -> Dict [ str , Any ] A Trainer can use this to serialize the state of the metric tracker.","title":"state_dict"},{"location":"api/training/metric_tracker/#load_state_dict","text":"class MetricTracker : | ... | def load_state_dict ( self , state_dict : Dict [ str , Any ]) -> None A Trainer can use this to hydrate a metric tracker from a serialized state.","title":"load_state_dict"},{"location":"api/training/metric_tracker/#add_metrics","text":"class MetricTracker : | ... | def add_metrics ( self , metrics : Dict [ str , float ]) -> None Record a new value of the metric and update the various things that depend on it.","title":"add_metrics"},{"location":"api/training/metric_tracker/#is_best_so_far","text":"class MetricTracker : | ... | def is_best_so_far ( self ) -> bool Returns true if the most recent value of the metric is the best so far.","title":"is_best_so_far"},{"location":"api/training/metric_tracker/#should_stop_early","text":"class MetricTracker : | ... | def should_stop_early ( self ) -> bool Returns true if improvement has stopped for long enough.","title":"should_stop_early"},{"location":"api/training/metric_tracker/#combined_score","text":"class MetricTracker : | ... | def combined_score ( self , metrics : Dict [ str , float ]) -> float","title":"combined_score"},{"location":"api/training/moving_average/","text":"allennlp .training .moving_average [SOURCE] NamedParameter \u00b6 NamedParameter = Tuple [ str , torch . Tensor ] MovingAverage \u00b6 class MovingAverage ( Registrable ): | def __init__ ( self , parameters : Iterable [ NamedParameter ]) -> None Tracks a moving average of model parameters. default_implementation \u00b6 class MovingAverage ( Registrable ): | ... | default_implementation = \"exponential\" apply \u00b6 class MovingAverage ( Registrable ): | ... | def apply ( self , num_updates : Optional [ int ] = None ) Update the moving averages based on the latest values of the parameters. assign_average_value \u00b6 class MovingAverage ( Registrable ): | ... | def assign_average_value ( self ) -> None Replace all the parameter values with the averages. Save the current parameter values to restore later. restore \u00b6 class MovingAverage ( Registrable ): | ... | def restore ( self ) -> None Restore the backed-up (non-average) parameter values. state_dict \u00b6 class MovingAverage ( Registrable ): | ... | def state_dict ( self ) -> Dict [ str , Any ] load_state_dict \u00b6 class MovingAverage ( Registrable ): | ... | def load_state_dict ( self , state_dict : Dict [ str , Any ]) -> None ExponentialMovingAverage \u00b6 @MovingAverage . register ( \"exponential\" ) class ExponentialMovingAverage ( MovingAverage ): | def __init__ ( | self , | parameters : Iterable [ NamedParameter ], | decay : float = 0.9999 , | numerator : float = 1.0 , | denominator : float = 10.0 | ) -> None Create shadow variables and maintain exponential moving average for model parameters. Registered as a MovingAverage with name \"exponential\". Parameters \u00b6 parameters : Iterable[Tuple[str, Parameter]] The parameters whose averages we'll be tracking. In a typical AllenNLP configuration file, this argument does not get an entry under the \"moving_average\", it gets passed in separately. decay : float , optional (default = 0.9999 ) The decay rate that will be used if num_updates is not passed (and that will be used as an upper bound if num_updates is passed). numerator : float , optional (default = 1.0 ) The numerator used to compute the decay rate if num_updates is passed. denominator : float , optional (default = 10.0 ) The denominator used to compute the decay rate if num_updates is passed. apply \u00b6 class ExponentialMovingAverage ( MovingAverage ): | ... | def apply ( self , num_updates : Optional [ int ] = None ) -> None Apply exponential moving average to named_parameters if specified, or we will apply this to all the trainable parameters of the model. The optional num_updates parameter allows one to tweak the decay rate dynamically. If passed, the actual decay rate used is: `min(decay, (numerator + num_updates) / (denominator + num_updates))` (This logic is based on the Tensorflow exponential moving average https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage )","title":"moving_average"},{"location":"api/training/moving_average/#namedparameter","text":"NamedParameter = Tuple [ str , torch . Tensor ]","title":"NamedParameter"},{"location":"api/training/moving_average/#movingaverage","text":"class MovingAverage ( Registrable ): | def __init__ ( self , parameters : Iterable [ NamedParameter ]) -> None Tracks a moving average of model parameters.","title":"MovingAverage"},{"location":"api/training/moving_average/#default_implementation","text":"class MovingAverage ( Registrable ): | ... | default_implementation = \"exponential\"","title":"default_implementation"},{"location":"api/training/moving_average/#apply","text":"class MovingAverage ( Registrable ): | ... | def apply ( self , num_updates : Optional [ int ] = None ) Update the moving averages based on the latest values of the parameters.","title":"apply"},{"location":"api/training/moving_average/#assign_average_value","text":"class MovingAverage ( Registrable ): | ... | def assign_average_value ( self ) -> None Replace all the parameter values with the averages. Save the current parameter values to restore later.","title":"assign_average_value"},{"location":"api/training/moving_average/#restore","text":"class MovingAverage ( Registrable ): | ... | def restore ( self ) -> None Restore the backed-up (non-average) parameter values.","title":"restore"},{"location":"api/training/moving_average/#state_dict","text":"class MovingAverage ( Registrable ): | ... | def state_dict ( self ) -> Dict [ str , Any ]","title":"state_dict"},{"location":"api/training/moving_average/#load_state_dict","text":"class MovingAverage ( Registrable ): | ... | def load_state_dict ( self , state_dict : Dict [ str , Any ]) -> None","title":"load_state_dict"},{"location":"api/training/moving_average/#exponentialmovingaverage","text":"@MovingAverage . register ( \"exponential\" ) class ExponentialMovingAverage ( MovingAverage ): | def __init__ ( | self , | parameters : Iterable [ NamedParameter ], | decay : float = 0.9999 , | numerator : float = 1.0 , | denominator : float = 10.0 | ) -> None Create shadow variables and maintain exponential moving average for model parameters. Registered as a MovingAverage with name \"exponential\".","title":"ExponentialMovingAverage"},{"location":"api/training/moving_average/#apply_1","text":"class ExponentialMovingAverage ( MovingAverage ): | ... | def apply ( self , num_updates : Optional [ int ] = None ) -> None Apply exponential moving average to named_parameters if specified, or we will apply this to all the trainable parameters of the model. The optional num_updates parameter allows one to tweak the decay rate dynamically. If passed, the actual decay rate used is: `min(decay, (numerator + num_updates) / (denominator + num_updates))` (This logic is based on the Tensorflow exponential moving average https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage )","title":"apply"},{"location":"api/training/no_op_trainer/","text":"allennlp .training .no_op_trainer [SOURCE] NoOpTrainer \u00b6 @Trainer . register ( \"no_op\" ) class NoOpTrainer ( Trainer ): | def __init__ ( self , serialization_dir : str , model : Model ) -> None Registered as a Trainer with name \"no_op\". train \u00b6 class NoOpTrainer ( Trainer ): | ... | def train ( self ) -> Dict [ str , Any ] get_checkpoint_state \u00b6 class NoOpTrainer ( Trainer ): | ... | def get_checkpoint_state ( self ) -> TrainerCheckpoint get_best_weights_path \u00b6 class NoOpTrainer ( Trainer ): | ... | def get_best_weights_path ( self ) -> Optional [ str ]","title":"no_op_trainer"},{"location":"api/training/no_op_trainer/#nooptrainer","text":"@Trainer . register ( \"no_op\" ) class NoOpTrainer ( Trainer ): | def __init__ ( self , serialization_dir : str , model : Model ) -> None Registered as a Trainer with name \"no_op\".","title":"NoOpTrainer"},{"location":"api/training/no_op_trainer/#train","text":"class NoOpTrainer ( Trainer ): | ... | def train ( self ) -> Dict [ str , Any ]","title":"train"},{"location":"api/training/no_op_trainer/#get_checkpoint_state","text":"class NoOpTrainer ( Trainer ): | ... | def get_checkpoint_state ( self ) -> TrainerCheckpoint","title":"get_checkpoint_state"},{"location":"api/training/no_op_trainer/#get_best_weights_path","text":"class NoOpTrainer ( Trainer ): | ... | def get_best_weights_path ( self ) -> Optional [ str ]","title":"get_best_weights_path"},{"location":"api/training/optimizers/","text":"allennlp .training .optimizers [SOURCE] AllenNLP just uses PyTorch optimizers , with a thin wrapper to allow registering them and instantiating them from_params . The available optimizers are adadelta adagrad adam adamw huggingface_adamw huggingface_adafactor sparse_adam sgd rmsprop adamax averaged_sgd ParameterGroupsType \u00b6 ParameterGroupsType = List [ Tuple [ List [ str ], Dict [ str , Any ]]] make_parameter_groups \u00b6 def make_parameter_groups ( model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], groups : Optional [ ParameterGroupsType ] = None ) -> Union [ List [ Dict [ str , Any ]], List [ torch . nn . Parameter ]] Takes a list of model parameters with associated names (typically coming from something like model.named_parameters() ), along with a grouping (as specified below), and prepares them to be passed to the __init__ function of a torch.Optimizer . This means separating the parameters into groups with the given regexes, and prepping whatever keyword arguments are given for those regexes in groups . groups contains something like: [ ([\"regex1\", \"regex2\"], {\"lr\": 1e-3}), ([\"regex3\"], {\"lr\": 1e-4}) ] All of key-value pairs specified in each of these dictionaries will passed passed as-is to the optimizer, with the exception of a dictionaries that specify requires_grad to be False : [ ... ([\"regex\"], {\"requires_grad\": False}) ] When a parameter group has {\"requires_grad\": False} , the gradient on all matching parameters will be disabled and that group will be dropped so that it's not actually passed to the optimizer. Ultimately, the return value of this function is in the right format to be passed directly as the params argument to a pytorch Optimizer . If there are multiple groups specified, this is a list of dictionaries, where each dict contains a \"parameter group\" and groups specific options, e.g., {'params': [list of parameters], 'lr': 1e-3, ...}. Any config option not specified in the additional options (e.g. for the default group) is inherited from the top level arguments given in the constructor. See: https://pytorch.org/docs/0.3.0/optim.html?#per-parameter-options . See also our test_optimizer_parameter_groups test for an example of how this works in this code. The dictionary's return type is labeled as Any , because it can be a List[torch.nn.Parameter] (for the \"params\" key), or anything else (typically a float) for the other keys. Optimizer \u00b6 class Optimizer ( torch . optim . Optimizer , Registrable ) This class just allows us to implement Registrable for Pytorch Optimizers. We do something a little bit different with Optimizers , because they are implemented as classes in PyTorch, and we want to use those classes. To make things easy, we just inherit from those classes, using multiple inheritance to also inherit from Optimizer . The only reason we do this is to make type inference on parameters possible, so we can construct these objects using our configuration framework. If you are writing your own script, you can safely ignore these classes and just use the torch.optim classes directly. If you are implementing one of these classes, the model_parameters and parameter_groups arguments to __init__ are important, and should always be present. The trainer will pass the trainable parameters in the model to the optimizer using the name model_parameters , so if you use a different name, your code will crash. Nothing will technically crash if you use a name other than parameter_groups for your second argument, it will just be annoyingly inconsistent. Most subclasses of Optimizer take both a model_parameters and a parameter_groups constructor argument. The model_parameters argument does not get an entry in a typical AllenNLP configuration file, but the parameter_groups argument does (if you want a non-default value). See the documentation for the make_parameter_groups function for more information on how the parameter_groups argument should be specified. default_implementation \u00b6 class Optimizer ( torch . optim . Optimizer , Registrable ): | ... | default_implementation = \"adam\" default \u00b6 class Optimizer ( torch . optim . Optimizer , Registrable ): | ... | @staticmethod | def default ( model_parameters : List ) -> \"Optimizer\" MultiOptimizer \u00b6 @Optimizer . register ( \"multi\" ) class MultiOptimizer ( Optimizer ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | optimizers : Dict [ str , Lazy [ Optimizer ]], | parameter_groups : ParameterGroupsType | ) A MultiOptimizer creates a dictionary of Optimizer s keyed on some 'name'. Each Optimizer contains its own set of parameters which are obtained using regex matches for certain model parameters. This optimizer works by taking in a parameter optimizers which contains a list of Optimizers with their keyword arguments, and a parameter parameter_groups , which contains regexes and their corresponding optimizer and optional non-default optimizer options for this group. The regexes in the parameter groups are assigned to their optimizer based on the 'name' argument where the 'name' value should be the same for the optimizer and parameter group. You should specify a default optimizer with 'name': 'default' which will be used for all parameters which didn't obtain a regex match or when your parameter group doesn't contain a 'name' parameter. Parameters \u00b6 optimizers : List[Dict[str, Any]] A list of optimizers to use. Each entry in the list is a dictionary of keyword arguments. A 'name' keyword argument should be given which will serve as the key to match the optimizer with a specific parameter group. You should also supply an entry for the default parameter group, e.g. 'name': 'default'. parameter_groups : List[Tuple[List[str], Dict[str, Any]] , optional (default = None ) See the docstring of make_parameter_groups for what this parameter should look like. It should follow the same format as there, except an additional 'optimizer_name' argument should be provided to match this group to its own optimizer. Optimizer options can also be set for this group which will override the default options. step \u00b6 class MultiOptimizer ( Optimizer ): | ... | def step ( self ) Takes an optimization step for each optimizer. state_dict \u00b6 class MultiOptimizer ( Optimizer ): | ... | def state_dict ( self ) Creates an object optimizer_state_dict , which is a dictionary mapping an optimizer key to its state_dict . This dictionary is used as the value for 'optimizer' in the 'training_states' dictionary in the gradient_descent Trainer , e.g. \"optimizer\" : { \"optimizer1\": `optimizer1_state_dict`, \"optimizer2\": `optimizer2_state_dict` }. load_state_dict \u00b6 class MultiOptimizer ( Optimizer ): | ... | def load_state_dict ( self , training_state : Dict [ str , Any ]) Loads each optimizer's state_dict . zero_grad \u00b6 class MultiOptimizer ( Optimizer ): | ... | def zero_grad ( self , set_to_none : bool = False ) Sets parameter gradients to zero or None. AdamOptimizer \u00b6 @Optimizer . register ( \"adam\" ) class AdamOptimizer ( Optimizer , torch . optim . Adam ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 0.001 , | betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), | eps : float = 1e-08 , | weight_decay : float = 0.0 , | amsgrad : bool = False | ) Registered as an Optimizer with name \"adam\". SparseAdamOptimizer \u00b6 @Optimizer . register ( \"sparse_adam\" ) class SparseAdamOptimizer ( Optimizer , torch . optim . SparseAdam ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 0.001 , | betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), | eps : float = 1e-08 | ) Registered as an Optimizer with name \"sparse_adam\". AdamaxOptimizer \u00b6 @Optimizer . register ( \"adamax\" ) class AdamaxOptimizer ( Optimizer , torch . optim . Adamax ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 0.002 , | betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), | eps : float = 1e-08 , | weight_decay : float = 0.0 | ) Registered as an Optimizer with name \"adamax\". AdamWOptimizer \u00b6 @Optimizer . register ( \"adamw\" ) class AdamWOptimizer ( Optimizer , torch . optim . AdamW ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 0.001 , | betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), | eps : float = 1e-08 , | weight_decay : float = 0.01 , | amsgrad : bool = False | ) Registered as an Optimizer with name \"adamw\". HuggingfaceAdamWOptimizer \u00b6 @Optimizer . register ( \"huggingface_adamw\" ) class HuggingfaceAdamWOptimizer ( Optimizer , transformers . AdamW ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 1e-5 , | betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), | eps : float = 1e-08 , | weight_decay : float = 0.0 , | correct_bias : bool = True | ) Registered as an Optimizer with name \"huggingface_adamw\". HuggingfaceAdafactor \u00b6 @Optimizer . register ( \"huggingface_adafactor\" ) class HuggingfaceAdafactor ( Optimizer , transformers . Adafactor ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : Optional [ float ] = None , | eps : Tuple [ float , float ] = ( 1e-30 , 1e-3 ), | clip_threshold : float = 1.0 , | decay_rate : float = - 0.8 , | beta1 : Optional [ float ] = None , | weight_decay : float = 0.0 , | scale_parameter : bool = True , | relative_step : bool = True , | warmup_init : bool = False | ) Registered as an Optimizer with name \"huggingface_adafactor\". AdagradOptimizer \u00b6 @Optimizer . register ( \"adagrad\" ) class AdagradOptimizer ( Optimizer , torch . optim . Adagrad ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 0.01 , | lr_decay : float = 0.0 , | weight_decay : float = 0.0 , | initial_accumulator_value : float = 0.0 , | eps : float = 1e-10 | ) Registered as an Optimizer with name \"adagrad\". AdadeltaOptimizer \u00b6 @Optimizer . register ( \"adadelta\" ) class AdadeltaOptimizer ( Optimizer , torch . optim . Adadelta ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 1.0 , | rho : float = 0.9 , | eps : float = 1e-06 , | weight_decay : float = 0.0 | ) Registered as an Optimizer with name \"adadelta\". SgdOptimizer \u00b6 @Optimizer . register ( \"sgd\" ) class SgdOptimizer ( Optimizer , torch . optim . SGD ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | lr : float , | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | momentum : float = 0.0 , | dampening : float = 0 , | weight_decay : float = 0.0 , | nesterov : bool = False | ) Registered as an Optimizer with name \"sgd\". RmsPropOptimizer \u00b6 @Optimizer . register ( \"rmsprop\" ) class RmsPropOptimizer ( Optimizer , torch . optim . RMSprop ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 0.01 , | alpha : float = 0.99 , | eps : float = 1e-08 , | weight_decay : float = 0.0 , | momentum : float = 0.0 , | centered : bool = False | ) Registered as an Optimizer with name \"rmsprop\". AveragedSgdOptimizer \u00b6 @Optimizer . register ( \"averaged_sgd\" ) class AveragedSgdOptimizer ( Optimizer , torch . optim . ASGD ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 0.01 , | lambd : float = 0.0001 , | alpha : float = 0.75 , | t0 : float = 1000000.0 , | weight_decay : float = 0.0 | ) Registered as an Optimizer with name \"averaged_sgd\". DenseSparseAdam \u00b6 @Optimizer . register ( \"dense_sparse_adam\" ) class DenseSparseAdam ( Optimizer , torch . optim . Optimizer ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr = 1e-3 , | betas = ( 0.9 , 0.999 ), | eps = 1e-8 | ) NOTE: This class has been copied verbatim from the separate Dense and Sparse versions of Adam in Pytorch. Implements Adam algorithm with dense & sparse gradients. It has been proposed in Adam: A Method for Stochastic Optimization. Registered as an Optimizer with name \"dense_sparse_adam\". Parameters \u00b6 params : iterable iterable of parameters to optimize or dicts defining parameter groups lr : float , optional (default = 1e-3 ) The learning rate. betas : Tuple[float, float] , optional (default = (0.9, 0.999) ) coefficients used for computing running averages of gradient and its square. eps : float , optional (default = 1e-8 ) A term added to the denominator to improve numerical stability. step \u00b6 class DenseSparseAdam ( Optimizer , torch . optim . Optimizer ): | ... | def step ( self , closure = None ) Performs a single optimization step. Parameters \u00b6 closure : callable , optional A closure that reevaluates the model and returns the loss.","title":"optimizers"},{"location":"api/training/optimizers/#parametergroupstype","text":"ParameterGroupsType = List [ Tuple [ List [ str ], Dict [ str , Any ]]]","title":"ParameterGroupsType"},{"location":"api/training/optimizers/#make_parameter_groups","text":"def make_parameter_groups ( model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], groups : Optional [ ParameterGroupsType ] = None ) -> Union [ List [ Dict [ str , Any ]], List [ torch . nn . Parameter ]] Takes a list of model parameters with associated names (typically coming from something like model.named_parameters() ), along with a grouping (as specified below), and prepares them to be passed to the __init__ function of a torch.Optimizer . This means separating the parameters into groups with the given regexes, and prepping whatever keyword arguments are given for those regexes in groups . groups contains something like: [ ([\"regex1\", \"regex2\"], {\"lr\": 1e-3}), ([\"regex3\"], {\"lr\": 1e-4}) ] All of key-value pairs specified in each of these dictionaries will passed passed as-is to the optimizer, with the exception of a dictionaries that specify requires_grad to be False : [ ... ([\"regex\"], {\"requires_grad\": False}) ] When a parameter group has {\"requires_grad\": False} , the gradient on all matching parameters will be disabled and that group will be dropped so that it's not actually passed to the optimizer. Ultimately, the return value of this function is in the right format to be passed directly as the params argument to a pytorch Optimizer . If there are multiple groups specified, this is a list of dictionaries, where each dict contains a \"parameter group\" and groups specific options, e.g., {'params': [list of parameters], 'lr': 1e-3, ...}. Any config option not specified in the additional options (e.g. for the default group) is inherited from the top level arguments given in the constructor. See: https://pytorch.org/docs/0.3.0/optim.html?#per-parameter-options . See also our test_optimizer_parameter_groups test for an example of how this works in this code. The dictionary's return type is labeled as Any , because it can be a List[torch.nn.Parameter] (for the \"params\" key), or anything else (typically a float) for the other keys.","title":"make_parameter_groups"},{"location":"api/training/optimizers/#optimizer","text":"class Optimizer ( torch . optim . Optimizer , Registrable ) This class just allows us to implement Registrable for Pytorch Optimizers. We do something a little bit different with Optimizers , because they are implemented as classes in PyTorch, and we want to use those classes. To make things easy, we just inherit from those classes, using multiple inheritance to also inherit from Optimizer . The only reason we do this is to make type inference on parameters possible, so we can construct these objects using our configuration framework. If you are writing your own script, you can safely ignore these classes and just use the torch.optim classes directly. If you are implementing one of these classes, the model_parameters and parameter_groups arguments to __init__ are important, and should always be present. The trainer will pass the trainable parameters in the model to the optimizer using the name model_parameters , so if you use a different name, your code will crash. Nothing will technically crash if you use a name other than parameter_groups for your second argument, it will just be annoyingly inconsistent. Most subclasses of Optimizer take both a model_parameters and a parameter_groups constructor argument. The model_parameters argument does not get an entry in a typical AllenNLP configuration file, but the parameter_groups argument does (if you want a non-default value). See the documentation for the make_parameter_groups function for more information on how the parameter_groups argument should be specified.","title":"Optimizer"},{"location":"api/training/optimizers/#default_implementation","text":"class Optimizer ( torch . optim . Optimizer , Registrable ): | ... | default_implementation = \"adam\"","title":"default_implementation"},{"location":"api/training/optimizers/#default","text":"class Optimizer ( torch . optim . Optimizer , Registrable ): | ... | @staticmethod | def default ( model_parameters : List ) -> \"Optimizer\"","title":"default"},{"location":"api/training/optimizers/#multioptimizer","text":"@Optimizer . register ( \"multi\" ) class MultiOptimizer ( Optimizer ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | optimizers : Dict [ str , Lazy [ Optimizer ]], | parameter_groups : ParameterGroupsType | ) A MultiOptimizer creates a dictionary of Optimizer s keyed on some 'name'. Each Optimizer contains its own set of parameters which are obtained using regex matches for certain model parameters. This optimizer works by taking in a parameter optimizers which contains a list of Optimizers with their keyword arguments, and a parameter parameter_groups , which contains regexes and their corresponding optimizer and optional non-default optimizer options for this group. The regexes in the parameter groups are assigned to their optimizer based on the 'name' argument where the 'name' value should be the same for the optimizer and parameter group. You should specify a default optimizer with 'name': 'default' which will be used for all parameters which didn't obtain a regex match or when your parameter group doesn't contain a 'name' parameter.","title":"MultiOptimizer"},{"location":"api/training/optimizers/#step","text":"class MultiOptimizer ( Optimizer ): | ... | def step ( self ) Takes an optimization step for each optimizer.","title":"step"},{"location":"api/training/optimizers/#state_dict","text":"class MultiOptimizer ( Optimizer ): | ... | def state_dict ( self ) Creates an object optimizer_state_dict , which is a dictionary mapping an optimizer key to its state_dict . This dictionary is used as the value for 'optimizer' in the 'training_states' dictionary in the gradient_descent Trainer , e.g. \"optimizer\" : { \"optimizer1\": `optimizer1_state_dict`, \"optimizer2\": `optimizer2_state_dict` }.","title":"state_dict"},{"location":"api/training/optimizers/#load_state_dict","text":"class MultiOptimizer ( Optimizer ): | ... | def load_state_dict ( self , training_state : Dict [ str , Any ]) Loads each optimizer's state_dict .","title":"load_state_dict"},{"location":"api/training/optimizers/#zero_grad","text":"class MultiOptimizer ( Optimizer ): | ... | def zero_grad ( self , set_to_none : bool = False ) Sets parameter gradients to zero or None.","title":"zero_grad"},{"location":"api/training/optimizers/#adamoptimizer","text":"@Optimizer . register ( \"adam\" ) class AdamOptimizer ( Optimizer , torch . optim . Adam ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 0.001 , | betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), | eps : float = 1e-08 , | weight_decay : float = 0.0 , | amsgrad : bool = False | ) Registered as an Optimizer with name \"adam\".","title":"AdamOptimizer"},{"location":"api/training/optimizers/#sparseadamoptimizer","text":"@Optimizer . register ( \"sparse_adam\" ) class SparseAdamOptimizer ( Optimizer , torch . optim . SparseAdam ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 0.001 , | betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), | eps : float = 1e-08 | ) Registered as an Optimizer with name \"sparse_adam\".","title":"SparseAdamOptimizer"},{"location":"api/training/optimizers/#adamaxoptimizer","text":"@Optimizer . register ( \"adamax\" ) class AdamaxOptimizer ( Optimizer , torch . optim . Adamax ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 0.002 , | betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), | eps : float = 1e-08 , | weight_decay : float = 0.0 | ) Registered as an Optimizer with name \"adamax\".","title":"AdamaxOptimizer"},{"location":"api/training/optimizers/#adamwoptimizer","text":"@Optimizer . register ( \"adamw\" ) class AdamWOptimizer ( Optimizer , torch . optim . AdamW ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 0.001 , | betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), | eps : float = 1e-08 , | weight_decay : float = 0.01 , | amsgrad : bool = False | ) Registered as an Optimizer with name \"adamw\".","title":"AdamWOptimizer"},{"location":"api/training/optimizers/#huggingfaceadamwoptimizer","text":"@Optimizer . register ( \"huggingface_adamw\" ) class HuggingfaceAdamWOptimizer ( Optimizer , transformers . AdamW ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 1e-5 , | betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), | eps : float = 1e-08 , | weight_decay : float = 0.0 , | correct_bias : bool = True | ) Registered as an Optimizer with name \"huggingface_adamw\".","title":"HuggingfaceAdamWOptimizer"},{"location":"api/training/optimizers/#huggingfaceadafactor","text":"@Optimizer . register ( \"huggingface_adafactor\" ) class HuggingfaceAdafactor ( Optimizer , transformers . Adafactor ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : Optional [ float ] = None , | eps : Tuple [ float , float ] = ( 1e-30 , 1e-3 ), | clip_threshold : float = 1.0 , | decay_rate : float = - 0.8 , | beta1 : Optional [ float ] = None , | weight_decay : float = 0.0 , | scale_parameter : bool = True , | relative_step : bool = True , | warmup_init : bool = False | ) Registered as an Optimizer with name \"huggingface_adafactor\".","title":"HuggingfaceAdafactor"},{"location":"api/training/optimizers/#adagradoptimizer","text":"@Optimizer . register ( \"adagrad\" ) class AdagradOptimizer ( Optimizer , torch . optim . Adagrad ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 0.01 , | lr_decay : float = 0.0 , | weight_decay : float = 0.0 , | initial_accumulator_value : float = 0.0 , | eps : float = 1e-10 | ) Registered as an Optimizer with name \"adagrad\".","title":"AdagradOptimizer"},{"location":"api/training/optimizers/#adadeltaoptimizer","text":"@Optimizer . register ( \"adadelta\" ) class AdadeltaOptimizer ( Optimizer , torch . optim . Adadelta ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 1.0 , | rho : float = 0.9 , | eps : float = 1e-06 , | weight_decay : float = 0.0 | ) Registered as an Optimizer with name \"adadelta\".","title":"AdadeltaOptimizer"},{"location":"api/training/optimizers/#sgdoptimizer","text":"@Optimizer . register ( \"sgd\" ) class SgdOptimizer ( Optimizer , torch . optim . SGD ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | lr : float , | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | momentum : float = 0.0 , | dampening : float = 0 , | weight_decay : float = 0.0 , | nesterov : bool = False | ) Registered as an Optimizer with name \"sgd\".","title":"SgdOptimizer"},{"location":"api/training/optimizers/#rmspropoptimizer","text":"@Optimizer . register ( \"rmsprop\" ) class RmsPropOptimizer ( Optimizer , torch . optim . RMSprop ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 0.01 , | alpha : float = 0.99 , | eps : float = 1e-08 , | weight_decay : float = 0.0 , | momentum : float = 0.0 , | centered : bool = False | ) Registered as an Optimizer with name \"rmsprop\".","title":"RmsPropOptimizer"},{"location":"api/training/optimizers/#averagedsgdoptimizer","text":"@Optimizer . register ( \"averaged_sgd\" ) class AveragedSgdOptimizer ( Optimizer , torch . optim . ASGD ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 0.01 , | lambd : float = 0.0001 , | alpha : float = 0.75 , | t0 : float = 1000000.0 , | weight_decay : float = 0.0 | ) Registered as an Optimizer with name \"averaged_sgd\".","title":"AveragedSgdOptimizer"},{"location":"api/training/optimizers/#densesparseadam","text":"@Optimizer . register ( \"dense_sparse_adam\" ) class DenseSparseAdam ( Optimizer , torch . optim . Optimizer ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr = 1e-3 , | betas = ( 0.9 , 0.999 ), | eps = 1e-8 | ) NOTE: This class has been copied verbatim from the separate Dense and Sparse versions of Adam in Pytorch. Implements Adam algorithm with dense & sparse gradients. It has been proposed in Adam: A Method for Stochastic Optimization. Registered as an Optimizer with name \"dense_sparse_adam\".","title":"DenseSparseAdam"},{"location":"api/training/optimizers/#step_1","text":"class DenseSparseAdam ( Optimizer , torch . optim . Optimizer ): | ... | def step ( self , closure = None ) Performs a single optimization step.","title":"step"},{"location":"api/training/scheduler/","text":"allennlp .training .scheduler [SOURCE] Scheduler \u00b6 class Scheduler : | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | param_group_field : str , | last_epoch : int = - 1 | ) -> None A Scheduler is a generalization of PyTorch learning rate schedulers. A scheduler can be used to update any field in an optimizer's parameter groups, not just the learning rate. During training using the AllenNLP Trainer , this is the API and calling sequence for step and step_batch :: scheduler = ... # creates scheduler batch_num_total = 0 for epoch in range(num_epochs): for batch in batchs_in_epoch: # compute loss, update parameters with current learning rates # call step_batch AFTER updating parameters batch_num_total += 1 scheduler.step_batch(batch_num_total) # call step() at the END of each epoch scheduler.step(validation_metrics, epoch) state_dict \u00b6 class Scheduler : | ... | def state_dict ( self ) -> Dict [ str , Any ] Returns the state of the scheduler as a dict . load_state_dict \u00b6 class Scheduler : | ... | def load_state_dict ( self , state_dict : Dict [ str , Any ]) -> None Load the schedulers state. Parameters \u00b6 state_dict : Dict[str, Any] Scheduler state. Should be an object returned from a call to state_dict . get_values \u00b6 class Scheduler : | ... | def get_values ( self ) step \u00b6 class Scheduler : | ... | def step ( self , metric : float = None ) -> None step_batch \u00b6 class Scheduler : | ... | def step_batch ( self , batch_num_total : int = None ) -> None By default, a scheduler is assumed to only update every epoch, not every batch. So this does nothing unless it's overriden.","title":"scheduler"},{"location":"api/training/scheduler/#scheduler","text":"class Scheduler : | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | param_group_field : str , | last_epoch : int = - 1 | ) -> None A Scheduler is a generalization of PyTorch learning rate schedulers. A scheduler can be used to update any field in an optimizer's parameter groups, not just the learning rate. During training using the AllenNLP Trainer , this is the API and calling sequence for step and step_batch :: scheduler = ... # creates scheduler batch_num_total = 0 for epoch in range(num_epochs): for batch in batchs_in_epoch: # compute loss, update parameters with current learning rates # call step_batch AFTER updating parameters batch_num_total += 1 scheduler.step_batch(batch_num_total) # call step() at the END of each epoch scheduler.step(validation_metrics, epoch)","title":"Scheduler"},{"location":"api/training/scheduler/#state_dict","text":"class Scheduler : | ... | def state_dict ( self ) -> Dict [ str , Any ] Returns the state of the scheduler as a dict .","title":"state_dict"},{"location":"api/training/scheduler/#load_state_dict","text":"class Scheduler : | ... | def load_state_dict ( self , state_dict : Dict [ str , Any ]) -> None Load the schedulers state.","title":"load_state_dict"},{"location":"api/training/scheduler/#get_values","text":"class Scheduler : | ... | def get_values ( self )","title":"get_values"},{"location":"api/training/scheduler/#step","text":"class Scheduler : | ... | def step ( self , metric : float = None ) -> None","title":"step"},{"location":"api/training/scheduler/#step_batch","text":"class Scheduler : | ... | def step_batch ( self , batch_num_total : int = None ) -> None By default, a scheduler is assumed to only update every epoch, not every batch. So this does nothing unless it's overriden.","title":"step_batch"},{"location":"api/training/trainer/","text":"allennlp .training .trainer [SOURCE] TrainerCheckpoint \u00b6 class TrainerCheckpoint ( NamedTuple ) model_state \u00b6 class TrainerCheckpoint ( NamedTuple ): | ... | model_state : Dict [ str , Any ] = None trainer_state \u00b6 class TrainerCheckpoint ( NamedTuple ): | ... | trainer_state : Dict [ str , Any ] = None Trainer \u00b6 class Trainer ( Registrable ): | def __init__ ( | self , | serialization_dir : Union [ str , os . PathLike ] = None , | cuda_device : Optional [ Union [ int , torch . device ]] = None , | distributed : bool = False , | local_rank : int = 0 , | world_size : int = 1 | ) -> None The base class for an AllenNLP trainer. It can do pretty much anything you want. Your subclass should implement train and also probably from_params . default_implementation \u00b6 class Trainer ( Registrable ): | ... | default_implementation = \"gradient_descent\" train \u00b6 class Trainer ( Registrable ): | ... | def train ( self ) -> Dict [ str , Any ] Train a model and return the results. get_checkpoint_state \u00b6 class Trainer ( Registrable ): | ... | def get_checkpoint_state ( self ) -> Optional [ TrainerCheckpoint ] Returns a tuple of (model state, training state), where training state could have several internal components (e.g., for an, optimizer, learning rate scheduler, etc.). get_best_weights_path \u00b6 class Trainer ( Registrable ): | ... | def get_best_weights_path ( self ) -> Optional [ str ] Returns the path to file containing the current best weights.","title":"trainer"},{"location":"api/training/trainer/#trainercheckpoint","text":"class TrainerCheckpoint ( NamedTuple )","title":"TrainerCheckpoint"},{"location":"api/training/trainer/#model_state","text":"class TrainerCheckpoint ( NamedTuple ): | ... | model_state : Dict [ str , Any ] = None","title":"model_state"},{"location":"api/training/trainer/#trainer_state","text":"class TrainerCheckpoint ( NamedTuple ): | ... | trainer_state : Dict [ str , Any ] = None","title":"trainer_state"},{"location":"api/training/trainer/#trainer","text":"class Trainer ( Registrable ): | def __init__ ( | self , | serialization_dir : Union [ str , os . PathLike ] = None , | cuda_device : Optional [ Union [ int , torch . device ]] = None , | distributed : bool = False , | local_rank : int = 0 , | world_size : int = 1 | ) -> None The base class for an AllenNLP trainer. It can do pretty much anything you want. Your subclass should implement train and also probably from_params .","title":"Trainer"},{"location":"api/training/trainer/#default_implementation","text":"class Trainer ( Registrable ): | ... | default_implementation = \"gradient_descent\"","title":"default_implementation"},{"location":"api/training/trainer/#train","text":"class Trainer ( Registrable ): | ... | def train ( self ) -> Dict [ str , Any ] Train a model and return the results.","title":"train"},{"location":"api/training/trainer/#get_checkpoint_state","text":"class Trainer ( Registrable ): | ... | def get_checkpoint_state ( self ) -> Optional [ TrainerCheckpoint ] Returns a tuple of (model state, training state), where training state could have several internal components (e.g., for an, optimizer, learning rate scheduler, etc.).","title":"get_checkpoint_state"},{"location":"api/training/trainer/#get_best_weights_path","text":"class Trainer ( Registrable ): | ... | def get_best_weights_path ( self ) -> Optional [ str ] Returns the path to file containing the current best weights.","title":"get_best_weights_path"},{"location":"api/training/util/","text":"allennlp .training .util [SOURCE] Helper functions for Trainers HasBeenWarned \u00b6 class HasBeenWarned tqdm_ignores_underscores \u00b6 class HasBeenWarned : | ... | tqdm_ignores_underscores = False move_optimizer_to_cuda \u00b6 def move_optimizer_to_cuda ( optimizer ) Move the optimizer state to GPU, if necessary. After calling, any parameter specific state in the optimizer will be located on the same device as the parameter. get_batch_size \u00b6 def get_batch_size ( batch : Union [ Dict , torch . Tensor ]) -> int Returns the size of the batch dimension. Assumes a well-formed batch, returns 0 otherwise. time_to_str \u00b6 def time_to_str ( timestamp : int ) -> str Convert seconds past Epoch to human readable string. str_to_time \u00b6 def str_to_time ( time_str : str ) -> datetime . datetime Convert human readable string to datetime.datetime. data_loaders_from_params \u00b6 def data_loaders_from_params ( params : Params , train : bool = True , validation : bool = True , test : bool = True , serialization_dir : Optional [ Union [ str , PathLike ]] = None ) -> Dict [ str , DataLoader ] Instantiate data loaders specified by the config. create_serialization_dir \u00b6 def create_serialization_dir ( params : Params , serialization_dir : Union [ str , PathLike ], recover : bool , force : bool ) -> None This function creates the serialization directory if it doesn't exist. If it already exists and is non-empty, then it verifies that we're recovering from a training with an identical configuration. Parameters \u00b6 params : Params A parameter object specifying an AllenNLP Experiment. serialization_dir : str The directory in which to save results and logs. recover : bool If True , we will try to recover from an existing serialization directory, and crash if the directory doesn't exist, or doesn't match the configuration we're given. force : bool If True , we will overwrite the serialization directory if it already exists. enable_gradient_clipping \u00b6 def enable_gradient_clipping ( model : Model , grad_clipping : Optional [ float ] ) -> None rescale_gradients \u00b6 def rescale_gradients ( model : Model , grad_norm : Optional [ float ] = None ) -> Optional [ float ] Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled. get_metrics \u00b6 def get_metrics ( model : Model , total_loss : float , total_reg_loss : Optional [ float ], batch_loss : Optional [ float ], batch_reg_loss : Optional [ float ], num_batches : int , reset : bool = False ) -> Dict [ str , float ] Gets the metrics but sets \"loss\" to the total loss divided by the num_batches so that the \"loss\" metric is \"average loss per batch\". Returns the \"batch_loss\" separately. get_train_and_validation_metrics \u00b6 def get_train_and_validation_metrics ( metrics : Dict ) -> Tuple [ Dict [ str , Any ], Dict [ str , Any ]] Utility function to separate out train_metrics and val_metrics. evaluate \u00b6 def evaluate ( model : Model , data_loader : DataLoader , cuda_device : Union [ int , torch . device ] = - 1 , batch_weight_key : str = None , output_file : str = None , predictions_output_file : str = None ) -> Dict [ str , Any ] Parameters \u00b6 model : Model The model to evaluate data_loader : DataLoader The DataLoader that will iterate over the evaluation data (data loaders already contain their data). cuda_device : Union[int, torch.device] , optional (default = -1 ) The cuda device to use for this evaluation. The model is assumed to already be using this device; this parameter is only used for moving the input data to the correct device. batch_weight_key : str , optional (default = None ) If given, this is a key in the output dictionary for each batch that specifies how to weight the loss for that batch. If this is not given, we use a weight of 1 for every batch. metrics_output_file : str , optional (default = None ) Optional path to write the final metrics to. predictions_output_file : str , optional (default = None ) Optional path to write the predictions to. Returns \u00b6 Dict[str, Any] The final metrics. description_from_metrics \u00b6 def description_from_metrics ( metrics : Dict [ str , float ]) -> str make_vocab_from_params \u00b6 def make_vocab_from_params ( params : Params , serialization_dir : Union [ str , PathLike ], print_statistics : bool = False ) -> Vocabulary ngrams \u00b6 def ngrams ( tensor : torch . LongTensor , ngram_size : int , exclude_indices : Set [ int ] ) -> Dict [ Tuple [ int , ... ], int ] get_valid_tokens_mask \u00b6 def get_valid_tokens_mask ( tensor : torch . LongTensor , exclude_indices : Set [ int ] ) -> torch . ByteTensor","title":"util"},{"location":"api/training/util/#hasbeenwarned","text":"class HasBeenWarned","title":"HasBeenWarned"},{"location":"api/training/util/#tqdm_ignores_underscores","text":"class HasBeenWarned : | ... | tqdm_ignores_underscores = False","title":"tqdm_ignores_underscores"},{"location":"api/training/util/#move_optimizer_to_cuda","text":"def move_optimizer_to_cuda ( optimizer ) Move the optimizer state to GPU, if necessary. After calling, any parameter specific state in the optimizer will be located on the same device as the parameter.","title":"move_optimizer_to_cuda"},{"location":"api/training/util/#get_batch_size","text":"def get_batch_size ( batch : Union [ Dict , torch . Tensor ]) -> int Returns the size of the batch dimension. Assumes a well-formed batch, returns 0 otherwise.","title":"get_batch_size"},{"location":"api/training/util/#time_to_str","text":"def time_to_str ( timestamp : int ) -> str Convert seconds past Epoch to human readable string.","title":"time_to_str"},{"location":"api/training/util/#str_to_time","text":"def str_to_time ( time_str : str ) -> datetime . datetime Convert human readable string to datetime.datetime.","title":"str_to_time"},{"location":"api/training/util/#data_loaders_from_params","text":"def data_loaders_from_params ( params : Params , train : bool = True , validation : bool = True , test : bool = True , serialization_dir : Optional [ Union [ str , PathLike ]] = None ) -> Dict [ str , DataLoader ] Instantiate data loaders specified by the config.","title":"data_loaders_from_params"},{"location":"api/training/util/#create_serialization_dir","text":"def create_serialization_dir ( params : Params , serialization_dir : Union [ str , PathLike ], recover : bool , force : bool ) -> None This function creates the serialization directory if it doesn't exist. If it already exists and is non-empty, then it verifies that we're recovering from a training with an identical configuration.","title":"create_serialization_dir"},{"location":"api/training/util/#enable_gradient_clipping","text":"def enable_gradient_clipping ( model : Model , grad_clipping : Optional [ float ] ) -> None","title":"enable_gradient_clipping"},{"location":"api/training/util/#rescale_gradients","text":"def rescale_gradients ( model : Model , grad_norm : Optional [ float ] = None ) -> Optional [ float ] Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.","title":"rescale_gradients"},{"location":"api/training/util/#get_metrics","text":"def get_metrics ( model : Model , total_loss : float , total_reg_loss : Optional [ float ], batch_loss : Optional [ float ], batch_reg_loss : Optional [ float ], num_batches : int , reset : bool = False ) -> Dict [ str , float ] Gets the metrics but sets \"loss\" to the total loss divided by the num_batches so that the \"loss\" metric is \"average loss per batch\". Returns the \"batch_loss\" separately.","title":"get_metrics"},{"location":"api/training/util/#get_train_and_validation_metrics","text":"def get_train_and_validation_metrics ( metrics : Dict ) -> Tuple [ Dict [ str , Any ], Dict [ str , Any ]] Utility function to separate out train_metrics and val_metrics.","title":"get_train_and_validation_metrics"},{"location":"api/training/util/#evaluate","text":"def evaluate ( model : Model , data_loader : DataLoader , cuda_device : Union [ int , torch . device ] = - 1 , batch_weight_key : str = None , output_file : str = None , predictions_output_file : str = None ) -> Dict [ str , Any ]","title":"evaluate"},{"location":"api/training/util/#description_from_metrics","text":"def description_from_metrics ( metrics : Dict [ str , float ]) -> str","title":"description_from_metrics"},{"location":"api/training/util/#make_vocab_from_params","text":"def make_vocab_from_params ( params : Params , serialization_dir : Union [ str , PathLike ], print_statistics : bool = False ) -> Vocabulary","title":"make_vocab_from_params"},{"location":"api/training/util/#ngrams","text":"def ngrams ( tensor : torch . LongTensor , ngram_size : int , exclude_indices : Set [ int ] ) -> Dict [ Tuple [ int , ... ], int ]","title":"ngrams"},{"location":"api/training/util/#get_valid_tokens_mask","text":"def get_valid_tokens_mask ( tensor : torch . LongTensor , exclude_indices : Set [ int ] ) -> torch . ByteTensor","title":"get_valid_tokens_mask"},{"location":"api/training/callbacks/backward/","text":"allennlp .training .callbacks .backward [SOURCE] MixedPrecisionBackwardCallback \u00b6 @TrainerCallback . register ( \"mixed_precision_backward\" ) class MixedPrecisionBackwardCallback ( TrainerCallback ) Performs backpropagation for mixed precision training. on_backward \u00b6 class MixedPrecisionBackwardCallback ( TrainerCallback ): | ... | def on_backward ( | self , | trainer : \"GradientDescentTrainer\" , | batch_outputs : Dict [ str , torch . Tensor ], | backward_called : bool , | ** kwargs | ) -> bool OnBackwardException \u00b6 class OnBackwardException ( Exception ): | def __init__ ( self , message = \"\" ) -> None The exception type raised if an on_backward callback attempts to call backward when backward_called is True .","title":"backward"},{"location":"api/training/callbacks/backward/#mixedprecisionbackwardcallback","text":"@TrainerCallback . register ( \"mixed_precision_backward\" ) class MixedPrecisionBackwardCallback ( TrainerCallback ) Performs backpropagation for mixed precision training.","title":"MixedPrecisionBackwardCallback"},{"location":"api/training/callbacks/backward/#on_backward","text":"class MixedPrecisionBackwardCallback ( TrainerCallback ): | ... | def on_backward ( | self , | trainer : \"GradientDescentTrainer\" , | batch_outputs : Dict [ str , torch . Tensor ], | backward_called : bool , | ** kwargs | ) -> bool","title":"on_backward"},{"location":"api/training/callbacks/backward/#onbackwardexception","text":"class OnBackwardException ( Exception ): | def __init__ ( self , message = \"\" ) -> None The exception type raised if an on_backward callback attempts to call backward when backward_called is True .","title":"OnBackwardException"},{"location":"api/training/callbacks/callback/","text":"allennlp .training .callbacks .callback [SOURCE] TrainerCallback \u00b6 class TrainerCallback ( Registrable ): | def __init__ ( self , serialization_dir : str ) -> None A general callback object that handles multiple events. This class has on_backward , on_batch , on_epoch , and on_end methods, corresponding to each callback type. Each one receives the state of the wrapper object as self . This enables easier state sharing between related callbacks. Also, this callback type is instantiated with serialization_dir and on_start is called with the trainer instance as an argument. This might be handy in case of callback logging and saving its own files next to the config/checkpoints/logs/etc. on_start \u00b6 class TrainerCallback ( Registrable ): | ... | def on_start ( | self , | trainer : \"GradientDescentTrainer\" , | is_primary : bool = True , | ** kwargs | ) -> None This callback hook is called before the training is started. on_backward \u00b6 class TrainerCallback ( Registrable ): | ... | def on_backward ( | self , | trainer : \"GradientDescentTrainer\" , | batch_outputs : Dict [ str , torch . Tensor ], | backward_called : bool , | ** kwargs | ) -> bool This callback hook performs backpropagation and allows for gradient manipulation. backward_called indicates if loss.backward has been called prior to this callback. on_backward should return True if and only if loss.backward is called in its body. on_batch \u00b6 class TrainerCallback ( Registrable ): | ... | def on_batch ( | self , | trainer : \"GradientDescentTrainer\" , | batch_inputs : List [ TensorDict ], | batch_outputs : List [ Dict [ str , Any ]], | batch_metrics : Dict [ str , Any ], | epoch : int , | batch_number : int , | is_training : bool , | is_primary : bool = True , | batch_grad_norm : Optional [ float ] = None , | ** kwargs | ) -> None This callback hook is called after the end of each batch. on_epoch \u00b6 class TrainerCallback ( Registrable ): | ... | def on_epoch ( | self , | trainer : \"GradientDescentTrainer\" , | metrics : Dict [ str , Any ], | epoch : int , | is_primary : bool = True , | ** kwargs | ) -> None This callback hook is called after the end of each epoch. on_end \u00b6 class TrainerCallback ( Registrable ): | ... | def on_end ( | self , | trainer : \"GradientDescentTrainer\" , | metrics : Dict [ str , Any ] = None , | epoch : int = None , | is_primary : bool = True , | ** kwargs | ) -> None This callback hook is called after the final training epoch. state_dict \u00b6 class TrainerCallback ( Registrable ): | ... | def state_dict ( self ) -> Dict [ str , Any ] load_state_dict \u00b6 class TrainerCallback ( Registrable ): | ... | def load_state_dict ( self , state_dict : Dict [ str , Any ]) -> None","title":"callback"},{"location":"api/training/callbacks/callback/#trainercallback","text":"class TrainerCallback ( Registrable ): | def __init__ ( self , serialization_dir : str ) -> None A general callback object that handles multiple events. This class has on_backward , on_batch , on_epoch , and on_end methods, corresponding to each callback type. Each one receives the state of the wrapper object as self . This enables easier state sharing between related callbacks. Also, this callback type is instantiated with serialization_dir and on_start is called with the trainer instance as an argument. This might be handy in case of callback logging and saving its own files next to the config/checkpoints/logs/etc.","title":"TrainerCallback"},{"location":"api/training/callbacks/callback/#on_start","text":"class TrainerCallback ( Registrable ): | ... | def on_start ( | self , | trainer : \"GradientDescentTrainer\" , | is_primary : bool = True , | ** kwargs | ) -> None This callback hook is called before the training is started.","title":"on_start"},{"location":"api/training/callbacks/callback/#on_backward","text":"class TrainerCallback ( Registrable ): | ... | def on_backward ( | self , | trainer : \"GradientDescentTrainer\" , | batch_outputs : Dict [ str , torch . Tensor ], | backward_called : bool , | ** kwargs | ) -> bool This callback hook performs backpropagation and allows for gradient manipulation. backward_called indicates if loss.backward has been called prior to this callback. on_backward should return True if and only if loss.backward is called in its body.","title":"on_backward"},{"location":"api/training/callbacks/callback/#on_batch","text":"class TrainerCallback ( Registrable ): | ... | def on_batch ( | self , | trainer : \"GradientDescentTrainer\" , | batch_inputs : List [ TensorDict ], | batch_outputs : List [ Dict [ str , Any ]], | batch_metrics : Dict [ str , Any ], | epoch : int , | batch_number : int , | is_training : bool , | is_primary : bool = True , | batch_grad_norm : Optional [ float ] = None , | ** kwargs | ) -> None This callback hook is called after the end of each batch.","title":"on_batch"},{"location":"api/training/callbacks/callback/#on_epoch","text":"class TrainerCallback ( Registrable ): | ... | def on_epoch ( | self , | trainer : \"GradientDescentTrainer\" , | metrics : Dict [ str , Any ], | epoch : int , | is_primary : bool = True , | ** kwargs | ) -> None This callback hook is called after the end of each epoch.","title":"on_epoch"},{"location":"api/training/callbacks/callback/#on_end","text":"class TrainerCallback ( Registrable ): | ... | def on_end ( | self , | trainer : \"GradientDescentTrainer\" , | metrics : Dict [ str , Any ] = None , | epoch : int = None , | is_primary : bool = True , | ** kwargs | ) -> None This callback hook is called after the final training epoch.","title":"on_end"},{"location":"api/training/callbacks/callback/#state_dict","text":"class TrainerCallback ( Registrable ): | ... | def state_dict ( self ) -> Dict [ str , Any ]","title":"state_dict"},{"location":"api/training/callbacks/callback/#load_state_dict","text":"class TrainerCallback ( Registrable ): | ... | def load_state_dict ( self , state_dict : Dict [ str , Any ]) -> None","title":"load_state_dict"},{"location":"api/training/callbacks/confidence_checks/","text":"allennlp .training .callbacks .confidence_checks [SOURCE] ConfidenceChecksCallback \u00b6 @TrainerCallback . register ( \"sanity_checks\" ) @TrainerCallback . register ( \"confidence_checks\" ) class ConfidenceChecksCallback ( TrainerCallback ) Performs model confidence checks. Checks performed: NormalizationBiasVerification for detecting invalid combinations of bias and normalization layers. See allennlp.confidence_checks.normalization_bias_verification for more details. Note: Any new confidence checks should also be added to this callback. on_start \u00b6 class ConfidenceChecksCallback ( TrainerCallback ): | ... | def on_start ( | self , | trainer : \"GradientDescentTrainer\" , | is_primary : bool = True , | ** kwargs | ) -> None on_batch \u00b6 class ConfidenceChecksCallback ( TrainerCallback ): | ... | def on_batch ( | self , | trainer : \"GradientDescentTrainer\" , | batch_inputs : List [ TensorDict ], | batch_outputs : List [ Dict [ str , Any ]], | batch_metrics : Dict [ str , Any ], | epoch : int , | batch_number : int , | is_training : bool , | is_primary : bool = True , | batch_grad_norm : Optional [ float ] = None , | ** kwargs | ) -> None ConfidenceCheckError \u00b6 class ConfidenceCheckError ( Exception ): | def __init__ ( self , message ) -> None The error type raised when a confidence check fails.","title":"confidence_checks"},{"location":"api/training/callbacks/confidence_checks/#confidencecheckscallback","text":"@TrainerCallback . register ( \"sanity_checks\" ) @TrainerCallback . register ( \"confidence_checks\" ) class ConfidenceChecksCallback ( TrainerCallback ) Performs model confidence checks. Checks performed: NormalizationBiasVerification for detecting invalid combinations of bias and normalization layers. See allennlp.confidence_checks.normalization_bias_verification for more details. Note: Any new confidence checks should also be added to this callback.","title":"ConfidenceChecksCallback"},{"location":"api/training/callbacks/confidence_checks/#on_start","text":"class ConfidenceChecksCallback ( TrainerCallback ): | ... | def on_start ( | self , | trainer : \"GradientDescentTrainer\" , | is_primary : bool = True , | ** kwargs | ) -> None","title":"on_start"},{"location":"api/training/callbacks/confidence_checks/#on_batch","text":"class ConfidenceChecksCallback ( TrainerCallback ): | ... | def on_batch ( | self , | trainer : \"GradientDescentTrainer\" , | batch_inputs : List [ TensorDict ], | batch_outputs : List [ Dict [ str , Any ]], | batch_metrics : Dict [ str , Any ], | epoch : int , | batch_number : int , | is_training : bool , | is_primary : bool = True , | batch_grad_norm : Optional [ float ] = None , | ** kwargs | ) -> None","title":"on_batch"},{"location":"api/training/callbacks/confidence_checks/#confidencecheckerror","text":"class ConfidenceCheckError ( Exception ): | def __init__ ( self , message ) -> None The error type raised when a confidence check fails.","title":"ConfidenceCheckError"},{"location":"api/training/callbacks/console_logger/","text":"allennlp .training .callbacks .console_logger [SOURCE] ConsoleLoggerCallback \u00b6 @TrainerCallback . register ( \"console_logger\" ) class ConsoleLoggerCallback ( TrainerCallback ): | def __init__ ( | self , | serialization_dir : str , | should_log_inputs : bool = True | ) -> None on_batch \u00b6 class ConsoleLoggerCallback ( TrainerCallback ): | ... | def on_batch ( | self , | trainer : \"GradientDescentTrainer\" , | batch_inputs : List [ TensorDict ], | batch_outputs : List [ Dict [ str , Any ]], | batch_metrics : Dict [ str , Any ], | epoch : int , | batch_number : int , | is_training : bool , | is_primary : bool = True , | batch_grad_norm : Optional [ float ] = None , | ** kwargs | ) -> None on_epoch \u00b6 class ConsoleLoggerCallback ( TrainerCallback ): | ... | def on_epoch ( | self , | trainer : \"GradientDescentTrainer\" , | metrics : Dict [ str , Any ], | epoch : int , | is_primary : bool = True , | ** kwargs | ) -> None","title":"console_logger"},{"location":"api/training/callbacks/console_logger/#consoleloggercallback","text":"@TrainerCallback . register ( \"console_logger\" ) class ConsoleLoggerCallback ( TrainerCallback ): | def __init__ ( | self , | serialization_dir : str , | should_log_inputs : bool = True | ) -> None","title":"ConsoleLoggerCallback"},{"location":"api/training/callbacks/console_logger/#on_batch","text":"class ConsoleLoggerCallback ( TrainerCallback ): | ... | def on_batch ( | self , | trainer : \"GradientDescentTrainer\" , | batch_inputs : List [ TensorDict ], | batch_outputs : List [ Dict [ str , Any ]], | batch_metrics : Dict [ str , Any ], | epoch : int , | batch_number : int , | is_training : bool , | is_primary : bool = True , | batch_grad_norm : Optional [ float ] = None , | ** kwargs | ) -> None","title":"on_batch"},{"location":"api/training/callbacks/console_logger/#on_epoch","text":"class ConsoleLoggerCallback ( TrainerCallback ): | ... | def on_epoch ( | self , | trainer : \"GradientDescentTrainer\" , | metrics : Dict [ str , Any ], | epoch : int , | is_primary : bool = True , | ** kwargs | ) -> None","title":"on_epoch"},{"location":"api/training/callbacks/log_writer/","text":"allennlp .training .callbacks .log_writer [SOURCE] LogWriterCallback \u00b6 class LogWriterCallback ( TrainerCallback ): | def __init__ ( | self , | serialization_dir : str , | summary_interval : int = 100 , | distribution_interval : Optional [ int ] = None , | batch_size_interval : Optional [ int ] = None , | should_log_parameter_statistics : bool = True , | should_log_learning_rate : bool = False , | batch_loss_moving_average_count : int = 100 | ) -> None An abstract baseclass for callbacks that Log training statistics and metrics. Examples of concrete implementations are the TensorBoardCallback and WandBCallback . Parameters \u00b6 serialization_dir : str The training serialization directory. In a typical AllenNLP configuration file, this parameter does not get an entry in the file, it gets passed in separately. summary_interval : int , optional (default = 100 ) Most statistics will be written out only every this many batches. distribution_interval : int , optional (default = None ) When this parameter is specified, the following additional logging is enabled every this many batches: * Distributions of model parameters * The ratio of parameter update norm to parameter norm * Distribution of layer activations The layer activations are logged for any modules in the Model that have the attribute should_log_activations set to True . Logging distributions requires a number of GPU-CPU copies during training and is typically slow, so we recommend logging distributions relatively infrequently. Note Only Modules that return tensors, tuples of tensors or dicts with tensors as values currently support activation logging. batch_size_interval : int , optional (default = None ) If defined, how often to log the average batch size. should_log_parameter_statistics : bool , optional (default = True ) Whether to log parameter statistics (mean and standard deviation of parameters and gradients). If True , parameter stats are logged every summary_interval batches. should_log_learning_rate : bool , optional (default = False ) Whether to log (parameter-specific) learning rate. If True , learning rates are logged every summary_interval batches. batch_loss_moving_average_count : int , optional (default = 100 ) The length of the moving average for batch loss. log_scalars \u00b6 class LogWriterCallback ( TrainerCallback ): | ... | def log_scalars ( | self , | scalars : Dict [ str , Union [ int , float ]], | log_prefix : str = \"\" , | epoch : Optional [ int ] = None | ) -> None Required to be implemented by subclasses. Defines how batch or epoch scalar metrics are logged. log_tensors \u00b6 class LogWriterCallback ( TrainerCallback ): | ... | def log_tensors ( | self , | tensors : Dict [ str , torch . Tensor ], | log_prefix : str = \"\" , | epoch : Optional [ int ] = None | ) -> None Required to be implemented by subclasses. Defines how batch or epoch tensor metrics are logged. log_inputs \u00b6 class LogWriterCallback ( TrainerCallback ): | ... | def log_inputs ( | self , | inputs : List [ TensorDict ], | log_prefix : str = \"\" | ) -> None Can be optionally implemented by subclasses. Defines how batch inputs are logged. This is called once at the start of each epoch. close \u00b6 class LogWriterCallback ( TrainerCallback ): | ... | def close ( self ) -> None Called at the end of training to remove any module hooks and close out any other logging resources. on_start \u00b6 class LogWriterCallback ( TrainerCallback ): | ... | def on_start ( | self , | trainer : \"GradientDescentTrainer\" , | is_primary : bool = True , | ** kwargs | ) -> None on_batch \u00b6 class LogWriterCallback ( TrainerCallback ): | ... | def on_batch ( | self , | trainer : \"GradientDescentTrainer\" , | batch_inputs : List [ TensorDict ], | batch_outputs : List [ Dict [ str , Any ]], | batch_metrics : Dict [ str , Any ], | epoch : int , | batch_number : int , | is_training : bool , | is_primary : bool = True , | batch_grad_norm : Optional [ float ] = None , | ** kwargs | ) -> None on_epoch \u00b6 class LogWriterCallback ( TrainerCallback ): | ... | def on_epoch ( | self , | trainer : \"GradientDescentTrainer\" , | metrics : Dict [ str , Any ], | epoch : int , | is_primary : bool = True , | ** kwargs | ) -> None on_end \u00b6 class LogWriterCallback ( TrainerCallback ): | ... | def on_end ( | self , | trainer : \"GradientDescentTrainer\" , | metrics : Dict [ str , Any ] = None , | epoch : int = None , | is_primary : bool = True , | ** kwargs | ) -> None log_batch \u00b6 class LogWriterCallback ( TrainerCallback ): | ... | def log_batch ( | self , | batch_grad_norm : Optional [ float ], | metrics : Dict [ str , float ], | batch_group : List [ TensorDict ], | param_updates : Optional [ Dict [ str , torch . Tensor ]], | batch_number : int | ) -> None Called every batch to perform all of the logging that is due. log_epoch \u00b6 class LogWriterCallback ( TrainerCallback ): | ... | def log_epoch ( | self , | train_metrics : Dict [ str , Any ], | val_metrics : Dict [ str , Any ], | epoch : int | ) -> None Called at the end of every epoch to log training and validation metrics.","title":"log_writer"},{"location":"api/training/callbacks/log_writer/#logwritercallback","text":"class LogWriterCallback ( TrainerCallback ): | def __init__ ( | self , | serialization_dir : str , | summary_interval : int = 100 , | distribution_interval : Optional [ int ] = None , | batch_size_interval : Optional [ int ] = None , | should_log_parameter_statistics : bool = True , | should_log_learning_rate : bool = False , | batch_loss_moving_average_count : int = 100 | ) -> None An abstract baseclass for callbacks that Log training statistics and metrics. Examples of concrete implementations are the TensorBoardCallback and WandBCallback .","title":"LogWriterCallback"},{"location":"api/training/callbacks/log_writer/#log_scalars","text":"class LogWriterCallback ( TrainerCallback ): | ... | def log_scalars ( | self , | scalars : Dict [ str , Union [ int , float ]], | log_prefix : str = \"\" , | epoch : Optional [ int ] = None | ) -> None Required to be implemented by subclasses. Defines how batch or epoch scalar metrics are logged.","title":"log_scalars"},{"location":"api/training/callbacks/log_writer/#log_tensors","text":"class LogWriterCallback ( TrainerCallback ): | ... | def log_tensors ( | self , | tensors : Dict [ str , torch . Tensor ], | log_prefix : str = \"\" , | epoch : Optional [ int ] = None | ) -> None Required to be implemented by subclasses. Defines how batch or epoch tensor metrics are logged.","title":"log_tensors"},{"location":"api/training/callbacks/log_writer/#log_inputs","text":"class LogWriterCallback ( TrainerCallback ): | ... | def log_inputs ( | self , | inputs : List [ TensorDict ], | log_prefix : str = \"\" | ) -> None Can be optionally implemented by subclasses. Defines how batch inputs are logged. This is called once at the start of each epoch.","title":"log_inputs"},{"location":"api/training/callbacks/log_writer/#close","text":"class LogWriterCallback ( TrainerCallback ): | ... | def close ( self ) -> None Called at the end of training to remove any module hooks and close out any other logging resources.","title":"close"},{"location":"api/training/callbacks/log_writer/#on_start","text":"class LogWriterCallback ( TrainerCallback ): | ... | def on_start ( | self , | trainer : \"GradientDescentTrainer\" , | is_primary : bool = True , | ** kwargs | ) -> None","title":"on_start"},{"location":"api/training/callbacks/log_writer/#on_batch","text":"class LogWriterCallback ( TrainerCallback ): | ... | def on_batch ( | self , | trainer : \"GradientDescentTrainer\" , | batch_inputs : List [ TensorDict ], | batch_outputs : List [ Dict [ str , Any ]], | batch_metrics : Dict [ str , Any ], | epoch : int , | batch_number : int , | is_training : bool , | is_primary : bool = True , | batch_grad_norm : Optional [ float ] = None , | ** kwargs | ) -> None","title":"on_batch"},{"location":"api/training/callbacks/log_writer/#on_epoch","text":"class LogWriterCallback ( TrainerCallback ): | ... | def on_epoch ( | self , | trainer : \"GradientDescentTrainer\" , | metrics : Dict [ str , Any ], | epoch : int , | is_primary : bool = True , | ** kwargs | ) -> None","title":"on_epoch"},{"location":"api/training/callbacks/log_writer/#on_end","text":"class LogWriterCallback ( TrainerCallback ): | ... | def on_end ( | self , | trainer : \"GradientDescentTrainer\" , | metrics : Dict [ str , Any ] = None , | epoch : int = None , | is_primary : bool = True , | ** kwargs | ) -> None","title":"on_end"},{"location":"api/training/callbacks/log_writer/#log_batch","text":"class LogWriterCallback ( TrainerCallback ): | ... | def log_batch ( | self , | batch_grad_norm : Optional [ float ], | metrics : Dict [ str , float ], | batch_group : List [ TensorDict ], | param_updates : Optional [ Dict [ str , torch . Tensor ]], | batch_number : int | ) -> None Called every batch to perform all of the logging that is due.","title":"log_batch"},{"location":"api/training/callbacks/log_writer/#log_epoch","text":"class LogWriterCallback ( TrainerCallback ): | ... | def log_epoch ( | self , | train_metrics : Dict [ str , Any ], | val_metrics : Dict [ str , Any ], | epoch : int | ) -> None Called at the end of every epoch to log training and validation metrics.","title":"log_epoch"},{"location":"api/training/callbacks/should_validate/","text":"allennlp .training .callbacks .should_validate [SOURCE] ShouldValidateCallback \u00b6 @TrainerCallback . register ( \"should_validate_callback\" ) class ShouldValidateCallback ( TrainerCallback ): | def __init__ ( | self , | serialization_dir : str , | validation_start : Optional [ int ] = None , | validation_interval : Optional [ int ] = None | ) -> None A callback that you can pass to the GradientDescentTrainer to change the frequency of validation during training. If validation_start is not None , validation will not occur until validation_start epochs have elapsed. If validation_interval is not None , validation will run every validation_interval number of epochs epochs. on_start \u00b6 class ShouldValidateCallback ( TrainerCallback ): | ... | def on_start ( | self , | trainer : \"GradientDescentTrainer\" , | is_primary : bool = True , | ** kwargs | ) -> None on_epoch \u00b6 class ShouldValidateCallback ( TrainerCallback ): | ... | def on_epoch ( | self , | trainer : \"GradientDescentTrainer\" , | metrics : Dict [ str , Any ], | epoch : int , | is_primary : bool = True , | ** kwargs | ) -> None on_end \u00b6 class ShouldValidateCallback ( TrainerCallback ): | ... | def on_end ( | self , | trainer : \"GradientDescentTrainer\" , | metrics : Dict [ str , Any ] = None , | epoch : int = None , | is_primary : bool = True , | ** kwargs | ) -> None","title":"should_validate"},{"location":"api/training/callbacks/should_validate/#shouldvalidatecallback","text":"@TrainerCallback . register ( \"should_validate_callback\" ) class ShouldValidateCallback ( TrainerCallback ): | def __init__ ( | self , | serialization_dir : str , | validation_start : Optional [ int ] = None , | validation_interval : Optional [ int ] = None | ) -> None A callback that you can pass to the GradientDescentTrainer to change the frequency of validation during training. If validation_start is not None , validation will not occur until validation_start epochs have elapsed. If validation_interval is not None , validation will run every validation_interval number of epochs epochs.","title":"ShouldValidateCallback"},{"location":"api/training/callbacks/should_validate/#on_start","text":"class ShouldValidateCallback ( TrainerCallback ): | ... | def on_start ( | self , | trainer : \"GradientDescentTrainer\" , | is_primary : bool = True , | ** kwargs | ) -> None","title":"on_start"},{"location":"api/training/callbacks/should_validate/#on_epoch","text":"class ShouldValidateCallback ( TrainerCallback ): | ... | def on_epoch ( | self , | trainer : \"GradientDescentTrainer\" , | metrics : Dict [ str , Any ], | epoch : int , | is_primary : bool = True , | ** kwargs | ) -> None","title":"on_epoch"},{"location":"api/training/callbacks/should_validate/#on_end","text":"class ShouldValidateCallback ( TrainerCallback ): | ... | def on_end ( | self , | trainer : \"GradientDescentTrainer\" , | metrics : Dict [ str , Any ] = None , | epoch : int = None , | is_primary : bool = True , | ** kwargs | ) -> None","title":"on_end"},{"location":"api/training/callbacks/tensorboard/","text":"allennlp .training .callbacks .tensorboard [SOURCE] TensorBoardCallback \u00b6 @TrainerCallback . register ( \"tensorboard\" ) class TensorBoardCallback ( LogWriterCallback ): | def __init__ ( | self , | serialization_dir : str , | summary_interval : int = 100 , | distribution_interval : Optional [ int ] = None , | batch_size_interval : Optional [ int ] = None , | should_log_parameter_statistics : bool = False , | should_log_learning_rate : bool = False | ) -> None A callback that writes training statistics/metrics to TensorBoard. log_scalars \u00b6 class TensorBoardCallback ( LogWriterCallback ): | ... | def log_scalars ( | self , | scalars : Dict [ str , Union [ int , float ]], | log_prefix : str = \"\" , | epoch : Optional [ int ] = None | ) -> None log_tensors \u00b6 class TensorBoardCallback ( LogWriterCallback ): | ... | def log_tensors ( | self , | tensors : Dict [ str , torch . Tensor ], | log_prefix : str = \"\" , | epoch : Optional [ int ] = None | ) -> None close \u00b6 class TensorBoardCallback ( LogWriterCallback ): | ... | def close ( self ) -> None Calls the close method of the SummaryWriter s which makes sure that pending scalars are flushed to disk and the tensorboard event files are closed properly.","title":"tensorboard"},{"location":"api/training/callbacks/tensorboard/#tensorboardcallback","text":"@TrainerCallback . register ( \"tensorboard\" ) class TensorBoardCallback ( LogWriterCallback ): | def __init__ ( | self , | serialization_dir : str , | summary_interval : int = 100 , | distribution_interval : Optional [ int ] = None , | batch_size_interval : Optional [ int ] = None , | should_log_parameter_statistics : bool = False , | should_log_learning_rate : bool = False | ) -> None A callback that writes training statistics/metrics to TensorBoard.","title":"TensorBoardCallback"},{"location":"api/training/callbacks/tensorboard/#log_scalars","text":"class TensorBoardCallback ( LogWriterCallback ): | ... | def log_scalars ( | self , | scalars : Dict [ str , Union [ int , float ]], | log_prefix : str = \"\" , | epoch : Optional [ int ] = None | ) -> None","title":"log_scalars"},{"location":"api/training/callbacks/tensorboard/#log_tensors","text":"class TensorBoardCallback ( LogWriterCallback ): | ... | def log_tensors ( | self , | tensors : Dict [ str , torch . Tensor ], | log_prefix : str = \"\" , | epoch : Optional [ int ] = None | ) -> None","title":"log_tensors"},{"location":"api/training/callbacks/tensorboard/#close","text":"class TensorBoardCallback ( LogWriterCallback ): | ... | def close ( self ) -> None Calls the close method of the SummaryWriter s which makes sure that pending scalars are flushed to disk and the tensorboard event files are closed properly.","title":"close"},{"location":"api/training/callbacks/track_epoch/","text":"allennlp .training .callbacks .track_epoch [SOURCE] TrackEpochCallback \u00b6 @TrainerCallback . register ( \"track_epoch_callback\" ) class TrackEpochCallback ( TrainerCallback ) A callback that you can pass to the GradientDescentTrainer to access the current epoch number in your model during training. This callback sets model.epoch , which can be read inside of model.forward() . We set model.epoch = epoch + 1 which now denotes the number of completed epochs at a given training state. on_start \u00b6 class TrackEpochCallback ( TrainerCallback ): | ... | def on_start ( | self , | trainer : \"GradientDescentTrainer\" , | is_primary : bool = True , | ** kwargs | ) -> None on_epoch \u00b6 class TrackEpochCallback ( TrainerCallback ): | ... | def on_epoch ( | self , | trainer : \"GradientDescentTrainer\" , | metrics : Dict [ str , Any ], | epoch : int , | is_primary : bool = True , | ** kwargs | ) -> None","title":"track_epoch"},{"location":"api/training/callbacks/track_epoch/#trackepochcallback","text":"@TrainerCallback . register ( \"track_epoch_callback\" ) class TrackEpochCallback ( TrainerCallback ) A callback that you can pass to the GradientDescentTrainer to access the current epoch number in your model during training. This callback sets model.epoch , which can be read inside of model.forward() . We set model.epoch = epoch + 1 which now denotes the number of completed epochs at a given training state.","title":"TrackEpochCallback"},{"location":"api/training/callbacks/track_epoch/#on_start","text":"class TrackEpochCallback ( TrainerCallback ): | ... | def on_start ( | self , | trainer : \"GradientDescentTrainer\" , | is_primary : bool = True , | ** kwargs | ) -> None","title":"on_start"},{"location":"api/training/callbacks/track_epoch/#on_epoch","text":"class TrackEpochCallback ( TrainerCallback ): | ... | def on_epoch ( | self , | trainer : \"GradientDescentTrainer\" , | metrics : Dict [ str , Any ], | epoch : int , | is_primary : bool = True , | ** kwargs | ) -> None","title":"on_epoch"},{"location":"api/training/callbacks/wandb/","text":"allennlp .training .callbacks .wandb [SOURCE] WandBCallback \u00b6 @TrainerCallback . register ( \"wandb\" ) class WandBCallback ( LogWriterCallback ): | def __init__ ( | self , | serialization_dir : str , | summary_interval : int = 100 , | distribution_interval : Optional [ int ] = None , | batch_size_interval : Optional [ int ] = None , | should_log_parameter_statistics : bool = True , | should_log_learning_rate : bool = False , | project : Optional [ str ] = None , | entity : Optional [ str ] = None , | group : Optional [ str ] = None , | name : Optional [ str ] = None , | notes : Optional [ str ] = None , | tags : Optional [ List [ str ]] = None , | watch_model : bool = True , | files_to_save : Tuple [ str , ... ] = ( \"config.json\" , \"out.log\" ), | wandb_kwargs : Optional [ Dict [ str , Any ]] = None | ) -> None Logs training runs to Weights & Biases. Note This requires the environment variable 'WANDB_API_KEY' to be set in order to authenticate with Weights & Biases. If not set, you may be prompted to log in or upload the experiment to an anonymous account. In addition to the parameters that LogWriterCallback takes, there are several other parameters specific to WandBWriter listed below. Parameters \u00b6 project : Optional[str] , optional (default = None ) The name of the W&B project to save the training run to. entity : Optional[str] , optional (default = None ) The username or team name to send the run to. If not specified, the default will be used. group : Optional[str] , optional (default = None ) Specify a group to organize individual runs into a larger experiment. name : Optional[str] , optional (default = None ) A short display name for this run, which is how you'll identify this run in the W&B UI. By default a random name is generated. notes : Optional[str] , optional (default = None ) A description of the run. tags : Optional[List[str]] , optional (default = None ) Tags to assign to the training run in W&B. watch_model : bool , optional (default = True ) Whether or not W&B should watch the Model . files_to_save : Tuple[str, ...] , optional (default = (\"config.json\", \"out.log\") ) Extra files in the serialization directory to save to the W&B training run. wandb_kwargs : Optional[Dict[str, Any]] , optional (default = None ) Additional key word arguments to pass to wandb.init() . log_scalars \u00b6 class WandBCallback ( LogWriterCallback ): | ... | def log_scalars ( | self , | scalars : Dict [ str , Union [ int , float ]], | log_prefix : str = \"\" , | epoch : Optional [ int ] = None | ) -> None log_tensors \u00b6 class WandBCallback ( LogWriterCallback ): | ... | def log_tensors ( | self , | tensors : Dict [ str , torch . Tensor ], | log_prefix : str = \"\" , | epoch : Optional [ int ] = None | ) -> None on_start \u00b6 class WandBCallback ( LogWriterCallback ): | ... | def on_start ( | self , | trainer : \"GradientDescentTrainer\" , | is_primary : bool = True , | ** kwargs | ) -> None close \u00b6 class WandBCallback ( LogWriterCallback ): | ... | def close ( self ) -> None state_dict \u00b6 class WandBCallback ( LogWriterCallback ): | ... | def state_dict ( self ) -> Dict [ str , Any ] load_state_dict \u00b6 class WandBCallback ( LogWriterCallback ): | ... | def load_state_dict ( self , state_dict : Dict [ str , Any ]) -> None","title":"wandb"},{"location":"api/training/callbacks/wandb/#wandbcallback","text":"@TrainerCallback . register ( \"wandb\" ) class WandBCallback ( LogWriterCallback ): | def __init__ ( | self , | serialization_dir : str , | summary_interval : int = 100 , | distribution_interval : Optional [ int ] = None , | batch_size_interval : Optional [ int ] = None , | should_log_parameter_statistics : bool = True , | should_log_learning_rate : bool = False , | project : Optional [ str ] = None , | entity : Optional [ str ] = None , | group : Optional [ str ] = None , | name : Optional [ str ] = None , | notes : Optional [ str ] = None , | tags : Optional [ List [ str ]] = None , | watch_model : bool = True , | files_to_save : Tuple [ str , ... ] = ( \"config.json\" , \"out.log\" ), | wandb_kwargs : Optional [ Dict [ str , Any ]] = None | ) -> None Logs training runs to Weights & Biases. Note This requires the environment variable 'WANDB_API_KEY' to be set in order to authenticate with Weights & Biases. If not set, you may be prompted to log in or upload the experiment to an anonymous account. In addition to the parameters that LogWriterCallback takes, there are several other parameters specific to WandBWriter listed below.","title":"WandBCallback"},{"location":"api/training/callbacks/wandb/#log_scalars","text":"class WandBCallback ( LogWriterCallback ): | ... | def log_scalars ( | self , | scalars : Dict [ str , Union [ int , float ]], | log_prefix : str = \"\" , | epoch : Optional [ int ] = None | ) -> None","title":"log_scalars"},{"location":"api/training/callbacks/wandb/#log_tensors","text":"class WandBCallback ( LogWriterCallback ): | ... | def log_tensors ( | self , | tensors : Dict [ str , torch . Tensor ], | log_prefix : str = \"\" , | epoch : Optional [ int ] = None | ) -> None","title":"log_tensors"},{"location":"api/training/callbacks/wandb/#on_start","text":"class WandBCallback ( LogWriterCallback ): | ... | def on_start ( | self , | trainer : \"GradientDescentTrainer\" , | is_primary : bool = True , | ** kwargs | ) -> None","title":"on_start"},{"location":"api/training/callbacks/wandb/#close","text":"class WandBCallback ( LogWriterCallback ): | ... | def close ( self ) -> None","title":"close"},{"location":"api/training/callbacks/wandb/#state_dict","text":"class WandBCallback ( LogWriterCallback ): | ... | def state_dict ( self ) -> Dict [ str , Any ]","title":"state_dict"},{"location":"api/training/callbacks/wandb/#load_state_dict","text":"class WandBCallback ( LogWriterCallback ): | ... | def load_state_dict ( self , state_dict : Dict [ str , Any ]) -> None","title":"load_state_dict"},{"location":"api/training/learning_rate_schedulers/combined/","text":"allennlp .training .learning_rate_schedulers .combined [SOURCE] CombinedLearningRateScheduler \u00b6 @LearningRateScheduler . register ( \"combined\" ) class CombinedLearningRateScheduler ( LearningRateScheduler ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | schedulers : List [ Tuple [ int , Lazy [ LearningRateScheduler ]]], | num_steps_per_epoch : Optional [ int ] = None , | last_epoch : int = - 1 | ) -> None This LearningRateScheduler can be used to apply an arbitrary number of other schedulers one after the other. These schedulers are defined though the schedulers parameter, which takes a list of Tuple[int, Lazy[LearningRateScheduler]] . The first field of the tuple, the int , specifies how many epochs the corresponding scheduler will be used before the next scheduler takes its place. While it usually makes sense for the sum sum ( n_epochs for ( n_epochs , _ ) in schedulers ) to equal the total number of training epochs, it is not a requirement. If training continues beyond the last defined scheduler, both step() and step_batch() will be a no-op. In effect, this causes the learning rate to stay constant. Example \u00b6 Config for using the CombinedLearningRateScheduler Learning Rate Scheduler with the following arguments: Use PolynomialDecay for the first 15 epochs. Use NoamLR for the next 15 epochs. Use a constant LR for the remaining epochs. { ... \"trainer\" :{ ... \"learning_rate_scheduler\" : { \"type\" : \"combined\" , \"schedulers\" : [ [ 15 , { \"type\" : \"polynomial_decay\" , \"power\" : 2 , \"warmup_steps\" : 50 , \"end_learning_rate\" : 1e-10 } ], [ 15 , { \"type\" : \"noam\" , \"warmup_steps\" : 1 , \"model_size\" : 128 , \"factor\" : 0.5 } ] ] }, ... } } Note that you do NOT pass a optimizer key to the Learning rate scheduler. current_scheduler \u00b6 class CombinedLearningRateScheduler ( LearningRateScheduler ): | ... | @property | def current_scheduler ( self ) -> Optional [ LearningRateScheduler ] state_dict \u00b6 class CombinedLearningRateScheduler ( LearningRateScheduler ): | ... | def state_dict ( self ) -> Dict [ str , Any ] load_state_dict \u00b6 class CombinedLearningRateScheduler ( LearningRateScheduler ): | ... | def load_state_dict ( self , state_dict : Dict [ str , Any ]) -> None get_values \u00b6 class CombinedLearningRateScheduler ( LearningRateScheduler ): | ... | def get_values ( self ) This should never be called directly. step_batch \u00b6 class CombinedLearningRateScheduler ( LearningRateScheduler ): | ... | def step_batch ( self , batch_num_total : int = None ) -> None step \u00b6 class CombinedLearningRateScheduler ( LearningRateScheduler ): | ... | def step ( self , metric : float = None ) -> None","title":"combined"},{"location":"api/training/learning_rate_schedulers/combined/#combinedlearningratescheduler","text":"@LearningRateScheduler . register ( \"combined\" ) class CombinedLearningRateScheduler ( LearningRateScheduler ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | schedulers : List [ Tuple [ int , Lazy [ LearningRateScheduler ]]], | num_steps_per_epoch : Optional [ int ] = None , | last_epoch : int = - 1 | ) -> None This LearningRateScheduler can be used to apply an arbitrary number of other schedulers one after the other. These schedulers are defined though the schedulers parameter, which takes a list of Tuple[int, Lazy[LearningRateScheduler]] . The first field of the tuple, the int , specifies how many epochs the corresponding scheduler will be used before the next scheduler takes its place. While it usually makes sense for the sum sum ( n_epochs for ( n_epochs , _ ) in schedulers ) to equal the total number of training epochs, it is not a requirement. If training continues beyond the last defined scheduler, both step() and step_batch() will be a no-op. In effect, this causes the learning rate to stay constant.","title":"CombinedLearningRateScheduler"},{"location":"api/training/learning_rate_schedulers/combined/#current_scheduler","text":"class CombinedLearningRateScheduler ( LearningRateScheduler ): | ... | @property | def current_scheduler ( self ) -> Optional [ LearningRateScheduler ]","title":"current_scheduler"},{"location":"api/training/learning_rate_schedulers/combined/#state_dict","text":"class CombinedLearningRateScheduler ( LearningRateScheduler ): | ... | def state_dict ( self ) -> Dict [ str , Any ]","title":"state_dict"},{"location":"api/training/learning_rate_schedulers/combined/#load_state_dict","text":"class CombinedLearningRateScheduler ( LearningRateScheduler ): | ... | def load_state_dict ( self , state_dict : Dict [ str , Any ]) -> None","title":"load_state_dict"},{"location":"api/training/learning_rate_schedulers/combined/#get_values","text":"class CombinedLearningRateScheduler ( LearningRateScheduler ): | ... | def get_values ( self ) This should never be called directly.","title":"get_values"},{"location":"api/training/learning_rate_schedulers/combined/#step_batch","text":"class CombinedLearningRateScheduler ( LearningRateScheduler ): | ... | def step_batch ( self , batch_num_total : int = None ) -> None","title":"step_batch"},{"location":"api/training/learning_rate_schedulers/combined/#step","text":"class CombinedLearningRateScheduler ( LearningRateScheduler ): | ... | def step ( self , metric : float = None ) -> None","title":"step"},{"location":"api/training/learning_rate_schedulers/cosine/","text":"allennlp .training .learning_rate_schedulers .cosine [SOURCE] CosineWithRestarts \u00b6 @LearningRateScheduler . register ( \"cosine\" ) class CosineWithRestarts ( LearningRateScheduler ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | t_initial : int , | t_mul : float = 1.0 , | eta_min : float = 0.0 , | eta_mul : float = 1.0 , | last_epoch : int = - 1 | ) -> None Cosine annealing with restarts. This is described in the paper https://arxiv.org/abs/1608.03983. Note that early stopping should typically be avoided when using this schedule. Registered as a LearningRateScheduler with name \"cosine\". Parameters \u00b6 optimizer : torch.optim.Optimizer This argument does not get an entry in a configuration file for the object. t_initial : int The number of iterations (epochs) within the first cycle. t_mul : float , optional (default = 1 ) Determines the number of iterations (epochs) in the i-th decay cycle, which is the length of the last cycle multiplied by t_mul . eta_min : float , optional (default = 0 ) The minimum learning rate. eta_mul : float , optional (default = 1 ) Determines the initial learning rate for the i-th decay cycle, which is the last initial learning rate multiplied by m_mul . last_epoch : int , optional (default = -1 ) The index of the last epoch. This is used when restarting. Example \u00b6 Config for using the CosineWithRestarts Learning Rate Scheduler with the following arguments: t_initial set to 5 t_mul set to 0.9 eta_min set to 1e-12 eta_mul set to 0.8 last_epoch set to 10 { ... \"trainer\" :{ ... \"learning_rate_scheduler\" : { \"type\" : \"cosine\" , \"t_initial\" : 5 , \"t_mul\" : 0.9 , \"eta_min\" : 1e-12 \"eta_mul\" : 0.8 \"last_epoch\" : 10 }, ... } } Note that you do NOT pass a optimizer key to the Learning rate scheduler. get_values \u00b6 class CosineWithRestarts ( LearningRateScheduler ): | ... | def get_values ( self ) Get updated learning rate.","title":"cosine"},{"location":"api/training/learning_rate_schedulers/cosine/#cosinewithrestarts","text":"@LearningRateScheduler . register ( \"cosine\" ) class CosineWithRestarts ( LearningRateScheduler ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | t_initial : int , | t_mul : float = 1.0 , | eta_min : float = 0.0 , | eta_mul : float = 1.0 , | last_epoch : int = - 1 | ) -> None Cosine annealing with restarts. This is described in the paper https://arxiv.org/abs/1608.03983. Note that early stopping should typically be avoided when using this schedule. Registered as a LearningRateScheduler with name \"cosine\".","title":"CosineWithRestarts"},{"location":"api/training/learning_rate_schedulers/cosine/#get_values","text":"class CosineWithRestarts ( LearningRateScheduler ): | ... | def get_values ( self ) Get updated learning rate.","title":"get_values"},{"location":"api/training/learning_rate_schedulers/learning_rate_scheduler/","text":"allennlp .training .learning_rate_schedulers .learning_rate_scheduler [SOURCE] LearningRateScheduler \u00b6 class LearningRateScheduler ( Scheduler , Registrable ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | last_epoch : int = - 1 | ) -> None get_values \u00b6 class LearningRateScheduler ( Scheduler , Registrable ): | ... | def get_values ( self ) ConstantLearningRateScheduler \u00b6 @LearningRateScheduler . register ( \"constant\" ) class ConstantLearningRateScheduler ( _PyTorchLearningRateSchedulerWrapper ): | def __init__ ( self , optimizer : Optimizer , last_epoch : int = - 1 ) -> None Registered as a LearningRateScheduler with name \"constant\". The \"optimizer\" argument does not get an entry in a configuration file for the object. Example \u00b6 Config for using the ConstantLearningRateScheduler Learning Rate Scheduler. { ... \"trainer\" :{ ... \"learning_rate_scheduler\" : \"constant\" , ... } } Note that you do NOT pass a optimizer key to the Learning rate scheduler. ConstantWithWarmupLearningRateScheduler \u00b6 @LearningRateScheduler . register ( \"constant_with_warmup\" ) class ConstantWithWarmupLearningRateScheduler ( _PyTorchLearningRateSchedulerWrapper ): | def __init__ ( | self , | optimizer : Optimizer , | num_warmup_steps : int , | last_epoch : int = - 1 | ) -> None Registered as a LearningRateScheduler with name \"constant_with_warmup\". The \"optimizer\" argument does not get an entry in a configuration file for the object. Parameters \u00b6 optimizer : torch.optim.Optimizer This argument does not get an entry in a configuration file for the object. num_warmup_steps : int The number of steps to linearly increase the learning rate. Example \u00b6 Config for using the ConstantWithWarmupLearningRateScheduler Learning Rate Scheduler with num_warmup_steps set 100 . { ... \"trainer\" :{ ... \"learning_rate_scheduler\" : { \"type\" : \"constant_with_warmup\" , \"num_warmup_steps\" : 100 }, ... } } <a id= \"allennlp.training.learning_rate_schedulers.learning_rate_scheduler.CosineWithWarmupLearningRateScheduler\" ></a> ## Cosi ne Wi t hWarmupLear n i n gRa te Scheduler ```py t ho n @Lear n i n gRa te Scheduler.regis ter ( \"cosine_with_warmup\" ) class Cosi ne Wi t hWarmupLear n i n gRa te Scheduler(_PyTorchLear n i n gRa te SchedulerWrapper) : | de f __i n i t __( | sel f , | op t imizer : Op t imizer , | nu m_warmup_s te ps : i nt , | nu m_ tra i n i n g_s te ps : i nt , | nu m_cycles : fl oa t = 0.5 , | las t _epoch : i nt = -1 | ) - > No ne Registered as a LearningRateScheduler with name \"cosine_with_warmup\". The \"optimizer\" argument does not get an entry in a configuration file for the object. Parameters \u00b6 optimizer : torch.optim.Optimizer This argument does not get an entry in a configuration file for the object. num_warmup_steps : int The number of steps to linearly increase the learning rate. Example \u00b6 Config for using the CosineWithWarmupLearningRateScheduler Learning Rate Scheduler with num_warmup_steps set 100 . { ... \"trainer\" :{ ... \"learning_rate_scheduler\" : { \"type\" : \"cosine_with_warmup\" , \"num_warmup_steps\" : 100 }, ... } } <a id= \"allennlp.training.learning_rate_schedulers.learning_rate_scheduler.CosineHardRestartsWithWarmupLearningRateScheduler\" ></a> ## Cosi ne HardRes tarts Wi t hWarmupLear n i n gRa te Scheduler ```py t ho n @Lear n i n gRa te Scheduler.regis ter ( \"cosine_hard_restarts_with_warmup\" ) class Cosi ne HardRes tarts Wi t hWarmupLear n i n gRa te Scheduler(_PyTorchLear n i n gRa te SchedulerWrapper) : | de f __i n i t __( | sel f , | op t imizer : Op t imizer , | nu m_warmup_s te ps : i nt , | nu m_ tra i n i n g_s te ps : i nt , | nu m_cycles : i nt = 1 , | las t _epoch : i nt = -1 | ) - > No ne Registered as a LearningRateScheduler with name \"cosine_hard_restarts_with_warmup\". The \"optimizer\" argument does not get an entry in a configuration file for the object. Example \u00b6 Config for using the CosineHardRestartsWithWarmupLearningRateScheduler Learning Rate Scheduler with num_warmup_steps set 100 . ```json { ... \"trainer\":{ ... \"learning_rate_scheduler\": { \"type\": \"cosine_hard_restarts_with_warmup\", \"num_warmup_steps\": 100 }, ... } }","title":"learning_rate_scheduler"},{"location":"api/training/learning_rate_schedulers/learning_rate_scheduler/#learningratescheduler","text":"class LearningRateScheduler ( Scheduler , Registrable ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | last_epoch : int = - 1 | ) -> None","title":"LearningRateScheduler"},{"location":"api/training/learning_rate_schedulers/learning_rate_scheduler/#get_values","text":"class LearningRateScheduler ( Scheduler , Registrable ): | ... | def get_values ( self )","title":"get_values"},{"location":"api/training/learning_rate_schedulers/learning_rate_scheduler/#constantlearningratescheduler","text":"@LearningRateScheduler . register ( \"constant\" ) class ConstantLearningRateScheduler ( _PyTorchLearningRateSchedulerWrapper ): | def __init__ ( self , optimizer : Optimizer , last_epoch : int = - 1 ) -> None Registered as a LearningRateScheduler with name \"constant\". The \"optimizer\" argument does not get an entry in a configuration file for the object.","title":"ConstantLearningRateScheduler"},{"location":"api/training/learning_rate_schedulers/learning_rate_scheduler/#constantwithwarmuplearningratescheduler","text":"@LearningRateScheduler . register ( \"constant_with_warmup\" ) class ConstantWithWarmupLearningRateScheduler ( _PyTorchLearningRateSchedulerWrapper ): | def __init__ ( | self , | optimizer : Optimizer , | num_warmup_steps : int , | last_epoch : int = - 1 | ) -> None Registered as a LearningRateScheduler with name \"constant_with_warmup\". The \"optimizer\" argument does not get an entry in a configuration file for the object.","title":"ConstantWithWarmupLearningRateScheduler"},{"location":"api/training/learning_rate_schedulers/linear_with_warmup/","text":"allennlp .training .learning_rate_schedulers .linear_with_warmup [SOURCE] LinearWithWarmup \u00b6 @LearningRateScheduler . register ( \"linear_with_warmup\" ) class LinearWithWarmup ( PolynomialDecay ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | num_epochs : int , | num_steps_per_epoch : int , | warmup_steps : int = 100 , | last_epoch : int = - 1 | ) -> None Implements a learning rate scheduler that increases the learning rate to lr during the first warmup_steps steps, and then decreases it to zero over the rest of the training steps. In practice, this is a wrapper of PolynomialDecay with power=1 and end_learning_rate=0 . Parameters \u00b6 optimizer : torch.optim.Optimizer This argument does not get an entry in a configuration file for the object. num_epochs : int The number of epochs in the experiment. this does NOT get an entry in the config. num_steps_per_epoch : int The number of steps per epoch. this does NOT get an entry in the config. warmup_steps : int The number of steps to linearly increase the learning rate. Example \u00b6 Config for using the LinearWithWarmup Learning Rate Scheduler with warmup_steps set 100 . { ... \"trainer\" :{ ... \"learning_rate_scheduler\" : { \"type\" : \"linear_with_warmup\" , \"warmup_steps\" : 100 }, ... } } Note that you do NOT pass a optimizer , num_epochs , nor num_steps_per_epoch key to the Learning rate scheduler.","title":"linear_with_warmup"},{"location":"api/training/learning_rate_schedulers/linear_with_warmup/#linearwithwarmup","text":"@LearningRateScheduler . register ( \"linear_with_warmup\" ) class LinearWithWarmup ( PolynomialDecay ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | num_epochs : int , | num_steps_per_epoch : int , | warmup_steps : int = 100 , | last_epoch : int = - 1 | ) -> None Implements a learning rate scheduler that increases the learning rate to lr during the first warmup_steps steps, and then decreases it to zero over the rest of the training steps. In practice, this is a wrapper of PolynomialDecay with power=1 and end_learning_rate=0 .","title":"LinearWithWarmup"},{"location":"api/training/learning_rate_schedulers/noam/","text":"allennlp .training .learning_rate_schedulers .noam [SOURCE] NoamLR \u00b6 @LearningRateScheduler . register ( \"noam\" ) class NoamLR ( LearningRateScheduler ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | model_size : int , | warmup_steps : int , | factor : float = 1.0 , | last_epoch : int = - 1 | ) -> None Implements the Noam Learning rate schedule. This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number, scaled by the inverse square root of the dimensionality of the model. Time will tell if this is just madness or it's actually important. The formula for learning rate when using NoamLR is: lr = factor * ( model_size ** ( -0.5 ) * min( step** ( -0.5 ), step * warmup_steps ** ( -1.5 )) ) Registered as a LearningRateScheduler with name \"noam\". Parameters \u00b6 optimizer : torch.optim.Optimizer This argument does not get an entry in a configuration file for the object. model_size : int The hidden size parameter which dominates the number of parameters in your model. warmup_steps : int The number of steps to linearly increase the learning rate. factor : float , optional (default = 1.0 ) The overall scale factor for the learning rate decay. Example \u00b6 Config for using NoamLR with a model size of 1024 , warmup steps of 5 , and factor of .25 . { ... \"trainer\" :{ ... \"learning_rate_scheduler\" : { \"type\" : \"noam\" , \"model_size\" : 1024 , \"warmup_steps\" : 5 , \"factor\" : 0.25 }, ... } } Note that you do NOT pass an optimizer key to the Learning rate scheduler. step \u00b6 class NoamLR ( LearningRateScheduler ): | ... | def step ( self , metric : float = None ) -> None step_batch \u00b6 class NoamLR ( LearningRateScheduler ): | ... | def step_batch ( self , batch_num_total : int = None ) -> None get_values \u00b6 class NoamLR ( LearningRateScheduler ): | ... | def get_values ( self )","title":"noam"},{"location":"api/training/learning_rate_schedulers/noam/#noamlr","text":"@LearningRateScheduler . register ( \"noam\" ) class NoamLR ( LearningRateScheduler ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | model_size : int , | warmup_steps : int , | factor : float = 1.0 , | last_epoch : int = - 1 | ) -> None Implements the Noam Learning rate schedule. This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number, scaled by the inverse square root of the dimensionality of the model. Time will tell if this is just madness or it's actually important. The formula for learning rate when using NoamLR is: lr = factor * ( model_size ** ( -0.5 ) * min( step** ( -0.5 ), step * warmup_steps ** ( -1.5 )) ) Registered as a LearningRateScheduler with name \"noam\".","title":"NoamLR"},{"location":"api/training/learning_rate_schedulers/noam/#step","text":"class NoamLR ( LearningRateScheduler ): | ... | def step ( self , metric : float = None ) -> None","title":"step"},{"location":"api/training/learning_rate_schedulers/noam/#step_batch","text":"class NoamLR ( LearningRateScheduler ): | ... | def step_batch ( self , batch_num_total : int = None ) -> None","title":"step_batch"},{"location":"api/training/learning_rate_schedulers/noam/#get_values","text":"class NoamLR ( LearningRateScheduler ): | ... | def get_values ( self )","title":"get_values"},{"location":"api/training/learning_rate_schedulers/polynomial_decay/","text":"allennlp .training .learning_rate_schedulers .polynomial_decay [SOURCE] PolynomialDecay \u00b6 @LearningRateScheduler . register ( \"polynomial_decay\" ) class PolynomialDecay ( LearningRateScheduler ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | num_epochs : int , | num_steps_per_epoch : int , | power = 1.0 , | warmup_steps = 0 , | end_learning_rate = 0.0 , | last_epoch : int = - 1 | ) Implements polynomial decay Learning rate scheduling. The learning rate is first linearly increased for the first warmup_steps training steps. Then it is decayed for total_steps - warmup_steps from the initial learning rate to end_learning_rate using a polynomial of degree power . Formally, lr = ( initial_lr - end_learning_rate ) * (( total_steps - steps )/( total_steps - warmup_steps )) ** power Parameters \u00b6 optimizer : torch.optim.Optimizer This argument does not get an entry in a configuration file for the object. num_epochs : int The number of epochs in the experiment. this does NOT get an entry in the config. num_steps_per_epoch : int The number of steps per epoch. this does NOT get an entry in the config. warmup_steps : int The number of steps to linearly increase the learning rate. power : float , optional (default = 1.0 ) The power of the polynomial used for decaying. end_learning_rate : float , optional (default = 0.0 ) Final learning rate to decay towards. Example \u00b6 Config for using the PolynomialDecay Learning Rate Scheduler with warmup_steps set 100 , power set to 2 , and end_learning_rate set to 1e-10 . { ... \"trainer\" :{ ... \"learning_rate_scheduler\" : { \"type\" : \"polynomial_decay\" , \"power\" : 2 , \"warmup_steps\" : 100 , \"end_learning_rate\" : 1e-10 }, ... } } Note that you do NOT pass a optimizer , num_epochs , nor num_steps_per_epoch key to the Learning rate scheduler. get_values \u00b6 class PolynomialDecay ( LearningRateScheduler ): | ... | def get_values ( self ) step \u00b6 class PolynomialDecay ( LearningRateScheduler ): | ... | def step ( self , metric : float = None ) -> None step_batch \u00b6 class PolynomialDecay ( LearningRateScheduler ): | ... | def step_batch ( self , batch_num_total : int = None ) -> None","title":"polynomial_decay"},{"location":"api/training/learning_rate_schedulers/polynomial_decay/#polynomialdecay","text":"@LearningRateScheduler . register ( \"polynomial_decay\" ) class PolynomialDecay ( LearningRateScheduler ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | num_epochs : int , | num_steps_per_epoch : int , | power = 1.0 , | warmup_steps = 0 , | end_learning_rate = 0.0 , | last_epoch : int = - 1 | ) Implements polynomial decay Learning rate scheduling. The learning rate is first linearly increased for the first warmup_steps training steps. Then it is decayed for total_steps - warmup_steps from the initial learning rate to end_learning_rate using a polynomial of degree power . Formally, lr = ( initial_lr - end_learning_rate ) * (( total_steps - steps )/( total_steps - warmup_steps )) ** power","title":"PolynomialDecay"},{"location":"api/training/learning_rate_schedulers/polynomial_decay/#get_values","text":"class PolynomialDecay ( LearningRateScheduler ): | ... | def get_values ( self )","title":"get_values"},{"location":"api/training/learning_rate_schedulers/polynomial_decay/#step","text":"class PolynomialDecay ( LearningRateScheduler ): | ... | def step ( self , metric : float = None ) -> None","title":"step"},{"location":"api/training/learning_rate_schedulers/polynomial_decay/#step_batch","text":"class PolynomialDecay ( LearningRateScheduler ): | ... | def step_batch ( self , batch_num_total : int = None ) -> None","title":"step_batch"},{"location":"api/training/learning_rate_schedulers/pytorch_lr_schedulers/","text":"allennlp .training .learning_rate_schedulers .pytorch_lr_schedulers [SOURCE] Wrappers for PyTorch's Learning Rate Schedulers so that they work with AllenNLP StepLearningRateScheduler \u00b6 @LearningRateScheduler . register ( \"step\" ) class StepLearningRateScheduler ( _PyTorchLearningRateSchedulerWrapper ): | def __init__ ( | self , | optimizer : Optimizer , | step_size : int , | gamma : float = 0.1 , | last_epoch : int = - 1 | ) -> None Wrapper for PyTorch's StepLR . Registered as a LearningRateScheduler with name \"step\". The \"optimizer\" argument does not get an entry in a configuration file for the object. Parameters \u00b6 optimizer : torch.optim.Optimizer This argument does not get an entry in a configuration file for the object. step_size : int Period of learning rate decay. gamma : float , optional (default = 0.1 ) Multiplicative factor of learning rate decay. last_epoch : int , optional (default = -1 ) The index of the last epoch. This is used when restarting. Example \u00b6 Config for using the StepLearningRateScheduler Learning Rate Scheduler with step_size of 100 and gamma set 0.2 . { ... \"trainer\" :{ ... \"learning_rate_scheduler\" : { \"type\" : \"step\" , \"step_size\" : 100 , \"gamma\" : 0.2 }, ... } } Note that you do NOT pass a optimizer key to the Learning rate scheduler. MultiStepLearningRateScheduler \u00b6 @LearningRateScheduler . register ( \"multi_step\" ) class MultiStepLearningRateScheduler ( _PyTorchLearningRateSchedulerWrapper ): | def __init__ ( | self , | optimizer : Optimizer , | milestones : List [ int ], | gamma : float = 0.1 , | last_epoch : int = - 1 | ) -> None Wrapper for PyTorch's MultiStepLR . Registered as a LearningRateScheduler with name \"multi_step\". The \"optimizer\" argument does not get an entry in a configuration file for the object. Example \u00b6 Config for using the MultiStepLearningRateScheduler Learning Rate Scheduler with milestones of [10,20,40] and gamma set 0.2 . { ... \"trainer\" :{ ... \"learning_rate_scheduler\" : { \"type\" : \"multi_step\" , \"milestones\" : [ 10 , 20 , 40 ], \"gamma\" : 0.2 }, ... } } Note that you do NOT pass a optimizer key to the Learning rate scheduler. ExponentialLearningRateScheduler \u00b6 @LearningRateScheduler . register ( \"exponential\" ) class ExponentialLearningRateScheduler ( _PyTorchLearningRateSchedulerWrapper ): | def __init__ ( | self , | optimizer : Optimizer , | gamma : float = 0.1 , | last_epoch : int = - 1 | ) -> None Wrapper for PyTorch's ExponentialLR . Registered as a LearningRateScheduler with name \"exponential\". The \"optimizer\" argument does not get an entry in a configuration file for the object. Example \u00b6 Config for using the ExponentialLearningRateScheduler Learning Rate Scheduler with gamma set 0.2 . { ... \"trainer\" :{ ... \"learning_rate_scheduler\" : { \"type\" : \"exponential\" , \"gamma\" : 0.2 }, ... } } Note that you do NOT pass a optimizer key to the Learning rate scheduler. ReduceOnPlateauLearningRateScheduler \u00b6 @LearningRateScheduler . register ( \"reduce_on_plateau\" ) class ReduceOnPlateauLearningRateScheduler ( _PyTorchLearningRateSchedulerWithMetricsWrapper ): | def __init__ ( | self , | optimizer : Optimizer , | mode : str = \"min\" , | factor : float = 0.1 , | patience : int = 10 , | verbose : bool = False , | threshold_mode : str = \"rel\" , | threshold : float = 1e-4 , | cooldown : int = 0 , | min_lr : Union [ float , List [ float ]] = 0 , | eps : float = 1e-8 | ) -> None Wrapper for PyTorch's ReduceLROnPlateau . Registered as a LearningRateScheduler with name \"reduce_on_plateau\". The \"optimizer\" argument does not get an entry in a configuration file for the object. # Example Config for using the ReduceOnPlateauLearningRateScheduler Learning Rate Scheduler with the following init arguments: mode=\"max\" factor=0.2 patience=5 threshold=5e-3 threshold_mode=\"abs\" cooldown=2 min_lr=1e-12 eps=1e-10 { ... \"trainer\" :{ ... \"learning_rate_scheduler\" : { \"type\" : \"reduce_on_plateau\" , \"mode\" : \"max\" , \"factor\" : 0.2 , \"patience\" : 5 , \"threshold\" : 5e-3 , \"threshold_mode\" : \"abs\" , \"cooldown\" : 2 , \"min_lr\" : 1e-12 , \"eps\" : 1e-10 }, ... } } Note that you do NOT pass a optimizer key to the Learning rate scheduler.","title":"pytorch_lr_schedulers"},{"location":"api/training/learning_rate_schedulers/pytorch_lr_schedulers/#steplearningratescheduler","text":"@LearningRateScheduler . register ( \"step\" ) class StepLearningRateScheduler ( _PyTorchLearningRateSchedulerWrapper ): | def __init__ ( | self , | optimizer : Optimizer , | step_size : int , | gamma : float = 0.1 , | last_epoch : int = - 1 | ) -> None Wrapper for PyTorch's StepLR . Registered as a LearningRateScheduler with name \"step\". The \"optimizer\" argument does not get an entry in a configuration file for the object.","title":"StepLearningRateScheduler"},{"location":"api/training/learning_rate_schedulers/pytorch_lr_schedulers/#multisteplearningratescheduler","text":"@LearningRateScheduler . register ( \"multi_step\" ) class MultiStepLearningRateScheduler ( _PyTorchLearningRateSchedulerWrapper ): | def __init__ ( | self , | optimizer : Optimizer , | milestones : List [ int ], | gamma : float = 0.1 , | last_epoch : int = - 1 | ) -> None Wrapper for PyTorch's MultiStepLR . Registered as a LearningRateScheduler with name \"multi_step\". The \"optimizer\" argument does not get an entry in a configuration file for the object.","title":"MultiStepLearningRateScheduler"},{"location":"api/training/learning_rate_schedulers/pytorch_lr_schedulers/#exponentiallearningratescheduler","text":"@LearningRateScheduler . register ( \"exponential\" ) class ExponentialLearningRateScheduler ( _PyTorchLearningRateSchedulerWrapper ): | def __init__ ( | self , | optimizer : Optimizer , | gamma : float = 0.1 , | last_epoch : int = - 1 | ) -> None Wrapper for PyTorch's ExponentialLR . Registered as a LearningRateScheduler with name \"exponential\". The \"optimizer\" argument does not get an entry in a configuration file for the object.","title":"ExponentialLearningRateScheduler"},{"location":"api/training/learning_rate_schedulers/pytorch_lr_schedulers/#reduceonplateaulearningratescheduler","text":"@LearningRateScheduler . register ( \"reduce_on_plateau\" ) class ReduceOnPlateauLearningRateScheduler ( _PyTorchLearningRateSchedulerWithMetricsWrapper ): | def __init__ ( | self , | optimizer : Optimizer , | mode : str = \"min\" , | factor : float = 0.1 , | patience : int = 10 , | verbose : bool = False , | threshold_mode : str = \"rel\" , | threshold : float = 1e-4 , | cooldown : int = 0 , | min_lr : Union [ float , List [ float ]] = 0 , | eps : float = 1e-8 | ) -> None Wrapper for PyTorch's ReduceLROnPlateau . Registered as a LearningRateScheduler with name \"reduce_on_plateau\". The \"optimizer\" argument does not get an entry in a configuration file for the object. # Example Config for using the ReduceOnPlateauLearningRateScheduler Learning Rate Scheduler with the following init arguments: mode=\"max\" factor=0.2 patience=5 threshold=5e-3 threshold_mode=\"abs\" cooldown=2 min_lr=1e-12 eps=1e-10 { ... \"trainer\" :{ ... \"learning_rate_scheduler\" : { \"type\" : \"reduce_on_plateau\" , \"mode\" : \"max\" , \"factor\" : 0.2 , \"patience\" : 5 , \"threshold\" : 5e-3 , \"threshold_mode\" : \"abs\" , \"cooldown\" : 2 , \"min_lr\" : 1e-12 , \"eps\" : 1e-10 }, ... } } Note that you do NOT pass a optimizer key to the Learning rate scheduler.","title":"ReduceOnPlateauLearningRateScheduler"},{"location":"api/training/learning_rate_schedulers/slanted_triangular/","text":"allennlp .training .learning_rate_schedulers .slanted_triangular [SOURCE] SlantedTriangular \u00b6 @LearningRateScheduler . register ( \"slanted_triangular\" ) class SlantedTriangular ( LearningRateScheduler ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | num_epochs : int , | num_steps_per_epoch : Optional [ int ] = None , | cut_frac : float = 0.1 , | ratio : int = 32 , | last_epoch : int = - 1 , | gradual_unfreezing : bool = False , | discriminative_fine_tuning : bool = False , | decay_factor : float = 0.38 | ) -> None Implements the Slanted Triangular Learning Rate schedule with optional gradual unfreezing and discriminative fine-tuning. The schedule corresponds to first linearly increasing the learning rate over some number of epochs, and then linearly decreasing it over the remaining epochs. If we gradually unfreeze, then in the first epoch of training, only the top layer is trained; in the second epoch, the top two layers are trained, etc. During freezing, the learning rate is increased and annealed over one epoch. After freezing finished, the learning rate is increased and annealed over the remaining training iterations. Note that with this schedule, early stopping should typically be avoided. Registered as a LearningRateScheduler with name \"slanted_triangular\". Parameters \u00b6 optimizer : torch.optim.Optimizer This argument does not get an entry in a configuration file for the object. num_epochs : int The total number of epochs for which the model should be trained. num_steps_per_epoch : Optional[int] , optional (default = None ) The number of steps (updates, batches) per training epoch. cut_frac : float , optional (default = 0.1 ) The fraction of the steps to increase the learning rate. ratio : float , optional (default = 32 ) The ratio of the smallest to the (largest) base learning rate. gradual_unfreezing : bool , optional (default = False ) Whether gradual unfreezing should be used. discriminative_fine_tuning : bool , optional (default = False ) Whether discriminative fine-tuning (different learning rates per layer) are used. decay_factor : float , optional (default = 0.38 ) The decay factor by which the learning rate is reduced with discriminative fine-tuning when going a layer deeper. step \u00b6 class SlantedTriangular ( LearningRateScheduler ): | ... | def step ( self , metric : float = None ) -> None step_batch \u00b6 class SlantedTriangular ( LearningRateScheduler ): | ... | def step_batch ( self , batch_num_total : int = None ) get_values \u00b6 class SlantedTriangular ( LearningRateScheduler ): | ... | def get_values ( self )","title":"slanted_triangular"},{"location":"api/training/learning_rate_schedulers/slanted_triangular/#slantedtriangular","text":"@LearningRateScheduler . register ( \"slanted_triangular\" ) class SlantedTriangular ( LearningRateScheduler ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | num_epochs : int , | num_steps_per_epoch : Optional [ int ] = None , | cut_frac : float = 0.1 , | ratio : int = 32 , | last_epoch : int = - 1 , | gradual_unfreezing : bool = False , | discriminative_fine_tuning : bool = False , | decay_factor : float = 0.38 | ) -> None Implements the Slanted Triangular Learning Rate schedule with optional gradual unfreezing and discriminative fine-tuning. The schedule corresponds to first linearly increasing the learning rate over some number of epochs, and then linearly decreasing it over the remaining epochs. If we gradually unfreeze, then in the first epoch of training, only the top layer is trained; in the second epoch, the top two layers are trained, etc. During freezing, the learning rate is increased and annealed over one epoch. After freezing finished, the learning rate is increased and annealed over the remaining training iterations. Note that with this schedule, early stopping should typically be avoided. Registered as a LearningRateScheduler with name \"slanted_triangular\".","title":"SlantedTriangular"},{"location":"api/training/learning_rate_schedulers/slanted_triangular/#step","text":"class SlantedTriangular ( LearningRateScheduler ): | ... | def step ( self , metric : float = None ) -> None","title":"step"},{"location":"api/training/learning_rate_schedulers/slanted_triangular/#step_batch","text":"class SlantedTriangular ( LearningRateScheduler ): | ... | def step_batch ( self , batch_num_total : int = None )","title":"step_batch"},{"location":"api/training/learning_rate_schedulers/slanted_triangular/#get_values","text":"class SlantedTriangular ( LearningRateScheduler ): | ... | def get_values ( self )","title":"get_values"},{"location":"api/training/metrics/attachment_scores/","text":"allennlp .training .metrics .attachment_scores [SOURCE] AttachmentScores \u00b6 @Metric . register ( \"attachment_scores\" ) class AttachmentScores ( Metric ): | def __init__ ( self , ignore_classes : List [ int ] = None ) -> None Computes labeled and unlabeled attachment scores for a dependency parse, as well as sentence level exact match for both labeled and unlabeled trees. Note that the input to this metric is the sampled predictions, not the distribution itself. Parameters \u00b6 ignore_classes : List[int] , optional (default = None ) A list of label ids to ignore when computing metrics. __call__ \u00b6 class AttachmentScores ( Metric ): | ... | def __call__ ( | self , | predicted_indices : torch . Tensor , | predicted_labels : torch . Tensor , | gold_indices : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters \u00b6 predicted_indices : torch.Tensor A tensor of head index predictions of shape (batch_size, timesteps). predicted_labels : torch.Tensor A tensor of arc label predictions of shape (batch_size, timesteps). gold_indices : torch.Tensor A tensor of the same shape as predicted_indices . gold_labels : torch.Tensor A tensor of the same shape as predicted_labels . mask : torch.BoolTensor , optional (default = None ) A tensor of the same shape as predicted_indices . get_metric \u00b6 class AttachmentScores ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns \u00b6 The accumulated metrics as a dictionary. reset \u00b6 class AttachmentScores ( Metric ): | ... | def reset ( self )","title":"attachment_scores"},{"location":"api/training/metrics/attachment_scores/#attachmentscores","text":"@Metric . register ( \"attachment_scores\" ) class AttachmentScores ( Metric ): | def __init__ ( self , ignore_classes : List [ int ] = None ) -> None Computes labeled and unlabeled attachment scores for a dependency parse, as well as sentence level exact match for both labeled and unlabeled trees. Note that the input to this metric is the sampled predictions, not the distribution itself.","title":"AttachmentScores"},{"location":"api/training/metrics/attachment_scores/#__call__","text":"class AttachmentScores ( Metric ): | ... | def __call__ ( | self , | predicted_indices : torch . Tensor , | predicted_labels : torch . Tensor , | gold_indices : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | )","title":"__call__"},{"location":"api/training/metrics/attachment_scores/#get_metric","text":"class AttachmentScores ( Metric ): | ... | def get_metric ( self , reset : bool = False )","title":"get_metric"},{"location":"api/training/metrics/attachment_scores/#reset","text":"class AttachmentScores ( Metric ): | ... | def reset ( self )","title":"reset"},{"location":"api/training/metrics/auc/","text":"allennlp .training .metrics .auc [SOURCE] Auc \u00b6 @Metric . register ( \"auc\" ) class Auc ( Metric ): | def __init__ ( self , positive_label = 1 ) The AUC Metric measures the area under the receiver-operating characteristic (ROC) curve for binary classification problems. __call__ \u00b6 class Auc ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters \u00b6 predictions : torch.Tensor A one-dimensional tensor of prediction scores of shape (batch_size). gold_labels : torch.Tensor A one-dimensional label tensor of shape (batch_size), with {1, 0} entries for positive and negative class. If it's not binary, positive_label should be passed in the initialization. mask : torch.BoolTensor , optional (default = None ) A one-dimensional label tensor of shape (batch_size). get_metric \u00b6 class Auc ( Metric ): | ... | def get_metric ( self , reset : bool = False ) reset \u00b6 class Auc ( Metric ): | ... | def reset ( self )","title":"auc"},{"location":"api/training/metrics/auc/#auc","text":"@Metric . register ( \"auc\" ) class Auc ( Metric ): | def __init__ ( self , positive_label = 1 ) The AUC Metric measures the area under the receiver-operating characteristic (ROC) curve for binary classification problems.","title":"Auc"},{"location":"api/training/metrics/auc/#__call__","text":"class Auc ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | )","title":"__call__"},{"location":"api/training/metrics/auc/#get_metric","text":"class Auc ( Metric ): | ... | def get_metric ( self , reset : bool = False )","title":"get_metric"},{"location":"api/training/metrics/auc/#reset","text":"class Auc ( Metric ): | ... | def reset ( self )","title":"reset"},{"location":"api/training/metrics/average/","text":"allennlp .training .metrics .average [SOURCE] Average \u00b6 @Metric . register ( \"average\" ) class Average ( Metric ): | def __init__ ( self ) -> None This Metric breaks with the typical Metric API and just stores values that were computed in some fashion outside of a Metric . If you have some external code that computes the metric for you, for instance, you can use this to report the average result using our Metric API. __call__ \u00b6 class Average ( Metric ): | ... | def __call__ ( self , value ) Parameters \u00b6 value : float The value to average. get_metric \u00b6 class Average ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns \u00b6 The average of all values that were passed to __call__ . reset \u00b6 class Average ( Metric ): | ... | def reset ( self )","title":"average"},{"location":"api/training/metrics/average/#average","text":"@Metric . register ( \"average\" ) class Average ( Metric ): | def __init__ ( self ) -> None This Metric breaks with the typical Metric API and just stores values that were computed in some fashion outside of a Metric . If you have some external code that computes the metric for you, for instance, you can use this to report the average result using our Metric API.","title":"Average"},{"location":"api/training/metrics/average/#__call__","text":"class Average ( Metric ): | ... | def __call__ ( self , value )","title":"__call__"},{"location":"api/training/metrics/average/#get_metric","text":"class Average ( Metric ): | ... | def get_metric ( self , reset : bool = False )","title":"get_metric"},{"location":"api/training/metrics/average/#reset","text":"class Average ( Metric ): | ... | def reset ( self )","title":"reset"},{"location":"api/training/metrics/bleu/","text":"allennlp .training .metrics .bleu [SOURCE] BLEU \u00b6 @Metric . register ( \"bleu\" ) class BLEU ( Metric ): | def __init__ ( | self , | ngram_weights : Iterable [ float ] = ( 0.25 , 0.25 , 0.25 , 0.25 ), | exclude_indices : Set [ int ] = None | ) -> None Bilingual Evaluation Understudy (BLEU). BLEU is a common metric used for evaluating the quality of machine translations against a set of reference translations. See Papineni et. al., \"BLEU: a method for automatic evaluation of machine translation\", 2002 . Parameters \u00b6 ngram_weights : Iterable[float] , optional (default = (0.25, 0.25, 0.25, 0.25) ) Weights to assign to scores for each ngram size. exclude_indices : Set[int] , optional (default = None ) Indices to exclude when calculating ngrams. This should usually include the indices of the start, end, and pad tokens. Notes \u00b6 We chose to implement this from scratch instead of wrapping an existing implementation (such as nltk.translate.bleu_score ) for a two reasons. First, so that we could pass tensors directly to this metric instead of first converting the tensors to lists of strings. And second, because functions like nltk.translate.bleu_score.corpus_bleu() are meant to be called once over the entire corpus, whereas it is more efficient in our use case to update the running precision counts every batch. This implementation only considers a reference set of size 1, i.e. a single gold target sequence for each predicted sequence. reset \u00b6 class BLEU ( Metric ): | ... | def reset ( self ) -> None __call__ \u00b6 class BLEU ( Metric ): | ... | def __call__ ( | self , | predictions : torch . LongTensor , | gold_targets : torch . LongTensor , | mask : Optional [ torch . BoolTensor ] = None | ) -> None Update precision counts. Parameters \u00b6 predictions : torch.LongTensor Batched predicted tokens of shape (batch_size, max_sequence_length) . references : torch.LongTensor Batched reference (gold) translations with shape (batch_size, max_gold_sequence_length) . Returns \u00b6 None get_metric \u00b6 class BLEU ( Metric ): | ... | def get_metric ( self , reset : bool = False ) -> Dict [ str , float ]","title":"bleu"},{"location":"api/training/metrics/bleu/#bleu","text":"@Metric . register ( \"bleu\" ) class BLEU ( Metric ): | def __init__ ( | self , | ngram_weights : Iterable [ float ] = ( 0.25 , 0.25 , 0.25 , 0.25 ), | exclude_indices : Set [ int ] = None | ) -> None Bilingual Evaluation Understudy (BLEU). BLEU is a common metric used for evaluating the quality of machine translations against a set of reference translations. See Papineni et. al., \"BLEU: a method for automatic evaluation of machine translation\", 2002 .","title":"BLEU"},{"location":"api/training/metrics/bleu/#reset","text":"class BLEU ( Metric ): | ... | def reset ( self ) -> None","title":"reset"},{"location":"api/training/metrics/bleu/#__call__","text":"class BLEU ( Metric ): | ... | def __call__ ( | self , | predictions : torch . LongTensor , | gold_targets : torch . LongTensor , | mask : Optional [ torch . BoolTensor ] = None | ) -> None Update precision counts.","title":"__call__"},{"location":"api/training/metrics/bleu/#get_metric","text":"class BLEU ( Metric ): | ... | def get_metric ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metric"},{"location":"api/training/metrics/boolean_accuracy/","text":"allennlp .training .metrics .boolean_accuracy [SOURCE] BooleanAccuracy \u00b6 @Metric . register ( \"boolean_accuracy\" ) class BooleanAccuracy ( Metric ): | def __init__ ( self ) -> None Just checks batch-equality of two tensors and computes an accuracy metric based on that. That is, if your prediction has shape (batch_size, dim_1, ..., dim_n), this metric considers that as a set of batch_size predictions and checks that each is entirely correct across the remaining dims. This means the denominator in the accuracy computation is batch_size , with the caveat that predictions that are totally masked are ignored (in which case the denominator is the number of predictions that have at least one unmasked element). This is similar to CategoricalAccuracy , if you've already done a .max() on your predictions. If you have categorical output, though, you should typically just use CategoricalAccuracy . The reason you might want to use this instead is if you've done some kind of constrained inference and don't have a prediction tensor that matches the API of CategoricalAccuracy , which assumes a final dimension of size num_classes . __call__ \u00b6 class BooleanAccuracy ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters \u00b6 predictions : torch.Tensor A tensor of predictions of shape (batch_size, ...). gold_labels : torch.Tensor A tensor of the same shape as predictions . mask : torch.BoolTensor , optional (default = None ) A tensor of the same shape as predictions . get_metric \u00b6 class BooleanAccuracy ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns \u00b6 The accumulated accuracy. reset \u00b6 class BooleanAccuracy ( Metric ): | ... | def reset ( self )","title":"boolean_accuracy"},{"location":"api/training/metrics/boolean_accuracy/#booleanaccuracy","text":"@Metric . register ( \"boolean_accuracy\" ) class BooleanAccuracy ( Metric ): | def __init__ ( self ) -> None Just checks batch-equality of two tensors and computes an accuracy metric based on that. That is, if your prediction has shape (batch_size, dim_1, ..., dim_n), this metric considers that as a set of batch_size predictions and checks that each is entirely correct across the remaining dims. This means the denominator in the accuracy computation is batch_size , with the caveat that predictions that are totally masked are ignored (in which case the denominator is the number of predictions that have at least one unmasked element). This is similar to CategoricalAccuracy , if you've already done a .max() on your predictions. If you have categorical output, though, you should typically just use CategoricalAccuracy . The reason you might want to use this instead is if you've done some kind of constrained inference and don't have a prediction tensor that matches the API of CategoricalAccuracy , which assumes a final dimension of size num_classes .","title":"BooleanAccuracy"},{"location":"api/training/metrics/boolean_accuracy/#__call__","text":"class BooleanAccuracy ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | )","title":"__call__"},{"location":"api/training/metrics/boolean_accuracy/#get_metric","text":"class BooleanAccuracy ( Metric ): | ... | def get_metric ( self , reset : bool = False )","title":"get_metric"},{"location":"api/training/metrics/boolean_accuracy/#reset","text":"class BooleanAccuracy ( Metric ): | ... | def reset ( self )","title":"reset"},{"location":"api/training/metrics/categorical_accuracy/","text":"allennlp .training .metrics .categorical_accuracy [SOURCE] CategoricalAccuracy \u00b6 @Metric . register ( \"categorical_accuracy\" ) class CategoricalAccuracy ( Metric ): | def __init__ ( self , top_k : int = 1 , tie_break : bool = False ) -> None Categorical Top-K accuracy. Assumes integer labels, with each item to be classified having a single correct class. Tie break enables equal distribution of scores among the classes with same maximum predicted scores. supports_distributed \u00b6 class CategoricalAccuracy ( Metric ): | ... | supports_distributed = True __call__ \u00b6 class CategoricalAccuracy ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters \u00b6 predictions : torch.Tensor A tensor of predictions of shape (batch_size, ..., num_classes). gold_labels : torch.Tensor A tensor of integer class label of shape (batch_size, ...). It must be the same shape as the predictions tensor without the num_classes dimension. mask : torch.BoolTensor , optional (default = None ) A masking tensor the same size as gold_labels . get_metric \u00b6 class CategoricalAccuracy ( Metric ): | ... | def get_metric ( self , reset : bool = False ) -> float Returns \u00b6 The accumulated accuracy. reset \u00b6 class CategoricalAccuracy ( Metric ): | ... | def reset ( self )","title":"categorical_accuracy"},{"location":"api/training/metrics/categorical_accuracy/#categoricalaccuracy","text":"@Metric . register ( \"categorical_accuracy\" ) class CategoricalAccuracy ( Metric ): | def __init__ ( self , top_k : int = 1 , tie_break : bool = False ) -> None Categorical Top-K accuracy. Assumes integer labels, with each item to be classified having a single correct class. Tie break enables equal distribution of scores among the classes with same maximum predicted scores.","title":"CategoricalAccuracy"},{"location":"api/training/metrics/categorical_accuracy/#supports_distributed","text":"class CategoricalAccuracy ( Metric ): | ... | supports_distributed = True","title":"supports_distributed"},{"location":"api/training/metrics/categorical_accuracy/#__call__","text":"class CategoricalAccuracy ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | )","title":"__call__"},{"location":"api/training/metrics/categorical_accuracy/#get_metric","text":"class CategoricalAccuracy ( Metric ): | ... | def get_metric ( self , reset : bool = False ) -> float","title":"get_metric"},{"location":"api/training/metrics/categorical_accuracy/#reset","text":"class CategoricalAccuracy ( Metric ): | ... | def reset ( self )","title":"reset"},{"location":"api/training/metrics/covariance/","text":"allennlp .training .metrics .covariance [SOURCE] Covariance \u00b6 @Metric . register ( \"covariance\" ) class Covariance ( Metric ): | def __init__ ( self ) -> None This Metric calculates the unbiased sample covariance between two tensors. Each element in the two tensors is assumed to be a different observation of the variable (i.e., the input tensors are implicitly flattened into vectors and the covariance is calculated between the vectors). This implementation is mostly modeled after the streaming_covariance function in Tensorflow. See: https://github.com/tensorflow/tensorflow/blob/v1.10.1/tensorflow/contrib/metrics/python/ops/metric_ops.py#L3127 The following is copied from the Tensorflow documentation: The algorithm used for this online computation is described in https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online . Specifically, the formula used to combine two sample comoments is C_AB = C_A + C_B + (E[x_A] - E[x_B]) * (E[y_A] - E[y_B]) * n_A * n_B / n_AB The comoment for a single batch of data is simply sum((x - E[x]) * (y - E[y])) , optionally masked. __call__ \u00b6 class Covariance ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters \u00b6 predictions : torch.Tensor A tensor of predictions of shape (batch_size, ...). gold_labels : torch.Tensor A tensor of the same shape as predictions . mask : torch.BoolTensor , optional (default = None ) A tensor of the same shape as predictions . get_metric \u00b6 class Covariance ( Metric ): | ... | def get_metric ( self , reset : bool = False ) -> float Returns \u00b6 The accumulated covariance. reset \u00b6 class Covariance ( Metric ): | ... | def reset ( self )","title":"covariance"},{"location":"api/training/metrics/covariance/#covariance","text":"@Metric . register ( \"covariance\" ) class Covariance ( Metric ): | def __init__ ( self ) -> None This Metric calculates the unbiased sample covariance between two tensors. Each element in the two tensors is assumed to be a different observation of the variable (i.e., the input tensors are implicitly flattened into vectors and the covariance is calculated between the vectors). This implementation is mostly modeled after the streaming_covariance function in Tensorflow. See: https://github.com/tensorflow/tensorflow/blob/v1.10.1/tensorflow/contrib/metrics/python/ops/metric_ops.py#L3127 The following is copied from the Tensorflow documentation: The algorithm used for this online computation is described in https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online . Specifically, the formula used to combine two sample comoments is C_AB = C_A + C_B + (E[x_A] - E[x_B]) * (E[y_A] - E[y_B]) * n_A * n_B / n_AB The comoment for a single batch of data is simply sum((x - E[x]) * (y - E[y])) , optionally masked.","title":"Covariance"},{"location":"api/training/metrics/covariance/#__call__","text":"class Covariance ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | )","title":"__call__"},{"location":"api/training/metrics/covariance/#get_metric","text":"class Covariance ( Metric ): | ... | def get_metric ( self , reset : bool = False ) -> float","title":"get_metric"},{"location":"api/training/metrics/covariance/#reset","text":"class Covariance ( Metric ): | ... | def reset ( self )","title":"reset"},{"location":"api/training/metrics/entropy/","text":"allennlp .training .metrics .entropy [SOURCE] Entropy \u00b6 @Metric . register ( \"entropy\" ) class Entropy ( Metric ): | def __init__ ( self ) -> None __call__ \u00b6 class Entropy ( Metric ): | ... | def __call__ ( | self , | logits : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters \u00b6 logits : torch.Tensor A tensor of unnormalized log probabilities of shape (batch_size, ..., num_classes). mask : torch.BoolTensor , optional (default = None ) A masking tensor of shape (batch_size, ...). get_metric \u00b6 class Entropy ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns \u00b6 The scalar average entropy. reset \u00b6 class Entropy ( Metric ): | ... | def reset ( self )","title":"entropy"},{"location":"api/training/metrics/entropy/#entropy","text":"@Metric . register ( \"entropy\" ) class Entropy ( Metric ): | def __init__ ( self ) -> None","title":"Entropy"},{"location":"api/training/metrics/entropy/#__call__","text":"class Entropy ( Metric ): | ... | def __call__ ( | self , | logits : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | )","title":"__call__"},{"location":"api/training/metrics/entropy/#get_metric","text":"class Entropy ( Metric ): | ... | def get_metric ( self , reset : bool = False )","title":"get_metric"},{"location":"api/training/metrics/entropy/#reset","text":"class Entropy ( Metric ): | ... | def reset ( self )","title":"reset"},{"location":"api/training/metrics/evalb_bracketing_scorer/","text":"allennlp .training .metrics .evalb_bracketing_scorer [SOURCE] DEFAULT_EVALB_DIR \u00b6 DEFAULT_EVALB_DIR = os . path . abspath ( os . path . join ( os . path . dirname ( os . path . realpath ( __file__ )), os . pardir , o ... EvalbBracketingScorer \u00b6 @Metric . register ( \"evalb\" ) class EvalbBracketingScorer ( Metric ): | def __init__ ( | self , | evalb_directory_path : str = DEFAULT_EVALB_DIR , | evalb_param_filename : str = \"COLLINS.prm\" , | evalb_num_errors_to_kill : int = 10 | ) -> None This class uses the external EVALB software for computing a broad range of metrics on parse trees. Here, we use it to compute the Precision, Recall and F1 metrics. You can download the source for EVALB from here: https://nlp.cs.nyu.edu/evalb/ . Note that this software is 20 years old. In order to compile it on modern hardware, you may need to remove an include <malloc.h> statement in evalb.c before it will compile. AllenNLP contains the EVALB software, but you will need to compile it yourself before using it because the binary it generates is system dependent. To build it, run make inside the allennlp/tools/EVALB directory. Note that this metric reads and writes from disk quite a bit. You probably don't want to include it in your training loop; instead, you should calculate this on a validation set only. Parameters \u00b6 evalb_directory_path : str The directory containing the EVALB executable. evalb_param_filename : str , optional (default = \"COLLINS.prm\" ) The relative name of the EVALB configuration file used when scoring the trees. By default, this uses the COLLINS.prm configuration file which comes with EVALB. This configuration ignores POS tags and some punctuation labels. evalb_num_errors_to_kill : int , optional (default = \"10\" ) The number of errors to tolerate from EVALB before terminating evaluation. __call__ \u00b6 class EvalbBracketingScorer ( Metric ): | ... | def __call__ ( | self , | predicted_trees : List [ Tree ], | gold_trees : List [ Tree ] | ) -> None Parameters \u00b6 predicted_trees : List[Tree] A list of predicted NLTK Trees to compute score for. gold_trees : List[Tree] A list of gold NLTK Trees to use as a reference. get_metric \u00b6 class EvalbBracketingScorer ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns \u00b6 The average precision, recall and f1. reset \u00b6 class EvalbBracketingScorer ( Metric ): | ... | def reset ( self ) compile_evalb \u00b6 class EvalbBracketingScorer ( Metric ): | ... | @staticmethod | def compile_evalb ( evalb_directory_path : str = DEFAULT_EVALB_DIR ) clean_evalb \u00b6 class EvalbBracketingScorer ( Metric ): | ... | @staticmethod | def clean_evalb ( evalb_directory_path : str = DEFAULT_EVALB_DIR )","title":"evalb_bracketing_scorer"},{"location":"api/training/metrics/evalb_bracketing_scorer/#default_evalb_dir","text":"DEFAULT_EVALB_DIR = os . path . abspath ( os . path . join ( os . path . dirname ( os . path . realpath ( __file__ )), os . pardir , o ...","title":"DEFAULT_EVALB_DIR"},{"location":"api/training/metrics/evalb_bracketing_scorer/#evalbbracketingscorer","text":"@Metric . register ( \"evalb\" ) class EvalbBracketingScorer ( Metric ): | def __init__ ( | self , | evalb_directory_path : str = DEFAULT_EVALB_DIR , | evalb_param_filename : str = \"COLLINS.prm\" , | evalb_num_errors_to_kill : int = 10 | ) -> None This class uses the external EVALB software for computing a broad range of metrics on parse trees. Here, we use it to compute the Precision, Recall and F1 metrics. You can download the source for EVALB from here: https://nlp.cs.nyu.edu/evalb/ . Note that this software is 20 years old. In order to compile it on modern hardware, you may need to remove an include <malloc.h> statement in evalb.c before it will compile. AllenNLP contains the EVALB software, but you will need to compile it yourself before using it because the binary it generates is system dependent. To build it, run make inside the allennlp/tools/EVALB directory. Note that this metric reads and writes from disk quite a bit. You probably don't want to include it in your training loop; instead, you should calculate this on a validation set only.","title":"EvalbBracketingScorer"},{"location":"api/training/metrics/evalb_bracketing_scorer/#__call__","text":"class EvalbBracketingScorer ( Metric ): | ... | def __call__ ( | self , | predicted_trees : List [ Tree ], | gold_trees : List [ Tree ] | ) -> None","title":"__call__"},{"location":"api/training/metrics/evalb_bracketing_scorer/#get_metric","text":"class EvalbBracketingScorer ( Metric ): | ... | def get_metric ( self , reset : bool = False )","title":"get_metric"},{"location":"api/training/metrics/evalb_bracketing_scorer/#reset","text":"class EvalbBracketingScorer ( Metric ): | ... | def reset ( self )","title":"reset"},{"location":"api/training/metrics/evalb_bracketing_scorer/#compile_evalb","text":"class EvalbBracketingScorer ( Metric ): | ... | @staticmethod | def compile_evalb ( evalb_directory_path : str = DEFAULT_EVALB_DIR )","title":"compile_evalb"},{"location":"api/training/metrics/evalb_bracketing_scorer/#clean_evalb","text":"class EvalbBracketingScorer ( Metric ): | ... | @staticmethod | def clean_evalb ( evalb_directory_path : str = DEFAULT_EVALB_DIR )","title":"clean_evalb"},{"location":"api/training/metrics/f1_measure/","text":"allennlp .training .metrics .f1_measure [SOURCE] F1Measure \u00b6 @Metric . register ( \"f1\" ) class F1Measure ( FBetaMeasure ): | def __init__ ( self , positive_label : int ) -> None Computes Precision, Recall and F1 with respect to a given positive_label . For example, for a BIO tagging scheme, you would pass the classification index of the tag you are interested in, resulting in the Precision, Recall and F1 score being calculated for this tag only. get_metric \u00b6 class F1Measure ( FBetaMeasure ): | ... | def get_metric ( self , reset : bool = False ) -> Dict [ str , float ] Returns \u00b6 precision : float recall : float f1-measure : float","title":"f1_measure"},{"location":"api/training/metrics/f1_measure/#f1measure","text":"@Metric . register ( \"f1\" ) class F1Measure ( FBetaMeasure ): | def __init__ ( self , positive_label : int ) -> None Computes Precision, Recall and F1 with respect to a given positive_label . For example, for a BIO tagging scheme, you would pass the classification index of the tag you are interested in, resulting in the Precision, Recall and F1 score being calculated for this tag only.","title":"F1Measure"},{"location":"api/training/metrics/f1_measure/#get_metric","text":"class F1Measure ( FBetaMeasure ): | ... | def get_metric ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metric"},{"location":"api/training/metrics/fbeta_measure/","text":"allennlp .training .metrics .fbeta_measure [SOURCE] FBetaMeasure \u00b6 @Metric . register ( \"fbeta\" ) class FBetaMeasure ( Metric ): | def __init__ ( | self , | beta : float = 1.0 , | average : str = None , | labels : List [ int ] = None | ) -> None Compute precision, recall, F-measure and support for each class. The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples. The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0. If we have precision and recall, the F-beta score is simply: F-beta = (1 + beta ** 2) * precision * recall / (beta ** 2 * precision + recall) The F-beta score weights recall more than precision by a factor of beta . beta == 1.0 means recall and precision are equally important. The support is the number of occurrences of each class in y_true . Parameters \u00b6 beta : float , optional (default = 1.0 ) The strength of recall versus precision in the F-score. average : str , optional (default = None ) If None , the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data: 'micro' : Calculate metrics globally by counting the total true positives, false negatives and false positives. 'macro' : Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account. 'weighted' : Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters 'macro' to account for label imbalance; it can result in an F-score that is not between precision and recall. labels : list , optional The set of labels to include and their order if average is None . Labels present in the data can be excluded, for example to calculate a multi-class average ignoring a majority negative class. Labels not present in the data will result in 0 components in a macro or weighted average. __call__ \u00b6 class FBetaMeasure ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters \u00b6 predictions : torch.Tensor A tensor of predictions of shape (batch_size, ..., num_classes). gold_labels : torch.Tensor A tensor of integer class label of shape (batch_size, ...). It must be the same shape as the predictions tensor without the num_classes dimension. mask : torch.BoolTensor , optional (default = None ) A masking tensor the same size as gold_labels . get_metric \u00b6 class FBetaMeasure ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns \u00b6 precisions : List[float] recalls : List[float] f1-measures : List[float] Note If self.average is not None , you will get float instead of List[float] . reset \u00b6 class FBetaMeasure ( Metric ): | ... | def reset ( self ) -> None","title":"fbeta_measure"},{"location":"api/training/metrics/fbeta_measure/#fbetameasure","text":"@Metric . register ( \"fbeta\" ) class FBetaMeasure ( Metric ): | def __init__ ( | self , | beta : float = 1.0 , | average : str = None , | labels : List [ int ] = None | ) -> None Compute precision, recall, F-measure and support for each class. The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples. The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0. If we have precision and recall, the F-beta score is simply: F-beta = (1 + beta ** 2) * precision * recall / (beta ** 2 * precision + recall) The F-beta score weights recall more than precision by a factor of beta . beta == 1.0 means recall and precision are equally important. The support is the number of occurrences of each class in y_true .","title":"FBetaMeasure"},{"location":"api/training/metrics/fbeta_measure/#__call__","text":"class FBetaMeasure ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | )","title":"__call__"},{"location":"api/training/metrics/fbeta_measure/#get_metric","text":"class FBetaMeasure ( Metric ): | ... | def get_metric ( self , reset : bool = False )","title":"get_metric"},{"location":"api/training/metrics/fbeta_measure/#reset","text":"class FBetaMeasure ( Metric ): | ... | def reset ( self ) -> None","title":"reset"},{"location":"api/training/metrics/fbeta_multi_label_measure/","text":"allennlp .training .metrics .fbeta_multi_label_measure [SOURCE] FBetaMultiLabelMeasure \u00b6 @Metric . register ( \"fbeta_multi_label\" ) class FBetaMultiLabelMeasure ( FBetaMeasure ): | def __init__ ( | self , | beta : float = 1.0 , | average : str = None , | labels : List [ int ] = None , | threshold : float = 0.5 | ) -> None Compute precision, recall, F-measure and support for multi-label classification. The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples. The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0. If we have precision and recall, the F-beta score is simply: F-beta = (1 + beta ** 2) * precision * recall / (beta ** 2 * precision + recall) The F-beta score weights recall more than precision by a factor of beta . beta == 1.0 means recall and precision are equally important. The support is the number of occurrences of each class in y_true . Parameters \u00b6 beta : float , optional (default = 1.0 ) The strength of recall versus precision in the F-score. average : str , optional (default = None ) If None , the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data: 'micro' : Calculate metrics globally by counting the total true positives, false negatives and false positives. 'macro' : Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account. 'weighted' : Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters 'macro' to account for label imbalance; it can result in an F-score that is not between precision and recall. labels : list , optional The set of labels to include and their order if average is None . Labels present in the data can be excluded, for example to calculate a multi-class average ignoring a majority negative class. Labels not present in the data will result in 0 components in a macro or weighted average. threshold : float , optional (default = 0.5 ) Probabilities over this threshold will be considered predictions for the corresponding class. Note that you can also use this metric with logits, in which case it would make more sense to set the threshold value to 0.0 . __call__ \u00b6 class FBetaMultiLabelMeasure ( FBetaMeasure ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters \u00b6 predictions : torch.Tensor A tensor of predictions of shape (batch_size, ..., num_classes). gold_labels : torch.Tensor A tensor of boolean labels of shape (batch_size, ..., num_classes). It must be the same shape as the predictions . mask : torch.BoolTensor , optional (default = None ) A masking tensor the same size as gold_labels . F1MultiLabelMeasure \u00b6 @Metric . register ( \"f1_multi_label\" ) class F1MultiLabelMeasure ( FBetaMultiLabelMeasure ): | def __init__ ( | self , | average : str = None , | labels : List [ int ] = None , | threshold : float = 0.5 | ) -> None","title":"fbeta_multi_label_measure"},{"location":"api/training/metrics/fbeta_multi_label_measure/#fbetamultilabelmeasure","text":"@Metric . register ( \"fbeta_multi_label\" ) class FBetaMultiLabelMeasure ( FBetaMeasure ): | def __init__ ( | self , | beta : float = 1.0 , | average : str = None , | labels : List [ int ] = None , | threshold : float = 0.5 | ) -> None Compute precision, recall, F-measure and support for multi-label classification. The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples. The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0. If we have precision and recall, the F-beta score is simply: F-beta = (1 + beta ** 2) * precision * recall / (beta ** 2 * precision + recall) The F-beta score weights recall more than precision by a factor of beta . beta == 1.0 means recall and precision are equally important. The support is the number of occurrences of each class in y_true .","title":"FBetaMultiLabelMeasure"},{"location":"api/training/metrics/fbeta_multi_label_measure/#__call__","text":"class FBetaMultiLabelMeasure ( FBetaMeasure ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | )","title":"__call__"},{"location":"api/training/metrics/fbeta_multi_label_measure/#f1multilabelmeasure","text":"@Metric . register ( \"f1_multi_label\" ) class F1MultiLabelMeasure ( FBetaMultiLabelMeasure ): | def __init__ ( | self , | average : str = None , | labels : List [ int ] = None , | threshold : float = 0.5 | ) -> None","title":"F1MultiLabelMeasure"},{"location":"api/training/metrics/mean_absolute_error/","text":"allennlp .training .metrics .mean_absolute_error [SOURCE] MeanAbsoluteError \u00b6 @Metric . register ( \"mean_absolute_error\" ) class MeanAbsoluteError ( Metric ): | def __init__ ( self ) -> None This Metric calculates the mean absolute error (MAE) between two tensors. __call__ \u00b6 class MeanAbsoluteError ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) -> None Parameters \u00b6 predictions : torch.Tensor A tensor of predictions of shape (batch_size, ...). gold_labels : torch.Tensor A tensor of the same shape as predictions . mask : torch.BoolTensor , optional (default = None ) A tensor of the same shape as predictions . get_metric \u00b6 class MeanAbsoluteError ( Metric ): | ... | def get_metric ( self , reset : bool = False ) -> Dict [ str , float ] Returns \u00b6 The accumulated mean absolute error. reset \u00b6 class MeanAbsoluteError ( Metric ): | ... | def reset ( self ) -> None","title":"mean_absolute_error"},{"location":"api/training/metrics/mean_absolute_error/#meanabsoluteerror","text":"@Metric . register ( \"mean_absolute_error\" ) class MeanAbsoluteError ( Metric ): | def __init__ ( self ) -> None This Metric calculates the mean absolute error (MAE) between two tensors.","title":"MeanAbsoluteError"},{"location":"api/training/metrics/mean_absolute_error/#__call__","text":"class MeanAbsoluteError ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) -> None","title":"__call__"},{"location":"api/training/metrics/mean_absolute_error/#get_metric","text":"class MeanAbsoluteError ( Metric ): | ... | def get_metric ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metric"},{"location":"api/training/metrics/mean_absolute_error/#reset","text":"class MeanAbsoluteError ( Metric ): | ... | def reset ( self ) -> None","title":"reset"},{"location":"api/training/metrics/metric/","text":"allennlp .training .metrics .metric [SOURCE] Metric \u00b6 class Metric ( Registrable ) A very general abstract class representing a metric which can be accumulated. supports_distributed \u00b6 class Metric ( Registrable ): | ... | supports_distributed = False __call__ \u00b6 class Metric ( Registrable ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] | ) Parameters \u00b6 predictions : torch.Tensor A tensor of predictions. gold_labels : torch.Tensor A tensor corresponding to some gold label to evaluate against. mask : torch.BoolTensor , optional (default = None ) A mask can be passed, in order to deal with metrics which are computed over potentially padded elements, such as sequence labels. get_metric \u00b6 class Metric ( Registrable ): | ... | def get_metric ( self , reset : bool ) Compute and return the metric. Optionally also call self.reset . reset \u00b6 class Metric ( Registrable ): | ... | def reset ( self ) -> None Reset any accumulators or internal state. detach_tensors \u00b6 class Metric ( Registrable ): | ... | @staticmethod | def detach_tensors ( * tensors : torch . Tensor ) -> Iterable [ torch . Tensor ] If you actually passed gradient-tracking Tensors to a Metric, there will be a huge memory leak, because it will prevent garbage collection for the computation graph. This method ensures the tensors are detached.","title":"metric"},{"location":"api/training/metrics/metric/#metric","text":"class Metric ( Registrable ) A very general abstract class representing a metric which can be accumulated.","title":"Metric"},{"location":"api/training/metrics/metric/#supports_distributed","text":"class Metric ( Registrable ): | ... | supports_distributed = False","title":"supports_distributed"},{"location":"api/training/metrics/metric/#__call__","text":"class Metric ( Registrable ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] | )","title":"__call__"},{"location":"api/training/metrics/metric/#get_metric","text":"class Metric ( Registrable ): | ... | def get_metric ( self , reset : bool ) Compute and return the metric. Optionally also call self.reset .","title":"get_metric"},{"location":"api/training/metrics/metric/#reset","text":"class Metric ( Registrable ): | ... | def reset ( self ) -> None Reset any accumulators or internal state.","title":"reset"},{"location":"api/training/metrics/metric/#detach_tensors","text":"class Metric ( Registrable ): | ... | @staticmethod | def detach_tensors ( * tensors : torch . Tensor ) -> Iterable [ torch . Tensor ] If you actually passed gradient-tracking Tensors to a Metric, there will be a huge memory leak, because it will prevent garbage collection for the computation graph. This method ensures the tensors are detached.","title":"detach_tensors"},{"location":"api/training/metrics/pearson_correlation/","text":"allennlp .training .metrics .pearson_correlation [SOURCE] PearsonCorrelation \u00b6 @Metric . register ( \"pearson_correlation\" ) class PearsonCorrelation ( Metric ): | def __init__ ( self ) -> None This Metric calculates the sample Pearson correlation coefficient (r) between two tensors. Each element in the two tensors is assumed to be a different observation of the variable (i.e., the input tensors are implicitly flattened into vectors and the correlation is calculated between the vectors). This implementation is mostly modeled after the streaming_pearson_correlation function in Tensorflow. See https://github.com/tensorflow/tensorflow/blob/v1.10.1/tensorflow/contrib/metrics/python/ops/metric_ops.py#L3267 . This metric delegates to the Covariance metric the tracking of three [co]variances: covariance(predictions, labels) , i.e. covariance covariance(predictions, predictions) , i.e. variance of predictions covariance(labels, labels) , i.e. variance of labels If we have these values, the sample Pearson correlation coefficient is simply: r = covariance / (sqrt(predictions_variance) * sqrt(labels_variance)) if predictions_variance or labels_variance is 0, r is 0 __call__ \u00b6 class PearsonCorrelation ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters \u00b6 predictions : torch.Tensor A tensor of predictions of shape (batch_size, ...). gold_labels : torch.Tensor A tensor of the same shape as predictions . mask : torch.BoolTensor , optional (default = None ) A tensor of the same shape as predictions . get_metric \u00b6 class PearsonCorrelation ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns \u00b6 The accumulated sample Pearson correlation. reset \u00b6 class PearsonCorrelation ( Metric ): | ... | def reset ( self )","title":"pearson_correlation"},{"location":"api/training/metrics/pearson_correlation/#pearsoncorrelation","text":"@Metric . register ( \"pearson_correlation\" ) class PearsonCorrelation ( Metric ): | def __init__ ( self ) -> None This Metric calculates the sample Pearson correlation coefficient (r) between two tensors. Each element in the two tensors is assumed to be a different observation of the variable (i.e., the input tensors are implicitly flattened into vectors and the correlation is calculated between the vectors). This implementation is mostly modeled after the streaming_pearson_correlation function in Tensorflow. See https://github.com/tensorflow/tensorflow/blob/v1.10.1/tensorflow/contrib/metrics/python/ops/metric_ops.py#L3267 . This metric delegates to the Covariance metric the tracking of three [co]variances: covariance(predictions, labels) , i.e. covariance covariance(predictions, predictions) , i.e. variance of predictions covariance(labels, labels) , i.e. variance of labels If we have these values, the sample Pearson correlation coefficient is simply: r = covariance / (sqrt(predictions_variance) * sqrt(labels_variance)) if predictions_variance or labels_variance is 0, r is 0","title":"PearsonCorrelation"},{"location":"api/training/metrics/pearson_correlation/#__call__","text":"class PearsonCorrelation ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | )","title":"__call__"},{"location":"api/training/metrics/pearson_correlation/#get_metric","text":"class PearsonCorrelation ( Metric ): | ... | def get_metric ( self , reset : bool = False )","title":"get_metric"},{"location":"api/training/metrics/pearson_correlation/#reset","text":"class PearsonCorrelation ( Metric ): | ... | def reset ( self )","title":"reset"},{"location":"api/training/metrics/perplexity/","text":"allennlp .training .metrics .perplexity [SOURCE] Perplexity \u00b6 @Metric . register ( \"perplexity\" ) class Perplexity ( Average ) Perplexity is a common metric used for evaluating how well a language model predicts a sample. NotesAssumes negative log likelihood loss of each batch (base e). Provides the \u00b6 average perplexity of the batches. get_metric \u00b6 class Perplexity ( Average ): | ... | def get_metric ( self , reset : bool = False ) Returns \u00b6 The accumulated perplexity.","title":"perplexity"},{"location":"api/training/metrics/perplexity/#perplexity","text":"@Metric . register ( \"perplexity\" ) class Perplexity ( Average ) Perplexity is a common metric used for evaluating how well a language model predicts a sample.","title":"Perplexity"},{"location":"api/training/metrics/perplexity/#get_metric","text":"class Perplexity ( Average ): | ... | def get_metric ( self , reset : bool = False )","title":"get_metric"},{"location":"api/training/metrics/rouge/","text":"allennlp .training .metrics .rouge [SOURCE] ROUGE \u00b6 @Metric . register ( \"rouge\" ) class ROUGE ( Metric ): | def __init__ ( | self , | ngram_size : int = 2 , | exclude_indices : Set [ int ] = None | ) -> None Recall-Oriented Understudy for Gisting Evaluation (ROUGE) ROUGE is a metric for measuring the quality of summaries. It is based on calculating the recall between ngrams in the predicted summary and a set of reference summaries. See Lin, \"ROUGE: A Package For Automatic Evaluation Of Summaries\", 2004 . Parameters \u00b6 ngram_size : int , optional (default = 2 ) ROUGE scores are calculate for ROUGE-1 .. ROUGE- ngram_size exclude_indices : Set[int] , optional (default = None ) Indices to exclude when calculating ngrams. This should usually include the indices of the start, end, and pad tokens. reset \u00b6 class ROUGE ( Metric ): | ... | def reset ( self ) -> None __call__ \u00b6 class ROUGE ( Metric ): | ... | def __call__ ( | self , | predictions : torch . LongTensor , | gold_targets : torch . LongTensor , | mask : Optional [ torch . BoolTensor ] = None | ) -> None Update recall counts. Parameters \u00b6 predictions : torch.LongTensor Batched predicted tokens of shape (batch_size, max_sequence_length) . references : torch.LongTensor Batched reference (gold) sequences with shape (batch_size, max_gold_sequence_length) . Returns \u00b6 None get_metric \u00b6 class ROUGE ( Metric ): | ... | def get_metric ( self , reset : bool = False ) -> Dict [ str , float ] Parameters \u00b6 reset : bool , optional (default = False ) Reset any accumulators or internal state. Returns \u00b6 Dict[str, float]: A dictionary containing ROUGE-1 .. ROUGE-ngram_size scores.","title":"rouge"},{"location":"api/training/metrics/rouge/#rouge","text":"@Metric . register ( \"rouge\" ) class ROUGE ( Metric ): | def __init__ ( | self , | ngram_size : int = 2 , | exclude_indices : Set [ int ] = None | ) -> None Recall-Oriented Understudy for Gisting Evaluation (ROUGE) ROUGE is a metric for measuring the quality of summaries. It is based on calculating the recall between ngrams in the predicted summary and a set of reference summaries. See Lin, \"ROUGE: A Package For Automatic Evaluation Of Summaries\", 2004 .","title":"ROUGE"},{"location":"api/training/metrics/rouge/#reset","text":"class ROUGE ( Metric ): | ... | def reset ( self ) -> None","title":"reset"},{"location":"api/training/metrics/rouge/#__call__","text":"class ROUGE ( Metric ): | ... | def __call__ ( | self , | predictions : torch . LongTensor , | gold_targets : torch . LongTensor , | mask : Optional [ torch . BoolTensor ] = None | ) -> None Update recall counts.","title":"__call__"},{"location":"api/training/metrics/rouge/#get_metric","text":"class ROUGE ( Metric ): | ... | def get_metric ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metric"},{"location":"api/training/metrics/sequence_accuracy/","text":"allennlp .training .metrics .sequence_accuracy [SOURCE] SequenceAccuracy \u00b6 @Metric . register ( \"sequence_accuracy\" ) class SequenceAccuracy ( Metric ): | def __init__ ( self ) -> None Sequence Top-K accuracy. Assumes integer labels, with each item to be classified having a single correct class. __call__ \u00b6 class SequenceAccuracy ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters \u00b6 predictions : torch.Tensor A tensor of predictions of shape (batch_size, k, sequence_length). gold_labels : torch.Tensor A tensor of integer class label of shape (batch_size, sequence_length). mask : torch.BoolTensor , optional (default = None ) A masking tensor the same size as gold_labels . get_metric \u00b6 class SequenceAccuracy ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns \u00b6 The accumulated accuracy. reset \u00b6 class SequenceAccuracy ( Metric ): | ... | def reset ( self )","title":"sequence_accuracy"},{"location":"api/training/metrics/sequence_accuracy/#sequenceaccuracy","text":"@Metric . register ( \"sequence_accuracy\" ) class SequenceAccuracy ( Metric ): | def __init__ ( self ) -> None Sequence Top-K accuracy. Assumes integer labels, with each item to be classified having a single correct class.","title":"SequenceAccuracy"},{"location":"api/training/metrics/sequence_accuracy/#__call__","text":"class SequenceAccuracy ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | )","title":"__call__"},{"location":"api/training/metrics/sequence_accuracy/#get_metric","text":"class SequenceAccuracy ( Metric ): | ... | def get_metric ( self , reset : bool = False )","title":"get_metric"},{"location":"api/training/metrics/sequence_accuracy/#reset","text":"class SequenceAccuracy ( Metric ): | ... | def reset ( self )","title":"reset"},{"location":"api/training/metrics/span_based_f1_measure/","text":"allennlp .training .metrics .span_based_f1_measure [SOURCE] TAGS_TO_SPANS_FUNCTION_TYPE \u00b6 TAGS_TO_SPANS_FUNCTION_TYPE = Callable [[ List [ str ], Optional [ List [ str ]]], List [ TypedStringSpan ]] SpanBasedF1Measure \u00b6 @Metric . register ( \"span_f1\" ) class SpanBasedF1Measure ( Metric ): | def __init__ ( | self , | vocabulary : Vocabulary , | tag_namespace : str = \"tags\" , | ignore_classes : List [ str ] = None , | label_encoding : Optional [ str ] = \"BIO\" , | tags_to_spans_function : Optional [ TAGS_TO_SPANS_FUNCTION_TYPE ] = None | ) -> None The Conll SRL metrics are based on exact span matching. This metric implements span-based precision and recall metrics for a BIO tagging scheme. It will produce precision, recall and F1 measures per tag, as well as overall statistics. Note that the implementation of this metric is not exactly the same as the perl script used to evaluate the CONLL 2005 data - particularly, it does not consider continuations or reference spans as constituents of the original span. However, it is a close proxy, which can be helpful for judging model performance during training. This metric works properly when the spans are unlabeled (i.e., your labels are simply \"B\", \"I\", \"O\" if using the \"BIO\" label encoding). __call__ \u00b6 class SpanBasedF1Measure ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None , | prediction_map : Optional [ torch . Tensor ] = None | ) Parameters \u00b6 predictions : torch.Tensor A tensor of predictions of shape (batch_size, sequence_length, num_classes). gold_labels : torch.Tensor A tensor of integer class label of shape (batch_size, sequence_length). It must be the same shape as the predictions tensor without the num_classes dimension. mask : torch.BoolTensor , optional (default = None ) A masking tensor the same size as gold_labels . prediction_map : torch.Tensor , optional (default = None ) A tensor of size (batch_size, num_classes) which provides a mapping from the index of predictions to the indices of the label vocabulary. If provided, the output label at each timestep will be vocabulary.get_index_to_token_vocabulary(prediction_map[batch, argmax(predictions[batch, t])) , rather than simply vocabulary.get_index_to_token_vocabulary(argmax(predictions[batch, t])) . This is useful in cases where each Instance in the dataset is associated with a different possible subset of labels from a large label-space (IE FrameNet, where each frame has a different set of possible roles associated with it). get_metric \u00b6 class SpanBasedF1Measure ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns \u00b6 Dict[str, float] A Dict per label containing following the span based metrics: precision : float recall : float f1-measure : float Additionally, an overall key is included, which provides the precision, recall and f1-measure for all spans. reset \u00b6 class SpanBasedF1Measure ( Metric ): | ... | def reset ( self )","title":"span_based_f1_measure"},{"location":"api/training/metrics/span_based_f1_measure/#tags_to_spans_function_type","text":"TAGS_TO_SPANS_FUNCTION_TYPE = Callable [[ List [ str ], Optional [ List [ str ]]], List [ TypedStringSpan ]]","title":"TAGS_TO_SPANS_FUNCTION_TYPE"},{"location":"api/training/metrics/span_based_f1_measure/#spanbasedf1measure","text":"@Metric . register ( \"span_f1\" ) class SpanBasedF1Measure ( Metric ): | def __init__ ( | self , | vocabulary : Vocabulary , | tag_namespace : str = \"tags\" , | ignore_classes : List [ str ] = None , | label_encoding : Optional [ str ] = \"BIO\" , | tags_to_spans_function : Optional [ TAGS_TO_SPANS_FUNCTION_TYPE ] = None | ) -> None The Conll SRL metrics are based on exact span matching. This metric implements span-based precision and recall metrics for a BIO tagging scheme. It will produce precision, recall and F1 measures per tag, as well as overall statistics. Note that the implementation of this metric is not exactly the same as the perl script used to evaluate the CONLL 2005 data - particularly, it does not consider continuations or reference spans as constituents of the original span. However, it is a close proxy, which can be helpful for judging model performance during training. This metric works properly when the spans are unlabeled (i.e., your labels are simply \"B\", \"I\", \"O\" if using the \"BIO\" label encoding).","title":"SpanBasedF1Measure"},{"location":"api/training/metrics/span_based_f1_measure/#__call__","text":"class SpanBasedF1Measure ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None , | prediction_map : Optional [ torch . Tensor ] = None | )","title":"__call__"},{"location":"api/training/metrics/span_based_f1_measure/#get_metric","text":"class SpanBasedF1Measure ( Metric ): | ... | def get_metric ( self , reset : bool = False )","title":"get_metric"},{"location":"api/training/metrics/span_based_f1_measure/#reset","text":"class SpanBasedF1Measure ( Metric ): | ... | def reset ( self )","title":"reset"},{"location":"api/training/metrics/spearman_correlation/","text":"allennlp .training .metrics .spearman_correlation [SOURCE] SpearmanCorrelation \u00b6 @Metric . register ( \"spearman_correlation\" ) class SpearmanCorrelation ( Metric ): | def __init__ ( self ) -> None This Metric calculates the sample Spearman correlation coefficient (r) between two tensors. Each element in the two tensors is assumed to be a different observation of the variable (i.e., the input tensors are implicitly flattened into vectors and the correlation is calculated between the vectors). https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient __call__ \u00b6 class SpearmanCorrelation ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters \u00b6 predictions : torch.Tensor A tensor of predictions of shape (batch_size, ...). gold_labels : torch.Tensor A tensor of the same shape as predictions . mask : torch.BoolTensor , optional (default = None ) A tensor of the same shape as predictions . get_metric \u00b6 class SpearmanCorrelation ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns \u00b6 The accumulated sample Spearman correlation. reset \u00b6 class SpearmanCorrelation ( Metric ): | ... | def reset ( self )","title":"spearman_correlation"},{"location":"api/training/metrics/spearman_correlation/#spearmancorrelation","text":"@Metric . register ( \"spearman_correlation\" ) class SpearmanCorrelation ( Metric ): | def __init__ ( self ) -> None This Metric calculates the sample Spearman correlation coefficient (r) between two tensors. Each element in the two tensors is assumed to be a different observation of the variable (i.e., the input tensors are implicitly flattened into vectors and the correlation is calculated between the vectors). https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient","title":"SpearmanCorrelation"},{"location":"api/training/metrics/spearman_correlation/#__call__","text":"class SpearmanCorrelation ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | )","title":"__call__"},{"location":"api/training/metrics/spearman_correlation/#get_metric","text":"class SpearmanCorrelation ( Metric ): | ... | def get_metric ( self , reset : bool = False )","title":"get_metric"},{"location":"api/training/metrics/spearman_correlation/#reset","text":"class SpearmanCorrelation ( Metric ): | ... | def reset ( self )","title":"reset"},{"location":"api/training/metrics/unigram_recall/","text":"allennlp .training .metrics .unigram_recall [SOURCE] UnigramRecall \u00b6 @Metric . register ( \"unigram_recall\" ) class UnigramRecall ( Metric ): | def __init__ ( self ) -> None Unigram top-K recall. This does not take word order into account. Assumes integer labels, with each item to be classified having a single correct class. __call__ \u00b6 class UnigramRecall ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None , | end_index : int = sys . maxsize | ) Parameters \u00b6 predictions : torch.Tensor A tensor of predictions of shape (batch_size, k, sequence_length). gold_labels : torch.Tensor A tensor of integer class label of shape (batch_size, sequence_length). mask : torch.BoolTensor , optional (default = None ) A masking tensor the same size as gold_labels . get_metric \u00b6 class UnigramRecall ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns \u00b6 The accumulated recall. reset \u00b6 class UnigramRecall ( Metric ): | ... | def reset ( self )","title":"unigram_recall"},{"location":"api/training/metrics/unigram_recall/#unigramrecall","text":"@Metric . register ( \"unigram_recall\" ) class UnigramRecall ( Metric ): | def __init__ ( self ) -> None Unigram top-K recall. This does not take word order into account. Assumes integer labels, with each item to be classified having a single correct class.","title":"UnigramRecall"},{"location":"api/training/metrics/unigram_recall/#__call__","text":"class UnigramRecall ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None , | end_index : int = sys . maxsize | )","title":"__call__"},{"location":"api/training/metrics/unigram_recall/#get_metric","text":"class UnigramRecall ( Metric ): | ... | def get_metric ( self , reset : bool = False )","title":"get_metric"},{"location":"api/training/metrics/unigram_recall/#reset","text":"class UnigramRecall ( Metric ): | ... | def reset ( self )","title":"reset"},{"location":"api/training/momentum_schedulers/inverted_triangular/","text":"allennlp .training .momentum_schedulers .inverted_triangular [SOURCE] InvertedTriangular \u00b6 @MomentumScheduler . register ( \"inverted_triangular\" ) class InvertedTriangular ( MomentumScheduler ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | cool_down : int , | warm_up : int , | ratio : int = 10 , | last_epoch : int = - 1 | ) -> None Adjust momentum during training according to an inverted triangle-like schedule. The momentum starts off high, then decreases linearly for cool_down epochs, until reaching 1 / ratio th of the original value. Then the momentum increases linearly for warm_up epochs until reaching its original value again. If there are still more epochs left over to train, the momentum will stay flat at the original value. Registered as a MomentumScheduler with name \"inverted_triangular\". The \"optimizer\" argument does not get an entry in a configuration file for the object. get_values \u00b6 class InvertedTriangular ( MomentumScheduler ): | ... | def get_values ( self )","title":"inverted_triangular"},{"location":"api/training/momentum_schedulers/inverted_triangular/#invertedtriangular","text":"@MomentumScheduler . register ( \"inverted_triangular\" ) class InvertedTriangular ( MomentumScheduler ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | cool_down : int , | warm_up : int , | ratio : int = 10 , | last_epoch : int = - 1 | ) -> None Adjust momentum during training according to an inverted triangle-like schedule. The momentum starts off high, then decreases linearly for cool_down epochs, until reaching 1 / ratio th of the original value. Then the momentum increases linearly for warm_up epochs until reaching its original value again. If there are still more epochs left over to train, the momentum will stay flat at the original value. Registered as a MomentumScheduler with name \"inverted_triangular\". The \"optimizer\" argument does not get an entry in a configuration file for the object.","title":"InvertedTriangular"},{"location":"api/training/momentum_schedulers/inverted_triangular/#get_values","text":"class InvertedTriangular ( MomentumScheduler ): | ... | def get_values ( self )","title":"get_values"},{"location":"api/training/momentum_schedulers/momentum_scheduler/","text":"allennlp .training .momentum_schedulers .momentum_scheduler [SOURCE] MomentumScheduler \u00b6 class MomentumScheduler ( Scheduler , Registrable ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | last_epoch : int = - 1 | ) -> None get_values \u00b6 class MomentumScheduler ( Scheduler , Registrable ): | ... | def get_values ( self ) -> None","title":"momentum_scheduler"},{"location":"api/training/momentum_schedulers/momentum_scheduler/#momentumscheduler","text":"class MomentumScheduler ( Scheduler , Registrable ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | last_epoch : int = - 1 | ) -> None","title":"MomentumScheduler"},{"location":"api/training/momentum_schedulers/momentum_scheduler/#get_values","text":"class MomentumScheduler ( Scheduler , Registrable ): | ... | def get_values ( self ) -> None","title":"get_values"}]}