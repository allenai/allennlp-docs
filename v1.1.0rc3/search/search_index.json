{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"An Apache 2.0 NLP research library, built on PyTorch, for developing state-of-the-art deep learning models on a wide variety of linguistic tasks. Quick Links # Website Guide Forum Documentation ( latest | stable | master ) Contributing Guidelines Officially Supported Models Pretrained Models Documentation ( latest | stable | master ) Continuous Build Nightly Releases Getting Started Using the Library # If you're interested in using AllenNLP for model development, we recommend you check out the AllenNLP Guide . When you're ready to start your project, we've created a couple of template repositories that you can use as a starting place: If you want to use allennlp train and config files to specify experiments, use this template . We recommend this approach. If you'd prefer to use python code to configure your experiments and run your training loop, use this template . There are a few things that are currently a little harder in this setup (loading a saved model, and using distributed training), but except for those its functionality is equivalent to the config files setup. In addition, there are external tutorials: Hyperparameter optimization for AllenNLP using Optuna Package Overview # allennlp an open-source NLP research library, built on PyTorch allennlp.commands functionality for a CLI and web service allennlp.data a data processing module for loading datasets and encoding strings as integers for representation in matrices allennlp.models a collection of state-of-the-art models allennlp.modules a collection of PyTorch modules for use with text allennlp.nn tensor utility functions, such as initializers and activation functions allennlp.training functionality for training models Installation # AllenNLP requires Python 3.6.1 or later. The preferred way to install AllenNLP is via pip . Just run pip install allennlp in your Python environment and you're good to go! If you need pointers on setting up an appropriate Python environment or would like to install AllenNLP using a different method, see below. We support AllenNLP on Mac and Linux environments. We presently do not support Windows but are open to contributions. Installing via pip # Setting up a virtual environment # Conda can be used set up a virtual environment with the version of Python required for AllenNLP. If you already have a Python 3.6 or 3.7 environment you want to use, you can skip to the 'installing via pip' section. Download and install Conda . Create a Conda environment with Python 3.7: conda create -n allennlp python=3.7 Activate the Conda environment. You will need to activate the Conda environment in each terminal in which you want to use AllenNLP: conda activate allennlp Installing the library and dependencies # Installing the library and dependencies is simple using pip . pip install allennlp Looking for bleeding edge features? You can install nightly releases directly from pypi AllenNLP installs a script when you install the python package, so you can run allennlp commands just by typing allennlp into a terminal. For example, you can now test your installation with allennlp test-install . You may also want to install allennlp-models , which contains the NLP constructs to train and run our officially supported models, many of which are hosted at https://demo.allennlp.org . pip install allennlp-models Installing using Docker # Docker provides a virtual machine with everything set up to run AllenNLP-- whether you will leverage a GPU or just run on a CPU. Docker provides more isolation and consistency, and also makes it easy to distribute your environment to a compute cluster. Once you have installed Docker just run the following command to get an environment that will run on either the cpu or gpu. mkdir -p $HOME /.allennlp/ docker run --rm -v $HOME /.allennlp:/root/.allennlp allennlp/allennlp:latest You can test the Docker environment with docker run --rm -v $HOME /.allennlp:/root/.allennlp allennlp/allennlp:latest test-install Installing from source # You can also install AllenNLP by cloning our git repository: git clone https://github.com/allenai/allennlp.git Create a Python 3.7 virtual environment, and install AllenNLP in editable mode by running: pip install --editable . pip install -r dev-requirements.txt This will make allennlp available on your system but it will use the sources from the local clone you made of the source repository. You can test your installation with allennlp test-install . See https://github.com/allenai/allennlp-models for instructions on installing allennlp-models from source. Running AllenNLP # Once you've installed AllenNLP, you can run the command-line interface with the allennlp command (whether you installed from pip or from source). allennlp has various subcommands such as train , evaluate , and predict . To see the full usage information, run allennlp --help . Docker images # AllenNLP releases Docker images to Docker Hub for each release. For information on how to run these releases, see Installing using Docker . Building a Docker image # For various reasons you may need to create your own AllenNLP Docker image. The same image can be used either with a CPU or a GPU. First, you need to install Docker . Then you will need a wheel of allennlp in the dist/ directory. You can either obtain a pre-built wheel from a PyPI release or build a new wheel from source. PyPI release wheels can be downloaded by going to https://pypi.org/project/allennlp/#history, clicking on the desired release, and then clicking \"Download files\" in the left sidebar. After downloading, make you sure you put the wheel in the dist/ directory (which may not exist if you haven't built a wheel from source yet). To build a wheel from source, just run python setup.py wheel . Before building the image, make sure you only have one wheel in the dist/ directory. Once you have your wheel, run make docker-image . By default this builds an image with the tag allennlp/allennlp . You can change this to anything you want by setting the DOCKER_TAG flag when you call make . For example, make docker-image DOCKER_TAG=my-allennlp . You should now be able to see this image listed by running docker images allennlp . REPOSITORY TAG IMAGE ID CREATED SIZE allennlp/allennlp latest b66aee6cb593 5 minutes ago 2.38GB Running the Docker image # You can run the image with docker run --rm -it allennlp/allennlp:latest . The --rm flag cleans up the image on exit and the -it flags make the session interactive so you can use the bash shell the Docker image starts. You can test your installation by running allennlp test-install . Issues # Everyone is welcome to file issues with either feature requests, bug reports, or general questions. As a small team with our own internal goals, we may ask for contributions if a prompt fix doesn't fit into our roadmap. To keep things tidy we will often close issues we think are answered, but don't hesitate to follow up if further discussion is needed. Contributions # The AllenNLP team at AI2 (@allenai) welcomes contributions from the greater AllenNLP community, and, if you would like to get a change into the library, this is likely the fastest approach. If you would like to contribute a larger feature, we recommend first creating an issue with a proposed design for discussion. This will prevent you from spending significant time on an implementation which has a technical limitation someone could have pointed out early on. Small contributions can be made directly in a pull request. Pull requests (PRs) must have one approving review and no requested changes before they are merged. As AllenNLP is primarily driven by AI2 (@allenai) we reserve the right to reject or revert contributions that we don't think are good additions. Citing # If you use AllenNLP in your research, please cite AllenNLP: A Deep Semantic Natural Language Processing Platform . @inproceedings { Gardner2017AllenNLP , title = {AllenNLP: A Deep Semantic Natural Language Processing Platform} , author = {Matt Gardner and Joel Grus and Mark Neumann and Oyvind Tafjord and Pradeep Dasigi and Nelson F. Liu and Matthew Peters and Michael Schmitz and Luke S. Zettlemoyer} , year = {2017} , Eprint = {arXiv:1803.07640} , } Team # AllenNLP is an open-source project backed by the Allen Institute for Artificial Intelligence (AI2) . AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering. To learn more about who specifically contributed to this codebase, see our contributors page.","title":"Home"},{"location":"#quick-links","text":"Website Guide Forum Documentation ( latest | stable | master ) Contributing Guidelines Officially Supported Models Pretrained Models Documentation ( latest | stable | master ) Continuous Build Nightly Releases","title":"Quick Links"},{"location":"#getting-started-using-the-library","text":"If you're interested in using AllenNLP for model development, we recommend you check out the AllenNLP Guide . When you're ready to start your project, we've created a couple of template repositories that you can use as a starting place: If you want to use allennlp train and config files to specify experiments, use this template . We recommend this approach. If you'd prefer to use python code to configure your experiments and run your training loop, use this template . There are a few things that are currently a little harder in this setup (loading a saved model, and using distributed training), but except for those its functionality is equivalent to the config files setup. In addition, there are external tutorials: Hyperparameter optimization for AllenNLP using Optuna","title":"Getting Started Using the Library"},{"location":"#package-overview","text":"allennlp an open-source NLP research library, built on PyTorch allennlp.commands functionality for a CLI and web service allennlp.data a data processing module for loading datasets and encoding strings as integers for representation in matrices allennlp.models a collection of state-of-the-art models allennlp.modules a collection of PyTorch modules for use with text allennlp.nn tensor utility functions, such as initializers and activation functions allennlp.training functionality for training models","title":"Package Overview"},{"location":"#installation","text":"AllenNLP requires Python 3.6.1 or later. The preferred way to install AllenNLP is via pip . Just run pip install allennlp in your Python environment and you're good to go! If you need pointers on setting up an appropriate Python environment or would like to install AllenNLP using a different method, see below. We support AllenNLP on Mac and Linux environments. We presently do not support Windows but are open to contributions.","title":"Installation"},{"location":"#installing-via-pip","text":"","title":"Installing via pip"},{"location":"#setting-up-a-virtual-environment","text":"Conda can be used set up a virtual environment with the version of Python required for AllenNLP. If you already have a Python 3.6 or 3.7 environment you want to use, you can skip to the 'installing via pip' section. Download and install Conda . Create a Conda environment with Python 3.7: conda create -n allennlp python=3.7 Activate the Conda environment. You will need to activate the Conda environment in each terminal in which you want to use AllenNLP: conda activate allennlp","title":"Setting up a virtual environment"},{"location":"#installing-the-library-and-dependencies","text":"Installing the library and dependencies is simple using pip . pip install allennlp Looking for bleeding edge features? You can install nightly releases directly from pypi AllenNLP installs a script when you install the python package, so you can run allennlp commands just by typing allennlp into a terminal. For example, you can now test your installation with allennlp test-install . You may also want to install allennlp-models , which contains the NLP constructs to train and run our officially supported models, many of which are hosted at https://demo.allennlp.org . pip install allennlp-models","title":"Installing the library and dependencies"},{"location":"#installing-using-docker","text":"Docker provides a virtual machine with everything set up to run AllenNLP-- whether you will leverage a GPU or just run on a CPU. Docker provides more isolation and consistency, and also makes it easy to distribute your environment to a compute cluster. Once you have installed Docker just run the following command to get an environment that will run on either the cpu or gpu. mkdir -p $HOME /.allennlp/ docker run --rm -v $HOME /.allennlp:/root/.allennlp allennlp/allennlp:latest You can test the Docker environment with docker run --rm -v $HOME /.allennlp:/root/.allennlp allennlp/allennlp:latest test-install","title":"Installing using Docker"},{"location":"#installing-from-source","text":"You can also install AllenNLP by cloning our git repository: git clone https://github.com/allenai/allennlp.git Create a Python 3.7 virtual environment, and install AllenNLP in editable mode by running: pip install --editable . pip install -r dev-requirements.txt This will make allennlp available on your system but it will use the sources from the local clone you made of the source repository. You can test your installation with allennlp test-install . See https://github.com/allenai/allennlp-models for instructions on installing allennlp-models from source.","title":"Installing from source"},{"location":"#running-allennlp","text":"Once you've installed AllenNLP, you can run the command-line interface with the allennlp command (whether you installed from pip or from source). allennlp has various subcommands such as train , evaluate , and predict . To see the full usage information, run allennlp --help .","title":"Running AllenNLP"},{"location":"#docker-images","text":"AllenNLP releases Docker images to Docker Hub for each release. For information on how to run these releases, see Installing using Docker .","title":"Docker images"},{"location":"#building-a-docker-image","text":"For various reasons you may need to create your own AllenNLP Docker image. The same image can be used either with a CPU or a GPU. First, you need to install Docker . Then you will need a wheel of allennlp in the dist/ directory. You can either obtain a pre-built wheel from a PyPI release or build a new wheel from source. PyPI release wheels can be downloaded by going to https://pypi.org/project/allennlp/#history, clicking on the desired release, and then clicking \"Download files\" in the left sidebar. After downloading, make you sure you put the wheel in the dist/ directory (which may not exist if you haven't built a wheel from source yet). To build a wheel from source, just run python setup.py wheel . Before building the image, make sure you only have one wheel in the dist/ directory. Once you have your wheel, run make docker-image . By default this builds an image with the tag allennlp/allennlp . You can change this to anything you want by setting the DOCKER_TAG flag when you call make . For example, make docker-image DOCKER_TAG=my-allennlp . You should now be able to see this image listed by running docker images allennlp . REPOSITORY TAG IMAGE ID CREATED SIZE allennlp/allennlp latest b66aee6cb593 5 minutes ago 2.38GB","title":"Building a Docker image"},{"location":"#running-the-docker-image","text":"You can run the image with docker run --rm -it allennlp/allennlp:latest . The --rm flag cleans up the image on exit and the -it flags make the session interactive so you can use the bash shell the Docker image starts. You can test your installation by running allennlp test-install .","title":"Running the Docker image"},{"location":"#issues","text":"Everyone is welcome to file issues with either feature requests, bug reports, or general questions. As a small team with our own internal goals, we may ask for contributions if a prompt fix doesn't fit into our roadmap. To keep things tidy we will often close issues we think are answered, but don't hesitate to follow up if further discussion is needed.","title":"Issues"},{"location":"#contributions","text":"The AllenNLP team at AI2 (@allenai) welcomes contributions from the greater AllenNLP community, and, if you would like to get a change into the library, this is likely the fastest approach. If you would like to contribute a larger feature, we recommend first creating an issue with a proposed design for discussion. This will prevent you from spending significant time on an implementation which has a technical limitation someone could have pointed out early on. Small contributions can be made directly in a pull request. Pull requests (PRs) must have one approving review and no requested changes before they are merged. As AllenNLP is primarily driven by AI2 (@allenai) we reserve the right to reject or revert contributions that we don't think are good additions.","title":"Contributions"},{"location":"#citing","text":"If you use AllenNLP in your research, please cite AllenNLP: A Deep Semantic Natural Language Processing Platform . @inproceedings { Gardner2017AllenNLP , title = {AllenNLP: A Deep Semantic Natural Language Processing Platform} , author = {Matt Gardner and Joel Grus and Mark Neumann and Oyvind Tafjord and Pradeep Dasigi and Nelson F. Liu and Matthew Peters and Michael Schmitz and Luke S. Zettlemoyer} , year = {2017} , Eprint = {arXiv:1803.07640} , }","title":"Citing"},{"location":"#team","text":"AllenNLP is an open-source project backed by the Allen Institute for Artificial Intelligence (AI2) . AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering. To learn more about who specifically contributed to this codebase, see our contributors page.","title":"Team"},{"location":"CHANGELOG/","text":"Changelog # All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . Unreleased # v1.1.0rc3 - 2020-08-12 # Fixed # Fixed how truncation was handled with PretrainedTransformerTokenizer . Previously, if max_length was set to None , the tokenizer would still do truncation if the transformer model had a default max length in its config. Also, when max_length was set to a non- None value, several warnings would appear for certain transformer models around the use of the truncation parameter. Fixed evaluation of all metrics when using distributed training. v1.1.0rc2 - 2020-07-31 # Changed # Upgraded PyTorch requirement to 1.6. Replaced the NVIDIA Apex AMP module with torch's native AMP module. The default trainer ( GradientDescentTrainer ) now takes a use_amp: bool parameter instead of the old opt_level: str parameter. Fixed # Removed unnecessary warning about deadlocks in DataLoader . Fixed testing models that only return a loss when they are in training mode. Fixed a bug in FromParams that caused silent failure in case of the parameter type being Optional[Union[...]] . Fixed a bug where the program crashes if evaluation_data_loader is a AllennlpLazyDataset . Added # Added the option to specify requires_grad: false within an optimizer's parameter groups. Added the file-friendly-logging flag back to the train command. Also added this flag to the predict , evaluate , and find-learning-rate commands. Added an EpochCallback to track current epoch as a model class member. Added the option to enable or disable gradient checkpointing for transformer token embedders via boolean parameter gradient_checkpointing . Removed # Removed the opt_level parameter to Model.load and load_archive . In order to use AMP with a loaded model now, just run the model's forward pass within torch's autocast context. v1.1.0rc1 - 2020-07-14 # Fixed # Reduced the amount of log messages produced by allennlp.common.file_utils . Fixed a bug where PretrainedTransformerEmbedder parameters appeared to be trainable in the log output even when train_parameters was set to False . Fixed a bug with the sharded dataset reader where it would only read a fraction of the instances in distributed training. Fixed checking equality of ArrayField s. Fixed a bug where NamespaceSwappingField did not work correctly with .empty_field() . Put more sensible defaults on the huggingface_adamw optimizer. Simplified logging so that all logging output always goes to one file. Fixed interaction with the python command line debugger. Log the grad norm properly even when we're not clipping it. Fixed a bug where PretrainedModelInitializer fails to initialize a model with a 0-dim tensor Fixed a bug with the layer unfreezing schedule of the SlantedTriangular learning rate scheduler. Fixed a regression with logging in the distributed setting. Only the main worker should write log output to the terminal. Pinned the version of boto3 for package managers (e.g. poetry). Fixed issue #4330 by updating the tokenizers dependency. Fixed a bug in TextClassificationPredictor so that it passes tokenized inputs to the DatasetReader in case it does not have a tokenizer. reg_loss is only now returned for models that have some regularization penalty configured. Fixed a bug that prevented cached_path from downloading assets from GitHub releases. Fixed a bug that erroneously increased last label's false positive count in calculating fbeta metrics. Tqdm output now looks much better when the output is being piped or redirected. Small improvements to how the API documentation is rendered. Only show validation progress bar from main process in distributed training. Added # Adjust beam search to support multi-layer decoder. A method to ModelTestCase for running basic model tests when you aren't using config files. Added some convenience methods for reading files. Added an option to file_utils.cached_path to automatically extract archives. Added the ability to pass an archive file instead of a local directory to Vocab.from_files . Added the ability to pass an archive file instead of a glob to ShardedDatasetReader . Added a new \"linear_with_warmup\" learning rate scheduler. Added a check in ShardedDatasetReader that ensures the base reader doesn't implement manual distributed sharding itself. Added an option to PretrainedTransformerEmbedder and PretrainedTransformerMismatchedEmbedder to use a scalar mix of all hidden layers from the transformer model instead of just the last layer. To utilize this, just set last_layer_only to False . cached_path() can now read files inside of archives. Training metrics now include batch_loss and batch_reg_loss in addition to aggregate loss across number of batches. Changed # Not specifying a cuda_device now automatically determines whether to use a GPU or not. Discovered plugins are logged so you can see what was loaded. allennlp.data.DataLoader is now an abstract registrable class. The default implementation remains the same, but was renamed to allennlp.data.PyTorchDataLoader . BertPooler can now unwrap and re-wrap extra dimensions if necessary. New transformers dependency. Only version >=3.0 now supported. v1.0.0 - 2020-06-16 # Fixed # Lazy dataset readers now work correctly with multi-process data loading. Fixed race conditions that could occur when using a dataset cache. Added # A bug where where all datasets would be loaded for vocab creation even if not needed. A parameter to the DatasetReader class: manual_multi_process_sharding . This is similar to the manual_distributed_sharding parameter, but applies when using a multi-process DataLoader . v1.0.0rc6 - 2020-06-11 # Fixed # A bug where TextField s could not be duplicated since some tokenizers cannot be deep-copied. See https://github.com/allenai/allennlp/issues/4270. Our caching mechanism had the potential to introduce race conditions if multiple processes were attempting to cache the same file at once. This was fixed by using a lock file tied to each cached file. get_text_field_mask() now supports padding indices that are not 0 . A bug where predictor.get_gradients() would return an empty dictionary if an embedding layer had trainable set to false Fixes PretrainedTransformerMismatchedIndexer in the case where a token consists of zero word pieces. Fixes a bug when using a lazy dataset reader that results in a UserWarning from PyTorch being printed at every iteration during training. Predictor names were inconsistently switching between dashes and underscores. Now they all use underscores. Predictor.from_path now automatically loads plugins (unless you specify load_plugins=False ) so that you don't have to manually import a bunch of modules when instantiating predictors from an archive path. allennlp-server automatically found as a plugin once again. Added # A duplicate() method on Instance s and Field s, to be used instead of copy.deepcopy() A batch sampler that makes sure each batch contains approximately the same number of tokens ( MaxTokensBatchSampler ) Functions to turn a sequence of token indices back into tokens The ability to use Huggingface encoder/decoder models as token embedders Improvements to beam search ROUGE metric Polynomial decay learning rate scheduler A BatchCallback for logging CPU and GPU memory usage to tensorboard. This is mainly for debugging because using it can cause a significant slowdown in training. Ability to run pretrained transformers as an embedder without training the weights Add Optuna Integrated badge to README.md Changed # Similar to our caching mechanism, we introduced a lock file to the vocab to avoid race conditions when saving/loading the vocab from/to the same serialization directory in different processes. Changed the Token , Instance , and Batch classes along with all Field classes to \"slots\" classes. This dramatically reduces the size in memory of instances. SimpleTagger will no longer calculate span-based F1 metric when calculate_span_f1 is False . CPU memory for every worker is now reported in the logs and the metrics. Previously this was only reporting the CPU memory of the master process, and so it was only correct in the non-distributed setting. To be consistent with PyTorch IterableDataset , AllennlpLazyDataset no longer implements __len__() . Previously it would always return 1. Removed old tutorials, in favor of the new AllenNLP Guide Changed the vocabulary loading to consider new lines for Windows/Linux and Mac. v1.0.0rc5 - 2020-05-26 # Fixed # Fix bug where PretrainedTransformerTokenizer crashed with some transformers (#4267) Make cached_path work offline. Tons of docstring inconsistencies resolved. Nightly builds no longer run on forks. Distributed training now automatically figures out which worker should see which instances A race condition bug in distributed training caused from saving the vocab to file from the master process while other processing might be reading those files. Unused dependencies in setup.py removed. Added # Additional CI checks to ensure docstrings are consistently formatted. Ability to train on CPU with multiple processes by setting cuda_devices to a list of negative integers in your training config. For example: \"distributed\": {\"cuda_devices\": [-1, -1]} . This is mainly to make it easier to test and debug distributed training code.. Documentation for when parameters don't need config file entries. Changed # The allennlp test-install command now just ensures the core submodules can be imported successfully, and prints out some other useful information such as the version, PyTorch version, and the number of GPU devices available. All of the tests moved from allennlp/tests to tests at the root level, and allennlp/tests/fixtures moved to test_fixtures at the root level. The PyPI source and wheel distributions will no longer include tests and fixtures. v1.0.0rc4 - 2020-05-14 # We first introduced this CHANGELOG after release v1.0.0rc4 , so please refer to the GitHub release notes for this and earlier releases.","title":"CHANGELOG"},{"location":"CHANGELOG/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"CHANGELOG/#unreleased","text":"","title":"Unreleased"},{"location":"CHANGELOG/#v110rc3-2020-08-12","text":"","title":"v1.1.0rc3 - 2020-08-12"},{"location":"CHANGELOG/#fixed","text":"Fixed how truncation was handled with PretrainedTransformerTokenizer . Previously, if max_length was set to None , the tokenizer would still do truncation if the transformer model had a default max length in its config. Also, when max_length was set to a non- None value, several warnings would appear for certain transformer models around the use of the truncation parameter. Fixed evaluation of all metrics when using distributed training.","title":"Fixed"},{"location":"CHANGELOG/#v110rc2-2020-07-31","text":"","title":"v1.1.0rc2 - 2020-07-31"},{"location":"CHANGELOG/#changed","text":"Upgraded PyTorch requirement to 1.6. Replaced the NVIDIA Apex AMP module with torch's native AMP module. The default trainer ( GradientDescentTrainer ) now takes a use_amp: bool parameter instead of the old opt_level: str parameter.","title":"Changed"},{"location":"CHANGELOG/#fixed_1","text":"Removed unnecessary warning about deadlocks in DataLoader . Fixed testing models that only return a loss when they are in training mode. Fixed a bug in FromParams that caused silent failure in case of the parameter type being Optional[Union[...]] . Fixed a bug where the program crashes if evaluation_data_loader is a AllennlpLazyDataset .","title":"Fixed"},{"location":"CHANGELOG/#added","text":"Added the option to specify requires_grad: false within an optimizer's parameter groups. Added the file-friendly-logging flag back to the train command. Also added this flag to the predict , evaluate , and find-learning-rate commands. Added an EpochCallback to track current epoch as a model class member. Added the option to enable or disable gradient checkpointing for transformer token embedders via boolean parameter gradient_checkpointing .","title":"Added"},{"location":"CHANGELOG/#removed","text":"Removed the opt_level parameter to Model.load and load_archive . In order to use AMP with a loaded model now, just run the model's forward pass within torch's autocast context.","title":"Removed"},{"location":"CHANGELOG/#v110rc1-2020-07-14","text":"","title":"v1.1.0rc1 - 2020-07-14"},{"location":"CHANGELOG/#fixed_2","text":"Reduced the amount of log messages produced by allennlp.common.file_utils . Fixed a bug where PretrainedTransformerEmbedder parameters appeared to be trainable in the log output even when train_parameters was set to False . Fixed a bug with the sharded dataset reader where it would only read a fraction of the instances in distributed training. Fixed checking equality of ArrayField s. Fixed a bug where NamespaceSwappingField did not work correctly with .empty_field() . Put more sensible defaults on the huggingface_adamw optimizer. Simplified logging so that all logging output always goes to one file. Fixed interaction with the python command line debugger. Log the grad norm properly even when we're not clipping it. Fixed a bug where PretrainedModelInitializer fails to initialize a model with a 0-dim tensor Fixed a bug with the layer unfreezing schedule of the SlantedTriangular learning rate scheduler. Fixed a regression with logging in the distributed setting. Only the main worker should write log output to the terminal. Pinned the version of boto3 for package managers (e.g. poetry). Fixed issue #4330 by updating the tokenizers dependency. Fixed a bug in TextClassificationPredictor so that it passes tokenized inputs to the DatasetReader in case it does not have a tokenizer. reg_loss is only now returned for models that have some regularization penalty configured. Fixed a bug that prevented cached_path from downloading assets from GitHub releases. Fixed a bug that erroneously increased last label's false positive count in calculating fbeta metrics. Tqdm output now looks much better when the output is being piped or redirected. Small improvements to how the API documentation is rendered. Only show validation progress bar from main process in distributed training.","title":"Fixed"},{"location":"CHANGELOG/#added_1","text":"Adjust beam search to support multi-layer decoder. A method to ModelTestCase for running basic model tests when you aren't using config files. Added some convenience methods for reading files. Added an option to file_utils.cached_path to automatically extract archives. Added the ability to pass an archive file instead of a local directory to Vocab.from_files . Added the ability to pass an archive file instead of a glob to ShardedDatasetReader . Added a new \"linear_with_warmup\" learning rate scheduler. Added a check in ShardedDatasetReader that ensures the base reader doesn't implement manual distributed sharding itself. Added an option to PretrainedTransformerEmbedder and PretrainedTransformerMismatchedEmbedder to use a scalar mix of all hidden layers from the transformer model instead of just the last layer. To utilize this, just set last_layer_only to False . cached_path() can now read files inside of archives. Training metrics now include batch_loss and batch_reg_loss in addition to aggregate loss across number of batches.","title":"Added"},{"location":"CHANGELOG/#changed_1","text":"Not specifying a cuda_device now automatically determines whether to use a GPU or not. Discovered plugins are logged so you can see what was loaded. allennlp.data.DataLoader is now an abstract registrable class. The default implementation remains the same, but was renamed to allennlp.data.PyTorchDataLoader . BertPooler can now unwrap and re-wrap extra dimensions if necessary. New transformers dependency. Only version >=3.0 now supported.","title":"Changed"},{"location":"CHANGELOG/#v100-2020-06-16","text":"","title":"v1.0.0 - 2020-06-16"},{"location":"CHANGELOG/#fixed_3","text":"Lazy dataset readers now work correctly with multi-process data loading. Fixed race conditions that could occur when using a dataset cache.","title":"Fixed"},{"location":"CHANGELOG/#added_2","text":"A bug where where all datasets would be loaded for vocab creation even if not needed. A parameter to the DatasetReader class: manual_multi_process_sharding . This is similar to the manual_distributed_sharding parameter, but applies when using a multi-process DataLoader .","title":"Added"},{"location":"CHANGELOG/#v100rc6-2020-06-11","text":"","title":"v1.0.0rc6 - 2020-06-11"},{"location":"CHANGELOG/#fixed_4","text":"A bug where TextField s could not be duplicated since some tokenizers cannot be deep-copied. See https://github.com/allenai/allennlp/issues/4270. Our caching mechanism had the potential to introduce race conditions if multiple processes were attempting to cache the same file at once. This was fixed by using a lock file tied to each cached file. get_text_field_mask() now supports padding indices that are not 0 . A bug where predictor.get_gradients() would return an empty dictionary if an embedding layer had trainable set to false Fixes PretrainedTransformerMismatchedIndexer in the case where a token consists of zero word pieces. Fixes a bug when using a lazy dataset reader that results in a UserWarning from PyTorch being printed at every iteration during training. Predictor names were inconsistently switching between dashes and underscores. Now they all use underscores. Predictor.from_path now automatically loads plugins (unless you specify load_plugins=False ) so that you don't have to manually import a bunch of modules when instantiating predictors from an archive path. allennlp-server automatically found as a plugin once again.","title":"Fixed"},{"location":"CHANGELOG/#added_3","text":"A duplicate() method on Instance s and Field s, to be used instead of copy.deepcopy() A batch sampler that makes sure each batch contains approximately the same number of tokens ( MaxTokensBatchSampler ) Functions to turn a sequence of token indices back into tokens The ability to use Huggingface encoder/decoder models as token embedders Improvements to beam search ROUGE metric Polynomial decay learning rate scheduler A BatchCallback for logging CPU and GPU memory usage to tensorboard. This is mainly for debugging because using it can cause a significant slowdown in training. Ability to run pretrained transformers as an embedder without training the weights Add Optuna Integrated badge to README.md","title":"Added"},{"location":"CHANGELOG/#changed_2","text":"Similar to our caching mechanism, we introduced a lock file to the vocab to avoid race conditions when saving/loading the vocab from/to the same serialization directory in different processes. Changed the Token , Instance , and Batch classes along with all Field classes to \"slots\" classes. This dramatically reduces the size in memory of instances. SimpleTagger will no longer calculate span-based F1 metric when calculate_span_f1 is False . CPU memory for every worker is now reported in the logs and the metrics. Previously this was only reporting the CPU memory of the master process, and so it was only correct in the non-distributed setting. To be consistent with PyTorch IterableDataset , AllennlpLazyDataset no longer implements __len__() . Previously it would always return 1. Removed old tutorials, in favor of the new AllenNLP Guide Changed the vocabulary loading to consider new lines for Windows/Linux and Mac.","title":"Changed"},{"location":"CHANGELOG/#v100rc5-2020-05-26","text":"","title":"v1.0.0rc5 - 2020-05-26"},{"location":"CHANGELOG/#fixed_5","text":"Fix bug where PretrainedTransformerTokenizer crashed with some transformers (#4267) Make cached_path work offline. Tons of docstring inconsistencies resolved. Nightly builds no longer run on forks. Distributed training now automatically figures out which worker should see which instances A race condition bug in distributed training caused from saving the vocab to file from the master process while other processing might be reading those files. Unused dependencies in setup.py removed.","title":"Fixed"},{"location":"CHANGELOG/#added_4","text":"Additional CI checks to ensure docstrings are consistently formatted. Ability to train on CPU with multiple processes by setting cuda_devices to a list of negative integers in your training config. For example: \"distributed\": {\"cuda_devices\": [-1, -1]} . This is mainly to make it easier to test and debug distributed training code.. Documentation for when parameters don't need config file entries.","title":"Added"},{"location":"CHANGELOG/#changed_3","text":"The allennlp test-install command now just ensures the core submodules can be imported successfully, and prints out some other useful information such as the version, PyTorch version, and the number of GPU devices available. All of the tests moved from allennlp/tests to tests at the root level, and allennlp/tests/fixtures moved to test_fixtures at the root level. The PyPI source and wheel distributions will no longer include tests and fixtures.","title":"Changed"},{"location":"CHANGELOG/#v100rc4-2020-05-14","text":"We first introduced this CHANGELOG after release v1.0.0rc4 , so please refer to the GitHub release notes for this and earlier releases.","title":"v1.0.0rc4 - 2020-05-14"},{"location":"CONTRIBUTING/","text":"Contributing # Thanks for considering contributing! We want AllenNLP to be the way to do cutting-edge NLP research, but we cannot get there without community support. How Can I Contribute? # Bug fixes and new features # Did you find a bug? First, do a quick search to see whether your issue has already been reported. If your issue has already been reported, please comment on the existing issue. Otherwise, open a new GitHub issue . Be sure to include a clear title and description. The description should include as much relevant information as possible. The description should explain how to reproduce the erroneous behavior as well as the behavior you expect to see. Ideally you would include a code sample or an executable test case demonstrating the expected behavior. Do you have a suggestion for an enhancement? We use GitHub issues to track enhancement requests. Before you create an enhancement request: Make sure you have a clear idea of the enhancement you would like. If you have a vague idea, consider discussing it first on a GitHub issue. Check the documentation to make sure your feature does not already exist. Do a quick search to see whether your enhancement has already been suggested. When creating your enhancement request, please: Provide a clear title and description. Explain why the enhancement would be useful. It may be helpful to highlight the feature in other libraries. Include code examples to demonstrate how the enhancement would be used. Making a pull request # When you're ready to contribute code to address an open issue, please follow these guidelines to help us be able to review your pull request (PR) quickly. Initial setup (only do this once) Expand details \ud83d\udc47 If you haven't already done so, please fork this repository on GitHub. Then clone your fork locally with git clone https://github.com/USERNAME/allennlp.git or git clone git@github.com:USERNAME/allennlp.git At this point the local clone of your fork only knows that it came from your repo, github.com/USERNAME/allennlp.git, but doesn't know anything the main repo, https://github.com/allenai/allennlp.git . You can see this by running git remote -v which will output something like this: origin https://github.com/USERNAME/allennlp.git (fetch) origin https://github.com/USERNAME/allennlp.git (push) This means that your local clone can only track changes from your fork, but not from the main repo, and so you won't be able to keep your fork up-to-date with the main repo over time. Therefor you'll need to add another \"remote\" to your clone that points to https://github.com/allenai/allennlp.git . To do this, run the following: git remote add upstream https://github.com/allenai/allennlp.git Now if you do git remote -v again, you'll see origin https://github.com/USERNAME/allennlp.git (fetch) origin https://github.com/USERNAME/allennlp.git (push) upstream https://github.com/allenai/allennlp.git (fetch) upstream https://github.com/allenai/allennlp.git (push) Finally, you'll need to create a Python 3.6 or 3.7 virtual environment suitable for working on AllenNLP. There a number of tools out there that making working with virtual environments easier, but the most direct way is with the venv module in the standard library. Once your virtual environment is activated, you can install your local clone in \"editable mode\" with pip install -e . pip install -r dev-requirements.txt The \"editable mode\" comes from the -e argument to pip , and essential just creates a symbolic link from the site-packages directory of your virtual environment to the source code in your local clone. That way any changes you make will be immediately reflected in your virtual environment. Ensure your fork is up-to-date Expand details \ud83d\udc47 Once you've added an \"upstream\" remote pointing to https://github.com/allenai/allennlp.git , keeping your fork up-to-date is easy: git checkout master # if not already on master git pull --rebase upstream master git push Create a new branch to work on your fix or enhancement Expand details \ud83d\udc47 Commiting directly to the master branch of your fork is not recommended. It will be easier to keep your fork clean if you work on a seperate branch for each contribution you intend to make. You can create a new branch with # replace BRANCH with whatever name you want to give it git checkout -b BRANCH git push -u origin BRANCH Test your changes Expand details \ud83d\udc47 Our continuous integration (CI) testing runs a number of checks for each pull request on GitHub Actions . You can run most of these tests locally, which is something you should do before opening a PR to help speed up the review process and make it easier for us. First, you should run black to make sure you code is formatted consistently. Many IDEs support code formatters as plugins, so you may be able to setup black to run automatically everytime you save. black.vim will give you this functionality in Vim, for example. But black is also easy to run directly from the command line. Just run this from the root of your clone: black . Our CI also uses flake8 to lint the code base and mypy for type-checking. You should run both of these next with make lint and make typecheck We also strive to maintain high test coverage, so most contributions should include additions to the unit tests . These tests are run with pytest , which you can use to locally run any test modules that you've added or changed. For example, if you've fixed a bug in allennlp/nn/util.py , you can run the tests specific to that module with pytest -v tests/nn/util_test.py Our CI will automatically check that test coverage stays above a certain threshold (around 90%). To check the coverage locally in this example, you could run pytest -v --cov allennlp.nn.util tests/nn/util_test.py If your contribution involves changes to any docstrings, you should make sure the API documentation can build without errors. For that, just run make build-docs If the build fails, it's most likely due to small formatting issues. If the error message isn't clear, feel free to comment on this in your pull request. You can also serve and view the docs locally with make serve-docs And finally, please update the CHANGELOG with notes on your contribution in the \"Unreleased\" section at the top. After all of the above checks have passed, you can now open a new GitHub pull request . Make sure you have a clear description of the problem and the solution, and include a link to relevant issues. We look forward to reviewing your PR! New models # Do you have a new state-of-the-art model? We are always looking for new models to add to our collection. The most popular models are usually added to the official AllenNLP Models repository, and in some cases to the AllenNLP Demo . If you think your model should be part of AllenNLP Models, please create a pull request in the models repo that includes: Any code changes needed to support your new model. A link to the model itself. Please do not check your model into the GitHub repository, but instead upload it in the PR conversation or provide a link to it at an external location. In the description of your PR, please clearly explain the task your model performs along with the relevant metrics on an established dataset.","title":"Contributing"},{"location":"CONTRIBUTING/#contributing","text":"Thanks for considering contributing! We want AllenNLP to be the way to do cutting-edge NLP research, but we cannot get there without community support.","title":"Contributing"},{"location":"CONTRIBUTING/#how-can-i-contribute","text":"","title":"How Can I Contribute?"},{"location":"CONTRIBUTING/#bug-fixes-and-new-features","text":"Did you find a bug? First, do a quick search to see whether your issue has already been reported. If your issue has already been reported, please comment on the existing issue. Otherwise, open a new GitHub issue . Be sure to include a clear title and description. The description should include as much relevant information as possible. The description should explain how to reproduce the erroneous behavior as well as the behavior you expect to see. Ideally you would include a code sample or an executable test case demonstrating the expected behavior. Do you have a suggestion for an enhancement? We use GitHub issues to track enhancement requests. Before you create an enhancement request: Make sure you have a clear idea of the enhancement you would like. If you have a vague idea, consider discussing it first on a GitHub issue. Check the documentation to make sure your feature does not already exist. Do a quick search to see whether your enhancement has already been suggested. When creating your enhancement request, please: Provide a clear title and description. Explain why the enhancement would be useful. It may be helpful to highlight the feature in other libraries. Include code examples to demonstrate how the enhancement would be used.","title":"Bug fixes and new features"},{"location":"CONTRIBUTING/#making-a-pull-request","text":"When you're ready to contribute code to address an open issue, please follow these guidelines to help us be able to review your pull request (PR) quickly. Initial setup (only do this once) Expand details \ud83d\udc47 If you haven't already done so, please fork this repository on GitHub. Then clone your fork locally with git clone https://github.com/USERNAME/allennlp.git or git clone git@github.com:USERNAME/allennlp.git At this point the local clone of your fork only knows that it came from your repo, github.com/USERNAME/allennlp.git, but doesn't know anything the main repo, https://github.com/allenai/allennlp.git . You can see this by running git remote -v which will output something like this: origin https://github.com/USERNAME/allennlp.git (fetch) origin https://github.com/USERNAME/allennlp.git (push) This means that your local clone can only track changes from your fork, but not from the main repo, and so you won't be able to keep your fork up-to-date with the main repo over time. Therefor you'll need to add another \"remote\" to your clone that points to https://github.com/allenai/allennlp.git . To do this, run the following: git remote add upstream https://github.com/allenai/allennlp.git Now if you do git remote -v again, you'll see origin https://github.com/USERNAME/allennlp.git (fetch) origin https://github.com/USERNAME/allennlp.git (push) upstream https://github.com/allenai/allennlp.git (fetch) upstream https://github.com/allenai/allennlp.git (push) Finally, you'll need to create a Python 3.6 or 3.7 virtual environment suitable for working on AllenNLP. There a number of tools out there that making working with virtual environments easier, but the most direct way is with the venv module in the standard library. Once your virtual environment is activated, you can install your local clone in \"editable mode\" with pip install -e . pip install -r dev-requirements.txt The \"editable mode\" comes from the -e argument to pip , and essential just creates a symbolic link from the site-packages directory of your virtual environment to the source code in your local clone. That way any changes you make will be immediately reflected in your virtual environment. Ensure your fork is up-to-date Expand details \ud83d\udc47 Once you've added an \"upstream\" remote pointing to https://github.com/allenai/allennlp.git , keeping your fork up-to-date is easy: git checkout master # if not already on master git pull --rebase upstream master git push Create a new branch to work on your fix or enhancement Expand details \ud83d\udc47 Commiting directly to the master branch of your fork is not recommended. It will be easier to keep your fork clean if you work on a seperate branch for each contribution you intend to make. You can create a new branch with # replace BRANCH with whatever name you want to give it git checkout -b BRANCH git push -u origin BRANCH Test your changes Expand details \ud83d\udc47 Our continuous integration (CI) testing runs a number of checks for each pull request on GitHub Actions . You can run most of these tests locally, which is something you should do before opening a PR to help speed up the review process and make it easier for us. First, you should run black to make sure you code is formatted consistently. Many IDEs support code formatters as plugins, so you may be able to setup black to run automatically everytime you save. black.vim will give you this functionality in Vim, for example. But black is also easy to run directly from the command line. Just run this from the root of your clone: black . Our CI also uses flake8 to lint the code base and mypy for type-checking. You should run both of these next with make lint and make typecheck We also strive to maintain high test coverage, so most contributions should include additions to the unit tests . These tests are run with pytest , which you can use to locally run any test modules that you've added or changed. For example, if you've fixed a bug in allennlp/nn/util.py , you can run the tests specific to that module with pytest -v tests/nn/util_test.py Our CI will automatically check that test coverage stays above a certain threshold (around 90%). To check the coverage locally in this example, you could run pytest -v --cov allennlp.nn.util tests/nn/util_test.py If your contribution involves changes to any docstrings, you should make sure the API documentation can build without errors. For that, just run make build-docs If the build fails, it's most likely due to small formatting issues. If the error message isn't clear, feel free to comment on this in your pull request. You can also serve and view the docs locally with make serve-docs And finally, please update the CHANGELOG with notes on your contribution in the \"Unreleased\" section at the top. After all of the above checks have passed, you can now open a new GitHub pull request . Make sure you have a clear description of the problem and the solution, and include a link to relevant issues. We look forward to reviewing your PR!","title":"Making a pull request"},{"location":"CONTRIBUTING/#new-models","text":"Do you have a new state-of-the-art model? We are always looking for new models to add to our collection. The most popular models are usually added to the official AllenNLP Models repository, and in some cases to the AllenNLP Demo . If you think your model should be part of AllenNLP Models, please create a pull request in the models repo that includes: Any code changes needed to support your new model. A link to the model itself. Please do not check your model into the GitHub repository, but instead upload it in the PR conversation or provide a link to it at an external location. In the description of your PR, please clearly explain the task your model performs along with the relevant metrics on an established dataset.","title":"New models"},{"location":"api/commands/evaluate/","text":"[ allennlp .commands .evaluate ] The evaluate subcommand can be used to evaluate a trained model against a dataset and report any metrics calculated by the model. Evaluate # class Evaluate ( Subcommand ) add_subparser # class Evaluate ( Subcommand ): | ... | @overrides | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser evaluate_from_args # def evaluate_from_args ( args : argparse . Namespace ) -> Dict [ str , Any ]","title":"evaluate"},{"location":"api/commands/evaluate/#evaluate","text":"class Evaluate ( Subcommand )","title":"Evaluate"},{"location":"api/commands/evaluate/#add_subparser","text":"class Evaluate ( Subcommand ): | ... | @overrides | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser","title":"add_subparser"},{"location":"api/commands/evaluate/#evaluate_from_args","text":"def evaluate_from_args ( args : argparse . Namespace ) -> Dict [ str , Any ]","title":"evaluate_from_args"},{"location":"api/commands/find_learning_rate/","text":"[ allennlp .commands .find_learning_rate ] The find-lr subcommand can be used to find a good learning rate for a model. It requires a configuration file and a directory in which to write the results. FindLearningRate # class FindLearningRate ( Subcommand ) add_subparser # class FindLearningRate ( Subcommand ): | ... | @overrides | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser find_learning_rate_from_args # def find_learning_rate_from_args ( args : argparse . Namespace ) -> None Start learning rate finder for given args find_learning_rate_model # def find_learning_rate_model ( params : Params , serialization_dir : str , start_lr : float = 1e-5 , end_lr : float = 10 , num_batches : int = 100 , linear_steps : bool = False , stopping_factor : float = None , force : bool = False ) -> None Runs learning rate search for given num_batches and saves the results in serialization_dir Parameters params : Params A parameter object specifying an AllenNLP Experiment. serialization_dir : str The directory in which to save results. start_lr : float Learning rate to start the search. end_lr : float Learning rate upto which search is done. num_batches : int Number of mini-batches to run Learning rate finder. linear_steps : bool Increase learning rate linearly if False exponentially. stopping_factor : float Stop the search when the current loss exceeds the best loss recorded by multiple of stopping factor. If None search proceeds till the end_lr force : bool If True and the serialization directory already exists, everything in it will be removed prior to finding the learning rate. search_learning_rate # def search_learning_rate ( trainer : GradientDescentTrainer , start_lr : float = 1e-5 , end_lr : float = 10 , num_batches : int = 100 , linear_steps : bool = False , stopping_factor : float = None ) -> Tuple [ List [ float ], List [ float ]] Runs training loop on the model using GradientDescentTrainer increasing learning rate from start_lr to end_lr recording the losses. Parameters trainer : GradientDescentTrainer start_lr : float The learning rate to start the search. end_lr : float The learning rate upto which search is done. num_batches : int Number of batches to run the learning rate finder. linear_steps : bool Increase learning rate linearly if False exponentially. stopping_factor : float Stop the search when the current loss exceeds the best loss recorded by multiple of stopping factor. If None search proceeds till the end_lr Returns (learning_rates, losses) : Tuple[List[float], List[float]] Returns list of learning rates and corresponding losses. Note: The losses are recorded before applying the corresponding learning rate","title":"find_learning_rate"},{"location":"api/commands/find_learning_rate/#findlearningrate","text":"class FindLearningRate ( Subcommand )","title":"FindLearningRate"},{"location":"api/commands/find_learning_rate/#add_subparser","text":"class FindLearningRate ( Subcommand ): | ... | @overrides | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser","title":"add_subparser"},{"location":"api/commands/find_learning_rate/#find_learning_rate_from_args","text":"def find_learning_rate_from_args ( args : argparse . Namespace ) -> None Start learning rate finder for given args","title":"find_learning_rate_from_args"},{"location":"api/commands/find_learning_rate/#find_learning_rate_model","text":"def find_learning_rate_model ( params : Params , serialization_dir : str , start_lr : float = 1e-5 , end_lr : float = 10 , num_batches : int = 100 , linear_steps : bool = False , stopping_factor : float = None , force : bool = False ) -> None Runs learning rate search for given num_batches and saves the results in serialization_dir Parameters params : Params A parameter object specifying an AllenNLP Experiment. serialization_dir : str The directory in which to save results. start_lr : float Learning rate to start the search. end_lr : float Learning rate upto which search is done. num_batches : int Number of mini-batches to run Learning rate finder. linear_steps : bool Increase learning rate linearly if False exponentially. stopping_factor : float Stop the search when the current loss exceeds the best loss recorded by multiple of stopping factor. If None search proceeds till the end_lr force : bool If True and the serialization directory already exists, everything in it will be removed prior to finding the learning rate.","title":"find_learning_rate_model"},{"location":"api/commands/find_learning_rate/#search_learning_rate","text":"def search_learning_rate ( trainer : GradientDescentTrainer , start_lr : float = 1e-5 , end_lr : float = 10 , num_batches : int = 100 , linear_steps : bool = False , stopping_factor : float = None ) -> Tuple [ List [ float ], List [ float ]] Runs training loop on the model using GradientDescentTrainer increasing learning rate from start_lr to end_lr recording the losses. Parameters trainer : GradientDescentTrainer start_lr : float The learning rate to start the search. end_lr : float The learning rate upto which search is done. num_batches : int Number of batches to run the learning rate finder. linear_steps : bool Increase learning rate linearly if False exponentially. stopping_factor : float Stop the search when the current loss exceeds the best loss recorded by multiple of stopping factor. If None search proceeds till the end_lr Returns (learning_rates, losses) : Tuple[List[float], List[float]] Returns list of learning rates and corresponding losses. Note: The losses are recorded before applying the corresponding learning rate","title":"search_learning_rate"},{"location":"api/commands/predict/","text":"[ allennlp .commands .predict ] The predict subcommand allows you to make bulk JSON-to-JSON or dataset to JSON predictions using a trained model and its Predictor wrapper. Predict # class Predict ( Subcommand ) add_subparser # class Predict ( Subcommand ): | ... | @overrides | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser","title":"predict"},{"location":"api/commands/predict/#predict","text":"class Predict ( Subcommand )","title":"Predict"},{"location":"api/commands/predict/#add_subparser","text":"class Predict ( Subcommand ): | ... | @overrides | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser","title":"add_subparser"},{"location":"api/commands/print_results/","text":"[ allennlp .commands .print_results ] The print-results subcommand allows you to print results from multiple allennlp serialization directories to the console in a helpful csv format. PrintResults # class PrintResults ( Subcommand ) add_subparser # class PrintResults ( Subcommand ): | ... | @overrides | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser print_results_from_args # def print_results_from_args ( args : argparse . Namespace ) Prints results from an argparse.Namespace object.","title":"print_results"},{"location":"api/commands/print_results/#printresults","text":"class PrintResults ( Subcommand )","title":"PrintResults"},{"location":"api/commands/print_results/#add_subparser","text":"class PrintResults ( Subcommand ): | ... | @overrides | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser","title":"add_subparser"},{"location":"api/commands/print_results/#print_results_from_args","text":"def print_results_from_args ( args : argparse . Namespace ) Prints results from an argparse.Namespace object.","title":"print_results_from_args"},{"location":"api/commands/subcommand/","text":"[ allennlp .commands .subcommand ] Base class for subcommands under allennlp.run . T # T = TypeVar ( \"T\" , bound = \"Subcommand\" ) Subcommand # class Subcommand ( Registrable ) An abstract class representing subcommands for allennlp.run. If you wanted to (for example) create your own custom special-evaluate command to use like allennlp special-evaluate ... you would create a Subcommand subclass and then pass it as an override to main . reverse_registry # class Subcommand ( Registrable ): | ... | reverse_registry : Dict [ Type , str ] = {} add_subparser # class Subcommand ( Registrable ): | ... | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser register # class Subcommand ( Registrable ): | ... | @classmethod | @overrides | def register ( | cls : Type [ T ], | name : str , | constructor : Optional [ str ] = None , | exist_ok : bool = False | ) -> Callable [[ Type [ T ]], Type [ T ]] name # class Subcommand ( Registrable ): | ... | @property | def name ( self ) -> str","title":"subcommand"},{"location":"api/commands/subcommand/#t","text":"T = TypeVar ( \"T\" , bound = \"Subcommand\" )","title":"T"},{"location":"api/commands/subcommand/#subcommand","text":"class Subcommand ( Registrable ) An abstract class representing subcommands for allennlp.run. If you wanted to (for example) create your own custom special-evaluate command to use like allennlp special-evaluate ... you would create a Subcommand subclass and then pass it as an override to main .","title":"Subcommand"},{"location":"api/commands/subcommand/#reverse_registry","text":"class Subcommand ( Registrable ): | ... | reverse_registry : Dict [ Type , str ] = {}","title":"reverse_registry"},{"location":"api/commands/subcommand/#add_subparser","text":"class Subcommand ( Registrable ): | ... | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser","title":"add_subparser"},{"location":"api/commands/subcommand/#register","text":"class Subcommand ( Registrable ): | ... | @classmethod | @overrides | def register ( | cls : Type [ T ], | name : str , | constructor : Optional [ str ] = None , | exist_ok : bool = False | ) -> Callable [[ Type [ T ]], Type [ T ]]","title":"register"},{"location":"api/commands/subcommand/#name","text":"class Subcommand ( Registrable ): | ... | @property | def name ( self ) -> str","title":"name"},{"location":"api/commands/test_install/","text":"[ allennlp .commands .test_install ] The test-install subcommand provides a programmatic way to verify that AllenNLP has been successfully installed. TestInstall # class TestInstall ( Subcommand ) add_subparser # class TestInstall ( Subcommand ): | ... | @overrides | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser","title":"test_install"},{"location":"api/commands/test_install/#testinstall","text":"class TestInstall ( Subcommand )","title":"TestInstall"},{"location":"api/commands/test_install/#add_subparser","text":"class TestInstall ( Subcommand ): | ... | @overrides | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser","title":"add_subparser"},{"location":"api/commands/train/","text":"[ allennlp .commands .train ] The train subcommand can be used to train a model. It requires a configuration file and a directory in which to write the results. Train # class Train ( Subcommand ) add_subparser # class Train ( Subcommand ): | ... | @overrides | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser train_model_from_args # def train_model_from_args ( args : argparse . Namespace ) Just converts from an argparse.Namespace object to string paths. train_model_from_file # def train_model_from_file ( parameter_filename : Union [ str , PathLike ], serialization_dir : Union [ str , PathLike ], overrides : str = \"\" , recover : bool = False , force : bool = False , node_rank : int = 0 , include_package : List [ str ] = None , dry_run : bool = False , file_friendly_logging : bool = False ) -> Optional [ Model ] A wrapper around train_model which loads the params from a file. Parameters parameter_filename : str A json parameter file specifying an AllenNLP experiment. serialization_dir : str The directory in which to save results and logs. We just pass this along to train_model . overrides : str A JSON string that we will use to override values in the input parameter file. recover : bool , optional (default = False ) If True , we will try to recover a training run from an existing serialization directory. This is only intended for use when something actually crashed during the middle of a run. For continuing training a model on new data, see Model.from_archive . force : bool , optional (default = False ) If True , we will overwrite the serialization directory if it already exists. node_rank : int , optional Rank of the current node in distributed training include_package : str , optional In distributed mode, extra packages mentioned will be imported in trainer workers. dry_run : bool , optional (default = False ) Do not train a model, but create a vocabulary, show dataset statistics and other training information. file_friendly_logging : bool , optional (default = False ) If True , we add newlines to tqdm output, even on an interactive terminal, and we slow down tqdm's output to only once every 10 seconds. Returns best_model : Optional[Model] The model with the best epoch weights or None if in dry run. train_model # def train_model ( params : Params , serialization_dir : Union [ str , PathLike ], recover : bool = False , force : bool = False , node_rank : int = 0 , include_package : List [ str ] = None , dry_run : bool = False , file_friendly_logging : bool = False ) -> Optional [ Model ] Trains the model specified in the given Params object, using the data and training parameters also specified in that object, and saves the results in serialization_dir . Parameters params : Params A parameter object specifying an AllenNLP Experiment. serialization_dir : str The directory in which to save results and logs. recover : bool , optional (default = False ) If True , we will try to recover a training run from an existing serialization directory. This is only intended for use when something actually crashed during the middle of a run. For continuing training a model on new data, see Model.from_archive . force : bool , optional (default = False ) If True , we will overwrite the serialization directory if it already exists. node_rank : int , optional Rank of the current node in distributed training include_package : List[str] , optional In distributed mode, extra packages mentioned will be imported in trainer workers. dry_run : bool , optional (default = False ) Do not train a model, but create a vocabulary, show dataset statistics and other training information. file_friendly_logging : bool , optional (default = False ) If True , we add newlines to tqdm output, even on an interactive terminal, and we slow down tqdm's output to only once every 10 seconds. Returns best_model : Optional[Model] The model with the best epoch weights or None if in dry run. TrainModel # class TrainModel ( Registrable ): | def __init__ ( | self , | serialization_dir : str , | model : Model , | trainer : Trainer , | evaluation_data_loader : DataLoader = None , | evaluate_on_test : bool = False , | batch_weight_key : str = \"\" | ) -> None This class exists so that we can easily read a configuration file with the allennlp train command. The basic logic is that we call train_loop = TrainModel.from_params(params_from_config_file) , then train_loop.run() . This class performs very little logic, pushing most of it to the Trainer that has a train() method. The point here is to construct all of the dependencies for the Trainer in a way that we can do it using from_params() , while having all of those dependencies transparently documented and not hidden in calls to params.pop() . If you are writing your own training loop, you almost certainly should not use this class, but you might look at the code for this class to see what we do, to make writing your training loop easier. In particular, if you are tempted to call the __init__ method of this class, you are probably doing something unnecessary. Literally all we do after __init__ is call trainer.train() . You can do that yourself, if you've constructed a Trainer already. What this class gives you is a way to construct the Trainer by means of a config file. The actual constructor that we use with from_params in this class is from_partial_objects . See that method for a description of all of the allowed top-level keys in a configuration file used with allennlp train . default_implementation # class TrainModel ( Registrable ): | ... | default_implementation = \"default\" The default implementation is registered as 'default'. run # class TrainModel ( Registrable ): | ... | def run ( self ) -> Dict [ str , Any ] finish # class TrainModel ( Registrable ): | ... | def finish ( self , metrics : Dict [ str , Any ]) from_partial_objects # class TrainModel ( Registrable ): | ... | @classmethod | def from_partial_objects ( | cls , | serialization_dir : str , | local_rank : int , | dataset_reader : DatasetReader , | train_data_path : str , | model : Lazy [ Model ], | data_loader : Lazy [ DataLoader ], | trainer : Lazy [ Trainer ], | vocabulary : Lazy [ Vocabulary ] = None , | datasets_for_vocab_creation : List [ str ] = None , | validation_dataset_reader : DatasetReader = None , | validation_data_path : str = None , | validation_data_loader : Lazy [ DataLoader ] = None , | test_data_path : str = None , | evaluate_on_test : bool = False , | batch_weight_key : str = \"\" | ) -> \"TrainModel\" This method is intended for use with our FromParams logic, to construct a TrainModel object from a config file passed to the allennlp train command. The arguments to this method are the allowed top-level keys in a configuration file (except for the first three, which are obtained separately). You could use this outside of our FromParams logic if you really want to, but there might be easier ways to accomplish your goal than instantiating Lazy objects. If you are writing your own training loop, we recommend that you look at the implementation of this method for inspiration and possibly some utility functions you can call, but you very likely should not use this method directly. The Lazy type annotations here are a mechanism for building dependencies to an object sequentially - the TrainModel object needs data, a model, and a trainer, but the model needs to see the data before it's constructed (to create a vocabulary) and the trainer needs the data and the model before it's constructed. Objects that have sequential dependencies like this are labeled as Lazy in their type annotations, and we pass the missing dependencies when we call their construct() method, which you can see in the code below. Parameters serialization_dir : str The directory where logs and model archives will be saved. In a typical AllenNLP configuration file, this parameter does not get an entry as a top-level key, it gets passed in separately. - local_rank : int The process index that is initialized using the GPU device id. In a typical AllenNLP configuration file, this parameter does not get an entry as a top-level key, it gets passed in separately. - dataset_reader : DatasetReader The DatasetReader that will be used for training and (by default) for validation. - train_data_path : str The file (or directory) that will be passed to dataset_reader.read() to construct the training data. - model : Lazy[Model] The model that we will train. This is lazy because it depends on the Vocabulary ; after constructing the vocabulary we call model.construct(vocab=vocabulary) . - data_loader : Lazy[DataLoader] The data_loader we use to batch instances from the dataset reader at training and (by default) validation time. This is lazy because it takes a dataset in it's constructor. - trainer : Lazy[Trainer] The Trainer that actually implements the training loop. This is a lazy object because it depends on the model that's going to be trained. - vocabulary : Lazy[Vocabulary] , optional (default = None ) The Vocabulary that we will use to convert strings in the data to integer ids (and possibly set sizes of embedding matrices in the Model ). By default we construct the vocabulary from the instances that we read. - datasets_for_vocab_creation : List[str] , optional (default = None ) If you pass in more than one dataset but don't want to use all of them to construct a vocabulary, you can pass in this key to limit it. Valid entries in the list are \"train\", \"validation\" and \"test\". - validation_dataset_reader : DatasetReader , optional (default = None ) If given, we will use this dataset reader for the validation data instead of dataset_reader . - validation_data_path : str , optional (default = None ) If given, we will use this data for computing validation metrics and early stopping. - validation_data_loader : Lazy[DataLoader] , optional (default = None ) If given, the data_loader we use to batch instances from the dataset reader at validation and test time. This is lazy because it takes a dataset in it's constructor. - test_data_path : str , optional (default = None ) If given, we will use this as test data. This makes it available for vocab creation by default, but nothing else. - evaluate_on_test : bool , optional (default = False ) If given, we will evaluate the final model on this data at the end of training. Note that we do not recommend using this for actual test data in every-day experimentation; you should only very rarely evaluate your model on actual test data. - batch_weight_key : str , optional (default = \"\" ) The name of metric used to weight the loss on a per-batch basis. This is only used during evaluation on final test data, if you've specified evaluate_on_test=True .","title":"train"},{"location":"api/commands/train/#train","text":"class Train ( Subcommand )","title":"Train"},{"location":"api/commands/train/#add_subparser","text":"class Train ( Subcommand ): | ... | @overrides | def add_subparser ( | self , | parser : argparse . _SubParsersAction | ) -> argparse . ArgumentParser","title":"add_subparser"},{"location":"api/commands/train/#train_model_from_args","text":"def train_model_from_args ( args : argparse . Namespace ) Just converts from an argparse.Namespace object to string paths.","title":"train_model_from_args"},{"location":"api/commands/train/#train_model_from_file","text":"def train_model_from_file ( parameter_filename : Union [ str , PathLike ], serialization_dir : Union [ str , PathLike ], overrides : str = \"\" , recover : bool = False , force : bool = False , node_rank : int = 0 , include_package : List [ str ] = None , dry_run : bool = False , file_friendly_logging : bool = False ) -> Optional [ Model ] A wrapper around train_model which loads the params from a file. Parameters parameter_filename : str A json parameter file specifying an AllenNLP experiment. serialization_dir : str The directory in which to save results and logs. We just pass this along to train_model . overrides : str A JSON string that we will use to override values in the input parameter file. recover : bool , optional (default = False ) If True , we will try to recover a training run from an existing serialization directory. This is only intended for use when something actually crashed during the middle of a run. For continuing training a model on new data, see Model.from_archive . force : bool , optional (default = False ) If True , we will overwrite the serialization directory if it already exists. node_rank : int , optional Rank of the current node in distributed training include_package : str , optional In distributed mode, extra packages mentioned will be imported in trainer workers. dry_run : bool , optional (default = False ) Do not train a model, but create a vocabulary, show dataset statistics and other training information. file_friendly_logging : bool , optional (default = False ) If True , we add newlines to tqdm output, even on an interactive terminal, and we slow down tqdm's output to only once every 10 seconds. Returns best_model : Optional[Model] The model with the best epoch weights or None if in dry run.","title":"train_model_from_file"},{"location":"api/commands/train/#train_model","text":"def train_model ( params : Params , serialization_dir : Union [ str , PathLike ], recover : bool = False , force : bool = False , node_rank : int = 0 , include_package : List [ str ] = None , dry_run : bool = False , file_friendly_logging : bool = False ) -> Optional [ Model ] Trains the model specified in the given Params object, using the data and training parameters also specified in that object, and saves the results in serialization_dir . Parameters params : Params A parameter object specifying an AllenNLP Experiment. serialization_dir : str The directory in which to save results and logs. recover : bool , optional (default = False ) If True , we will try to recover a training run from an existing serialization directory. This is only intended for use when something actually crashed during the middle of a run. For continuing training a model on new data, see Model.from_archive . force : bool , optional (default = False ) If True , we will overwrite the serialization directory if it already exists. node_rank : int , optional Rank of the current node in distributed training include_package : List[str] , optional In distributed mode, extra packages mentioned will be imported in trainer workers. dry_run : bool , optional (default = False ) Do not train a model, but create a vocabulary, show dataset statistics and other training information. file_friendly_logging : bool , optional (default = False ) If True , we add newlines to tqdm output, even on an interactive terminal, and we slow down tqdm's output to only once every 10 seconds. Returns best_model : Optional[Model] The model with the best epoch weights or None if in dry run.","title":"train_model"},{"location":"api/commands/train/#trainmodel","text":"class TrainModel ( Registrable ): | def __init__ ( | self , | serialization_dir : str , | model : Model , | trainer : Trainer , | evaluation_data_loader : DataLoader = None , | evaluate_on_test : bool = False , | batch_weight_key : str = \"\" | ) -> None This class exists so that we can easily read a configuration file with the allennlp train command. The basic logic is that we call train_loop = TrainModel.from_params(params_from_config_file) , then train_loop.run() . This class performs very little logic, pushing most of it to the Trainer that has a train() method. The point here is to construct all of the dependencies for the Trainer in a way that we can do it using from_params() , while having all of those dependencies transparently documented and not hidden in calls to params.pop() . If you are writing your own training loop, you almost certainly should not use this class, but you might look at the code for this class to see what we do, to make writing your training loop easier. In particular, if you are tempted to call the __init__ method of this class, you are probably doing something unnecessary. Literally all we do after __init__ is call trainer.train() . You can do that yourself, if you've constructed a Trainer already. What this class gives you is a way to construct the Trainer by means of a config file. The actual constructor that we use with from_params in this class is from_partial_objects . See that method for a description of all of the allowed top-level keys in a configuration file used with allennlp train .","title":"TrainModel"},{"location":"api/commands/train/#default_implementation","text":"class TrainModel ( Registrable ): | ... | default_implementation = \"default\" The default implementation is registered as 'default'.","title":"default_implementation"},{"location":"api/commands/train/#run","text":"class TrainModel ( Registrable ): | ... | def run ( self ) -> Dict [ str , Any ]","title":"run"},{"location":"api/commands/train/#finish","text":"class TrainModel ( Registrable ): | ... | def finish ( self , metrics : Dict [ str , Any ])","title":"finish"},{"location":"api/commands/train/#from_partial_objects","text":"class TrainModel ( Registrable ): | ... | @classmethod | def from_partial_objects ( | cls , | serialization_dir : str , | local_rank : int , | dataset_reader : DatasetReader , | train_data_path : str , | model : Lazy [ Model ], | data_loader : Lazy [ DataLoader ], | trainer : Lazy [ Trainer ], | vocabulary : Lazy [ Vocabulary ] = None , | datasets_for_vocab_creation : List [ str ] = None , | validation_dataset_reader : DatasetReader = None , | validation_data_path : str = None , | validation_data_loader : Lazy [ DataLoader ] = None , | test_data_path : str = None , | evaluate_on_test : bool = False , | batch_weight_key : str = \"\" | ) -> \"TrainModel\" This method is intended for use with our FromParams logic, to construct a TrainModel object from a config file passed to the allennlp train command. The arguments to this method are the allowed top-level keys in a configuration file (except for the first three, which are obtained separately). You could use this outside of our FromParams logic if you really want to, but there might be easier ways to accomplish your goal than instantiating Lazy objects. If you are writing your own training loop, we recommend that you look at the implementation of this method for inspiration and possibly some utility functions you can call, but you very likely should not use this method directly. The Lazy type annotations here are a mechanism for building dependencies to an object sequentially - the TrainModel object needs data, a model, and a trainer, but the model needs to see the data before it's constructed (to create a vocabulary) and the trainer needs the data and the model before it's constructed. Objects that have sequential dependencies like this are labeled as Lazy in their type annotations, and we pass the missing dependencies when we call their construct() method, which you can see in the code below. Parameters serialization_dir : str The directory where logs and model archives will be saved. In a typical AllenNLP configuration file, this parameter does not get an entry as a top-level key, it gets passed in separately. - local_rank : int The process index that is initialized using the GPU device id. In a typical AllenNLP configuration file, this parameter does not get an entry as a top-level key, it gets passed in separately. - dataset_reader : DatasetReader The DatasetReader that will be used for training and (by default) for validation. - train_data_path : str The file (or directory) that will be passed to dataset_reader.read() to construct the training data. - model : Lazy[Model] The model that we will train. This is lazy because it depends on the Vocabulary ; after constructing the vocabulary we call model.construct(vocab=vocabulary) . - data_loader : Lazy[DataLoader] The data_loader we use to batch instances from the dataset reader at training and (by default) validation time. This is lazy because it takes a dataset in it's constructor. - trainer : Lazy[Trainer] The Trainer that actually implements the training loop. This is a lazy object because it depends on the model that's going to be trained. - vocabulary : Lazy[Vocabulary] , optional (default = None ) The Vocabulary that we will use to convert strings in the data to integer ids (and possibly set sizes of embedding matrices in the Model ). By default we construct the vocabulary from the instances that we read. - datasets_for_vocab_creation : List[str] , optional (default = None ) If you pass in more than one dataset but don't want to use all of them to construct a vocabulary, you can pass in this key to limit it. Valid entries in the list are \"train\", \"validation\" and \"test\". - validation_dataset_reader : DatasetReader , optional (default = None ) If given, we will use this dataset reader for the validation data instead of dataset_reader . - validation_data_path : str , optional (default = None ) If given, we will use this data for computing validation metrics and early stopping. - validation_data_loader : Lazy[DataLoader] , optional (default = None ) If given, the data_loader we use to batch instances from the dataset reader at validation and test time. This is lazy because it takes a dataset in it's constructor. - test_data_path : str , optional (default = None ) If given, we will use this as test data. This makes it available for vocab creation by default, but nothing else. - evaluate_on_test : bool , optional (default = False ) If given, we will evaluate the final model on this data at the end of training. Note that we do not recommend using this for actual test data in every-day experimentation; you should only very rarely evaluate your model on actual test data. - batch_weight_key : str , optional (default = \"\" ) The name of metric used to weight the loss on a per-batch basis. This is only used during evaluation on final test data, if you've specified evaluate_on_test=True .","title":"from_partial_objects"},{"location":"api/common/cached_transformers/","text":"[ allennlp .common .cached_transformers ] TransformerSpec # class TransformerSpec ( NamedTuple ) model_name # class TransformerSpec ( NamedTuple ): | ... | model_name : str = None override_weights_file # class TransformerSpec ( NamedTuple ): | ... | override_weights_file : Optional [ str ] = None override_weights_strip_prefix # class TransformerSpec ( NamedTuple ): | ... | override_weights_strip_prefix : Optional [ str ] = None get # def get ( model_name : str , make_copy : bool , override_weights_file : Optional [ str ] = None , override_weights_strip_prefix : Optional [ str ] = None ) -> transformers . PreTrainedModel Returns a transformer model from the cache. Parameters model_name : str The name of the transformer, for example \"bert-base-cased\" make_copy : bool If this is True , return a copy of the model instead of the cached model itself. If you want to modify the parameters of the model, set this to True . If you want only part of the model, set this to False , but make sure to copy.deepcopy() the bits you are keeping. override_weights_file : str , optional If set, this specifies a file from which to load alternate weights that override the weights from huggingface. The file is expected to contain a PyTorch state_dict , created with torch.save() . override_weights_strip_prefix : str , optional If set, strip the given prefix from the state dict when loading it. get_tokenizer # def get_tokenizer ( model_name : str , ** kwargs ) -> transformers . PreTrainedTokenizer","title":"cached_transformers"},{"location":"api/common/cached_transformers/#transformerspec","text":"class TransformerSpec ( NamedTuple )","title":"TransformerSpec"},{"location":"api/common/cached_transformers/#model_name","text":"class TransformerSpec ( NamedTuple ): | ... | model_name : str = None","title":"model_name"},{"location":"api/common/cached_transformers/#override_weights_file","text":"class TransformerSpec ( NamedTuple ): | ... | override_weights_file : Optional [ str ] = None","title":"override_weights_file"},{"location":"api/common/cached_transformers/#override_weights_strip_prefix","text":"class TransformerSpec ( NamedTuple ): | ... | override_weights_strip_prefix : Optional [ str ] = None","title":"override_weights_strip_prefix"},{"location":"api/common/cached_transformers/#get","text":"def get ( model_name : str , make_copy : bool , override_weights_file : Optional [ str ] = None , override_weights_strip_prefix : Optional [ str ] = None ) -> transformers . PreTrainedModel Returns a transformer model from the cache. Parameters model_name : str The name of the transformer, for example \"bert-base-cased\" make_copy : bool If this is True , return a copy of the model instead of the cached model itself. If you want to modify the parameters of the model, set this to True . If you want only part of the model, set this to False , but make sure to copy.deepcopy() the bits you are keeping. override_weights_file : str , optional If set, this specifies a file from which to load alternate weights that override the weights from huggingface. The file is expected to contain a PyTorch state_dict , created with torch.save() . override_weights_strip_prefix : str , optional If set, strip the given prefix from the state dict when loading it.","title":"get"},{"location":"api/common/cached_transformers/#get_tokenizer","text":"def get_tokenizer ( model_name : str , ** kwargs ) -> transformers . PreTrainedTokenizer","title":"get_tokenizer"},{"location":"api/common/checks/","text":"[ allennlp .common .checks ] Functions and exceptions for checking that AllenNLP and its models are configured correctly. ConfigurationError # class ConfigurationError ( Exception ): | def __init__ ( self , message ) The exception raised by any AllenNLP object when it's misconfigured (e.g. missing properties, invalid properties, unknown properties). ExperimentalFeatureWarning # class ExperimentalFeatureWarning ( RuntimeWarning ) A warning that you are using an experimental feature that may change or be deleted. log_pytorch_version_info # def log_pytorch_version_info () check_dimensions_match # def check_dimensions_match ( dimension_1 : int , dimension_2 : int , dim_1_name : str , dim_2_name : str ) -> None parse_cuda_device # def parse_cuda_device ( cuda_device : Union [ str , int , List [ int ]]) -> int Disambiguates single GPU and multiple GPU settings for cuda_device param. check_for_gpu # def check_for_gpu ( device : Union [ int , torch . device , List [ Union [ int , torch . device ]]] ) check_for_java # def check_for_java () -> bool","title":"checks"},{"location":"api/common/checks/#configurationerror","text":"class ConfigurationError ( Exception ): | def __init__ ( self , message ) The exception raised by any AllenNLP object when it's misconfigured (e.g. missing properties, invalid properties, unknown properties).","title":"ConfigurationError"},{"location":"api/common/checks/#experimentalfeaturewarning","text":"class ExperimentalFeatureWarning ( RuntimeWarning ) A warning that you are using an experimental feature that may change or be deleted.","title":"ExperimentalFeatureWarning"},{"location":"api/common/checks/#log_pytorch_version_info","text":"def log_pytorch_version_info ()","title":"log_pytorch_version_info"},{"location":"api/common/checks/#check_dimensions_match","text":"def check_dimensions_match ( dimension_1 : int , dimension_2 : int , dim_1_name : str , dim_2_name : str ) -> None","title":"check_dimensions_match"},{"location":"api/common/checks/#parse_cuda_device","text":"def parse_cuda_device ( cuda_device : Union [ str , int , List [ int ]]) -> int Disambiguates single GPU and multiple GPU settings for cuda_device param.","title":"parse_cuda_device"},{"location":"api/common/checks/#check_for_gpu","text":"def check_for_gpu ( device : Union [ int , torch . device , List [ Union [ int , torch . device ]]] )","title":"check_for_gpu"},{"location":"api/common/checks/#check_for_java","text":"def check_for_java () -> bool","title":"check_for_java"},{"location":"api/common/file_utils/","text":"[ allennlp .common .file_utils ] Utilities for working with the local dataset cache. CACHE_ROOT # CACHE_ROOT = Path ( os . getenv ( \"ALLENNLP_CACHE_ROOT\" , Path . home () / \".allennlp\" )) CACHE_DIRECTORY # CACHE_DIRECTORY = str ( CACHE_ROOT / \"cache\" ) DEPRECATED_CACHE_DIRECTORY # DEPRECATED_CACHE_DIRECTORY = str ( CACHE_ROOT / \"datasets\" ) DATASET_CACHE # DATASET_CACHE = CACHE_DIRECTORY url_to_filename # def url_to_filename ( url : str , etag : str = None ) -> str Convert url into a hashed filename in a repeatable way. If etag is specified, append its hash to the url's, delimited by a period. filename_to_url # def filename_to_url ( filename : str , cache_dir : Union [ str , Path ] = None ) -> Tuple [ str , str ] Return the url and etag (which may be None ) stored for filename . Raise FileNotFoundError if filename or its stored metadata do not exist. cached_path # def cached_path ( url_or_filename : Union [ str , PathLike ], cache_dir : Union [ str , Path ] = None , extract_archive : bool = False , force_extract : bool = False ) -> str Given something that might be a URL (or might be a local path), determine which. If it's a URL, download the file and cache it, and return the path to the cached file. If it's already a local path, make sure the file exists and then return the path. Parameters url_or_filename : Union[str, Path] A URL or local file to parse and possibly download. cache_dir : Union[str, Path] , optional (default = None ) The directory to cache downloads. extract_archive : bool , optional (default = False ) If True , then zip or tar.gz archives will be automatically extracted. In which case the directory is returned. force_extract : bool , optional (default = False ) If True and the file is an archive file, it will be extracted regardless of whether or not the extracted directory already exists. is_url_or_existing_file # def is_url_or_existing_file ( url_or_filename : Union [ str , Path , None ] ) -> bool Given something that might be a URL (or might be a local path), determine check if it's url or an existing file path. CacheFile # class CacheFile : | def __init__ ( | self , | cache_filename : Union [ Path , str ], | mode = \"w+b\" | ) -> None This is a context manager that makes robust caching easier. On __enter__ , an IO handle to a temporarily file is returned, which can be treated as if it's the actual cache file. On __exit__ , the temporarily file is renamed to the cache file. If anything goes wrong while writing to the temporary file, it will be removed. get_from_cache # def get_from_cache ( url : str , cache_dir : Union [ str , Path ] = None ) -> str Given a URL, look for the corresponding dataset in the local cache. If it's not there, download it. Then return the path to the cached file. read_set_from_file # def read_set_from_file ( filename : str ) -> Set [ str ] Extract a de-duped collection (set) of text from a file. Expected file format is one item per line. get_file_extension # def get_file_extension ( path : str , dot = True , lower : bool = True ) open_compressed # def open_compressed ( filename : Union [ str , Path ], mode : str = \"rt\" , encoding : Optional [ str ] = \"UTF-8\" , ** kwargs ) text_lines_from_file # def text_lines_from_file ( filename : Union [ str , Path ], strip_lines : bool = True ) -> Iterator [ str ] json_lines_from_file # def json_lines_from_file ( filename : Union [ str , Path ] ) -> Iterable [ Union [ list , dict ]]","title":"file_utils"},{"location":"api/common/file_utils/#cache_root","text":"CACHE_ROOT = Path ( os . getenv ( \"ALLENNLP_CACHE_ROOT\" , Path . home () / \".allennlp\" ))","title":"CACHE_ROOT"},{"location":"api/common/file_utils/#cache_directory","text":"CACHE_DIRECTORY = str ( CACHE_ROOT / \"cache\" )","title":"CACHE_DIRECTORY"},{"location":"api/common/file_utils/#deprecated_cache_directory","text":"DEPRECATED_CACHE_DIRECTORY = str ( CACHE_ROOT / \"datasets\" )","title":"DEPRECATED_CACHE_DIRECTORY"},{"location":"api/common/file_utils/#dataset_cache","text":"DATASET_CACHE = CACHE_DIRECTORY","title":"DATASET_CACHE"},{"location":"api/common/file_utils/#url_to_filename","text":"def url_to_filename ( url : str , etag : str = None ) -> str Convert url into a hashed filename in a repeatable way. If etag is specified, append its hash to the url's, delimited by a period.","title":"url_to_filename"},{"location":"api/common/file_utils/#filename_to_url","text":"def filename_to_url ( filename : str , cache_dir : Union [ str , Path ] = None ) -> Tuple [ str , str ] Return the url and etag (which may be None ) stored for filename . Raise FileNotFoundError if filename or its stored metadata do not exist.","title":"filename_to_url"},{"location":"api/common/file_utils/#cached_path","text":"def cached_path ( url_or_filename : Union [ str , PathLike ], cache_dir : Union [ str , Path ] = None , extract_archive : bool = False , force_extract : bool = False ) -> str Given something that might be a URL (or might be a local path), determine which. If it's a URL, download the file and cache it, and return the path to the cached file. If it's already a local path, make sure the file exists and then return the path. Parameters url_or_filename : Union[str, Path] A URL or local file to parse and possibly download. cache_dir : Union[str, Path] , optional (default = None ) The directory to cache downloads. extract_archive : bool , optional (default = False ) If True , then zip or tar.gz archives will be automatically extracted. In which case the directory is returned. force_extract : bool , optional (default = False ) If True and the file is an archive file, it will be extracted regardless of whether or not the extracted directory already exists.","title":"cached_path"},{"location":"api/common/file_utils/#is_url_or_existing_file","text":"def is_url_or_existing_file ( url_or_filename : Union [ str , Path , None ] ) -> bool Given something that might be a URL (or might be a local path), determine check if it's url or an existing file path.","title":"is_url_or_existing_file"},{"location":"api/common/file_utils/#cachefile","text":"class CacheFile : | def __init__ ( | self , | cache_filename : Union [ Path , str ], | mode = \"w+b\" | ) -> None This is a context manager that makes robust caching easier. On __enter__ , an IO handle to a temporarily file is returned, which can be treated as if it's the actual cache file. On __exit__ , the temporarily file is renamed to the cache file. If anything goes wrong while writing to the temporary file, it will be removed.","title":"CacheFile"},{"location":"api/common/file_utils/#get_from_cache","text":"def get_from_cache ( url : str , cache_dir : Union [ str , Path ] = None ) -> str Given a URL, look for the corresponding dataset in the local cache. If it's not there, download it. Then return the path to the cached file.","title":"get_from_cache"},{"location":"api/common/file_utils/#read_set_from_file","text":"def read_set_from_file ( filename : str ) -> Set [ str ] Extract a de-duped collection (set) of text from a file. Expected file format is one item per line.","title":"read_set_from_file"},{"location":"api/common/file_utils/#get_file_extension","text":"def get_file_extension ( path : str , dot = True , lower : bool = True )","title":"get_file_extension"},{"location":"api/common/file_utils/#open_compressed","text":"def open_compressed ( filename : Union [ str , Path ], mode : str = \"rt\" , encoding : Optional [ str ] = \"UTF-8\" , ** kwargs )","title":"open_compressed"},{"location":"api/common/file_utils/#text_lines_from_file","text":"def text_lines_from_file ( filename : Union [ str , Path ], strip_lines : bool = True ) -> Iterator [ str ]","title":"text_lines_from_file"},{"location":"api/common/file_utils/#json_lines_from_file","text":"def json_lines_from_file ( filename : Union [ str , Path ] ) -> Iterable [ Union [ list , dict ]]","title":"json_lines_from_file"},{"location":"api/common/from_params/","text":"[ allennlp .common .from_params ] T # T = TypeVar ( \"T\" , bound = \"FromParams\" ) takes_arg # def takes_arg ( obj , arg : str ) -> bool Checks whether the provided obj takes a certain arg. If it's a class, we're really checking whether its constructor does. If it's a function or method, we're checking the object itself. Otherwise, we raise an error. takes_kwargs # def takes_kwargs ( obj ) -> bool Checks whether a provided object takes in any positional arguments. Similar to takes_arg, we do this for both the init function of the class or a function / method Otherwise, we raise an error can_construct_from_params # def can_construct_from_params ( type_ : Type ) -> bool is_base_registrable # def is_base_registrable ( cls ) -> bool Checks whether this is a class that directly inherits from Registrable, or is a subclass of such a class. remove_optional # def remove_optional ( annotation : type ) Optional[X] annotations are actually represented as Union[X, NoneType]. For our purposes, the \"Optional\" part is not interesting, so here we throw it away. infer_params # def infer_params ( cls : Type [ T ], constructor : Callable [ ... , T ] = None ) create_kwargs # def create_kwargs ( constructor : Callable [ ... , T ], cls : Type [ T ], params : Params , ** extras ) -> Dict [ str , Any ] Given some class, a Params object, and potentially other keyword arguments, create a dict of keyword args suitable for passing to the class's constructor. The function does this by finding the class's constructor, matching the constructor arguments to entries in the params object, and instantiating values for the parameters using the type annotation and possibly a from_params method. Any values that are provided in the extras will just be used as is. For instance, you might provide an existing Vocabulary this way. create_extras # def create_extras ( cls : Type [ T ], extras : Dict [ str , Any ] ) -> Dict [ str , Any ] Given a dictionary of extra arguments, returns a dictionary of kwargs that actually are a part of the signature of the cls.from_params (or cls) method. pop_and_construct_arg # def pop_and_construct_arg ( class_name : str , argument_name : str , annotation : Type , default : Any , params : Params , ** extras ) -> Any Does the work of actually constructing an individual argument for create_kwargs . Here we're in the inner loop of iterating over the parameters to a particular constructor, trying to construct just one of them. The information we get for that parameter is its name, its type annotation, and its default value; we also get the full set of Params for constructing the object (which we may mutate), and any extras that the constructor might need. We take the type annotation and default value here separately, instead of using an inspect.Parameter object directly, so that we can handle Union types using recursion on this method, trying the different annotation types in the union in turn. construct_arg # def construct_arg ( class_name : str , argument_name : str , popped_params : Params , annotation : Type , default : Any , ** extras ) -> Any The first two parameters here are only used for logging if we encounter an error. FromParams # class FromParams Mixin to give a from_params method to classes. We create a distinct base class for this because sometimes we want non-Registrable classes to be instantiatable from_params. from_params # class FromParams : | ... | @classmethod | def from_params ( | cls : Type [ T ], | params : Params , | constructor_to_call : Callable [ ... , T ] = None , | constructor_to_inspect : Callable [ ... , T ] = None , | ** extras | ) -> T This is the automatic implementation of from_params . Any class that subclasses FromParams (or Registrable , which itself subclasses FromParams ) gets this implementation for free. If you want your class to be instantiated from params in the \"obvious\" way -- pop off parameters and hand them to your constructor with the same names -- this provides that functionality. If you need more complex logic in your from from_params method, you'll have to implement your own method that overrides this one. The constructor_to_call and constructor_to_inspect arguments deal with a bit of redirection that we do. We allow you to register particular @classmethods on a class as the constructor to use for a registered name. This lets you, e.g., have a single Vocabulary class that can be constructed in two different ways, with different names registered to each constructor. In order to handle this, we need to know not just the class we're trying to construct ( cls ), but also what method we should inspect to find its arguments ( constructor_to_inspect ), and what method to call when we're done constructing arguments ( constructor_to_call ). These two methods are the same when you've used a @classmethod as your constructor, but they are different when you use the default constructor (because you inspect __init__ , but call cls() ).","title":"from_params"},{"location":"api/common/from_params/#t","text":"T = TypeVar ( \"T\" , bound = \"FromParams\" )","title":"T"},{"location":"api/common/from_params/#takes_arg","text":"def takes_arg ( obj , arg : str ) -> bool Checks whether the provided obj takes a certain arg. If it's a class, we're really checking whether its constructor does. If it's a function or method, we're checking the object itself. Otherwise, we raise an error.","title":"takes_arg"},{"location":"api/common/from_params/#takes_kwargs","text":"def takes_kwargs ( obj ) -> bool Checks whether a provided object takes in any positional arguments. Similar to takes_arg, we do this for both the init function of the class or a function / method Otherwise, we raise an error","title":"takes_kwargs"},{"location":"api/common/from_params/#can_construct_from_params","text":"def can_construct_from_params ( type_ : Type ) -> bool","title":"can_construct_from_params"},{"location":"api/common/from_params/#is_base_registrable","text":"def is_base_registrable ( cls ) -> bool Checks whether this is a class that directly inherits from Registrable, or is a subclass of such a class.","title":"is_base_registrable"},{"location":"api/common/from_params/#remove_optional","text":"def remove_optional ( annotation : type ) Optional[X] annotations are actually represented as Union[X, NoneType]. For our purposes, the \"Optional\" part is not interesting, so here we throw it away.","title":"remove_optional"},{"location":"api/common/from_params/#infer_params","text":"def infer_params ( cls : Type [ T ], constructor : Callable [ ... , T ] = None )","title":"infer_params"},{"location":"api/common/from_params/#create_kwargs","text":"def create_kwargs ( constructor : Callable [ ... , T ], cls : Type [ T ], params : Params , ** extras ) -> Dict [ str , Any ] Given some class, a Params object, and potentially other keyword arguments, create a dict of keyword args suitable for passing to the class's constructor. The function does this by finding the class's constructor, matching the constructor arguments to entries in the params object, and instantiating values for the parameters using the type annotation and possibly a from_params method. Any values that are provided in the extras will just be used as is. For instance, you might provide an existing Vocabulary this way.","title":"create_kwargs"},{"location":"api/common/from_params/#create_extras","text":"def create_extras ( cls : Type [ T ], extras : Dict [ str , Any ] ) -> Dict [ str , Any ] Given a dictionary of extra arguments, returns a dictionary of kwargs that actually are a part of the signature of the cls.from_params (or cls) method.","title":"create_extras"},{"location":"api/common/from_params/#pop_and_construct_arg","text":"def pop_and_construct_arg ( class_name : str , argument_name : str , annotation : Type , default : Any , params : Params , ** extras ) -> Any Does the work of actually constructing an individual argument for create_kwargs . Here we're in the inner loop of iterating over the parameters to a particular constructor, trying to construct just one of them. The information we get for that parameter is its name, its type annotation, and its default value; we also get the full set of Params for constructing the object (which we may mutate), and any extras that the constructor might need. We take the type annotation and default value here separately, instead of using an inspect.Parameter object directly, so that we can handle Union types using recursion on this method, trying the different annotation types in the union in turn.","title":"pop_and_construct_arg"},{"location":"api/common/from_params/#construct_arg","text":"def construct_arg ( class_name : str , argument_name : str , popped_params : Params , annotation : Type , default : Any , ** extras ) -> Any The first two parameters here are only used for logging if we encounter an error.","title":"construct_arg"},{"location":"api/common/from_params/#fromparams","text":"class FromParams Mixin to give a from_params method to classes. We create a distinct base class for this because sometimes we want non-Registrable classes to be instantiatable from_params.","title":"FromParams"},{"location":"api/common/from_params/#from_params","text":"class FromParams : | ... | @classmethod | def from_params ( | cls : Type [ T ], | params : Params , | constructor_to_call : Callable [ ... , T ] = None , | constructor_to_inspect : Callable [ ... , T ] = None , | ** extras | ) -> T This is the automatic implementation of from_params . Any class that subclasses FromParams (or Registrable , which itself subclasses FromParams ) gets this implementation for free. If you want your class to be instantiated from params in the \"obvious\" way -- pop off parameters and hand them to your constructor with the same names -- this provides that functionality. If you need more complex logic in your from from_params method, you'll have to implement your own method that overrides this one. The constructor_to_call and constructor_to_inspect arguments deal with a bit of redirection that we do. We allow you to register particular @classmethods on a class as the constructor to use for a registered name. This lets you, e.g., have a single Vocabulary class that can be constructed in two different ways, with different names registered to each constructor. In order to handle this, we need to know not just the class we're trying to construct ( cls ), but also what method we should inspect to find its arguments ( constructor_to_inspect ), and what method to call when we're done constructing arguments ( constructor_to_call ). These two methods are the same when you've used a @classmethod as your constructor, but they are different when you use the default constructor (because you inspect __init__ , but call cls() ).","title":"from_params"},{"location":"api/common/lazy/","text":"[ allennlp .common .lazy ] T # T = TypeVar ( \"T\" ) Lazy # class Lazy ( Generic [ T ]): | def __init__ ( self , constructor : Callable [ ... , T ]) This class is for use when constructing objects using FromParams , when an argument to a constructor has a sequential dependency with another argument to the same constructor. For example, in a Trainer class you might want to take a Model and an Optimizer as arguments, but the Optimizer needs to be constructed using the parameters from the Model . You can give the type annotation Lazy[Optimizer] to the optimizer argument, then inside the constructor call optimizer.construct(parameters=model.parameters) . This is only recommended for use when you have registered a @classmethod as the constructor for your class, instead of using __init__ . Having a Lazy[] type annotation on an argument to an __init__ method makes your class completely dependent on being constructed using the FromParams pipeline, which is not a good idea. The actual implementation here is incredibly simple; the logic that handles the lazy construction is actually found in FromParams , where we have a special case for a Lazy type annotation. Warning The way this class is used in from_params means that optional constructor arguments CANNOT be compared to None before it is constructed. See the example below for correct usage. @classmethod def my_constructor ( cls , some_object : Lazy [ MyObject ] = None ) -> MyClass : ... # WRONG ! some_object will never be None at this point , it will be # a Lazy [] that returns None obj = some_object or MyObjectDefault () # CORRECT : obj = some_object . construct ( kwarg = kwarg ) or MyObjectDefault () ... construct # class Lazy ( Generic [ T ]): | ... | def construct ( self , ** kwargs ) -> Optional [ T ]","title":"lazy"},{"location":"api/common/lazy/#t","text":"T = TypeVar ( \"T\" )","title":"T"},{"location":"api/common/lazy/#lazy","text":"class Lazy ( Generic [ T ]): | def __init__ ( self , constructor : Callable [ ... , T ]) This class is for use when constructing objects using FromParams , when an argument to a constructor has a sequential dependency with another argument to the same constructor. For example, in a Trainer class you might want to take a Model and an Optimizer as arguments, but the Optimizer needs to be constructed using the parameters from the Model . You can give the type annotation Lazy[Optimizer] to the optimizer argument, then inside the constructor call optimizer.construct(parameters=model.parameters) . This is only recommended for use when you have registered a @classmethod as the constructor for your class, instead of using __init__ . Having a Lazy[] type annotation on an argument to an __init__ method makes your class completely dependent on being constructed using the FromParams pipeline, which is not a good idea. The actual implementation here is incredibly simple; the logic that handles the lazy construction is actually found in FromParams , where we have a special case for a Lazy type annotation. Warning The way this class is used in from_params means that optional constructor arguments CANNOT be compared to None before it is constructed. See the example below for correct usage. @classmethod def my_constructor ( cls , some_object : Lazy [ MyObject ] = None ) -> MyClass : ... # WRONG ! some_object will never be None at this point , it will be # a Lazy [] that returns None obj = some_object or MyObjectDefault () # CORRECT : obj = some_object . construct ( kwarg = kwarg ) or MyObjectDefault () ...","title":"Lazy"},{"location":"api/common/lazy/#construct","text":"class Lazy ( Generic [ T ]): | ... | def construct ( self , ** kwargs ) -> Optional [ T ]","title":"construct"},{"location":"api/common/logging/","text":"[ allennlp .common .logging ] AllenNlpLogger # class AllenNlpLogger ( logging . Logger ): | def __init__ ( self , name ) A custom subclass of 'logging.Logger' that keeps a set of messages to implement {debug,info,etc.}_once() methods. debug_once # class AllenNlpLogger ( logging . Logger ): | ... | def debug_once ( self , msg , * args , ** kwargs ) info_once # class AllenNlpLogger ( logging . Logger ): | ... | def info_once ( self , msg , * args , ** kwargs ) warning_once # class AllenNlpLogger ( logging . Logger ): | ... | def warning_once ( self , msg , * args , ** kwargs ) error_once # class AllenNlpLogger ( logging . Logger ): | ... | def error_once ( self , msg , * args , ** kwargs ) critical_once # class AllenNlpLogger ( logging . Logger ): | ... | def critical_once ( self , msg , * args , ** kwargs ) FILE_FRIENDLY_LOGGING # FILE_FRIENDLY_LOGGING : bool = False If this flag is set to True , we add newlines to tqdm output, even on an interactive terminal, and we slow down tqdm's output to only once every 10 seconds. By default, it is set to False . ErrorFilter # class ErrorFilter ( Filter ) Filters out everything that is at the ERROR level or higher. This is meant to be used with a stdout handler when a stderr handler is also configured. That way ERROR messages aren't duplicated. filter # class ErrorFilter ( Filter ): | ... | def filter ( self , record ) prepare_global_logging # def prepare_global_logging ( serialization_dir : Union [ str , PathLike ], rank : int = 0 , world_size : int = 1 ) -> None","title":"logging"},{"location":"api/common/logging/#allennlplogger","text":"class AllenNlpLogger ( logging . Logger ): | def __init__ ( self , name ) A custom subclass of 'logging.Logger' that keeps a set of messages to implement {debug,info,etc.}_once() methods.","title":"AllenNlpLogger"},{"location":"api/common/logging/#debug_once","text":"class AllenNlpLogger ( logging . Logger ): | ... | def debug_once ( self , msg , * args , ** kwargs )","title":"debug_once"},{"location":"api/common/logging/#info_once","text":"class AllenNlpLogger ( logging . Logger ): | ... | def info_once ( self , msg , * args , ** kwargs )","title":"info_once"},{"location":"api/common/logging/#warning_once","text":"class AllenNlpLogger ( logging . Logger ): | ... | def warning_once ( self , msg , * args , ** kwargs )","title":"warning_once"},{"location":"api/common/logging/#error_once","text":"class AllenNlpLogger ( logging . Logger ): | ... | def error_once ( self , msg , * args , ** kwargs )","title":"error_once"},{"location":"api/common/logging/#critical_once","text":"class AllenNlpLogger ( logging . Logger ): | ... | def critical_once ( self , msg , * args , ** kwargs )","title":"critical_once"},{"location":"api/common/logging/#file_friendly_logging","text":"FILE_FRIENDLY_LOGGING : bool = False If this flag is set to True , we add newlines to tqdm output, even on an interactive terminal, and we slow down tqdm's output to only once every 10 seconds. By default, it is set to False .","title":"FILE_FRIENDLY_LOGGING"},{"location":"api/common/logging/#errorfilter","text":"class ErrorFilter ( Filter ) Filters out everything that is at the ERROR level or higher. This is meant to be used with a stdout handler when a stderr handler is also configured. That way ERROR messages aren't duplicated.","title":"ErrorFilter"},{"location":"api/common/logging/#filter","text":"class ErrorFilter ( Filter ): | ... | def filter ( self , record )","title":"filter"},{"location":"api/common/logging/#prepare_global_logging","text":"def prepare_global_logging ( serialization_dir : Union [ str , PathLike ], rank : int = 0 , world_size : int = 1 ) -> None","title":"prepare_global_logging"},{"location":"api/common/params/","text":"[ allennlp .common .params ] infer_and_cast # def infer_and_cast ( value : Any ) In some cases we'll be feeding params dicts to functions we don't own; for example, PyTorch optimizers. In that case we can't use pop_int or similar to force casts (which means you can't specify int parameters using environment variables). This function takes something that looks JSON-like and recursively casts things that look like (bool, int, float) to (bool, int, float). unflatten # def unflatten ( flat_dict : Dict [ str , Any ]) -> Dict [ str , Any ] Given a \"flattened\" dict with compound keys, e.g. {\"a.b\": 0} unflatten it: {\"a\": {\"b\": 0}} with_fallback # def with_fallback ( preferred : Dict [ str , Any ], fallback : Dict [ str , Any ] ) -> Dict [ str , Any ] Deep merge two dicts, preferring values from preferred . parse_overrides # def parse_overrides ( serialized_overrides : str ) -> Dict [ str , Any ] Params # class Params ( MutableMapping ): | def __init__ ( self , params : Dict [ str , Any ], history : str = \"\" ) -> None Represents a parameter dictionary with a history, and contains other functionality around parameter passing and validation for AllenNLP. There are currently two benefits of a Params object over a plain dictionary for parameter passing: We handle a few kinds of parameter validation, including making sure that parameters representing discrete choices actually have acceptable values, and making sure no extra parameters are passed. We log all parameter reads, including default values. This gives a more complete specification of the actual parameters used than is given in a JSON file, because those may not specify what default values were used, whereas this will log them. Consumption The convention for using a Params object in AllenNLP is that you will consume the parameters as you read them, so that there are none left when you've read everything you expect. This lets us easily validate that you didn't pass in any extra parameters, just by making sure that the parameter dictionary is empty. You should do this when you're done handling parameters, by calling Params.assert_empty . DEFAULT # class Params ( MutableMapping ): | ... | DEFAULT = object () pop # class Params ( MutableMapping ): | ... | @overrides | def pop ( | self , | key : str , | default : Any = DEFAULT , | keep_as_dict : bool = False | ) -> Any Performs the functionality associated with dict.pop(key), along with checking for returned dictionaries, replacing them with Param objects with an updated history (unless keep_as_dict is True, in which case we leave them as dictionaries). If key is not present in the dictionary, and no default was specified, we raise a ConfigurationError , instead of the typical KeyError . pop_int # class Params ( MutableMapping ): | ... | def pop_int ( self , key : str , default : Any = DEFAULT ) -> int Performs a pop and coerces to an int. pop_float # class Params ( MutableMapping ): | ... | def pop_float ( self , key : str , default : Any = DEFAULT ) -> float Performs a pop and coerces to a float. pop_bool # class Params ( MutableMapping ): | ... | def pop_bool ( self , key : str , default : Any = DEFAULT ) -> bool Performs a pop and coerces to a bool. get # class Params ( MutableMapping ): | ... | @overrides | def get ( self , key : str , default : Any = DEFAULT ) Performs the functionality associated with dict.get(key) but also checks for returned dicts and returns a Params object in their place with an updated history. pop_choice # class Params ( MutableMapping ): | ... | def pop_choice ( | self , | key : str , | choices : List [ Any ], | default_to_first_choice : bool = False , | allow_class_names : bool = True | ) -> Any Gets the value of key in the params dictionary, ensuring that the value is one of the given choices. Note that this pops the key from params, modifying the dictionary, consistent with how parameters are processed in this codebase. Parameters key : str Key to get the value from in the param dictionary choices : List[Any] A list of valid options for values corresponding to key . For example, if you're specifying the type of encoder to use for some part of your model, the choices might be the list of encoder classes we know about and can instantiate. If the value we find in the param dictionary is not in choices , we raise a ConfigurationError , because the user specified an invalid value in their parameter file. default_to_first_choice : bool , optional (default = False ) If this is True , we allow the key to not be present in the parameter dictionary. If the key is not present, we will use the return as the value the first choice in the choices list. If this is False , we raise a ConfigurationError , because specifying the key is required (e.g., you have to specify your model class when running an experiment, but you can feel free to use default settings for encoders if you want). allow_class_names : bool , optional (default = True ) If this is True , then we allow unknown choices that look like fully-qualified class names. This is to allow e.g. specifying a model type as my_library.my_model.MyModel and importing it on the fly. Our check for \"looks like\" is extremely lenient and consists of checking that the value contains a '.'. as_dict # class Params ( MutableMapping ): | ... | def as_dict ( | self , | quiet : bool = False , | infer_type_and_cast : bool = False | ) Sometimes we need to just represent the parameters as a dict, for instance when we pass them to PyTorch code. Parameters quiet : bool , optional (default = False ) Whether to log the parameters before returning them as a dict. infer_type_and_cast : bool , optional (default = False ) If True, we infer types and cast (e.g. things that look like floats to floats). as_flat_dict # class Params ( MutableMapping ): | ... | def as_flat_dict ( self ) Returns the parameters of a flat dictionary from keys to values. Nested structure is collapsed with periods. duplicate # class Params ( MutableMapping ): | ... | def duplicate ( self ) -> \"Params\" Uses copy.deepcopy() to create a duplicate (but fully distinct) copy of these Params. assert_empty # class Params ( MutableMapping ): | ... | def assert_empty ( self , class_name : str ) Raises a ConfigurationError if self.params is not empty. We take class_name as an argument so that the error message gives some idea of where an error happened, if there was one. class_name should be the name of the calling class, the one that got extra parameters (if there are any). from_file # class Params ( MutableMapping ): | ... | @classmethod | def from_file ( | cls , | params_file : Union [ str , PathLike ], | params_overrides : str = \"\" , | ext_vars : dict = None | ) -> \"Params\" Load a Params object from a configuration file. Parameters params_file : str The path to the configuration file to load. params_overrides : str , optional A dict of overrides that can be applied to final object. e.g. {\"model.embedding_dim\": 10} ext_vars : dict , optional Our config files are Jsonnet, which allows specifying external variables for later substitution. Typically we substitute these using environment variables; however, you can also specify them here, in which case they take priority over environment variables. e.g. {\"HOME_DIR\": \"/Users/allennlp/home\"} to_file # class Params ( MutableMapping ): | ... | def to_file ( | self , | params_file : str , | preference_orders : List [ List [ str ]] = None | ) -> None as_ordered_dict # class Params ( MutableMapping ): | ... | def as_ordered_dict ( | self , | preference_orders : List [ List [ str ]] = None | ) -> OrderedDict Returns Ordered Dict of Params from list of partial order preferences. Parameters preference_orders : List[List[str]] , optional preference_orders is list of partial preference orders. [\"A\", \"B\", \"C\"] means \"A\" > \"B\" > \"C\". For multiple preference_orders first will be considered first. Keys not found, will have last but alphabetical preference. Default Preferences: [[\"dataset_reader\", \"iterator\", \"model\", \"train_data_path\", \"validation_data_path\", \"test_data_path\", \"trainer\", \"vocabulary\"], [\"type\"]] get_hash # class Params ( MutableMapping ): | ... | def get_hash ( self ) -> str Returns a hash code representing the current state of this Params object. We don't want to implement __hash__ because that has deeper python implications (and this is a mutable object), but this will give you a representation of the current state. We use zlib.adler32 instead of Python's builtin hash because the random seed for the latter is reset on each new program invocation, as discussed here: https://stackoverflow.com/questions/27954892/deterministic-hashing-in-python-3. pop_choice # def pop_choice ( params : Dict [ str , Any ], key : str , choices : List [ Any ], default_to_first_choice : bool = False , history : str = \"?.\" , allow_class_names : bool = True ) -> Any Performs the same function as Params.pop_choice , but is required in order to deal with places that the Params object is not welcome, such as inside Keras layers. See the docstring of that method for more detail on how this function works. This method adds a history parameter, in the off-chance that you know it, so that we can reproduce Params.pop_choice exactly. We default to using \"?.\" if you don't know the history, so you'll have to fix that in the log if you want to actually recover the logged parameters.","title":"params"},{"location":"api/common/params/#infer_and_cast","text":"def infer_and_cast ( value : Any ) In some cases we'll be feeding params dicts to functions we don't own; for example, PyTorch optimizers. In that case we can't use pop_int or similar to force casts (which means you can't specify int parameters using environment variables). This function takes something that looks JSON-like and recursively casts things that look like (bool, int, float) to (bool, int, float).","title":"infer_and_cast"},{"location":"api/common/params/#unflatten","text":"def unflatten ( flat_dict : Dict [ str , Any ]) -> Dict [ str , Any ] Given a \"flattened\" dict with compound keys, e.g. {\"a.b\": 0} unflatten it: {\"a\": {\"b\": 0}}","title":"unflatten"},{"location":"api/common/params/#with_fallback","text":"def with_fallback ( preferred : Dict [ str , Any ], fallback : Dict [ str , Any ] ) -> Dict [ str , Any ] Deep merge two dicts, preferring values from preferred .","title":"with_fallback"},{"location":"api/common/params/#parse_overrides","text":"def parse_overrides ( serialized_overrides : str ) -> Dict [ str , Any ]","title":"parse_overrides"},{"location":"api/common/params/#params","text":"class Params ( MutableMapping ): | def __init__ ( self , params : Dict [ str , Any ], history : str = \"\" ) -> None Represents a parameter dictionary with a history, and contains other functionality around parameter passing and validation for AllenNLP. There are currently two benefits of a Params object over a plain dictionary for parameter passing: We handle a few kinds of parameter validation, including making sure that parameters representing discrete choices actually have acceptable values, and making sure no extra parameters are passed. We log all parameter reads, including default values. This gives a more complete specification of the actual parameters used than is given in a JSON file, because those may not specify what default values were used, whereas this will log them. Consumption The convention for using a Params object in AllenNLP is that you will consume the parameters as you read them, so that there are none left when you've read everything you expect. This lets us easily validate that you didn't pass in any extra parameters, just by making sure that the parameter dictionary is empty. You should do this when you're done handling parameters, by calling Params.assert_empty .","title":"Params"},{"location":"api/common/params/#default","text":"class Params ( MutableMapping ): | ... | DEFAULT = object ()","title":"DEFAULT"},{"location":"api/common/params/#pop","text":"class Params ( MutableMapping ): | ... | @overrides | def pop ( | self , | key : str , | default : Any = DEFAULT , | keep_as_dict : bool = False | ) -> Any Performs the functionality associated with dict.pop(key), along with checking for returned dictionaries, replacing them with Param objects with an updated history (unless keep_as_dict is True, in which case we leave them as dictionaries). If key is not present in the dictionary, and no default was specified, we raise a ConfigurationError , instead of the typical KeyError .","title":"pop"},{"location":"api/common/params/#pop_int","text":"class Params ( MutableMapping ): | ... | def pop_int ( self , key : str , default : Any = DEFAULT ) -> int Performs a pop and coerces to an int.","title":"pop_int"},{"location":"api/common/params/#pop_float","text":"class Params ( MutableMapping ): | ... | def pop_float ( self , key : str , default : Any = DEFAULT ) -> float Performs a pop and coerces to a float.","title":"pop_float"},{"location":"api/common/params/#pop_bool","text":"class Params ( MutableMapping ): | ... | def pop_bool ( self , key : str , default : Any = DEFAULT ) -> bool Performs a pop and coerces to a bool.","title":"pop_bool"},{"location":"api/common/params/#get","text":"class Params ( MutableMapping ): | ... | @overrides | def get ( self , key : str , default : Any = DEFAULT ) Performs the functionality associated with dict.get(key) but also checks for returned dicts and returns a Params object in their place with an updated history.","title":"get"},{"location":"api/common/params/#pop_choice","text":"class Params ( MutableMapping ): | ... | def pop_choice ( | self , | key : str , | choices : List [ Any ], | default_to_first_choice : bool = False , | allow_class_names : bool = True | ) -> Any Gets the value of key in the params dictionary, ensuring that the value is one of the given choices. Note that this pops the key from params, modifying the dictionary, consistent with how parameters are processed in this codebase. Parameters key : str Key to get the value from in the param dictionary choices : List[Any] A list of valid options for values corresponding to key . For example, if you're specifying the type of encoder to use for some part of your model, the choices might be the list of encoder classes we know about and can instantiate. If the value we find in the param dictionary is not in choices , we raise a ConfigurationError , because the user specified an invalid value in their parameter file. default_to_first_choice : bool , optional (default = False ) If this is True , we allow the key to not be present in the parameter dictionary. If the key is not present, we will use the return as the value the first choice in the choices list. If this is False , we raise a ConfigurationError , because specifying the key is required (e.g., you have to specify your model class when running an experiment, but you can feel free to use default settings for encoders if you want). allow_class_names : bool , optional (default = True ) If this is True , then we allow unknown choices that look like fully-qualified class names. This is to allow e.g. specifying a model type as my_library.my_model.MyModel and importing it on the fly. Our check for \"looks like\" is extremely lenient and consists of checking that the value contains a '.'.","title":"pop_choice"},{"location":"api/common/params/#as_dict","text":"class Params ( MutableMapping ): | ... | def as_dict ( | self , | quiet : bool = False , | infer_type_and_cast : bool = False | ) Sometimes we need to just represent the parameters as a dict, for instance when we pass them to PyTorch code. Parameters quiet : bool , optional (default = False ) Whether to log the parameters before returning them as a dict. infer_type_and_cast : bool , optional (default = False ) If True, we infer types and cast (e.g. things that look like floats to floats).","title":"as_dict"},{"location":"api/common/params/#as_flat_dict","text":"class Params ( MutableMapping ): | ... | def as_flat_dict ( self ) Returns the parameters of a flat dictionary from keys to values. Nested structure is collapsed with periods.","title":"as_flat_dict"},{"location":"api/common/params/#duplicate","text":"class Params ( MutableMapping ): | ... | def duplicate ( self ) -> \"Params\" Uses copy.deepcopy() to create a duplicate (but fully distinct) copy of these Params.","title":"duplicate"},{"location":"api/common/params/#assert_empty","text":"class Params ( MutableMapping ): | ... | def assert_empty ( self , class_name : str ) Raises a ConfigurationError if self.params is not empty. We take class_name as an argument so that the error message gives some idea of where an error happened, if there was one. class_name should be the name of the calling class, the one that got extra parameters (if there are any).","title":"assert_empty"},{"location":"api/common/params/#from_file","text":"class Params ( MutableMapping ): | ... | @classmethod | def from_file ( | cls , | params_file : Union [ str , PathLike ], | params_overrides : str = \"\" , | ext_vars : dict = None | ) -> \"Params\" Load a Params object from a configuration file. Parameters params_file : str The path to the configuration file to load. params_overrides : str , optional A dict of overrides that can be applied to final object. e.g. {\"model.embedding_dim\": 10} ext_vars : dict , optional Our config files are Jsonnet, which allows specifying external variables for later substitution. Typically we substitute these using environment variables; however, you can also specify them here, in which case they take priority over environment variables. e.g. {\"HOME_DIR\": \"/Users/allennlp/home\"}","title":"from_file"},{"location":"api/common/params/#to_file","text":"class Params ( MutableMapping ): | ... | def to_file ( | self , | params_file : str , | preference_orders : List [ List [ str ]] = None | ) -> None","title":"to_file"},{"location":"api/common/params/#as_ordered_dict","text":"class Params ( MutableMapping ): | ... | def as_ordered_dict ( | self , | preference_orders : List [ List [ str ]] = None | ) -> OrderedDict Returns Ordered Dict of Params from list of partial order preferences. Parameters preference_orders : List[List[str]] , optional preference_orders is list of partial preference orders. [\"A\", \"B\", \"C\"] means \"A\" > \"B\" > \"C\". For multiple preference_orders first will be considered first. Keys not found, will have last but alphabetical preference. Default Preferences: [[\"dataset_reader\", \"iterator\", \"model\", \"train_data_path\", \"validation_data_path\", \"test_data_path\", \"trainer\", \"vocabulary\"], [\"type\"]]","title":"as_ordered_dict"},{"location":"api/common/params/#get_hash","text":"class Params ( MutableMapping ): | ... | def get_hash ( self ) -> str Returns a hash code representing the current state of this Params object. We don't want to implement __hash__ because that has deeper python implications (and this is a mutable object), but this will give you a representation of the current state. We use zlib.adler32 instead of Python's builtin hash because the random seed for the latter is reset on each new program invocation, as discussed here: https://stackoverflow.com/questions/27954892/deterministic-hashing-in-python-3.","title":"get_hash"},{"location":"api/common/params/#pop_choice_1","text":"def pop_choice ( params : Dict [ str , Any ], key : str , choices : List [ Any ], default_to_first_choice : bool = False , history : str = \"?.\" , allow_class_names : bool = True ) -> Any Performs the same function as Params.pop_choice , but is required in order to deal with places that the Params object is not welcome, such as inside Keras layers. See the docstring of that method for more detail on how this function works. This method adds a history parameter, in the off-chance that you know it, so that we can reproduce Params.pop_choice exactly. We default to using \"?.\" if you don't know the history, so you'll have to fix that in the log if you want to actually recover the logged parameters.","title":"pop_choice"},{"location":"api/common/plugins/","text":"[ allennlp .common .plugins ] Plugin management. AllenNLP supports loading \"plugins\" dynamically. A plugin is just a Python package that can be found and imported by AllenNLP. This is done by creating a file named .allennlp_plugins in the directory where the allennlp command is run that lists the modules that should be loaded, one per line. DEFAULT_PLUGINS # DEFAULT_PLUGINS = ( \"allennlp_models\" , \"allennlp_server\" ) discover_file_plugins # def discover_file_plugins ( plugins_filename : str = \".allennlp_plugins\" ) -> Iterable [ str ] Returns an iterable of the plugins found, declared within a file whose path is plugins_filename . discover_plugins # def discover_plugins () -> Iterable [ str ] Returns an iterable of the plugins found. import_plugins # def import_plugins () -> None Imports the plugins found with discover_plugins() .","title":"plugins"},{"location":"api/common/plugins/#default_plugins","text":"DEFAULT_PLUGINS = ( \"allennlp_models\" , \"allennlp_server\" )","title":"DEFAULT_PLUGINS"},{"location":"api/common/plugins/#discover_file_plugins","text":"def discover_file_plugins ( plugins_filename : str = \".allennlp_plugins\" ) -> Iterable [ str ] Returns an iterable of the plugins found, declared within a file whose path is plugins_filename .","title":"discover_file_plugins"},{"location":"api/common/plugins/#discover_plugins","text":"def discover_plugins () -> Iterable [ str ] Returns an iterable of the plugins found.","title":"discover_plugins"},{"location":"api/common/plugins/#import_plugins","text":"def import_plugins () -> None Imports the plugins found with discover_plugins() .","title":"import_plugins"},{"location":"api/common/registrable/","text":"[ allennlp .common .registrable ] allennlp.common.registrable.Registrable is a \"mixin\" for endowing any base class with a named registry for its subclasses and a decorator for registering them. T # T = TypeVar ( \"T\" , bound = \"Registrable\" ) Registrable # class Registrable ( FromParams ) Any class that inherits from Registrable gains access to a named registry for its subclasses. To register them, just decorate them with the classmethod @BaseClass.register(name) . After which you can call BaseClass.list_available() to get the keys for the registered subclasses, and BaseClass.by_name(name) to get the corresponding subclass. Note that the registry stores the subclasses themselves; not class instances. In most cases you would then call from_params(params) on the returned subclass. You can specify a default by setting BaseClass.default_implementation . If it is set, it will be the first element of list_available() . Note that if you use this class to implement a new Registrable abstract class, you must ensure that all subclasses of the abstract class are loaded when the module is loaded, because the subclasses register themselves in their respective files. You can achieve this by having the abstract class and all subclasses in the init .py of the module in which they reside (as this causes any import of either the abstract class or a subclass to load all other subclasses and the abstract class). default_implementation # class Registrable ( FromParams ): | ... | default_implementation : str = None register # class Registrable ( FromParams ): | ... | @classmethod | def register ( | cls : Type [ T ], | name : str , | constructor : str = None , | exist_ok : bool = False | ) Register a class under a particular name. Parameters name : str The name to register the class under. constructor : str , optional (default = None ) The name of the method to use on the class to construct the object. If this is given, we will use this method (which must be a @classmethod ) instead of the default constructor. exist_ok : bool , optional (default = False ) If True, overwrites any existing models registered under name . Else, throws an error if a model is already registered under name . Examples To use this class, you would typically have a base class that inherits from Registrable : class Vocabulary ( Registrable ): ... Then, if you want to register a subclass, you decorate it like this: @Vocabulary . register ( \"my-vocabulary\" ) class MyVocabulary ( Vocabulary ): def __init__ ( self , param1 : int , param2 : str ): ... Registering a class like this will let you instantiate a class from a config file, where you give \"type\": \"my-vocabulary\" , and keys corresponding to the parameters of the __init__ method (note that for this to work, those parameters must have type annotations). If you want to have the instantiation from a config file call a method other than the constructor, either because you have several different construction paths that could be taken for the same object (as we do in Vocabulary ) or because you have logic you want to happen before you get to the constructor (as we do in Embedding ), you can register a specific @classmethod as the constructor to use, like this: @Vocabulary . register ( \"my-vocabulary-from-instances\" , constructor = \"from_instances\" ) @Vocabulary . register ( \"my-vocabulary-from-files\" , constructor = \"from_files\" ) class MyVocabulary ( Vocabulary ): def __init__ ( self , some_params ): ... @classmethod def from_instances ( cls , some_other_params ) -> MyVocabulary : ... # construct some_params from instances return cls ( some_params ) @classmethod def from_files ( cls , still_other_params ) -> MyVocabulary : ... # construct some_params from files return cls ( some_params ) by_name # class Registrable ( FromParams ): | ... | @classmethod | def by_name ( cls : Type [ T ], name : str ) -> Callable [ ... , T ] Returns a callable function that constructs an argument of the registered class. Because you can register particular functions as constructors for specific names, this isn't necessarily the __init__ method of some class. resolve_class_name # class Registrable ( FromParams ): | ... | @classmethod | def resolve_class_name ( | cls : Type [ T ], | name : str | ) -> Tuple [ Type [ T ], Optional [ str ]] Returns the subclass that corresponds to the given name , along with the name of the method that was registered as a constructor for that name , if any. This method also allows name to be a fully-specified module name, instead of a name that was already added to the Registry . In that case, you cannot use a separate function as a constructor (as you need to call cls.register() in order to tell us what separate function to use). list_available # class Registrable ( FromParams ): | ... | @classmethod | def list_available ( cls ) -> List [ str ] List default first if it exists","title":"registrable"},{"location":"api/common/registrable/#t","text":"T = TypeVar ( \"T\" , bound = \"Registrable\" )","title":"T"},{"location":"api/common/registrable/#registrable","text":"class Registrable ( FromParams ) Any class that inherits from Registrable gains access to a named registry for its subclasses. To register them, just decorate them with the classmethod @BaseClass.register(name) . After which you can call BaseClass.list_available() to get the keys for the registered subclasses, and BaseClass.by_name(name) to get the corresponding subclass. Note that the registry stores the subclasses themselves; not class instances. In most cases you would then call from_params(params) on the returned subclass. You can specify a default by setting BaseClass.default_implementation . If it is set, it will be the first element of list_available() . Note that if you use this class to implement a new Registrable abstract class, you must ensure that all subclasses of the abstract class are loaded when the module is loaded, because the subclasses register themselves in their respective files. You can achieve this by having the abstract class and all subclasses in the init .py of the module in which they reside (as this causes any import of either the abstract class or a subclass to load all other subclasses and the abstract class).","title":"Registrable"},{"location":"api/common/registrable/#default_implementation","text":"class Registrable ( FromParams ): | ... | default_implementation : str = None","title":"default_implementation"},{"location":"api/common/registrable/#register","text":"class Registrable ( FromParams ): | ... | @classmethod | def register ( | cls : Type [ T ], | name : str , | constructor : str = None , | exist_ok : bool = False | ) Register a class under a particular name. Parameters name : str The name to register the class under. constructor : str , optional (default = None ) The name of the method to use on the class to construct the object. If this is given, we will use this method (which must be a @classmethod ) instead of the default constructor. exist_ok : bool , optional (default = False ) If True, overwrites any existing models registered under name . Else, throws an error if a model is already registered under name . Examples To use this class, you would typically have a base class that inherits from Registrable : class Vocabulary ( Registrable ): ... Then, if you want to register a subclass, you decorate it like this: @Vocabulary . register ( \"my-vocabulary\" ) class MyVocabulary ( Vocabulary ): def __init__ ( self , param1 : int , param2 : str ): ... Registering a class like this will let you instantiate a class from a config file, where you give \"type\": \"my-vocabulary\" , and keys corresponding to the parameters of the __init__ method (note that for this to work, those parameters must have type annotations). If you want to have the instantiation from a config file call a method other than the constructor, either because you have several different construction paths that could be taken for the same object (as we do in Vocabulary ) or because you have logic you want to happen before you get to the constructor (as we do in Embedding ), you can register a specific @classmethod as the constructor to use, like this: @Vocabulary . register ( \"my-vocabulary-from-instances\" , constructor = \"from_instances\" ) @Vocabulary . register ( \"my-vocabulary-from-files\" , constructor = \"from_files\" ) class MyVocabulary ( Vocabulary ): def __init__ ( self , some_params ): ... @classmethod def from_instances ( cls , some_other_params ) -> MyVocabulary : ... # construct some_params from instances return cls ( some_params ) @classmethod def from_files ( cls , still_other_params ) -> MyVocabulary : ... # construct some_params from files return cls ( some_params )","title":"register"},{"location":"api/common/registrable/#by_name","text":"class Registrable ( FromParams ): | ... | @classmethod | def by_name ( cls : Type [ T ], name : str ) -> Callable [ ... , T ] Returns a callable function that constructs an argument of the registered class. Because you can register particular functions as constructors for specific names, this isn't necessarily the __init__ method of some class.","title":"by_name"},{"location":"api/common/registrable/#resolve_class_name","text":"class Registrable ( FromParams ): | ... | @classmethod | def resolve_class_name ( | cls : Type [ T ], | name : str | ) -> Tuple [ Type [ T ], Optional [ str ]] Returns the subclass that corresponds to the given name , along with the name of the method that was registered as a constructor for that name , if any. This method also allows name to be a fully-specified module name, instead of a name that was already added to the Registry . In that case, you cannot use a separate function as a constructor (as you need to call cls.register() in order to tell us what separate function to use).","title":"resolve_class_name"},{"location":"api/common/registrable/#list_available","text":"class Registrable ( FromParams ): | ... | @classmethod | def list_available ( cls ) -> List [ str ] List default first if it exists","title":"list_available"},{"location":"api/common/tqdm/","text":"[ allennlp .common .tqdm ] allennlp.common.tqdm.Tqdm wraps tqdm so we can add configurable global defaults for certain tqdm parameters. logger.propagate # logger . propagate = False replace_cr_with_newline # def replace_cr_with_newline ( message : str ) -> str TQDM and requests use carriage returns to get the training line to update for each batch without adding more lines to the terminal output. Displaying those in a file won't work correctly, so we'll just make sure that each batch shows up on its one line. TqdmToLogsWriter # class TqdmToLogsWriter ( object ): | def __init__ ( self ) write # class TqdmToLogsWriter ( object ): | ... | def write ( self , message ) flush # class TqdmToLogsWriter ( object ): | ... | def flush ( self ) Tqdm # class Tqdm tqdm # class Tqdm : | ... | @staticmethod | def tqdm ( * args , ** kwargs ) Use a slower interval when FILE_FRIENDLY_LOGGING is set.","title":"tqdm"},{"location":"api/common/tqdm/#loggerpropagate","text":"logger . propagate = False","title":"logger.propagate"},{"location":"api/common/tqdm/#replace_cr_with_newline","text":"def replace_cr_with_newline ( message : str ) -> str TQDM and requests use carriage returns to get the training line to update for each batch without adding more lines to the terminal output. Displaying those in a file won't work correctly, so we'll just make sure that each batch shows up on its one line.","title":"replace_cr_with_newline"},{"location":"api/common/tqdm/#tqdmtologswriter","text":"class TqdmToLogsWriter ( object ): | def __init__ ( self )","title":"TqdmToLogsWriter"},{"location":"api/common/tqdm/#write","text":"class TqdmToLogsWriter ( object ): | ... | def write ( self , message )","title":"write"},{"location":"api/common/tqdm/#flush","text":"class TqdmToLogsWriter ( object ): | ... | def flush ( self )","title":"flush"},{"location":"api/common/tqdm/#tqdm","text":"class Tqdm","title":"Tqdm"},{"location":"api/common/tqdm/#tqdm_1","text":"class Tqdm : | ... | @staticmethod | def tqdm ( * args , ** kwargs ) Use a slower interval when FILE_FRIENDLY_LOGGING is set.","title":"tqdm"},{"location":"api/common/util/","text":"[ allennlp .common .util ] Various utilities that don't fit anywhere else. JsonDict # JsonDict = Dict [ str , Any ] START_SYMBOL # START_SYMBOL = \"@start@\" END_SYMBOL # END_SYMBOL = \"@end@\" PathType # PathType = Union [ os . PathLike , str ] T # T = TypeVar ( \"T\" ) ContextManagerFunctionReturnType # ContextManagerFunctionReturnType = Generator [ T , None , None ] sanitize # def sanitize ( x : Any ) -> Any Sanitize turns PyTorch and Numpy types into basic Python types so they can be serialized into JSON. group_by_count # def group_by_count ( iterable : List [ Any ], count : int , default_value : Any ) -> List [ List [ Any ]] Takes a list and groups it into sublists of size count , using default_value to pad the list at the end if the list is not divisable by count . For example: >>> group_by_count([1, 2, 3, 4, 5, 6, 7], 3, 0) [[1, 2, 3], [4, 5, 6], [7, 0, 0]] This is a short method, but it's complicated and hard to remember as a one-liner, so we just make a function out of it. A # A = TypeVar ( \"A\" ) lazy_groups_of # def lazy_groups_of ( iterable : Iterable [ A ], group_size : int ) -> Iterator [ List [ A ]] Takes an iterable and batches the individual instances into lists of the specified size. The last list may be smaller if there are instances left over. pad_sequence_to_length # def pad_sequence_to_length ( sequence : List , desired_length : int , default_value : Callable [[], Any ] = lambda : 0 , padding_on_right : bool = True ) -> List Take a list of objects and pads it to the desired length, returning the padded list. The original list is not modified. Parameters sequence : List A list of objects to be padded. desired_length : int Maximum length of each sequence. Longer sequences are truncated to this length, and shorter ones are padded to it. default_value : Callable , optional (default = lambda: 0 ) Callable that outputs a default value (of any type) to use as padding values. This is a lambda to avoid using the same object when the default value is more complex, like a list. padding_on_right : bool , optional (default = True ) When we add padding tokens (or truncate the sequence), should we do it on the right or the left? Returns padded_sequence : List add_noise_to_dict_values # def add_noise_to_dict_values ( dictionary : Dict [ A , float ], noise_param : float ) -> Dict [ A , float ] Returns a new dictionary with noise added to every key in dictionary . The noise is uniformly distributed within noise_param percent of the value for every value in the dictionary. namespace_match # def namespace_match ( pattern : str , namespace : str ) Matches a namespace pattern against a namespace string. For example, *tags matches passage_tags and question_tags and tokens matches tokens but not stemmed_tokens . prepare_environment # def prepare_environment ( params : Params ) Sets random seeds for reproducible experiments. This may not work as expected if you use this from within a python project in which you have already imported Pytorch. If you use the scripts/run_model.py entry point to training models with this library, your experiments should be reasonably reproducible. If you are using this from your own project, you will want to call this function before importing Pytorch. Complete determinism is very difficult to achieve with libraries doing optimized linear algebra due to massively parallel execution, which is exacerbated by using GPUs. Parameters params : Params A Params object or dict holding the json parameters. LOADED_SPACY_MODELS # LOADED_SPACY_MODELS : Dict [ Tuple [ str , bool , bool , bool ], SpacyModelType ] = {} get_spacy_model # def get_spacy_model ( spacy_model_name : str , pos_tags : bool , parse : bool , ner : bool ) -> SpacyModelType In order to avoid loading spacy models a whole bunch of times, we'll save references to them, keyed by the options we used to create the spacy model, so any particular configuration only gets loaded once. pushd # @contextmanager def pushd ( new_dir : PathType , verbose : bool = False ) -> ContextManagerFunctionReturnType [ None ] Changes the current directory to the given path and prepends it to sys.path . This method is intended to use with with , so after its usage, the current directory will be set to the previous value. push_python_path # @contextmanager def push_python_path ( path : PathType ) -> ContextManagerFunctionReturnType [ None ] Prepends the given path to sys.path . This method is intended to use with with , so after its usage, its value willbe removed from sys.path . import_module_and_submodules # def import_module_and_submodules ( package_name : str ) -> None Import all submodules under the given package. Primarily useful so that people using AllenNLP as a library can specify their own custom packages and have their custom classes get loaded and registered. peak_memory_mb # def peak_memory_mb () -> Dict [ int , float ] Get peak memory usage for each worker, as measured by max-resident-set size: https://unix.stackexchange.com/questions/30940/getrusage-system-call-what-is-maximum-resident-set-size Only works on OSX and Linux, otherwise the result will be 0.0 for every worker. gpu_memory_mb # def gpu_memory_mb () -> Dict [ int , int ] Get the current GPU memory usage. Based on https://discuss.pytorch.org/t/access-gpu-memory-usage-in-pytorch/3192/4 Returns Dict[int, int] Keys are device ids as integers. Values are memory usage as integers in MB. Returns an empty dict if GPUs are not available. ensure_list # def ensure_list ( iterable : Iterable [ A ]) -> List [ A ] An Iterable may be a list or a generator. This ensures we get a list without making an unnecessary copy. is_lazy # def is_lazy ( iterable : Iterable [ A ]) -> bool Checks if the given iterable is lazy, which here just means it's not a list. int_to_device # def int_to_device ( device : Union [ int , torch . device ]) -> torch . device log_frozen_and_tunable_parameter_names # def log_frozen_and_tunable_parameter_names ( model : torch . nn . Module ) -> None get_frozen_and_tunable_parameter_names # def get_frozen_and_tunable_parameter_names ( model : torch . nn . Module ) -> Tuple [ Iterable [ str ], Iterable [ str ]] dump_metrics # def dump_metrics ( file_path : Optional [ str ], metrics : Dict [ str , Any ], log : bool = False ) -> None flatten_filename # def flatten_filename ( file_path : str ) -> str is_master # def is_master ( global_rank : int = None , world_size : int = None , num_procs_per_node : int = None ) -> bool Checks if the process is a \"master\" of its node in a distributed process group. If a process group is not initialized, this returns True . Parameters global_rank : int , optional (default = None ) Global rank of the process if in a distributed process group. If not given, rank is obtained using torch.distributed.get_rank() world_size : int , optional (default = None ) Number of processes in the distributed group. If not given, this is obtained using torch.distributed.get_world_size() num_procs_per_node : int , optional (default = None ) Number of GPU processes running per node is_distributed # def is_distributed () -> bool Checks if the distributed process group is available and has been initialized sanitize_wordpiece # def sanitize_wordpiece ( wordpiece : str ) -> str Sanitizes wordpieces from BERT, RoBERTa or ALBERT tokenizers. sanitize_ptb_tokenized_string # def sanitize_ptb_tokenized_string ( text : str ) -> str Sanitizes string that was tokenized using PTBTokenizer","title":"util"},{"location":"api/common/util/#jsondict","text":"JsonDict = Dict [ str , Any ]","title":"JsonDict"},{"location":"api/common/util/#start_symbol","text":"START_SYMBOL = \"@start@\"","title":"START_SYMBOL"},{"location":"api/common/util/#end_symbol","text":"END_SYMBOL = \"@end@\"","title":"END_SYMBOL"},{"location":"api/common/util/#pathtype","text":"PathType = Union [ os . PathLike , str ]","title":"PathType"},{"location":"api/common/util/#t","text":"T = TypeVar ( \"T\" )","title":"T"},{"location":"api/common/util/#contextmanagerfunctionreturntype","text":"ContextManagerFunctionReturnType = Generator [ T , None , None ]","title":"ContextManagerFunctionReturnType"},{"location":"api/common/util/#sanitize","text":"def sanitize ( x : Any ) -> Any Sanitize turns PyTorch and Numpy types into basic Python types so they can be serialized into JSON.","title":"sanitize"},{"location":"api/common/util/#group_by_count","text":"def group_by_count ( iterable : List [ Any ], count : int , default_value : Any ) -> List [ List [ Any ]] Takes a list and groups it into sublists of size count , using default_value to pad the list at the end if the list is not divisable by count . For example: >>> group_by_count([1, 2, 3, 4, 5, 6, 7], 3, 0) [[1, 2, 3], [4, 5, 6], [7, 0, 0]] This is a short method, but it's complicated and hard to remember as a one-liner, so we just make a function out of it.","title":"group_by_count"},{"location":"api/common/util/#a","text":"A = TypeVar ( \"A\" )","title":"A"},{"location":"api/common/util/#lazy_groups_of","text":"def lazy_groups_of ( iterable : Iterable [ A ], group_size : int ) -> Iterator [ List [ A ]] Takes an iterable and batches the individual instances into lists of the specified size. The last list may be smaller if there are instances left over.","title":"lazy_groups_of"},{"location":"api/common/util/#pad_sequence_to_length","text":"def pad_sequence_to_length ( sequence : List , desired_length : int , default_value : Callable [[], Any ] = lambda : 0 , padding_on_right : bool = True ) -> List Take a list of objects and pads it to the desired length, returning the padded list. The original list is not modified. Parameters sequence : List A list of objects to be padded. desired_length : int Maximum length of each sequence. Longer sequences are truncated to this length, and shorter ones are padded to it. default_value : Callable , optional (default = lambda: 0 ) Callable that outputs a default value (of any type) to use as padding values. This is a lambda to avoid using the same object when the default value is more complex, like a list. padding_on_right : bool , optional (default = True ) When we add padding tokens (or truncate the sequence), should we do it on the right or the left? Returns padded_sequence : List","title":"pad_sequence_to_length"},{"location":"api/common/util/#add_noise_to_dict_values","text":"def add_noise_to_dict_values ( dictionary : Dict [ A , float ], noise_param : float ) -> Dict [ A , float ] Returns a new dictionary with noise added to every key in dictionary . The noise is uniformly distributed within noise_param percent of the value for every value in the dictionary.","title":"add_noise_to_dict_values"},{"location":"api/common/util/#namespace_match","text":"def namespace_match ( pattern : str , namespace : str ) Matches a namespace pattern against a namespace string. For example, *tags matches passage_tags and question_tags and tokens matches tokens but not stemmed_tokens .","title":"namespace_match"},{"location":"api/common/util/#prepare_environment","text":"def prepare_environment ( params : Params ) Sets random seeds for reproducible experiments. This may not work as expected if you use this from within a python project in which you have already imported Pytorch. If you use the scripts/run_model.py entry point to training models with this library, your experiments should be reasonably reproducible. If you are using this from your own project, you will want to call this function before importing Pytorch. Complete determinism is very difficult to achieve with libraries doing optimized linear algebra due to massively parallel execution, which is exacerbated by using GPUs. Parameters params : Params A Params object or dict holding the json parameters.","title":"prepare_environment"},{"location":"api/common/util/#loaded_spacy_models","text":"LOADED_SPACY_MODELS : Dict [ Tuple [ str , bool , bool , bool ], SpacyModelType ] = {}","title":"LOADED_SPACY_MODELS"},{"location":"api/common/util/#get_spacy_model","text":"def get_spacy_model ( spacy_model_name : str , pos_tags : bool , parse : bool , ner : bool ) -> SpacyModelType In order to avoid loading spacy models a whole bunch of times, we'll save references to them, keyed by the options we used to create the spacy model, so any particular configuration only gets loaded once.","title":"get_spacy_model"},{"location":"api/common/util/#pushd","text":"@contextmanager def pushd ( new_dir : PathType , verbose : bool = False ) -> ContextManagerFunctionReturnType [ None ] Changes the current directory to the given path and prepends it to sys.path . This method is intended to use with with , so after its usage, the current directory will be set to the previous value.","title":"pushd"},{"location":"api/common/util/#push_python_path","text":"@contextmanager def push_python_path ( path : PathType ) -> ContextManagerFunctionReturnType [ None ] Prepends the given path to sys.path . This method is intended to use with with , so after its usage, its value willbe removed from sys.path .","title":"push_python_path"},{"location":"api/common/util/#import_module_and_submodules","text":"def import_module_and_submodules ( package_name : str ) -> None Import all submodules under the given package. Primarily useful so that people using AllenNLP as a library can specify their own custom packages and have their custom classes get loaded and registered.","title":"import_module_and_submodules"},{"location":"api/common/util/#peak_memory_mb","text":"def peak_memory_mb () -> Dict [ int , float ] Get peak memory usage for each worker, as measured by max-resident-set size: https://unix.stackexchange.com/questions/30940/getrusage-system-call-what-is-maximum-resident-set-size Only works on OSX and Linux, otherwise the result will be 0.0 for every worker.","title":"peak_memory_mb"},{"location":"api/common/util/#gpu_memory_mb","text":"def gpu_memory_mb () -> Dict [ int , int ] Get the current GPU memory usage. Based on https://discuss.pytorch.org/t/access-gpu-memory-usage-in-pytorch/3192/4 Returns Dict[int, int] Keys are device ids as integers. Values are memory usage as integers in MB. Returns an empty dict if GPUs are not available.","title":"gpu_memory_mb"},{"location":"api/common/util/#ensure_list","text":"def ensure_list ( iterable : Iterable [ A ]) -> List [ A ] An Iterable may be a list or a generator. This ensures we get a list without making an unnecessary copy.","title":"ensure_list"},{"location":"api/common/util/#is_lazy","text":"def is_lazy ( iterable : Iterable [ A ]) -> bool Checks if the given iterable is lazy, which here just means it's not a list.","title":"is_lazy"},{"location":"api/common/util/#int_to_device","text":"def int_to_device ( device : Union [ int , torch . device ]) -> torch . device","title":"int_to_device"},{"location":"api/common/util/#log_frozen_and_tunable_parameter_names","text":"def log_frozen_and_tunable_parameter_names ( model : torch . nn . Module ) -> None","title":"log_frozen_and_tunable_parameter_names"},{"location":"api/common/util/#get_frozen_and_tunable_parameter_names","text":"def get_frozen_and_tunable_parameter_names ( model : torch . nn . Module ) -> Tuple [ Iterable [ str ], Iterable [ str ]]","title":"get_frozen_and_tunable_parameter_names"},{"location":"api/common/util/#dump_metrics","text":"def dump_metrics ( file_path : Optional [ str ], metrics : Dict [ str , Any ], log : bool = False ) -> None","title":"dump_metrics"},{"location":"api/common/util/#flatten_filename","text":"def flatten_filename ( file_path : str ) -> str","title":"flatten_filename"},{"location":"api/common/util/#is_master","text":"def is_master ( global_rank : int = None , world_size : int = None , num_procs_per_node : int = None ) -> bool Checks if the process is a \"master\" of its node in a distributed process group. If a process group is not initialized, this returns True . Parameters global_rank : int , optional (default = None ) Global rank of the process if in a distributed process group. If not given, rank is obtained using torch.distributed.get_rank() world_size : int , optional (default = None ) Number of processes in the distributed group. If not given, this is obtained using torch.distributed.get_world_size() num_procs_per_node : int , optional (default = None ) Number of GPU processes running per node","title":"is_master"},{"location":"api/common/util/#is_distributed","text":"def is_distributed () -> bool Checks if the distributed process group is available and has been initialized","title":"is_distributed"},{"location":"api/common/util/#sanitize_wordpiece","text":"def sanitize_wordpiece ( wordpiece : str ) -> str Sanitizes wordpieces from BERT, RoBERTa or ALBERT tokenizers.","title":"sanitize_wordpiece"},{"location":"api/common/util/#sanitize_ptb_tokenized_string","text":"def sanitize_ptb_tokenized_string ( text : str ) -> str Sanitizes string that was tokenized using PTBTokenizer","title":"sanitize_ptb_tokenized_string"},{"location":"api/common/testing/distributed_test/","text":"[ allennlp .common .testing .distributed_test ] init_process # def init_process ( process_rank : int , distributed_device_ids : List [ int ] = None , world_size : int = 1 , func : Callable = None , func_args : Tuple = None , func_kwargs : Dict [ str , Any ] = None , master_addr : str = \"127.0.0.1\" , master_port : int = 29500 ) run_distributed_test # def run_distributed_test ( device_ids : List [ int ] = [ - 1 , - 1 ], func : Callable = None , * args , ** kwargs ) This runs the func in a simulated distributed environment. Parameters device_ids : List[int] List of devices. There need to be at least 2 devices. Default is [-1, -1]. func : Callable func needs to be global for spawning the processes, so that it can be pickled.","title":"distributed_test"},{"location":"api/common/testing/distributed_test/#init_process","text":"def init_process ( process_rank : int , distributed_device_ids : List [ int ] = None , world_size : int = 1 , func : Callable = None , func_args : Tuple = None , func_kwargs : Dict [ str , Any ] = None , master_addr : str = \"127.0.0.1\" , master_port : int = 29500 )","title":"init_process"},{"location":"api/common/testing/distributed_test/#run_distributed_test","text":"def run_distributed_test ( device_ids : List [ int ] = [ - 1 , - 1 ], func : Callable = None , * args , ** kwargs ) This runs the func in a simulated distributed environment. Parameters device_ids : List[int] List of devices. There need to be at least 2 devices. Default is [-1, -1]. func : Callable func needs to be global for spawning the processes, so that it can be pickled.","title":"run_distributed_test"},{"location":"api/common/testing/model_test_case/","text":"[ allennlp .common .testing .model_test_case ] ModelTestCase # class ModelTestCase ( AllenNlpTestCase ) A subclass of AllenNlpTestCase with added methods for testing Model subclasses. set_up_model # class ModelTestCase ( AllenNlpTestCase ): | ... | def set_up_model ( self , param_file , dataset_file ) ensure_model_can_train_save_and_load # class ModelTestCase ( AllenNlpTestCase ): | ... | def ensure_model_can_train_save_and_load ( | self , | param_file : Union [ PathLike , str ], | tolerance : float = 1e-4 , | cuda_device : int = - 1 , | gradients_to_ignore : Set [ str ] = None , | overrides : str = \"\" , | metric_to_check : str = None , | metric_terminal_value : float = None , | metric_tolerance : float = 1e-4 , | disable_dropout : bool = True | ) Parameters param_file : str Path to a training configuration file that we will use to train the model for this test. tolerance : float , optional (default = 1e-4 ) When comparing model predictions between the originally-trained model and the model after saving and loading, we will use this tolerance value (passed as rtol to numpy.testing.assert_allclose ). cuda_device : int , optional (default = -1 ) The device to run the test on. gradients_to_ignore : Set[str] , optional (default = None ) This test runs a gradient check to make sure that we're actually computing gradients for all of the parameters in the model. If you really want to ignore certain parameters when doing that check, you can pass their names here. This is not recommended unless you're really sure you don't need to have non-zero gradients for those parameters (e.g., some of the beam search / state machine models have infrequently-used parameters that are hard to force the model to use in a small test). overrides : str , optional (default = \"\" ) A JSON string that we will use to override values in the input parameter file. metric_to_check : str , optional (default = None ) We may want to automatically perform a check that model reaches given metric when training (on validation set, if it is specified). It may be useful in CI, for example. You can pass any metric that is in your model returned metrics. metric_terminal_value : str , optional (default = None ) When you set metric_to_check , you need to set the value this metric must converge to metric_tolerance : float , optional (default = 1e-4 ) Tolerance to check you model metric against metric terminal value. One can expect some variance in model metrics when the training process is highly stochastic. disable_dropout : bool , optional (default = True ) If True we will set all dropout to 0 before checking gradients. (Otherwise, with small datasets, you may get zero gradients because of unlucky dropout.) ensure_model_can_train # class ModelTestCase ( AllenNlpTestCase ): | ... | def ensure_model_can_train ( | self , | trainer : GradientDescentTrainer , | gradients_to_ignore : Set [ str ] = None , | metric_to_check : str = None , | metric_terminal_value : float = None , | metric_tolerance : float = 1e-4 , | disable_dropout : bool = True | ) A simple test for model training behavior when you are not using configuration files. In this case, we don't have a story around saving and loading models (you need to handle that yourself), so we don't have tests for that. We just test that the model can train, and that it computes gradients for all parameters. Because the Trainer already has a reference to a model and to a data loader, we just take the Trainer object itself, and grab the Model and other necessary objects from there. Parameters trainer : GradientDescentTrainer The Trainer to use for the test, which already has references to a Model and a DataLoader , which we will use in the test. gradients_to_ignore : Set[str] , optional (default = None ) This test runs a gradient check to make sure that we're actually computing gradients for all of the parameters in the model. If you really want to ignore certain parameters when doing that check, you can pass their names here. This is not recommended unless you're really sure you don't need to have non-zero gradients for those parameters (e.g., some of the beam search / state machine models have infrequently-used parameters that are hard to force the model to use in a small test). metric_to_check : str , optional (default = None ) We may want to automatically perform a check that model reaches given metric when training (on validation set, if it is specified). It may be useful in CI, for example. You can pass any metric that is in your model returned metrics. metric_terminal_value : str , optional (default = None ) When you set metric_to_check , you need to set the value this metric must converge to metric_tolerance : float , optional (default = 1e-4 ) Tolerance to check you model metric against metric terminal value. One can expect some variance in model metrics when the training process is highly stochastic. disable_dropout : bool , optional (default = True ) If True we will set all dropout to 0 before checking gradients. (Otherwise, with small datasets, you may get zero gradients because of unlucky dropout.) assert_fields_equal # class ModelTestCase ( AllenNlpTestCase ): | ... | def assert_fields_equal ( | self , | field1 , | field2 , | name : str , | tolerance : float = 1e-6 | ) -> None check_model_computes_gradients_correctly # class ModelTestCase ( AllenNlpTestCase ): | ... | @staticmethod | def check_model_computes_gradients_correctly ( | model : Model , | model_batch : Dict [ str , Union [ Any , Dict [ str , Any ]]], | params_to_ignore : Set [ str ] = None , | disable_dropout : bool = True | ) ensure_batch_predictions_are_consistent # class ModelTestCase ( AllenNlpTestCase ): | ... | def ensure_batch_predictions_are_consistent ( | self , | keys_to_ignore : Iterable [ str ] = () | ) Ensures that the model performs the same on a batch of instances as on individual instances. Ignores metrics matching the regexp . loss. and those specified explicitly. Parameters keys_to_ignore : Iterable[str] , optional (default = () ) Names of metrics that should not be taken into account, e.g. \"batch_weight\".","title":"model_test_case"},{"location":"api/common/testing/model_test_case/#modeltestcase","text":"class ModelTestCase ( AllenNlpTestCase ) A subclass of AllenNlpTestCase with added methods for testing Model subclasses.","title":"ModelTestCase"},{"location":"api/common/testing/model_test_case/#set_up_model","text":"class ModelTestCase ( AllenNlpTestCase ): | ... | def set_up_model ( self , param_file , dataset_file )","title":"set_up_model"},{"location":"api/common/testing/model_test_case/#ensure_model_can_train_save_and_load","text":"class ModelTestCase ( AllenNlpTestCase ): | ... | def ensure_model_can_train_save_and_load ( | self , | param_file : Union [ PathLike , str ], | tolerance : float = 1e-4 , | cuda_device : int = - 1 , | gradients_to_ignore : Set [ str ] = None , | overrides : str = \"\" , | metric_to_check : str = None , | metric_terminal_value : float = None , | metric_tolerance : float = 1e-4 , | disable_dropout : bool = True | ) Parameters param_file : str Path to a training configuration file that we will use to train the model for this test. tolerance : float , optional (default = 1e-4 ) When comparing model predictions between the originally-trained model and the model after saving and loading, we will use this tolerance value (passed as rtol to numpy.testing.assert_allclose ). cuda_device : int , optional (default = -1 ) The device to run the test on. gradients_to_ignore : Set[str] , optional (default = None ) This test runs a gradient check to make sure that we're actually computing gradients for all of the parameters in the model. If you really want to ignore certain parameters when doing that check, you can pass their names here. This is not recommended unless you're really sure you don't need to have non-zero gradients for those parameters (e.g., some of the beam search / state machine models have infrequently-used parameters that are hard to force the model to use in a small test). overrides : str , optional (default = \"\" ) A JSON string that we will use to override values in the input parameter file. metric_to_check : str , optional (default = None ) We may want to automatically perform a check that model reaches given metric when training (on validation set, if it is specified). It may be useful in CI, for example. You can pass any metric that is in your model returned metrics. metric_terminal_value : str , optional (default = None ) When you set metric_to_check , you need to set the value this metric must converge to metric_tolerance : float , optional (default = 1e-4 ) Tolerance to check you model metric against metric terminal value. One can expect some variance in model metrics when the training process is highly stochastic. disable_dropout : bool , optional (default = True ) If True we will set all dropout to 0 before checking gradients. (Otherwise, with small datasets, you may get zero gradients because of unlucky dropout.)","title":"ensure_model_can_train_save_and_load"},{"location":"api/common/testing/model_test_case/#ensure_model_can_train","text":"class ModelTestCase ( AllenNlpTestCase ): | ... | def ensure_model_can_train ( | self , | trainer : GradientDescentTrainer , | gradients_to_ignore : Set [ str ] = None , | metric_to_check : str = None , | metric_terminal_value : float = None , | metric_tolerance : float = 1e-4 , | disable_dropout : bool = True | ) A simple test for model training behavior when you are not using configuration files. In this case, we don't have a story around saving and loading models (you need to handle that yourself), so we don't have tests for that. We just test that the model can train, and that it computes gradients for all parameters. Because the Trainer already has a reference to a model and to a data loader, we just take the Trainer object itself, and grab the Model and other necessary objects from there. Parameters trainer : GradientDescentTrainer The Trainer to use for the test, which already has references to a Model and a DataLoader , which we will use in the test. gradients_to_ignore : Set[str] , optional (default = None ) This test runs a gradient check to make sure that we're actually computing gradients for all of the parameters in the model. If you really want to ignore certain parameters when doing that check, you can pass their names here. This is not recommended unless you're really sure you don't need to have non-zero gradients for those parameters (e.g., some of the beam search / state machine models have infrequently-used parameters that are hard to force the model to use in a small test). metric_to_check : str , optional (default = None ) We may want to automatically perform a check that model reaches given metric when training (on validation set, if it is specified). It may be useful in CI, for example. You can pass any metric that is in your model returned metrics. metric_terminal_value : str , optional (default = None ) When you set metric_to_check , you need to set the value this metric must converge to metric_tolerance : float , optional (default = 1e-4 ) Tolerance to check you model metric against metric terminal value. One can expect some variance in model metrics when the training process is highly stochastic. disable_dropout : bool , optional (default = True ) If True we will set all dropout to 0 before checking gradients. (Otherwise, with small datasets, you may get zero gradients because of unlucky dropout.)","title":"ensure_model_can_train"},{"location":"api/common/testing/model_test_case/#assert_fields_equal","text":"class ModelTestCase ( AllenNlpTestCase ): | ... | def assert_fields_equal ( | self , | field1 , | field2 , | name : str , | tolerance : float = 1e-6 | ) -> None","title":"assert_fields_equal"},{"location":"api/common/testing/model_test_case/#check_model_computes_gradients_correctly","text":"class ModelTestCase ( AllenNlpTestCase ): | ... | @staticmethod | def check_model_computes_gradients_correctly ( | model : Model , | model_batch : Dict [ str , Union [ Any , Dict [ str , Any ]]], | params_to_ignore : Set [ str ] = None , | disable_dropout : bool = True | )","title":"check_model_computes_gradients_correctly"},{"location":"api/common/testing/model_test_case/#ensure_batch_predictions_are_consistent","text":"class ModelTestCase ( AllenNlpTestCase ): | ... | def ensure_batch_predictions_are_consistent ( | self , | keys_to_ignore : Iterable [ str ] = () | ) Ensures that the model performs the same on a batch of instances as on individual instances. Ignores metrics matching the regexp . loss. and those specified explicitly. Parameters keys_to_ignore : Iterable[str] , optional (default = () ) Names of metrics that should not be taken into account, e.g. \"batch_weight\".","title":"ensure_batch_predictions_are_consistent"},{"location":"api/common/testing/test_case/","text":"[ allennlp .common .testing .test_case ] TEST_DIR # TEST_DIR = tempfile . mkdtemp ( prefix = \"allennlp_tests\" ) AllenNlpTestCase # class AllenNlpTestCase A custom subclass of unittest.TestCase that disables some of the more verbose AllenNLP logging and that creates and destroys a temp directory as a test fixture. PROJECT_ROOT # class AllenNlpTestCase : | ... | PROJECT_ROOT = ( pathlib . Path ( __file__ ) . parent / \"..\" / \"..\" / \"..\" ) . resolve () MODULE_ROOT # class AllenNlpTestCase : | ... | MODULE_ROOT = PROJECT_ROOT / \"allennlp\" TOOLS_ROOT # class AllenNlpTestCase : | ... | TOOLS_ROOT = MODULE_ROOT / \"tools\" TESTS_ROOT # class AllenNlpTestCase : | ... | TESTS_ROOT = PROJECT_ROOT / \"tests\" FIXTURES_ROOT # class AllenNlpTestCase : | ... | FIXTURES_ROOT = PROJECT_ROOT / \"test_fixtures\" setup_method # class AllenNlpTestCase : | ... | def setup_method ( self ) teardown_method # class AllenNlpTestCase : | ... | def teardown_method ( self )","title":"test_case"},{"location":"api/common/testing/test_case/#test_dir","text":"TEST_DIR = tempfile . mkdtemp ( prefix = \"allennlp_tests\" )","title":"TEST_DIR"},{"location":"api/common/testing/test_case/#allennlptestcase","text":"class AllenNlpTestCase A custom subclass of unittest.TestCase that disables some of the more verbose AllenNLP logging and that creates and destroys a temp directory as a test fixture.","title":"AllenNlpTestCase"},{"location":"api/common/testing/test_case/#project_root","text":"class AllenNlpTestCase : | ... | PROJECT_ROOT = ( pathlib . Path ( __file__ ) . parent / \"..\" / \"..\" / \"..\" ) . resolve ()","title":"PROJECT_ROOT"},{"location":"api/common/testing/test_case/#module_root","text":"class AllenNlpTestCase : | ... | MODULE_ROOT = PROJECT_ROOT / \"allennlp\"","title":"MODULE_ROOT"},{"location":"api/common/testing/test_case/#tools_root","text":"class AllenNlpTestCase : | ... | TOOLS_ROOT = MODULE_ROOT / \"tools\"","title":"TOOLS_ROOT"},{"location":"api/common/testing/test_case/#tests_root","text":"class AllenNlpTestCase : | ... | TESTS_ROOT = PROJECT_ROOT / \"tests\"","title":"TESTS_ROOT"},{"location":"api/common/testing/test_case/#fixtures_root","text":"class AllenNlpTestCase : | ... | FIXTURES_ROOT = PROJECT_ROOT / \"test_fixtures\"","title":"FIXTURES_ROOT"},{"location":"api/common/testing/test_case/#setup_method","text":"class AllenNlpTestCase : | ... | def setup_method ( self )","title":"setup_method"},{"location":"api/common/testing/test_case/#teardown_method","text":"class AllenNlpTestCase : | ... | def teardown_method ( self )","title":"teardown_method"},{"location":"api/data/batch/","text":"[ allennlp .data .batch ] A Batch represents a collection of Instance s to be fed through a model. Batch # class Batch ( Iterable ): | def __init__ ( self , instances : Iterable [ Instance ]) -> None A batch of Instances. In addition to containing the instances themselves, it contains helper functions for converting the data into tensors. A Batch just takes an iterable of instances in its constructor and hangs onto them in a list. get_padding_lengths # class Batch ( Iterable ): | ... | def get_padding_lengths ( self ) -> Dict [ str , Dict [ str , int ]] Gets the maximum padding lengths from all Instances in this batch. Each Instance has multiple Fields , and each Field could have multiple things that need padding. We look at all fields in all instances, and find the max values for each (field_name, padding_key) pair, returning them in a dictionary. This can then be used to convert this batch into arrays of consistent length, or to set model parameters, etc. as_tensor_dict # class Batch ( Iterable ): | ... | def as_tensor_dict ( | self , | padding_lengths : Dict [ str , Dict [ str , int ]] = None , | verbose : bool = False | ) -> Dict [ str , Union [ torch . Tensor , Dict [ str , torch . Tensor ]]] This method converts this Batch into a set of pytorch Tensors that can be passed through a model. In order for the tensors to be valid tensors, all Instances in this batch need to be padded to the same lengths wherever padding is necessary, so we do that first, then we combine all of the tensors for each field in each instance into a set of batched tensors for each field. Parameters padding_lengths : Dict[str, Dict[str, int]] If a key is present in this dictionary with a non- None value, we will pad to that length instead of the length calculated from the data. This lets you, e.g., set a maximum value for sentence length if you want to throw out long sequences. Entries in this dictionary are keyed first by field name (e.g., \"question\"), then by padding key (e.g., \"num_tokens\"). verbose : bool , optional (default = False ) Should we output logging information when we're doing this padding? If the batch is large, this is nice to have, because padding a large batch could take a long time. But if you're doing this inside of a data generator, having all of this output per batch is a bit obnoxious (and really slow). Returns tensors : Dict[str, DataArray] A dictionary of tensors, keyed by field name, suitable for passing as input to a model. This is a batch of instances, so, e.g., if the instances have a \"question\" field and an \"answer\" field, the \"question\" fields for all of the instances will be grouped together into a single tensor, and the \"answer\" fields for all instances will be similarly grouped in a parallel set of tensors, for batched computation. Additionally, for complex Fields , the value of the dictionary key is not necessarily a single tensor. For example, with the TextField , the output is a dictionary mapping TokenIndexer keys to tensors. The number of elements in this sub-dictionary therefore corresponds to the number of TokenIndexers used to index the TextField . Each Field class is responsible for batching its own output. index_instances # class Batch ( Iterable ): | ... | def index_instances ( self , vocab : Vocabulary ) -> None print_statistics # class Batch ( Iterable ): | ... | def print_statistics ( self ) -> None Make sure if has been indexed first","title":"batch"},{"location":"api/data/batch/#batch","text":"class Batch ( Iterable ): | def __init__ ( self , instances : Iterable [ Instance ]) -> None A batch of Instances. In addition to containing the instances themselves, it contains helper functions for converting the data into tensors. A Batch just takes an iterable of instances in its constructor and hangs onto them in a list.","title":"Batch"},{"location":"api/data/batch/#get_padding_lengths","text":"class Batch ( Iterable ): | ... | def get_padding_lengths ( self ) -> Dict [ str , Dict [ str , int ]] Gets the maximum padding lengths from all Instances in this batch. Each Instance has multiple Fields , and each Field could have multiple things that need padding. We look at all fields in all instances, and find the max values for each (field_name, padding_key) pair, returning them in a dictionary. This can then be used to convert this batch into arrays of consistent length, or to set model parameters, etc.","title":"get_padding_lengths"},{"location":"api/data/batch/#as_tensor_dict","text":"class Batch ( Iterable ): | ... | def as_tensor_dict ( | self , | padding_lengths : Dict [ str , Dict [ str , int ]] = None , | verbose : bool = False | ) -> Dict [ str , Union [ torch . Tensor , Dict [ str , torch . Tensor ]]] This method converts this Batch into a set of pytorch Tensors that can be passed through a model. In order for the tensors to be valid tensors, all Instances in this batch need to be padded to the same lengths wherever padding is necessary, so we do that first, then we combine all of the tensors for each field in each instance into a set of batched tensors for each field. Parameters padding_lengths : Dict[str, Dict[str, int]] If a key is present in this dictionary with a non- None value, we will pad to that length instead of the length calculated from the data. This lets you, e.g., set a maximum value for sentence length if you want to throw out long sequences. Entries in this dictionary are keyed first by field name (e.g., \"question\"), then by padding key (e.g., \"num_tokens\"). verbose : bool , optional (default = False ) Should we output logging information when we're doing this padding? If the batch is large, this is nice to have, because padding a large batch could take a long time. But if you're doing this inside of a data generator, having all of this output per batch is a bit obnoxious (and really slow). Returns tensors : Dict[str, DataArray] A dictionary of tensors, keyed by field name, suitable for passing as input to a model. This is a batch of instances, so, e.g., if the instances have a \"question\" field and an \"answer\" field, the \"question\" fields for all of the instances will be grouped together into a single tensor, and the \"answer\" fields for all instances will be similarly grouped in a parallel set of tensors, for batched computation. Additionally, for complex Fields , the value of the dictionary key is not necessarily a single tensor. For example, with the TextField , the output is a dictionary mapping TokenIndexer keys to tensors. The number of elements in this sub-dictionary therefore corresponds to the number of TokenIndexers used to index the TextField . Each Field class is responsible for batching its own output.","title":"as_tensor_dict"},{"location":"api/data/batch/#index_instances","text":"class Batch ( Iterable ): | ... | def index_instances ( self , vocab : Vocabulary ) -> None","title":"index_instances"},{"location":"api/data/batch/#print_statistics","text":"class Batch ( Iterable ): | ... | def print_statistics ( self ) -> None Make sure if has been indexed first","title":"print_statistics"},{"location":"api/data/dataloader/","text":"[ allennlp .data .dataloader ] TensorDict # TensorDict = Dict [ str , Union [ torch . Tensor , Dict [ str , torch . Tensor ]]] allennlp_collate # def allennlp_collate ( instances : List [ Instance ]) -> TensorDict DataLoader # class DataLoader ( Registrable ) A DataLoader is responsible for generating batches of instances from a Dataset , or another source of data. This is essentially just an abstraction over torch.utils.data.DataLoader . This class only has one required method, __iter__() , that creates an iterable of TensorDict s. Additionally, this class comes with a __len__() method that just raises a TypeError by default. When possible, this should be overriden to return the number of batches that will be generated by the __iter__() method. default_implementation # class DataLoader ( Registrable ): | ... | default_implementation = \"pytorch_dataloader\" PyTorchDataLoader # class PyTorchDataLoader ( data . DataLoader , DataLoader ): | def __init__ ( | self , | dataset : data . Dataset , | batch_size : int = 1 , | shuffle : bool = False , | sampler : Sampler = None , | batch_sampler : BatchSampler = None , | num_workers : int = 0 , | collate_fn = allennlp_collate , | pin_memory : bool = False , | drop_last : bool = False , | timeout : int = 0 , | worker_init_fn = None , | multiprocessing_context : str = None , | batches_per_epoch : int = None | ) A registrable version of the pytorch DataLoader . Firstly, this class exists is so that we can construct a DataLoader from a configuration file and have a different default collate_fn . You can use this class directly in python code, but it is identical to using pytorch dataloader with allennlp's custom collate function: from torch.utils.data import DataLoader from allennlp.data import allennlp_collate # Construct a dataloader directly for a dataset which contains allennlp # Instances which have _already_ been indexed. my_loader = DataLoader ( dataset , batch_size = 32 , collate_fn = allennlp_collate ) Secondly, this class adds a batches_per_epoch parameter which, if given, determines the number of batches after which an epoch ends. If this is None , then an epoch is set to be one full pass through your data. You might use this if you have a very large dataset and want more frequent checkpoints and evaluations on validation data, for instance. In a typical AllenNLP configuration file, the dataset parameter does not get an entry under the \"data_loader\", it gets constructed separately. from_partial_objects # class PyTorchDataLoader ( data . DataLoader , DataLoader ): | ... | @classmethod | def from_partial_objects ( | cls , | dataset : data . Dataset , | batch_size : int = 1 , | shuffle : bool = False , | sampler : Lazy [ Sampler ] = None , | batch_sampler : Lazy [ BatchSampler ] = None , | num_workers : int = 0 , | pin_memory : bool = False , | drop_last : bool = False , | timeout : int = 0 , | worker_init_fn = None , | multiprocessing_context : str = None , | batches_per_epoch : int = None | ) -> \"PyTorchDataLoader\"","title":"dataloader"},{"location":"api/data/dataloader/#tensordict","text":"TensorDict = Dict [ str , Union [ torch . Tensor , Dict [ str , torch . Tensor ]]]","title":"TensorDict"},{"location":"api/data/dataloader/#allennlp_collate","text":"def allennlp_collate ( instances : List [ Instance ]) -> TensorDict","title":"allennlp_collate"},{"location":"api/data/dataloader/#dataloader","text":"class DataLoader ( Registrable ) A DataLoader is responsible for generating batches of instances from a Dataset , or another source of data. This is essentially just an abstraction over torch.utils.data.DataLoader . This class only has one required method, __iter__() , that creates an iterable of TensorDict s. Additionally, this class comes with a __len__() method that just raises a TypeError by default. When possible, this should be overriden to return the number of batches that will be generated by the __iter__() method.","title":"DataLoader"},{"location":"api/data/dataloader/#default_implementation","text":"class DataLoader ( Registrable ): | ... | default_implementation = \"pytorch_dataloader\"","title":"default_implementation"},{"location":"api/data/dataloader/#pytorchdataloader","text":"class PyTorchDataLoader ( data . DataLoader , DataLoader ): | def __init__ ( | self , | dataset : data . Dataset , | batch_size : int = 1 , | shuffle : bool = False , | sampler : Sampler = None , | batch_sampler : BatchSampler = None , | num_workers : int = 0 , | collate_fn = allennlp_collate , | pin_memory : bool = False , | drop_last : bool = False , | timeout : int = 0 , | worker_init_fn = None , | multiprocessing_context : str = None , | batches_per_epoch : int = None | ) A registrable version of the pytorch DataLoader . Firstly, this class exists is so that we can construct a DataLoader from a configuration file and have a different default collate_fn . You can use this class directly in python code, but it is identical to using pytorch dataloader with allennlp's custom collate function: from torch.utils.data import DataLoader from allennlp.data import allennlp_collate # Construct a dataloader directly for a dataset which contains allennlp # Instances which have _already_ been indexed. my_loader = DataLoader ( dataset , batch_size = 32 , collate_fn = allennlp_collate ) Secondly, this class adds a batches_per_epoch parameter which, if given, determines the number of batches after which an epoch ends. If this is None , then an epoch is set to be one full pass through your data. You might use this if you have a very large dataset and want more frequent checkpoints and evaluations on validation data, for instance. In a typical AllenNLP configuration file, the dataset parameter does not get an entry under the \"data_loader\", it gets constructed separately.","title":"PyTorchDataLoader"},{"location":"api/data/dataloader/#from_partial_objects","text":"class PyTorchDataLoader ( data . DataLoader , DataLoader ): | ... | @classmethod | def from_partial_objects ( | cls , | dataset : data . Dataset , | batch_size : int = 1 , | shuffle : bool = False , | sampler : Lazy [ Sampler ] = None , | batch_sampler : Lazy [ BatchSampler ] = None , | num_workers : int = 0 , | pin_memory : bool = False , | drop_last : bool = False , | timeout : int = 0 , | worker_init_fn = None , | multiprocessing_context : str = None , | batches_per_epoch : int = None | ) -> \"PyTorchDataLoader\"","title":"from_partial_objects"},{"location":"api/data/instance/","text":"[ allennlp .data .instance ] Instance # class Instance ( Mapping [ str , Field ]): | def __init__ ( self , fields : MutableMapping [ str , Field ]) -> None An Instance is a collection of Field objects, specifying the inputs and outputs to some model. We don't make a distinction between inputs and outputs here, though - all operations are done on all fields, and when we return arrays, we return them as dictionaries keyed by field name. A model can then decide which fields it wants to use as inputs as which as outputs. The Fields in an Instance can start out either indexed or un-indexed. During the data processing pipeline, all fields will be indexed, after which multiple instances can be combined into a Batch and then converted into padded arrays. Parameters fields : Dict[str, Field] The Field objects that will be used to produce data arrays for this instance. add_field # class Instance ( Mapping [ str , Field ]): | ... | def add_field ( | self , | field_name : str , | field : Field , | vocab : Vocabulary = None | ) -> None Add the field to the existing fields mapping. If we have already indexed the Instance, then we also index field , so it is necessary to supply the vocab. count_vocab_items # class Instance ( Mapping [ str , Field ]): | ... | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]]) Increments counts in the given counter for all of the vocabulary items in all of the Fields in this Instance . index_fields # class Instance ( Mapping [ str , Field ]): | ... | def index_fields ( self , vocab : Vocabulary ) -> None Indexes all fields in this Instance using the provided Vocabulary . This mutates the current object, it does not return a new Instance . A DataLoader will call this on each pass through a dataset; we use the indexed flag to make sure that indexing only happens once. This means that if for some reason you modify your vocabulary after you've indexed your instances, you might get unexpected behavior. get_padding_lengths # class Instance ( Mapping [ str , Field ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , Dict [ str , int ]] Returns a dictionary of padding lengths, keyed by field name. Each Field returns a mapping from padding keys to actual lengths, and we just key that dictionary by field name. as_tensor_dict # class Instance ( Mapping [ str , Field ]): | ... | def as_tensor_dict ( | self , | padding_lengths : Dict [ str , Dict [ str , int ]] = None | ) -> Dict [ str , DataArray ] Pads each Field in this instance to the lengths given in padding_lengths (which is keyed by field name, then by padding key, the same as the return value in get_padding_lengths ), returning a list of torch tensors for each field. If padding_lengths is omitted, we will call self.get_padding_lengths() to get the sizes of the tensors to create. duplicate # class Instance ( Mapping [ str , Field ]): | ... | def duplicate ( self ) -> \"Instance\"","title":"instance"},{"location":"api/data/instance/#instance","text":"class Instance ( Mapping [ str , Field ]): | def __init__ ( self , fields : MutableMapping [ str , Field ]) -> None An Instance is a collection of Field objects, specifying the inputs and outputs to some model. We don't make a distinction between inputs and outputs here, though - all operations are done on all fields, and when we return arrays, we return them as dictionaries keyed by field name. A model can then decide which fields it wants to use as inputs as which as outputs. The Fields in an Instance can start out either indexed or un-indexed. During the data processing pipeline, all fields will be indexed, after which multiple instances can be combined into a Batch and then converted into padded arrays. Parameters fields : Dict[str, Field] The Field objects that will be used to produce data arrays for this instance.","title":"Instance"},{"location":"api/data/instance/#add_field","text":"class Instance ( Mapping [ str , Field ]): | ... | def add_field ( | self , | field_name : str , | field : Field , | vocab : Vocabulary = None | ) -> None Add the field to the existing fields mapping. If we have already indexed the Instance, then we also index field , so it is necessary to supply the vocab.","title":"add_field"},{"location":"api/data/instance/#count_vocab_items","text":"class Instance ( Mapping [ str , Field ]): | ... | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]]) Increments counts in the given counter for all of the vocabulary items in all of the Fields in this Instance .","title":"count_vocab_items"},{"location":"api/data/instance/#index_fields","text":"class Instance ( Mapping [ str , Field ]): | ... | def index_fields ( self , vocab : Vocabulary ) -> None Indexes all fields in this Instance using the provided Vocabulary . This mutates the current object, it does not return a new Instance . A DataLoader will call this on each pass through a dataset; we use the indexed flag to make sure that indexing only happens once. This means that if for some reason you modify your vocabulary after you've indexed your instances, you might get unexpected behavior.","title":"index_fields"},{"location":"api/data/instance/#get_padding_lengths","text":"class Instance ( Mapping [ str , Field ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , Dict [ str , int ]] Returns a dictionary of padding lengths, keyed by field name. Each Field returns a mapping from padding keys to actual lengths, and we just key that dictionary by field name.","title":"get_padding_lengths"},{"location":"api/data/instance/#as_tensor_dict","text":"class Instance ( Mapping [ str , Field ]): | ... | def as_tensor_dict ( | self , | padding_lengths : Dict [ str , Dict [ str , int ]] = None | ) -> Dict [ str , DataArray ] Pads each Field in this instance to the lengths given in padding_lengths (which is keyed by field name, then by padding key, the same as the return value in get_padding_lengths ), returning a list of torch tensors for each field. If padding_lengths is omitted, we will call self.get_padding_lengths() to get the sizes of the tensors to create.","title":"as_tensor_dict"},{"location":"api/data/instance/#duplicate","text":"class Instance ( Mapping [ str , Field ]): | ... | def duplicate ( self ) -> \"Instance\"","title":"duplicate"},{"location":"api/data/vocabulary/","text":"[ allennlp .data .vocabulary ] A Vocabulary maps strings to integers, allowing for strings to be mapped to an out-of-vocabulary token. DEFAULT_NON_PADDED_NAMESPACES # DEFAULT_NON_PADDED_NAMESPACES = ( \"*tags\" , \"*labels\" ) DEFAULT_PADDING_TOKEN # DEFAULT_PADDING_TOKEN = \"@@PADDING@@\" DEFAULT_OOV_TOKEN # DEFAULT_OOV_TOKEN = \"@@UNKNOWN@@\" NAMESPACE_PADDING_FILE # NAMESPACE_PADDING_FILE = \"non_padded_namespaces.txt\" Vocabulary # class Vocabulary ( Registrable ): | def __init__ ( | self , | counter : Dict [ str , Dict [ str , int ]] = None , | min_count : Dict [ str , int ] = None , | max_vocab_size : Union [ int , Dict [ str , int ]] = None , | non_padded_namespaces : Iterable [ str ] = DEFAULT_NON_PADDED_NAMESPACES , | pretrained_files : Optional [ Dict [ str , str ]] = None , | only_include_pretrained_words : bool = False , | tokens_to_add : Dict [ str , List [ str ]] = None , | min_pretrained_embeddings : Dict [ str , int ] = None , | padding_token : Optional [ str ] = DEFAULT_PADDING_TOKEN , | oov_token : Optional [ str ] = DEFAULT_OOV_TOKEN | ) -> None A Vocabulary maps strings to integers, allowing for strings to be mapped to an out-of-vocabulary token. Vocabularies are fit to a particular dataset, which we use to decide which tokens are in-vocabulary. Vocabularies also allow for several different namespaces, so you can have separate indices for 'a' as a word, and 'a' as a character, for instance, and so we can use this object to also map tag and label strings to indices, for a unified .fields.field.Field API. Most of the methods on this class allow you to pass in a namespace; by default we use the 'tokens' namespace, and you can omit the namespace argument everywhere and just use the default. This class is registered as a Vocabulary with four different names, which all point to different @classmethod constructors found in this class. from_instances is registered as \"from_instances\", from_files is registered as \"from_files\", from_files_and_instances is registered as \"extend\", and empty is registered as \"empty\". If you are using a configuration file to construct a vocabulary, you can use any of those strings as the \"type\" key in the configuration file to use the corresponding @classmethod to construct the object. \"from_instances\" is the default. Look at the docstring for the @classmethod to see what keys are allowed in the configuration file (when there is an instances argument to the @classmethod , it will be passed in separately and does not need a corresponding key in the configuration file). Parameters counter : Dict[str, Dict[str, int]] , optional (default = None ) A collection of counts from which to initialize this vocabulary. We will examine the counts and, together with the other parameters to this class, use them to decide which words are in-vocabulary. If this is None , we just won't initialize the vocabulary with anything. min_count : Dict[str, int] , optional (default = None ) When initializing the vocab from a counter, you can specify a minimum count, and every token with a count less than this will not be added to the dictionary. These minimum counts are namespace-specific , so you can specify different minimums for labels versus words tokens, for example. If a namespace does not have a key in the given dictionary, we will add all seen tokens to that namespace. max_vocab_size : Union[int, Dict[str, int]] , optional (default = None ) If you want to cap the number of tokens in your vocabulary, you can do so with this parameter. If you specify a single integer, every namespace will have its vocabulary fixed to be no larger than this. If you specify a dictionary, then each namespace in the counter can have a separate maximum vocabulary size. Any missing key will have a value of None , which means no cap on the vocabulary size. non_padded_namespaces : Iterable[str] , optional By default, we assume you are mapping word / character tokens to integers, and so you want to reserve word indices for padding and out-of-vocabulary tokens. However, if you are mapping NER or SRL tags, or class labels, to integers, you probably do not want to reserve indices for padding and out-of-vocabulary tokens. Use this field to specify which namespaces should not have padding and OOV tokens added. The format of each element of this is either a string, which must match field names exactly, or * followed by a string, which we match as a suffix against field names. We try to make the default here reasonable, so that you don't have to think about this. The default is (\"*tags\", \"*labels\") , so as long as your namespace ends in \"tags\" or \"labels\" (which is true by default for all tag and label fields in this code), you don't have to specify anything here. pretrained_files : Dict[str, str] , optional If provided, this map specifies the path to optional pretrained embedding files for each namespace. This can be used to either restrict the vocabulary to only words which appear in this file, or to ensure that any words in this file are included in the vocabulary regardless of their count, depending on the value of only_include_pretrained_words . Words which appear in the pretrained embedding file but not in the data are NOT included in the Vocabulary. min_pretrained_embeddings : Dict[str, int] , optional If provided, specifies for each namespace a minimum number of lines (typically the most common words) to keep from pretrained embedding files, even for words not appearing in the data. only_include_pretrained_words : bool , optional (default = False ) This defines the strategy for using any pretrained embedding files which may have been specified in pretrained_files . If False, an inclusive strategy is used: and words which are in the counter and in the pretrained file are added to the Vocabulary , regardless of whether their count exceeds min_count or not. If True, we use an exclusive strategy: words are only included in the Vocabulary if they are in the pretrained embedding file (their count must still be at least min_count ). tokens_to_add : Dict[str, List[str]] , optional (default = None ) If given, this is a list of tokens to add to the vocabulary, keyed by the namespace to add the tokens to. This is a way to be sure that certain items appear in your vocabulary, regardless of any other vocabulary computation. padding_token : str , optional (default = DEFAULT_PADDING_TOKEN ) If given, this the string used for padding. oov_token : str , optional (default = DEFAULT_OOV_TOKEN ) If given, this the string used for the out of vocabulary (OOVs) tokens. default_implementation # class Vocabulary ( Registrable ): | ... | default_implementation = \"from_instances\" from_instances # class Vocabulary ( Registrable ): | ... | @classmethod | def from_instances ( | cls , | instances : Iterable [ \"adi.Instance\" ], | min_count : Dict [ str , int ] = None , | max_vocab_size : Union [ int , Dict [ str , int ]] = None , | non_padded_namespaces : Iterable [ str ] = DEFAULT_NON_PADDED_NAMESPACES , | pretrained_files : Optional [ Dict [ str , str ]] = None , | only_include_pretrained_words : bool = False , | tokens_to_add : Dict [ str , List [ str ]] = None , | min_pretrained_embeddings : Dict [ str , int ] = None , | padding_token : Optional [ str ] = DEFAULT_PADDING_TOKEN , | oov_token : Optional [ str ] = DEFAULT_OOV_TOKEN | ) -> \"Vocabulary\" Constructs a vocabulary given a collection of Instances and some parameters. We count all of the vocabulary items in the instances, then pass those counts and the other parameters, to __init__ . See that method for a description of what the other parameters do. The instances parameter does not get an entry in a typical AllenNLP configuration file, but the other parameters do (if you want non-default parameters). from_files # class Vocabulary ( Registrable ): | ... | @classmethod | def from_files ( | cls , | directory : str , | padding_token : Optional [ str ] = DEFAULT_PADDING_TOKEN , | oov_token : Optional [ str ] = DEFAULT_OOV_TOKEN | ) -> \"Vocabulary\" Loads a Vocabulary that was serialized either using save_to_files or inside a model archive file. Parameters directory : str The directory or archive file containing the serialized vocabulary. from_files_and_instances # class Vocabulary ( Registrable ): | ... | @classmethod | def from_files_and_instances ( | cls , | instances : Iterable [ \"adi.Instance\" ], | directory : str , | padding_token : Optional [ str ] = DEFAULT_PADDING_TOKEN , | oov_token : Optional [ str ] = DEFAULT_OOV_TOKEN , | min_count : Dict [ str , int ] = None , | max_vocab_size : Union [ int , Dict [ str , int ]] = None , | non_padded_namespaces : Iterable [ str ] = DEFAULT_NON_PADDED_NAMESPACES , | pretrained_files : Optional [ Dict [ str , str ]] = None , | only_include_pretrained_words : bool = False , | tokens_to_add : Dict [ str , List [ str ]] = None , | min_pretrained_embeddings : Dict [ str , int ] = None | ) -> \"Vocabulary\" Extends an already generated vocabulary using a collection of instances. The instances parameter does not get an entry in a typical AllenNLP configuration file, but the other parameters do (if you want non-default parameters). See __init__ for a description of what the other parameters mean. empty # class Vocabulary ( Registrable ): | ... | @classmethod | def empty ( cls ) -> \"Vocabulary\" This method returns a bare vocabulary instantiated with cls() (so, Vocabulary() if you haven't made a subclass of this object). The only reason to call Vocabulary.empty() instead of Vocabulary() is if you are instantiating this object from a config file. We register this constructor with the key \"empty\", so if you know that you don't need to compute a vocabulary (either because you're loading a pre-trained model from an archive file, you're using a pre-trained transformer that has its own vocabulary, or something else), you can use this to avoid having the default vocabulary construction code iterate through the data. set_from_file # class Vocabulary ( Registrable ): | ... | def set_from_file ( | self , | filename : str , | is_padded : bool = True , | oov_token : str = DEFAULT_OOV_TOKEN , | namespace : str = \"tokens\" | ) If you already have a vocabulary file for a trained model somewhere, and you really want to use that vocabulary file instead of just setting the vocabulary from a dataset, for whatever reason, you can do that with this method. You must specify the namespace to use, and we assume that you want to use padding and OOV tokens for this. Parameters filename : str The file containing the vocabulary to load. It should be formatted as one token per line, with nothing else in the line. The index we assign to the token is the line number in the file (1-indexed if is_padded , 0-indexed otherwise). Note that this file should contain the OOV token string! is_padded : bool , optional (default = True ) Is this vocabulary padded? For token / word / character vocabularies, this should be True ; while for tag or label vocabularies, this should typically be False . If True , we add a padding token with index 0, and we enforce that the oov_token is present in the file. oov_token : str , optional (default = DEFAULT_OOV_TOKEN ) What token does this vocabulary use to represent out-of-vocabulary characters? This must show up as a line in the vocabulary file. When we find it, we replace oov_token with self._oov_token , because we only use one OOV token across namespaces. namespace : str , optional (default = \"tokens\" ) What namespace should we overwrite with this vocab file? extend_from_instances # class Vocabulary ( Registrable ): | ... | def extend_from_instances ( | self , | instances : Iterable [ \"adi.Instance\" ] | ) -> None extend_from_vocab # class Vocabulary ( Registrable ): | ... | def extend_from_vocab ( self , vocab : \"Vocabulary\" ) -> None Adds all vocabulary items from all namespaces in the given vocabulary to this vocabulary. Useful if you want to load a model and extends its vocabulary from new instances. We also add all non-padded namespaces from the given vocabulary to this vocabulary. save_to_files # class Vocabulary ( Registrable ): | ... | def save_to_files ( self , directory : str ) -> None Persist this Vocabulary to files so it can be reloaded later. Each namespace corresponds to one file. Parameters directory : str The directory where we save the serialized vocabulary. is_padded # class Vocabulary ( Registrable ): | ... | def is_padded ( self , namespace : str ) -> bool Returns whether or not there are padding and OOV tokens added to the given namespace. add_token_to_namespace # class Vocabulary ( Registrable ): | ... | def add_token_to_namespace ( | self , | token : str , | namespace : str = \"tokens\" | ) -> int Adds token to the index, if it is not already present. Either way, we return the index of the token. add_tokens_to_namespace # class Vocabulary ( Registrable ): | ... | def add_tokens_to_namespace ( | self , | tokens : List [ str ], | namespace : str = \"tokens\" | ) -> List [ int ] Adds tokens to the index, if they are not already present. Either way, we return the indices of the tokens in the order that they were given. get_index_to_token_vocabulary # class Vocabulary ( Registrable ): | ... | def get_index_to_token_vocabulary ( | self , | namespace : str = \"tokens\" | ) -> Dict [ int , str ] get_token_to_index_vocabulary # class Vocabulary ( Registrable ): | ... | def get_token_to_index_vocabulary ( | self , | namespace : str = \"tokens\" | ) -> Dict [ str , int ] get_token_index # class Vocabulary ( Registrable ): | ... | def get_token_index ( | self , | token : str , | namespace : str = \"tokens\" | ) -> int get_token_from_index # class Vocabulary ( Registrable ): | ... | def get_token_from_index ( | self , | index : int , | namespace : str = \"tokens\" | ) -> str get_vocab_size # class Vocabulary ( Registrable ): | ... | def get_vocab_size ( self , namespace : str = \"tokens\" ) -> int get_namespaces # class Vocabulary ( Registrable ): | ... | def get_namespaces ( self ) -> Set [ str ] print_statistics # class Vocabulary ( Registrable ): | ... | def print_statistics ( self ) -> None","title":"vocabulary"},{"location":"api/data/vocabulary/#default_non_padded_namespaces","text":"DEFAULT_NON_PADDED_NAMESPACES = ( \"*tags\" , \"*labels\" )","title":"DEFAULT_NON_PADDED_NAMESPACES"},{"location":"api/data/vocabulary/#default_padding_token","text":"DEFAULT_PADDING_TOKEN = \"@@PADDING@@\"","title":"DEFAULT_PADDING_TOKEN"},{"location":"api/data/vocabulary/#default_oov_token","text":"DEFAULT_OOV_TOKEN = \"@@UNKNOWN@@\"","title":"DEFAULT_OOV_TOKEN"},{"location":"api/data/vocabulary/#namespace_padding_file","text":"NAMESPACE_PADDING_FILE = \"non_padded_namespaces.txt\"","title":"NAMESPACE_PADDING_FILE"},{"location":"api/data/vocabulary/#vocabulary","text":"class Vocabulary ( Registrable ): | def __init__ ( | self , | counter : Dict [ str , Dict [ str , int ]] = None , | min_count : Dict [ str , int ] = None , | max_vocab_size : Union [ int , Dict [ str , int ]] = None , | non_padded_namespaces : Iterable [ str ] = DEFAULT_NON_PADDED_NAMESPACES , | pretrained_files : Optional [ Dict [ str , str ]] = None , | only_include_pretrained_words : bool = False , | tokens_to_add : Dict [ str , List [ str ]] = None , | min_pretrained_embeddings : Dict [ str , int ] = None , | padding_token : Optional [ str ] = DEFAULT_PADDING_TOKEN , | oov_token : Optional [ str ] = DEFAULT_OOV_TOKEN | ) -> None A Vocabulary maps strings to integers, allowing for strings to be mapped to an out-of-vocabulary token. Vocabularies are fit to a particular dataset, which we use to decide which tokens are in-vocabulary. Vocabularies also allow for several different namespaces, so you can have separate indices for 'a' as a word, and 'a' as a character, for instance, and so we can use this object to also map tag and label strings to indices, for a unified .fields.field.Field API. Most of the methods on this class allow you to pass in a namespace; by default we use the 'tokens' namespace, and you can omit the namespace argument everywhere and just use the default. This class is registered as a Vocabulary with four different names, which all point to different @classmethod constructors found in this class. from_instances is registered as \"from_instances\", from_files is registered as \"from_files\", from_files_and_instances is registered as \"extend\", and empty is registered as \"empty\". If you are using a configuration file to construct a vocabulary, you can use any of those strings as the \"type\" key in the configuration file to use the corresponding @classmethod to construct the object. \"from_instances\" is the default. Look at the docstring for the @classmethod to see what keys are allowed in the configuration file (when there is an instances argument to the @classmethod , it will be passed in separately and does not need a corresponding key in the configuration file). Parameters counter : Dict[str, Dict[str, int]] , optional (default = None ) A collection of counts from which to initialize this vocabulary. We will examine the counts and, together with the other parameters to this class, use them to decide which words are in-vocabulary. If this is None , we just won't initialize the vocabulary with anything. min_count : Dict[str, int] , optional (default = None ) When initializing the vocab from a counter, you can specify a minimum count, and every token with a count less than this will not be added to the dictionary. These minimum counts are namespace-specific , so you can specify different minimums for labels versus words tokens, for example. If a namespace does not have a key in the given dictionary, we will add all seen tokens to that namespace. max_vocab_size : Union[int, Dict[str, int]] , optional (default = None ) If you want to cap the number of tokens in your vocabulary, you can do so with this parameter. If you specify a single integer, every namespace will have its vocabulary fixed to be no larger than this. If you specify a dictionary, then each namespace in the counter can have a separate maximum vocabulary size. Any missing key will have a value of None , which means no cap on the vocabulary size. non_padded_namespaces : Iterable[str] , optional By default, we assume you are mapping word / character tokens to integers, and so you want to reserve word indices for padding and out-of-vocabulary tokens. However, if you are mapping NER or SRL tags, or class labels, to integers, you probably do not want to reserve indices for padding and out-of-vocabulary tokens. Use this field to specify which namespaces should not have padding and OOV tokens added. The format of each element of this is either a string, which must match field names exactly, or * followed by a string, which we match as a suffix against field names. We try to make the default here reasonable, so that you don't have to think about this. The default is (\"*tags\", \"*labels\") , so as long as your namespace ends in \"tags\" or \"labels\" (which is true by default for all tag and label fields in this code), you don't have to specify anything here. pretrained_files : Dict[str, str] , optional If provided, this map specifies the path to optional pretrained embedding files for each namespace. This can be used to either restrict the vocabulary to only words which appear in this file, or to ensure that any words in this file are included in the vocabulary regardless of their count, depending on the value of only_include_pretrained_words . Words which appear in the pretrained embedding file but not in the data are NOT included in the Vocabulary. min_pretrained_embeddings : Dict[str, int] , optional If provided, specifies for each namespace a minimum number of lines (typically the most common words) to keep from pretrained embedding files, even for words not appearing in the data. only_include_pretrained_words : bool , optional (default = False ) This defines the strategy for using any pretrained embedding files which may have been specified in pretrained_files . If False, an inclusive strategy is used: and words which are in the counter and in the pretrained file are added to the Vocabulary , regardless of whether their count exceeds min_count or not. If True, we use an exclusive strategy: words are only included in the Vocabulary if they are in the pretrained embedding file (their count must still be at least min_count ). tokens_to_add : Dict[str, List[str]] , optional (default = None ) If given, this is a list of tokens to add to the vocabulary, keyed by the namespace to add the tokens to. This is a way to be sure that certain items appear in your vocabulary, regardless of any other vocabulary computation. padding_token : str , optional (default = DEFAULT_PADDING_TOKEN ) If given, this the string used for padding. oov_token : str , optional (default = DEFAULT_OOV_TOKEN ) If given, this the string used for the out of vocabulary (OOVs) tokens.","title":"Vocabulary"},{"location":"api/data/vocabulary/#default_implementation","text":"class Vocabulary ( Registrable ): | ... | default_implementation = \"from_instances\"","title":"default_implementation"},{"location":"api/data/vocabulary/#from_instances","text":"class Vocabulary ( Registrable ): | ... | @classmethod | def from_instances ( | cls , | instances : Iterable [ \"adi.Instance\" ], | min_count : Dict [ str , int ] = None , | max_vocab_size : Union [ int , Dict [ str , int ]] = None , | non_padded_namespaces : Iterable [ str ] = DEFAULT_NON_PADDED_NAMESPACES , | pretrained_files : Optional [ Dict [ str , str ]] = None , | only_include_pretrained_words : bool = False , | tokens_to_add : Dict [ str , List [ str ]] = None , | min_pretrained_embeddings : Dict [ str , int ] = None , | padding_token : Optional [ str ] = DEFAULT_PADDING_TOKEN , | oov_token : Optional [ str ] = DEFAULT_OOV_TOKEN | ) -> \"Vocabulary\" Constructs a vocabulary given a collection of Instances and some parameters. We count all of the vocabulary items in the instances, then pass those counts and the other parameters, to __init__ . See that method for a description of what the other parameters do. The instances parameter does not get an entry in a typical AllenNLP configuration file, but the other parameters do (if you want non-default parameters).","title":"from_instances"},{"location":"api/data/vocabulary/#from_files","text":"class Vocabulary ( Registrable ): | ... | @classmethod | def from_files ( | cls , | directory : str , | padding_token : Optional [ str ] = DEFAULT_PADDING_TOKEN , | oov_token : Optional [ str ] = DEFAULT_OOV_TOKEN | ) -> \"Vocabulary\" Loads a Vocabulary that was serialized either using save_to_files or inside a model archive file. Parameters directory : str The directory or archive file containing the serialized vocabulary.","title":"from_files"},{"location":"api/data/vocabulary/#from_files_and_instances","text":"class Vocabulary ( Registrable ): | ... | @classmethod | def from_files_and_instances ( | cls , | instances : Iterable [ \"adi.Instance\" ], | directory : str , | padding_token : Optional [ str ] = DEFAULT_PADDING_TOKEN , | oov_token : Optional [ str ] = DEFAULT_OOV_TOKEN , | min_count : Dict [ str , int ] = None , | max_vocab_size : Union [ int , Dict [ str , int ]] = None , | non_padded_namespaces : Iterable [ str ] = DEFAULT_NON_PADDED_NAMESPACES , | pretrained_files : Optional [ Dict [ str , str ]] = None , | only_include_pretrained_words : bool = False , | tokens_to_add : Dict [ str , List [ str ]] = None , | min_pretrained_embeddings : Dict [ str , int ] = None | ) -> \"Vocabulary\" Extends an already generated vocabulary using a collection of instances. The instances parameter does not get an entry in a typical AllenNLP configuration file, but the other parameters do (if you want non-default parameters). See __init__ for a description of what the other parameters mean.","title":"from_files_and_instances"},{"location":"api/data/vocabulary/#empty","text":"class Vocabulary ( Registrable ): | ... | @classmethod | def empty ( cls ) -> \"Vocabulary\" This method returns a bare vocabulary instantiated with cls() (so, Vocabulary() if you haven't made a subclass of this object). The only reason to call Vocabulary.empty() instead of Vocabulary() is if you are instantiating this object from a config file. We register this constructor with the key \"empty\", so if you know that you don't need to compute a vocabulary (either because you're loading a pre-trained model from an archive file, you're using a pre-trained transformer that has its own vocabulary, or something else), you can use this to avoid having the default vocabulary construction code iterate through the data.","title":"empty"},{"location":"api/data/vocabulary/#set_from_file","text":"class Vocabulary ( Registrable ): | ... | def set_from_file ( | self , | filename : str , | is_padded : bool = True , | oov_token : str = DEFAULT_OOV_TOKEN , | namespace : str = \"tokens\" | ) If you already have a vocabulary file for a trained model somewhere, and you really want to use that vocabulary file instead of just setting the vocabulary from a dataset, for whatever reason, you can do that with this method. You must specify the namespace to use, and we assume that you want to use padding and OOV tokens for this. Parameters filename : str The file containing the vocabulary to load. It should be formatted as one token per line, with nothing else in the line. The index we assign to the token is the line number in the file (1-indexed if is_padded , 0-indexed otherwise). Note that this file should contain the OOV token string! is_padded : bool , optional (default = True ) Is this vocabulary padded? For token / word / character vocabularies, this should be True ; while for tag or label vocabularies, this should typically be False . If True , we add a padding token with index 0, and we enforce that the oov_token is present in the file. oov_token : str , optional (default = DEFAULT_OOV_TOKEN ) What token does this vocabulary use to represent out-of-vocabulary characters? This must show up as a line in the vocabulary file. When we find it, we replace oov_token with self._oov_token , because we only use one OOV token across namespaces. namespace : str , optional (default = \"tokens\" ) What namespace should we overwrite with this vocab file?","title":"set_from_file"},{"location":"api/data/vocabulary/#extend_from_instances","text":"class Vocabulary ( Registrable ): | ... | def extend_from_instances ( | self , | instances : Iterable [ \"adi.Instance\" ] | ) -> None","title":"extend_from_instances"},{"location":"api/data/vocabulary/#extend_from_vocab","text":"class Vocabulary ( Registrable ): | ... | def extend_from_vocab ( self , vocab : \"Vocabulary\" ) -> None Adds all vocabulary items from all namespaces in the given vocabulary to this vocabulary. Useful if you want to load a model and extends its vocabulary from new instances. We also add all non-padded namespaces from the given vocabulary to this vocabulary.","title":"extend_from_vocab"},{"location":"api/data/vocabulary/#save_to_files","text":"class Vocabulary ( Registrable ): | ... | def save_to_files ( self , directory : str ) -> None Persist this Vocabulary to files so it can be reloaded later. Each namespace corresponds to one file. Parameters directory : str The directory where we save the serialized vocabulary.","title":"save_to_files"},{"location":"api/data/vocabulary/#is_padded","text":"class Vocabulary ( Registrable ): | ... | def is_padded ( self , namespace : str ) -> bool Returns whether or not there are padding and OOV tokens added to the given namespace.","title":"is_padded"},{"location":"api/data/vocabulary/#add_token_to_namespace","text":"class Vocabulary ( Registrable ): | ... | def add_token_to_namespace ( | self , | token : str , | namespace : str = \"tokens\" | ) -> int Adds token to the index, if it is not already present. Either way, we return the index of the token.","title":"add_token_to_namespace"},{"location":"api/data/vocabulary/#add_tokens_to_namespace","text":"class Vocabulary ( Registrable ): | ... | def add_tokens_to_namespace ( | self , | tokens : List [ str ], | namespace : str = \"tokens\" | ) -> List [ int ] Adds tokens to the index, if they are not already present. Either way, we return the indices of the tokens in the order that they were given.","title":"add_tokens_to_namespace"},{"location":"api/data/vocabulary/#get_index_to_token_vocabulary","text":"class Vocabulary ( Registrable ): | ... | def get_index_to_token_vocabulary ( | self , | namespace : str = \"tokens\" | ) -> Dict [ int , str ]","title":"get_index_to_token_vocabulary"},{"location":"api/data/vocabulary/#get_token_to_index_vocabulary","text":"class Vocabulary ( Registrable ): | ... | def get_token_to_index_vocabulary ( | self , | namespace : str = \"tokens\" | ) -> Dict [ str , int ]","title":"get_token_to_index_vocabulary"},{"location":"api/data/vocabulary/#get_token_index","text":"class Vocabulary ( Registrable ): | ... | def get_token_index ( | self , | token : str , | namespace : str = \"tokens\" | ) -> int","title":"get_token_index"},{"location":"api/data/vocabulary/#get_token_from_index","text":"class Vocabulary ( Registrable ): | ... | def get_token_from_index ( | self , | index : int , | namespace : str = \"tokens\" | ) -> str","title":"get_token_from_index"},{"location":"api/data/vocabulary/#get_vocab_size","text":"class Vocabulary ( Registrable ): | ... | def get_vocab_size ( self , namespace : str = \"tokens\" ) -> int","title":"get_vocab_size"},{"location":"api/data/vocabulary/#get_namespaces","text":"class Vocabulary ( Registrable ): | ... | def get_namespaces ( self ) -> Set [ str ]","title":"get_namespaces"},{"location":"api/data/vocabulary/#print_statistics","text":"class Vocabulary ( Registrable ): | ... | def print_statistics ( self ) -> None","title":"print_statistics"},{"location":"api/data/dataset_readers/babi/","text":"[ allennlp .data .dataset_readers .babi ] BabiReader # class BabiReader ( DatasetReader ): | def __init__ ( | self , | keep_sentences : bool = False , | token_indexers : Dict [ str , TokenIndexer ] = None , | ** kwargs | ) -> None Reads one single task in the bAbI tasks format as formulated in Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks (https://arxiv.org/abs/1502.05698). Since this class handle a single file, if one wants to load multiple tasks together it has to merge them into a single file and use this reader. Registered as a DatasetReader with name \"babi\". Parameters keep_sentences : bool , optional (default = False ) Whether to keep each sentence in the context or to concatenate them. Default is False that corresponds to concatenation. token_indexers : Dict[str, TokenIndexer] , optional (default = {\"tokens\": SingleIdTokenIndexer()} ) We use this to define the input representation for the text. See TokenIndexer . text_to_instance # class BabiReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | context : List [ List [ str ]], | question : List [ str ], | answer : str , | supports : List [ int ] | ) -> Instance","title":"babi"},{"location":"api/data/dataset_readers/babi/#babireader","text":"class BabiReader ( DatasetReader ): | def __init__ ( | self , | keep_sentences : bool = False , | token_indexers : Dict [ str , TokenIndexer ] = None , | ** kwargs | ) -> None Reads one single task in the bAbI tasks format as formulated in Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks (https://arxiv.org/abs/1502.05698). Since this class handle a single file, if one wants to load multiple tasks together it has to merge them into a single file and use this reader. Registered as a DatasetReader with name \"babi\". Parameters keep_sentences : bool , optional (default = False ) Whether to keep each sentence in the context or to concatenate them. Default is False that corresponds to concatenation. token_indexers : Dict[str, TokenIndexer] , optional (default = {\"tokens\": SingleIdTokenIndexer()} ) We use this to define the input representation for the text. See TokenIndexer .","title":"BabiReader"},{"location":"api/data/dataset_readers/babi/#text_to_instance","text":"class BabiReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | context : List [ List [ str ]], | question : List [ str ], | answer : str , | supports : List [ int ] | ) -> Instance","title":"text_to_instance"},{"location":"api/data/dataset_readers/conll2003/","text":"[ allennlp .data .dataset_readers .conll2003 ] Conll2003DatasetReader # class Conll2003DatasetReader ( DatasetReader ): | def __init__ ( | self , | token_indexers : Dict [ str , TokenIndexer ] = None , | tag_label : str = \"ner\" , | feature_labels : Sequence [ str ] = (), | coding_scheme : str = \"IOB1\" , | label_namespace : str = \"labels\" , | ** kwargs | ) -> None Reads instances from a pretokenised file where each line is in the following format: WORD POS-TAG CHUNK-TAG NER-TAG with a blank line indicating the end of each sentence and -DOCSTART- -X- -X- O indicating the end of each article, and converts it into a Dataset suitable for sequence tagging. Each Instance contains the words in the \"tokens\" TextField . The values corresponding to the tag_label values will get loaded into the \"tags\" SequenceLabelField . And if you specify any feature_labels (you probably shouldn't), the corresponding values will get loaded into their own SequenceLabelField s. This dataset reader ignores the \"article\" divisions and simply treats each sentence as an independent Instance . (Technically the reader splits sentences on any combination of blank lines and \"DOCSTART\" tags; in particular, it does the right thing on well formed inputs.) Registered as a DatasetReader with name \"conll2003\". Parameters token_indexers : Dict[str, TokenIndexer] , optional (default = {\"tokens\": SingleIdTokenIndexer()} ) We use this to define the input representation for the text. See TokenIndexer . tag_label : str , optional (default = ner ) Specify ner , pos , or chunk to have that tag loaded into the instance field tag . feature_labels : Sequence[str] , optional (default = () ) These labels will be loaded as features into the corresponding instance fields: pos -> pos_tags , chunk -> chunk_tags , ner -> ner_tags Each will have its own namespace : pos_tags , chunk_tags , ner_tags . If you want to use one of the tags as a feature in your model, it should be specified here. coding_scheme : str , optional (default = IOB1 ) Specifies the coding scheme for ner_labels and chunk_labels . Valid options are IOB1 and BIOUL . The IOB1 default maintains the original IOB1 scheme in the CoNLL 2003 NER data. In the IOB1 scheme, I is a token inside a span, O is a token outside a span and B is the beginning of span immediately following another span of the same type. label_namespace : str , optional (default = labels ) Specifies the namespace for the chosen tag_label . text_to_instance # class Conll2003DatasetReader ( DatasetReader ): | ... | def text_to_instance ( | self , | tokens : List [ Token ], | pos_tags : List [ str ] = None , | chunk_tags : List [ str ] = None , | ner_tags : List [ str ] = None | ) -> Instance We take pre-tokenized input here, because we don't have a tokenizer in this class.","title":"conll2003"},{"location":"api/data/dataset_readers/conll2003/#conll2003datasetreader","text":"class Conll2003DatasetReader ( DatasetReader ): | def __init__ ( | self , | token_indexers : Dict [ str , TokenIndexer ] = None , | tag_label : str = \"ner\" , | feature_labels : Sequence [ str ] = (), | coding_scheme : str = \"IOB1\" , | label_namespace : str = \"labels\" , | ** kwargs | ) -> None Reads instances from a pretokenised file where each line is in the following format: WORD POS-TAG CHUNK-TAG NER-TAG with a blank line indicating the end of each sentence and -DOCSTART- -X- -X- O indicating the end of each article, and converts it into a Dataset suitable for sequence tagging. Each Instance contains the words in the \"tokens\" TextField . The values corresponding to the tag_label values will get loaded into the \"tags\" SequenceLabelField . And if you specify any feature_labels (you probably shouldn't), the corresponding values will get loaded into their own SequenceLabelField s. This dataset reader ignores the \"article\" divisions and simply treats each sentence as an independent Instance . (Technically the reader splits sentences on any combination of blank lines and \"DOCSTART\" tags; in particular, it does the right thing on well formed inputs.) Registered as a DatasetReader with name \"conll2003\". Parameters token_indexers : Dict[str, TokenIndexer] , optional (default = {\"tokens\": SingleIdTokenIndexer()} ) We use this to define the input representation for the text. See TokenIndexer . tag_label : str , optional (default = ner ) Specify ner , pos , or chunk to have that tag loaded into the instance field tag . feature_labels : Sequence[str] , optional (default = () ) These labels will be loaded as features into the corresponding instance fields: pos -> pos_tags , chunk -> chunk_tags , ner -> ner_tags Each will have its own namespace : pos_tags , chunk_tags , ner_tags . If you want to use one of the tags as a feature in your model, it should be specified here. coding_scheme : str , optional (default = IOB1 ) Specifies the coding scheme for ner_labels and chunk_labels . Valid options are IOB1 and BIOUL . The IOB1 default maintains the original IOB1 scheme in the CoNLL 2003 NER data. In the IOB1 scheme, I is a token inside a span, O is a token outside a span and B is the beginning of span immediately following another span of the same type. label_namespace : str , optional (default = labels ) Specifies the namespace for the chosen tag_label .","title":"Conll2003DatasetReader"},{"location":"api/data/dataset_readers/conll2003/#text_to_instance","text":"class Conll2003DatasetReader ( DatasetReader ): | ... | def text_to_instance ( | self , | tokens : List [ Token ], | pos_tags : List [ str ] = None , | chunk_tags : List [ str ] = None , | ner_tags : List [ str ] = None | ) -> Instance We take pre-tokenized input here, because we don't have a tokenizer in this class.","title":"text_to_instance"},{"location":"api/data/dataset_readers/dataset_reader/","text":"[ allennlp .data .dataset_readers .dataset_reader ] AllennlpDataset # class AllennlpDataset ( Dataset ): | def __init__ ( | self , | instances : List [ Instance ], | vocab : Vocabulary = None | ) An AllennlpDataset is created by calling .read() on a non-lazy DatasetReader . It's essentially just a thin wrapper around a list of instances. index_with # class AllennlpDataset ( Dataset ): | ... | def index_with ( self , vocab : Vocabulary ) AllennlpLazyDataset # class AllennlpLazyDataset ( IterableDataset ): | def __init__ ( | self , | instance_generator : Callable [[ str ], Iterable [ Instance ]], | file_path : str , | vocab : Vocabulary = None | ) -> None An AllennlpLazyDataset is created by calling .read() on a lazy DatasetReader . Parameters instance_generator : Callable[[str], Iterable[Instance]] A factory function that creates an iterable of Instance s from a file path. This is usually just DatasetReader._instance_iterator . file_path : str The path to pass to the instance_generator function. vocab : Vocab , optional (default = None ) An optional vocab. This can also be set later with the .index_with method. index_with # class AllennlpLazyDataset ( IterableDataset ): | ... | def index_with ( self , vocab : Vocabulary ) DatasetReader # class DatasetReader ( Registrable ): | def __init__ ( | self , | lazy : bool = False , | cache_directory : Optional [ str ] = None , | max_instances : Optional [ int ] = None , | manual_distributed_sharding : bool = False , | manual_multi_process_sharding : bool = False | ) -> None A DatasetReader knows how to turn a file containing a dataset into a collection of Instances . To implement your own, just override the _read(file_path) method to return an Iterable of the instances. This could be a list containing the instances or a lazy generator that returns them one at a time. All parameters necessary to _read the data apart from the filepath should be passed to the constructor of the DatasetReader . Parameters lazy : bool , optional (default = False ) If this is true, instances() will return an object whose __iter__ method reloads the dataset each time it's called. Otherwise, instances() returns a list. cache_directory : str , optional (default = None ) If given, we will use this directory to store a cache of already-processed Instances in every file passed to read , serialized (by default, though you can override this) as one string-formatted Instance per line. If the cache file for a given file_path exists, we read the Instances from the cache instead of re-processing the data (using _instances_from_cache_file ). If the cache file does not exist, we will create it on our first pass through the data (using _instances_to_cache_file ). Note It is the caller's responsibility to make sure that this directory is unique for any combination of code and parameters that you use. That is, if you pass a directory here, we will use any existing cache files in that directory regardless of the parameters you set for this DatasetReader! max_instances : int , optional (default = None ) If given, will stop reading after this many instances. This is a useful setting for debugging. Setting this disables caching. manual_distributed_sharding : bool , optional (default = False ) By default, when used in a distributed setting, DatasetReader makes sure that each worker process only receives a subset of the data. It does this by reading the whole dataset in each worker, but filtering out the instances that are not needed. If you can implement a faster mechanism that only reads part of the data, set this to True, and do the sharding yourself. manual_multi_process_sharding : bool , optional (default = False ) This is similar to the manual_distributed_sharding parameter, but applies to multi-process data loading. By default, when this reader is used by a multi-process data loader (i.e. a DataLoader with num_workers > 1 ), each worker will filter out all but a subset of the instances that are needed so that you don't end up with duplicates. Note There is really no benefit of using a multi-process DataLoader unless you can specifically implement a faster sharding mechanism within _read() . In that case you should set manual_multi_process_sharding to True . CACHE_FILE_LOCK_TIMEOUT # class DatasetReader ( Registrable ): | ... | CACHE_FILE_LOCK_TIMEOUT : int = 10 The number of seconds to wait for the lock on a cache file to become available. read # class DatasetReader ( Registrable ): | ... | def read ( | self , | file_path : Union [ Path , str ] | ) -> Union [ AllennlpDataset , AllennlpLazyDataset ] Returns an dataset containing all the instances that can be read from the file path. If self.lazy is False , this eagerly reads all instances from self._read() and returns an AllennlpDataset . If self.lazy is True , this returns an AllennlpLazyDataset , which internally relies on the generator created from self._read() to lazily produce Instance s. In this case your implementation of _read() must also be lazy (that is, not load all instances into memory at once), otherwise you will get a ConfigurationError . In either case, the returned Iterable can be iterated over multiple times. It's unlikely you want to override this function, but if you do your result should likewise be repeatedly iterable. _read # class DatasetReader ( Registrable ): | ... | def _read ( self , file_path : str ) -> Iterable [ Instance ] Reads the instances from the given file_path and returns them as an Iterable (which could be a list or could be a generator). You are strongly encouraged to use a generator, so that users can read a dataset in a lazy way, if they so choose. text_to_instance # class DatasetReader ( Registrable ): | ... | def text_to_instance ( self , * inputs ) -> Instance Does whatever tokenization or processing is necessary to go from textual input to an Instance . The primary intended use for this is with a Predictor , which gets text input as a JSON object and needs to process it to be input to a model. The intent here is to share code between _read and what happens at model serving time, or any other time you want to make a prediction from new data. We need to process the data in the same way it was done at training time. Allowing the DatasetReader to process new text lets us accomplish this, as we can just call DatasetReader.text_to_instance when serving predictions. The input type here is rather vaguely specified, unfortunately. The Predictor will have to make some assumptions about the kind of DatasetReader that it's using, in order to pass it the right information. serialize_instance # class DatasetReader ( Registrable ): | ... | def serialize_instance ( self , instance : Instance ) -> str Serializes an Instance to a string. We use this for caching the processed data. The default implementation is to use jsonpickle . If you would like some other format for your pre-processed data, override this method. deserialize_instance # class DatasetReader ( Registrable ): | ... | def deserialize_instance ( self , string : str ) -> Instance Deserializes an Instance from a string. We use this when reading processed data from a cache. The default implementation is to use jsonpickle . If you would like some other format for your pre-processed data, override this method.","title":"dataset_reader"},{"location":"api/data/dataset_readers/dataset_reader/#allennlpdataset","text":"class AllennlpDataset ( Dataset ): | def __init__ ( | self , | instances : List [ Instance ], | vocab : Vocabulary = None | ) An AllennlpDataset is created by calling .read() on a non-lazy DatasetReader . It's essentially just a thin wrapper around a list of instances.","title":"AllennlpDataset"},{"location":"api/data/dataset_readers/dataset_reader/#index_with","text":"class AllennlpDataset ( Dataset ): | ... | def index_with ( self , vocab : Vocabulary )","title":"index_with"},{"location":"api/data/dataset_readers/dataset_reader/#allennlplazydataset","text":"class AllennlpLazyDataset ( IterableDataset ): | def __init__ ( | self , | instance_generator : Callable [[ str ], Iterable [ Instance ]], | file_path : str , | vocab : Vocabulary = None | ) -> None An AllennlpLazyDataset is created by calling .read() on a lazy DatasetReader . Parameters instance_generator : Callable[[str], Iterable[Instance]] A factory function that creates an iterable of Instance s from a file path. This is usually just DatasetReader._instance_iterator . file_path : str The path to pass to the instance_generator function. vocab : Vocab , optional (default = None ) An optional vocab. This can also be set later with the .index_with method.","title":"AllennlpLazyDataset"},{"location":"api/data/dataset_readers/dataset_reader/#index_with_1","text":"class AllennlpLazyDataset ( IterableDataset ): | ... | def index_with ( self , vocab : Vocabulary )","title":"index_with"},{"location":"api/data/dataset_readers/dataset_reader/#datasetreader","text":"class DatasetReader ( Registrable ): | def __init__ ( | self , | lazy : bool = False , | cache_directory : Optional [ str ] = None , | max_instances : Optional [ int ] = None , | manual_distributed_sharding : bool = False , | manual_multi_process_sharding : bool = False | ) -> None A DatasetReader knows how to turn a file containing a dataset into a collection of Instances . To implement your own, just override the _read(file_path) method to return an Iterable of the instances. This could be a list containing the instances or a lazy generator that returns them one at a time. All parameters necessary to _read the data apart from the filepath should be passed to the constructor of the DatasetReader . Parameters lazy : bool , optional (default = False ) If this is true, instances() will return an object whose __iter__ method reloads the dataset each time it's called. Otherwise, instances() returns a list. cache_directory : str , optional (default = None ) If given, we will use this directory to store a cache of already-processed Instances in every file passed to read , serialized (by default, though you can override this) as one string-formatted Instance per line. If the cache file for a given file_path exists, we read the Instances from the cache instead of re-processing the data (using _instances_from_cache_file ). If the cache file does not exist, we will create it on our first pass through the data (using _instances_to_cache_file ). Note It is the caller's responsibility to make sure that this directory is unique for any combination of code and parameters that you use. That is, if you pass a directory here, we will use any existing cache files in that directory regardless of the parameters you set for this DatasetReader! max_instances : int , optional (default = None ) If given, will stop reading after this many instances. This is a useful setting for debugging. Setting this disables caching. manual_distributed_sharding : bool , optional (default = False ) By default, when used in a distributed setting, DatasetReader makes sure that each worker process only receives a subset of the data. It does this by reading the whole dataset in each worker, but filtering out the instances that are not needed. If you can implement a faster mechanism that only reads part of the data, set this to True, and do the sharding yourself. manual_multi_process_sharding : bool , optional (default = False ) This is similar to the manual_distributed_sharding parameter, but applies to multi-process data loading. By default, when this reader is used by a multi-process data loader (i.e. a DataLoader with num_workers > 1 ), each worker will filter out all but a subset of the instances that are needed so that you don't end up with duplicates. Note There is really no benefit of using a multi-process DataLoader unless you can specifically implement a faster sharding mechanism within _read() . In that case you should set manual_multi_process_sharding to True .","title":"DatasetReader"},{"location":"api/data/dataset_readers/dataset_reader/#cache_file_lock_timeout","text":"class DatasetReader ( Registrable ): | ... | CACHE_FILE_LOCK_TIMEOUT : int = 10 The number of seconds to wait for the lock on a cache file to become available.","title":"CACHE_FILE_LOCK_TIMEOUT"},{"location":"api/data/dataset_readers/dataset_reader/#read","text":"class DatasetReader ( Registrable ): | ... | def read ( | self , | file_path : Union [ Path , str ] | ) -> Union [ AllennlpDataset , AllennlpLazyDataset ] Returns an dataset containing all the instances that can be read from the file path. If self.lazy is False , this eagerly reads all instances from self._read() and returns an AllennlpDataset . If self.lazy is True , this returns an AllennlpLazyDataset , which internally relies on the generator created from self._read() to lazily produce Instance s. In this case your implementation of _read() must also be lazy (that is, not load all instances into memory at once), otherwise you will get a ConfigurationError . In either case, the returned Iterable can be iterated over multiple times. It's unlikely you want to override this function, but if you do your result should likewise be repeatedly iterable.","title":"read"},{"location":"api/data/dataset_readers/dataset_reader/#_read","text":"class DatasetReader ( Registrable ): | ... | def _read ( self , file_path : str ) -> Iterable [ Instance ] Reads the instances from the given file_path and returns them as an Iterable (which could be a list or could be a generator). You are strongly encouraged to use a generator, so that users can read a dataset in a lazy way, if they so choose.","title":"_read"},{"location":"api/data/dataset_readers/dataset_reader/#text_to_instance","text":"class DatasetReader ( Registrable ): | ... | def text_to_instance ( self , * inputs ) -> Instance Does whatever tokenization or processing is necessary to go from textual input to an Instance . The primary intended use for this is with a Predictor , which gets text input as a JSON object and needs to process it to be input to a model. The intent here is to share code between _read and what happens at model serving time, or any other time you want to make a prediction from new data. We need to process the data in the same way it was done at training time. Allowing the DatasetReader to process new text lets us accomplish this, as we can just call DatasetReader.text_to_instance when serving predictions. The input type here is rather vaguely specified, unfortunately. The Predictor will have to make some assumptions about the kind of DatasetReader that it's using, in order to pass it the right information.","title":"text_to_instance"},{"location":"api/data/dataset_readers/dataset_reader/#serialize_instance","text":"class DatasetReader ( Registrable ): | ... | def serialize_instance ( self , instance : Instance ) -> str Serializes an Instance to a string. We use this for caching the processed data. The default implementation is to use jsonpickle . If you would like some other format for your pre-processed data, override this method.","title":"serialize_instance"},{"location":"api/data/dataset_readers/dataset_reader/#deserialize_instance","text":"class DatasetReader ( Registrable ): | ... | def deserialize_instance ( self , string : str ) -> Instance Deserializes an Instance from a string. We use this when reading processed data from a cache. The default implementation is to use jsonpickle . If you would like some other format for your pre-processed data, override this method.","title":"deserialize_instance"},{"location":"api/data/dataset_readers/interleaving_dataset_reader/","text":"[ allennlp .data .dataset_readers .interleaving_dataset_reader ] InterleavingDatasetReader # class InterleavingDatasetReader ( DatasetReader ): | def __init__ ( | self , | readers : Dict [ str , DatasetReader ], | dataset_field_name : str = \"dataset\" , | scheme : str = \"round_robin\" , | ** kwargs | ) -> None A DatasetReader that wraps multiple other dataset readers, and interleaves their instances, adding a MetadataField to indicate the provenance of each instance. Unlike most of our other dataset readers, here the file_path passed into read() should be a JSON-serialized dictionary with one file_path per wrapped dataset reader (and with corresponding keys). Registered as a DatasetReader with name \"interleaving\". Parameters readers : Dict[str, DatasetReader] The dataset readers to wrap. The keys of this dictionary will be used as the values in the MetadataField indicating provenance. dataset_field_name : str , optional (default = \"dataset\" ) The name of the MetadataField indicating which dataset an instance came from. scheme : str , optional (default = \"round_robin\" ) Indicates how to interleave instances. Currently the two options are \"round_robin\", which repeatedly cycles through the datasets grabbing one instance from each; and \"all_at_once\", which yields all the instances from the first dataset, then all the instances from the second dataset, and so on. You could imagine also implementing some sort of over- or under-sampling, although hasn't been done. text_to_instance # class InterleavingDatasetReader ( DatasetReader ): | ... | def text_to_instance ( self ) -> Instance","title":"interleaving_dataset_reader"},{"location":"api/data/dataset_readers/interleaving_dataset_reader/#interleavingdatasetreader","text":"class InterleavingDatasetReader ( DatasetReader ): | def __init__ ( | self , | readers : Dict [ str , DatasetReader ], | dataset_field_name : str = \"dataset\" , | scheme : str = \"round_robin\" , | ** kwargs | ) -> None A DatasetReader that wraps multiple other dataset readers, and interleaves their instances, adding a MetadataField to indicate the provenance of each instance. Unlike most of our other dataset readers, here the file_path passed into read() should be a JSON-serialized dictionary with one file_path per wrapped dataset reader (and with corresponding keys). Registered as a DatasetReader with name \"interleaving\". Parameters readers : Dict[str, DatasetReader] The dataset readers to wrap. The keys of this dictionary will be used as the values in the MetadataField indicating provenance. dataset_field_name : str , optional (default = \"dataset\" ) The name of the MetadataField indicating which dataset an instance came from. scheme : str , optional (default = \"round_robin\" ) Indicates how to interleave instances. Currently the two options are \"round_robin\", which repeatedly cycles through the datasets grabbing one instance from each; and \"all_at_once\", which yields all the instances from the first dataset, then all the instances from the second dataset, and so on. You could imagine also implementing some sort of over- or under-sampling, although hasn't been done.","title":"InterleavingDatasetReader"},{"location":"api/data/dataset_readers/interleaving_dataset_reader/#text_to_instance","text":"class InterleavingDatasetReader ( DatasetReader ): | ... | def text_to_instance ( self ) -> Instance","title":"text_to_instance"},{"location":"api/data/dataset_readers/sequence_tagging/","text":"[ allennlp .data .dataset_readers .sequence_tagging ] DEFAULT_WORD_TAG_DELIMITER # DEFAULT_WORD_TAG_DELIMITER = \"###\" SequenceTaggingDatasetReader # class SequenceTaggingDatasetReader ( DatasetReader ): | def __init__ ( | self , | word_tag_delimiter : str = DEFAULT_WORD_TAG_DELIMITER , | token_delimiter : str = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | ** kwargs | ) -> None Reads instances from a pretokenised file where each line is in the following format: WORD # ##TAG [ TAB ] WORD # ##TAG [ TAB ] ..... \\ n and converts it into a Dataset suitable for sequence tagging. You can also specify alternative delimiters in the constructor. Registered as a DatasetReader with name \"sequence_tagging\". Parameters word_tag_delimiter : str , optional (default = \"###\" ) The text that separates each WORD from its TAG. token_delimiter : str , optional (default = None ) The text that separates each WORD-TAG pair from the next pair. If None then the line will just be split on whitespace. token_indexers : Dict[str, TokenIndexer] , optional (default = {\"tokens\": SingleIdTokenIndexer()} ) We use this to define the input representation for the text. See TokenIndexer . Note that the output tags will always correspond to single token IDs based on how they are pre-tokenised in the data file. text_to_instance # class SequenceTaggingDatasetReader ( DatasetReader ): | ... | def text_to_instance ( | self , | tokens : List [ Token ], | tags : List [ str ] = None | ) -> Instance We take pre-tokenized input here, because we don't have a tokenizer in this class.","title":"sequence_tagging"},{"location":"api/data/dataset_readers/sequence_tagging/#default_word_tag_delimiter","text":"DEFAULT_WORD_TAG_DELIMITER = \"###\"","title":"DEFAULT_WORD_TAG_DELIMITER"},{"location":"api/data/dataset_readers/sequence_tagging/#sequencetaggingdatasetreader","text":"class SequenceTaggingDatasetReader ( DatasetReader ): | def __init__ ( | self , | word_tag_delimiter : str = DEFAULT_WORD_TAG_DELIMITER , | token_delimiter : str = None , | token_indexers : Dict [ str , TokenIndexer ] = None , | ** kwargs | ) -> None Reads instances from a pretokenised file where each line is in the following format: WORD # ##TAG [ TAB ] WORD # ##TAG [ TAB ] ..... \\ n and converts it into a Dataset suitable for sequence tagging. You can also specify alternative delimiters in the constructor. Registered as a DatasetReader with name \"sequence_tagging\". Parameters word_tag_delimiter : str , optional (default = \"###\" ) The text that separates each WORD from its TAG. token_delimiter : str , optional (default = None ) The text that separates each WORD-TAG pair from the next pair. If None then the line will just be split on whitespace. token_indexers : Dict[str, TokenIndexer] , optional (default = {\"tokens\": SingleIdTokenIndexer()} ) We use this to define the input representation for the text. See TokenIndexer . Note that the output tags will always correspond to single token IDs based on how they are pre-tokenised in the data file.","title":"SequenceTaggingDatasetReader"},{"location":"api/data/dataset_readers/sequence_tagging/#text_to_instance","text":"class SequenceTaggingDatasetReader ( DatasetReader ): | ... | def text_to_instance ( | self , | tokens : List [ Token ], | tags : List [ str ] = None | ) -> Instance We take pre-tokenized input here, because we don't have a tokenizer in this class.","title":"text_to_instance"},{"location":"api/data/dataset_readers/sharded_dataset_reader/","text":"[ allennlp .data .dataset_readers .sharded_dataset_reader ] ShardedDatasetReader # class ShardedDatasetReader ( DatasetReader ): | def __init__ ( self , base_reader : DatasetReader , ** kwargs ) -> None Wraps another dataset reader and uses it to read from multiple input files. Note that in this case the file_path passed to read() should either be a glob path or a path or URL to an archive file ('.zip' or '.tar.gz'). The dataset reader will return instances from all files matching the glob, or all files within the archive. The order the files are processed in is deterministic to enable the instances to be filtered according to worker rank in the distributed case. Registered as a DatasetReader with name \"sharded\". Parameters base_reader : DatasetReader Reader with a read method that accepts a single file. text_to_instance # class ShardedDatasetReader ( DatasetReader ): | ... | def text_to_instance ( self , * args , ** kwargs ) -> Instance Just delegate to the base reader text_to_instance.","title":"sharded_dataset_reader"},{"location":"api/data/dataset_readers/sharded_dataset_reader/#shardeddatasetreader","text":"class ShardedDatasetReader ( DatasetReader ): | def __init__ ( self , base_reader : DatasetReader , ** kwargs ) -> None Wraps another dataset reader and uses it to read from multiple input files. Note that in this case the file_path passed to read() should either be a glob path or a path or URL to an archive file ('.zip' or '.tar.gz'). The dataset reader will return instances from all files matching the glob, or all files within the archive. The order the files are processed in is deterministic to enable the instances to be filtered according to worker rank in the distributed case. Registered as a DatasetReader with name \"sharded\". Parameters base_reader : DatasetReader Reader with a read method that accepts a single file.","title":"ShardedDatasetReader"},{"location":"api/data/dataset_readers/sharded_dataset_reader/#text_to_instance","text":"class ShardedDatasetReader ( DatasetReader ): | ... | def text_to_instance ( self , * args , ** kwargs ) -> Instance Just delegate to the base reader text_to_instance.","title":"text_to_instance"},{"location":"api/data/dataset_readers/text_classification_json/","text":"[ allennlp .data .dataset_readers .text_classification_json ] TextClassificationJsonReader # class TextClassificationJsonReader ( DatasetReader ): | def __init__ ( | self , | token_indexers : Dict [ str , TokenIndexer ] = None , | tokenizer : Tokenizer = None , | segment_sentences : bool = False , | max_sequence_length : int = None , | skip_label_indexing : bool = False , | ** kwargs | ) -> None Reads tokens and their labels from a labeled text classification dataset. Expects a \"text\" field and a \"label\" field in JSON format. The output of read is a list of Instance s with the fields: tokens : TextField and label : LabelField Registered as a DatasetReader with name \"text_classification_json\". Parameters token_indexers : Dict[str, TokenIndexer] , optional optional (default= {\"tokens\": SingleIdTokenIndexer()} ) We use this to define the input representation for the text. See TokenIndexer . tokenizer : Tokenizer , optional (default = {\"tokens\": SpacyTokenizer()} ) Tokenizer to use to split the input text into words or other kinds of tokens. segment_sentences : bool , optional (default = False ) If True, we will first segment the text into sentences using SpaCy and then tokenize words. Necessary for some models that require pre-segmentation of sentences, like the Hierarchical Attention Network . max_sequence_length : int , optional (default = None ) If specified, will truncate tokens to specified maximum length. skip_label_indexing : bool , optional (default = False ) Whether or not to skip label indexing. You might want to skip label indexing if your labels are numbers, so the dataset reader doesn't re-number them starting from 0. text_to_instance # class TextClassificationJsonReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | text : str , | label : Union [ str , int ] = None | ) -> Instance Parameters text : str The text to classify label : str , optional (default = None ) The label for this text. Returns An Instance containing the following fields: tokens ( TextField ) : The tokens in the sentence or phrase. label ( LabelField ) : The label label of the sentence or phrase.","title":"text_classification_json"},{"location":"api/data/dataset_readers/text_classification_json/#textclassificationjsonreader","text":"class TextClassificationJsonReader ( DatasetReader ): | def __init__ ( | self , | token_indexers : Dict [ str , TokenIndexer ] = None , | tokenizer : Tokenizer = None , | segment_sentences : bool = False , | max_sequence_length : int = None , | skip_label_indexing : bool = False , | ** kwargs | ) -> None Reads tokens and their labels from a labeled text classification dataset. Expects a \"text\" field and a \"label\" field in JSON format. The output of read is a list of Instance s with the fields: tokens : TextField and label : LabelField Registered as a DatasetReader with name \"text_classification_json\". Parameters token_indexers : Dict[str, TokenIndexer] , optional optional (default= {\"tokens\": SingleIdTokenIndexer()} ) We use this to define the input representation for the text. See TokenIndexer . tokenizer : Tokenizer , optional (default = {\"tokens\": SpacyTokenizer()} ) Tokenizer to use to split the input text into words or other kinds of tokens. segment_sentences : bool , optional (default = False ) If True, we will first segment the text into sentences using SpaCy and then tokenize words. Necessary for some models that require pre-segmentation of sentences, like the Hierarchical Attention Network . max_sequence_length : int , optional (default = None ) If specified, will truncate tokens to specified maximum length. skip_label_indexing : bool , optional (default = False ) Whether or not to skip label indexing. You might want to skip label indexing if your labels are numbers, so the dataset reader doesn't re-number them starting from 0.","title":"TextClassificationJsonReader"},{"location":"api/data/dataset_readers/text_classification_json/#text_to_instance","text":"class TextClassificationJsonReader ( DatasetReader ): | ... | @overrides | def text_to_instance ( | self , | text : str , | label : Union [ str , int ] = None | ) -> Instance Parameters text : str The text to classify label : str , optional (default = None ) The label for this text. Returns An Instance containing the following fields: tokens ( TextField ) : The tokens in the sentence or phrase. label ( LabelField ) : The label label of the sentence or phrase.","title":"text_to_instance"},{"location":"api/data/dataset_readers/dataset_utils/span_utils/","text":"[ allennlp .data .dataset_readers .dataset_utils .span_utils ] TypedSpan # TypedSpan = Tuple [ int , Tuple [ int , int ]] TypedStringSpan # TypedStringSpan = Tuple [ str , Tuple [ int , int ]] InvalidTagSequence # class InvalidTagSequence ( Exception ): | def __init__ ( self , tag_sequence = None ) T # T = TypeVar ( \"T\" , str , Token ) enumerate_spans # def enumerate_spans ( sentence : List [ T ], offset : int = 0 , max_span_width : int = None , min_span_width : int = 1 , filter_function : Callable [[ List [ T ]], bool ] = None ) -> List [ Tuple [ int , int ]] Given a sentence, return all token spans within the sentence. Spans are inclusive . Additionally, you can provide a maximum and minimum span width, which will be used to exclude spans outside of this range. Finally, you can provide a function mapping List[T] -> bool , which will be applied to every span to decide whether that span should be included. This allows filtering by length, regex matches, pos tags or any Spacy Token attributes, for example. Parameters sentence : List[T] The sentence to generate spans for. The type is generic, as this function can be used with strings, or Spacy Tokens or other sequences. offset : int , optional (default = 0 ) A numeric offset to add to all span start and end indices. This is helpful if the sentence is part of a larger structure, such as a document, which the indices need to respect. max_span_width : int , optional (default = None ) The maximum length of spans which should be included. Defaults to len(sentence). min_span_width : int , optional (default = 1 ) The minimum length of spans which should be included. Defaults to 1. filter_function : Callable[[List[T]], bool] , optional (default = None ) A function mapping sequences of the passed type T to a boolean value. If True , the span is included in the returned spans from the sentence, otherwise it is excluded.. bio_tags_to_spans # def bio_tags_to_spans ( tag_sequence : List [ str ], classes_to_ignore : List [ str ] = None ) -> List [ TypedStringSpan ] Given a sequence corresponding to BIO tags, extracts spans. Spans are inclusive and can be of zero length, representing a single word span. Ill-formed spans are also included (i.e those which do not start with a \"B-LABEL\"), as otherwise it is possible to get a perfect precision score whilst still predicting ill-formed spans in addition to the correct spans. This function works properly when the spans are unlabeled (i.e., your labels are simply \"B\", \"I\", and \"O\"). Parameters tag_sequence : List[str] The integer class labels for a sequence. classes_to_ignore : List[str] , optional (default = None ) A list of string class labels excluding the bio tag which should be ignored when extracting spans. Returns spans : List[TypedStringSpan] The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)). Note that the label does not contain any BIO tag prefixes. iob1_tags_to_spans # def iob1_tags_to_spans ( tag_sequence : List [ str ], classes_to_ignore : List [ str ] = None ) -> List [ TypedStringSpan ] Given a sequence corresponding to IOB1 tags, extracts spans. Spans are inclusive and can be of zero length, representing a single word span. Ill-formed spans are also included (i.e., those where \"B-LABEL\" is not preceded by \"I-LABEL\" or \"B-LABEL\"). Parameters tag_sequence : List[str] The integer class labels for a sequence. classes_to_ignore : List[str] , optional (default = None ) A list of string class labels excluding the bio tag which should be ignored when extracting spans. Returns spans : List[TypedStringSpan] The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)). Note that the label does not contain any BIO tag prefixes. bioul_tags_to_spans # def bioul_tags_to_spans ( tag_sequence : List [ str ], classes_to_ignore : List [ str ] = None ) -> List [ TypedStringSpan ] Given a sequence corresponding to BIOUL tags, extracts spans. Spans are inclusive and can be of zero length, representing a single word span. Ill-formed spans are not allowed and will raise InvalidTagSequence . This function works properly when the spans are unlabeled (i.e., your labels are simply \"B\", \"I\", \"O\", \"U\", and \"L\"). Parameters tag_sequence : List[str] The tag sequence encoded in BIOUL, e.g. [\"B-PER\", \"L-PER\", \"O\"]. classes_to_ignore : List[str] , optional (default = None ) A list of string class labels excluding the bio tag which should be ignored when extracting spans. Returns spans : List[TypedStringSpan] The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)). iob1_to_bioul # def iob1_to_bioul ( tag_sequence : List [ str ]) -> List [ str ] to_bioul # def to_bioul ( tag_sequence : List [ str ], encoding : str = \"IOB1\" ) -> List [ str ] Given a tag sequence encoded with IOB1 labels, recode to BIOUL. In the IOB1 scheme, I is a token inside a span, O is a token outside a span and B is the beginning of span immediately following another span of the same type. In the BIO scheme, I is a token inside a span, O is a token outside a span and B is the beginning of a span. Parameters tag_sequence : List[str] The tag sequence encoded in IOB1, e.g. [\"I-PER\", \"I-PER\", \"O\"]. encoding : str , optional (default = \"IOB1\" ) The encoding type to convert from. Must be either \"IOB1\" or \"BIO\". Returns bioul_sequence : List[str] The tag sequence encoded in IOB1, e.g. [\"B-PER\", \"L-PER\", \"O\"]. bmes_tags_to_spans # def bmes_tags_to_spans ( tag_sequence : List [ str ], classes_to_ignore : List [ str ] = None ) -> List [ TypedStringSpan ] Given a sequence corresponding to BMES tags, extracts spans. Spans are inclusive and can be of zero length, representing a single word span. Ill-formed spans are also included (i.e those which do not start with a \"B-LABEL\"), as otherwise it is possible to get a perfect precision score whilst still predicting ill-formed spans in addition to the correct spans. This function works properly when the spans are unlabeled (i.e., your labels are simply \"B\", \"M\", \"E\" and \"S\"). Parameters tag_sequence : List[str] The integer class labels for a sequence. classes_to_ignore : List[str] , optional (default = None ) A list of string class labels excluding the bio tag which should be ignored when extracting spans. Returns spans : List[TypedStringSpan] The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)). Note that the label does not contain any BIO tag prefixes.","title":"span_utils"},{"location":"api/data/dataset_readers/dataset_utils/span_utils/#typedspan","text":"TypedSpan = Tuple [ int , Tuple [ int , int ]]","title":"TypedSpan"},{"location":"api/data/dataset_readers/dataset_utils/span_utils/#typedstringspan","text":"TypedStringSpan = Tuple [ str , Tuple [ int , int ]]","title":"TypedStringSpan"},{"location":"api/data/dataset_readers/dataset_utils/span_utils/#invalidtagsequence","text":"class InvalidTagSequence ( Exception ): | def __init__ ( self , tag_sequence = None )","title":"InvalidTagSequence"},{"location":"api/data/dataset_readers/dataset_utils/span_utils/#t","text":"T = TypeVar ( \"T\" , str , Token )","title":"T"},{"location":"api/data/dataset_readers/dataset_utils/span_utils/#enumerate_spans","text":"def enumerate_spans ( sentence : List [ T ], offset : int = 0 , max_span_width : int = None , min_span_width : int = 1 , filter_function : Callable [[ List [ T ]], bool ] = None ) -> List [ Tuple [ int , int ]] Given a sentence, return all token spans within the sentence. Spans are inclusive . Additionally, you can provide a maximum and minimum span width, which will be used to exclude spans outside of this range. Finally, you can provide a function mapping List[T] -> bool , which will be applied to every span to decide whether that span should be included. This allows filtering by length, regex matches, pos tags or any Spacy Token attributes, for example. Parameters sentence : List[T] The sentence to generate spans for. The type is generic, as this function can be used with strings, or Spacy Tokens or other sequences. offset : int , optional (default = 0 ) A numeric offset to add to all span start and end indices. This is helpful if the sentence is part of a larger structure, such as a document, which the indices need to respect. max_span_width : int , optional (default = None ) The maximum length of spans which should be included. Defaults to len(sentence). min_span_width : int , optional (default = 1 ) The minimum length of spans which should be included. Defaults to 1. filter_function : Callable[[List[T]], bool] , optional (default = None ) A function mapping sequences of the passed type T to a boolean value. If True , the span is included in the returned spans from the sentence, otherwise it is excluded..","title":"enumerate_spans"},{"location":"api/data/dataset_readers/dataset_utils/span_utils/#bio_tags_to_spans","text":"def bio_tags_to_spans ( tag_sequence : List [ str ], classes_to_ignore : List [ str ] = None ) -> List [ TypedStringSpan ] Given a sequence corresponding to BIO tags, extracts spans. Spans are inclusive and can be of zero length, representing a single word span. Ill-formed spans are also included (i.e those which do not start with a \"B-LABEL\"), as otherwise it is possible to get a perfect precision score whilst still predicting ill-formed spans in addition to the correct spans. This function works properly when the spans are unlabeled (i.e., your labels are simply \"B\", \"I\", and \"O\"). Parameters tag_sequence : List[str] The integer class labels for a sequence. classes_to_ignore : List[str] , optional (default = None ) A list of string class labels excluding the bio tag which should be ignored when extracting spans. Returns spans : List[TypedStringSpan] The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)). Note that the label does not contain any BIO tag prefixes.","title":"bio_tags_to_spans"},{"location":"api/data/dataset_readers/dataset_utils/span_utils/#iob1_tags_to_spans","text":"def iob1_tags_to_spans ( tag_sequence : List [ str ], classes_to_ignore : List [ str ] = None ) -> List [ TypedStringSpan ] Given a sequence corresponding to IOB1 tags, extracts spans. Spans are inclusive and can be of zero length, representing a single word span. Ill-formed spans are also included (i.e., those where \"B-LABEL\" is not preceded by \"I-LABEL\" or \"B-LABEL\"). Parameters tag_sequence : List[str] The integer class labels for a sequence. classes_to_ignore : List[str] , optional (default = None ) A list of string class labels excluding the bio tag which should be ignored when extracting spans. Returns spans : List[TypedStringSpan] The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)). Note that the label does not contain any BIO tag prefixes.","title":"iob1_tags_to_spans"},{"location":"api/data/dataset_readers/dataset_utils/span_utils/#bioul_tags_to_spans","text":"def bioul_tags_to_spans ( tag_sequence : List [ str ], classes_to_ignore : List [ str ] = None ) -> List [ TypedStringSpan ] Given a sequence corresponding to BIOUL tags, extracts spans. Spans are inclusive and can be of zero length, representing a single word span. Ill-formed spans are not allowed and will raise InvalidTagSequence . This function works properly when the spans are unlabeled (i.e., your labels are simply \"B\", \"I\", \"O\", \"U\", and \"L\"). Parameters tag_sequence : List[str] The tag sequence encoded in BIOUL, e.g. [\"B-PER\", \"L-PER\", \"O\"]. classes_to_ignore : List[str] , optional (default = None ) A list of string class labels excluding the bio tag which should be ignored when extracting spans. Returns spans : List[TypedStringSpan] The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).","title":"bioul_tags_to_spans"},{"location":"api/data/dataset_readers/dataset_utils/span_utils/#iob1_to_bioul","text":"def iob1_to_bioul ( tag_sequence : List [ str ]) -> List [ str ]","title":"iob1_to_bioul"},{"location":"api/data/dataset_readers/dataset_utils/span_utils/#to_bioul","text":"def to_bioul ( tag_sequence : List [ str ], encoding : str = \"IOB1\" ) -> List [ str ] Given a tag sequence encoded with IOB1 labels, recode to BIOUL. In the IOB1 scheme, I is a token inside a span, O is a token outside a span and B is the beginning of span immediately following another span of the same type. In the BIO scheme, I is a token inside a span, O is a token outside a span and B is the beginning of a span. Parameters tag_sequence : List[str] The tag sequence encoded in IOB1, e.g. [\"I-PER\", \"I-PER\", \"O\"]. encoding : str , optional (default = \"IOB1\" ) The encoding type to convert from. Must be either \"IOB1\" or \"BIO\". Returns bioul_sequence : List[str] The tag sequence encoded in IOB1, e.g. [\"B-PER\", \"L-PER\", \"O\"].","title":"to_bioul"},{"location":"api/data/dataset_readers/dataset_utils/span_utils/#bmes_tags_to_spans","text":"def bmes_tags_to_spans ( tag_sequence : List [ str ], classes_to_ignore : List [ str ] = None ) -> List [ TypedStringSpan ] Given a sequence corresponding to BMES tags, extracts spans. Spans are inclusive and can be of zero length, representing a single word span. Ill-formed spans are also included (i.e those which do not start with a \"B-LABEL\"), as otherwise it is possible to get a perfect precision score whilst still predicting ill-formed spans in addition to the correct spans. This function works properly when the spans are unlabeled (i.e., your labels are simply \"B\", \"M\", \"E\" and \"S\"). Parameters tag_sequence : List[str] The integer class labels for a sequence. classes_to_ignore : List[str] , optional (default = None ) A list of string class labels excluding the bio tag which should be ignored when extracting spans. Returns spans : List[TypedStringSpan] The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)). Note that the label does not contain any BIO tag prefixes.","title":"bmes_tags_to_spans"},{"location":"api/data/fields/adjacency_field/","text":"[ allennlp .data .fields .adjacency_field ] AdjacencyField # class AdjacencyField ( Field [ torch . Tensor ]): | def __init__ ( | self , | indices : List [ Tuple [ int , int ]], | sequence_field : SequenceField , | labels : List [ str ] = None , | label_namespace : str = \"labels\" , | padding_value : int = - 1 | ) -> None A AdjacencyField defines directed adjacency relations between elements in a SequenceField . Because it's a labeling of some other field, we take that field as input here and use it to determine our padding and other things. This field will get converted into an array of shape (sequence_field_length, sequence_field_length), where the (i, j)th array element is either a binary flag indicating there is an edge from i to j, or an integer label k, indicating there is a label from i to j of type k. Parameters indices : List[Tuple[int, int]] sequence_field : SequenceField A field containing the sequence that this AdjacencyField is labeling. Most often, this is a TextField , for tagging edge relations between tokens in a sentence. labels : List[str] , optional (default = None ) Optional labels for the edges of the adjacency matrix. label_namespace : str , optional (default = 'labels' ) The namespace to use for converting tag strings into integers. We convert tag strings to integers for you, and this parameter tells the Vocabulary object which mapping from strings to integers to use (so that \"O\" as a tag doesn't get the same id as \"O\" as a word). padding_value : int , optional (default = -1 ) The value to use as padding. count_vocab_items # class AdjacencyField ( Field [ torch . Tensor ]): | ... | @overrides | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]]) index # class AdjacencyField ( Field [ torch . Tensor ]): | ... | @overrides | def index ( self , vocab : Vocabulary ) get_padding_lengths # class AdjacencyField ( Field [ torch . Tensor ]): | ... | @overrides | def get_padding_lengths ( self ) -> Dict [ str , int ] as_tensor # class AdjacencyField ( Field [ torch . Tensor ]): | ... | @overrides | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor empty_field # class AdjacencyField ( Field [ torch . Tensor ]): | ... | @overrides | def empty_field ( self ) -> \"AdjacencyField\" The empty_list here is needed for mypy","title":"adjacency_field"},{"location":"api/data/fields/adjacency_field/#adjacencyfield","text":"class AdjacencyField ( Field [ torch . Tensor ]): | def __init__ ( | self , | indices : List [ Tuple [ int , int ]], | sequence_field : SequenceField , | labels : List [ str ] = None , | label_namespace : str = \"labels\" , | padding_value : int = - 1 | ) -> None A AdjacencyField defines directed adjacency relations between elements in a SequenceField . Because it's a labeling of some other field, we take that field as input here and use it to determine our padding and other things. This field will get converted into an array of shape (sequence_field_length, sequence_field_length), where the (i, j)th array element is either a binary flag indicating there is an edge from i to j, or an integer label k, indicating there is a label from i to j of type k. Parameters indices : List[Tuple[int, int]] sequence_field : SequenceField A field containing the sequence that this AdjacencyField is labeling. Most often, this is a TextField , for tagging edge relations between tokens in a sentence. labels : List[str] , optional (default = None ) Optional labels for the edges of the adjacency matrix. label_namespace : str , optional (default = 'labels' ) The namespace to use for converting tag strings into integers. We convert tag strings to integers for you, and this parameter tells the Vocabulary object which mapping from strings to integers to use (so that \"O\" as a tag doesn't get the same id as \"O\" as a word). padding_value : int , optional (default = -1 ) The value to use as padding.","title":"AdjacencyField"},{"location":"api/data/fields/adjacency_field/#count_vocab_items","text":"class AdjacencyField ( Field [ torch . Tensor ]): | ... | @overrides | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]])","title":"count_vocab_items"},{"location":"api/data/fields/adjacency_field/#index","text":"class AdjacencyField ( Field [ torch . Tensor ]): | ... | @overrides | def index ( self , vocab : Vocabulary )","title":"index"},{"location":"api/data/fields/adjacency_field/#get_padding_lengths","text":"class AdjacencyField ( Field [ torch . Tensor ]): | ... | @overrides | def get_padding_lengths ( self ) -> Dict [ str , int ]","title":"get_padding_lengths"},{"location":"api/data/fields/adjacency_field/#as_tensor","text":"class AdjacencyField ( Field [ torch . Tensor ]): | ... | @overrides | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor","title":"as_tensor"},{"location":"api/data/fields/adjacency_field/#empty_field","text":"class AdjacencyField ( Field [ torch . Tensor ]): | ... | @overrides | def empty_field ( self ) -> \"AdjacencyField\" The empty_list here is needed for mypy","title":"empty_field"},{"location":"api/data/fields/array_field/","text":"[ allennlp .data .fields .array_field ] ArrayField # class ArrayField ( Field [ numpy . ndarray ]): | def __init__ ( | self , | array : numpy . ndarray , | padding_value : int = 0 , | dtype : numpy . dtype = numpy . float32 | ) -> None A class representing an array, which could have arbitrary dimensions. A batch of these arrays are padded to the max dimension length in the batch for each dimension. get_padding_lengths # class ArrayField ( Field [ numpy . ndarray ]): | ... | @overrides | def get_padding_lengths ( self ) -> Dict [ str , int ] as_tensor # class ArrayField ( Field [ numpy . ndarray ]): | ... | @overrides | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor empty_field # class ArrayField ( Field [ numpy . ndarray ]): | ... | @overrides | def empty_field ( self ) Pass the padding_value, so that any outer field, e.g., ListField[ArrayField] uses the same padding_value in the padded ArrayFields","title":"array_field"},{"location":"api/data/fields/array_field/#arrayfield","text":"class ArrayField ( Field [ numpy . ndarray ]): | def __init__ ( | self , | array : numpy . ndarray , | padding_value : int = 0 , | dtype : numpy . dtype = numpy . float32 | ) -> None A class representing an array, which could have arbitrary dimensions. A batch of these arrays are padded to the max dimension length in the batch for each dimension.","title":"ArrayField"},{"location":"api/data/fields/array_field/#get_padding_lengths","text":"class ArrayField ( Field [ numpy . ndarray ]): | ... | @overrides | def get_padding_lengths ( self ) -> Dict [ str , int ]","title":"get_padding_lengths"},{"location":"api/data/fields/array_field/#as_tensor","text":"class ArrayField ( Field [ numpy . ndarray ]): | ... | @overrides | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor","title":"as_tensor"},{"location":"api/data/fields/array_field/#empty_field","text":"class ArrayField ( Field [ numpy . ndarray ]): | ... | @overrides | def empty_field ( self ) Pass the padding_value, so that any outer field, e.g., ListField[ArrayField] uses the same padding_value in the padded ArrayFields","title":"empty_field"},{"location":"api/data/fields/field/","text":"[ allennlp .data .fields .field ] DataArray # DataArray = TypeVar ( \"DataArray\" , torch . Tensor , Dict [ str , torch . Tensor ], Dict [ str , Dict [ str , torch . Tensor ]] ... Field # class Field ( Generic [ DataArray ]) A Field is some piece of a data instance that ends up as an tensor in a model (either as an input or an output). Data instances are just collections of fields. Fields go through up to two steps of processing: (1) tokenized fields are converted into token ids, (2) fields containing token ids (or any other numeric data) are padded (if necessary) and converted into tensors. The Field API has methods around both of these steps, though they may not be needed for some concrete Field classes - if your field doesn't have any strings that need indexing, you don't need to implement count_vocab_items or index . These methods pass by default. Once a vocabulary is computed and all fields are indexed, we will determine padding lengths, then intelligently batch together instances and pad them into actual tensors. count_vocab_items # class Field ( Generic [ DataArray ]): | ... | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]]) If there are strings in this field that need to be converted into integers through a Vocabulary , here is where we count them, to determine which tokens are in or out of the vocabulary. If your Field does not have any strings that need to be converted into indices, you do not need to implement this method. A note on this counter : because Fields can represent conceptually different things, we separate the vocabulary items by namespaces . This way, we can use a single shared mechanism to handle all mappings from strings to integers in all fields, while keeping words in a TextField from sharing the same ids with labels in a LabelField (e.g., \"entailment\" or \"contradiction\" are labels in an entailment task) Additionally, a single Field might want to use multiple namespaces - TextFields can be represented as a combination of word ids and character ids, and you don't want words and characters to share the same vocabulary - \"a\" as a word should get a different id from \"a\" as a character, and the vocabulary sizes of words and characters are very different. Because of this, the first key in the counter object is a namespace , like \"tokens\", \"token_characters\", \"tags\", or \"labels\", and the second key is the actual vocabulary item. index # class Field ( Generic [ DataArray ]): | ... | def index ( self , vocab : Vocabulary ) Given a Vocabulary , converts all strings in this field into (typically) integers. This modifies the Field object, it does not return anything. If your Field does not have any strings that need to be converted into indices, you do not need to implement this method. get_padding_lengths # class Field ( Generic [ DataArray ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ] If there are things in this field that need padding, note them here. In order to pad a batch of instance, we get all of the lengths from the batch, take the max, and pad everything to that length (or use a pre-specified maximum length). The return value is a dictionary mapping keys to lengths, like {'num_tokens': 13} . This is always called after index . as_tensor # class Field ( Generic [ DataArray ]): | ... | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> DataArray Given a set of specified padding lengths, actually pad the data in this field and return a torch Tensor (or a more complex data structure) of the correct shape. We also take a couple of parameters that are important when constructing torch Tensors. Parameters padding_lengths : Dict[str, int] This dictionary will have the same keys that were produced in get_padding_lengths . The values specify the lengths to use when padding each relevant dimension, aggregated across all instances in a batch. empty_field # class Field ( Generic [ DataArray ]): | ... | def empty_field ( self ) -> \"Field\" So that ListField can pad the number of fields in a list (e.g., the number of answer option TextFields ), we need a representation of an empty field of each type. This returns that. This will only ever be called when we're to the point of calling as_tensor , so you don't need to worry about get_padding_lengths , count_vocab_items , etc., being called on this empty field. We make this an instance method instead of a static method so that if there is any state in the Field, we can copy it over (e.g., the token indexers in TextField ). batch_tensors # class Field ( Generic [ DataArray ]): | ... | def batch_tensors ( self , tensor_list : List [ DataArray ]) -> DataArray Takes the output of Field.as_tensor() from a list of Instances and merges it into one batched tensor for this Field . The default implementation here in the base class handles cases where as_tensor returns a single torch tensor per instance. If your subclass returns something other than this, you need to override this method. This operation does not modify self , but in some cases we need the information contained in self in order to perform the batching, so this is an instance method, not a class method. duplicate # class Field ( Generic [ DataArray ]): | ... | def duplicate ( self )","title":"field"},{"location":"api/data/fields/field/#dataarray","text":"DataArray = TypeVar ( \"DataArray\" , torch . Tensor , Dict [ str , torch . Tensor ], Dict [ str , Dict [ str , torch . Tensor ]] ...","title":"DataArray"},{"location":"api/data/fields/field/#field","text":"class Field ( Generic [ DataArray ]) A Field is some piece of a data instance that ends up as an tensor in a model (either as an input or an output). Data instances are just collections of fields. Fields go through up to two steps of processing: (1) tokenized fields are converted into token ids, (2) fields containing token ids (or any other numeric data) are padded (if necessary) and converted into tensors. The Field API has methods around both of these steps, though they may not be needed for some concrete Field classes - if your field doesn't have any strings that need indexing, you don't need to implement count_vocab_items or index . These methods pass by default. Once a vocabulary is computed and all fields are indexed, we will determine padding lengths, then intelligently batch together instances and pad them into actual tensors.","title":"Field"},{"location":"api/data/fields/field/#count_vocab_items","text":"class Field ( Generic [ DataArray ]): | ... | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]]) If there are strings in this field that need to be converted into integers through a Vocabulary , here is where we count them, to determine which tokens are in or out of the vocabulary. If your Field does not have any strings that need to be converted into indices, you do not need to implement this method. A note on this counter : because Fields can represent conceptually different things, we separate the vocabulary items by namespaces . This way, we can use a single shared mechanism to handle all mappings from strings to integers in all fields, while keeping words in a TextField from sharing the same ids with labels in a LabelField (e.g., \"entailment\" or \"contradiction\" are labels in an entailment task) Additionally, a single Field might want to use multiple namespaces - TextFields can be represented as a combination of word ids and character ids, and you don't want words and characters to share the same vocabulary - \"a\" as a word should get a different id from \"a\" as a character, and the vocabulary sizes of words and characters are very different. Because of this, the first key in the counter object is a namespace , like \"tokens\", \"token_characters\", \"tags\", or \"labels\", and the second key is the actual vocabulary item.","title":"count_vocab_items"},{"location":"api/data/fields/field/#index","text":"class Field ( Generic [ DataArray ]): | ... | def index ( self , vocab : Vocabulary ) Given a Vocabulary , converts all strings in this field into (typically) integers. This modifies the Field object, it does not return anything. If your Field does not have any strings that need to be converted into indices, you do not need to implement this method.","title":"index"},{"location":"api/data/fields/field/#get_padding_lengths","text":"class Field ( Generic [ DataArray ]): | ... | def get_padding_lengths ( self ) -> Dict [ str , int ] If there are things in this field that need padding, note them here. In order to pad a batch of instance, we get all of the lengths from the batch, take the max, and pad everything to that length (or use a pre-specified maximum length). The return value is a dictionary mapping keys to lengths, like {'num_tokens': 13} . This is always called after index .","title":"get_padding_lengths"},{"location":"api/data/fields/field/#as_tensor","text":"class Field ( Generic [ DataArray ]): | ... | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> DataArray Given a set of specified padding lengths, actually pad the data in this field and return a torch Tensor (or a more complex data structure) of the correct shape. We also take a couple of parameters that are important when constructing torch Tensors. Parameters padding_lengths : Dict[str, int] This dictionary will have the same keys that were produced in get_padding_lengths . The values specify the lengths to use when padding each relevant dimension, aggregated across all instances in a batch.","title":"as_tensor"},{"location":"api/data/fields/field/#empty_field","text":"class Field ( Generic [ DataArray ]): | ... | def empty_field ( self ) -> \"Field\" So that ListField can pad the number of fields in a list (e.g., the number of answer option TextFields ), we need a representation of an empty field of each type. This returns that. This will only ever be called when we're to the point of calling as_tensor , so you don't need to worry about get_padding_lengths , count_vocab_items , etc., being called on this empty field. We make this an instance method instead of a static method so that if there is any state in the Field, we can copy it over (e.g., the token indexers in TextField ).","title":"empty_field"},{"location":"api/data/fields/field/#batch_tensors","text":"class Field ( Generic [ DataArray ]): | ... | def batch_tensors ( self , tensor_list : List [ DataArray ]) -> DataArray Takes the output of Field.as_tensor() from a list of Instances and merges it into one batched tensor for this Field . The default implementation here in the base class handles cases where as_tensor returns a single torch tensor per instance. If your subclass returns something other than this, you need to override this method. This operation does not modify self , but in some cases we need the information contained in self in order to perform the batching, so this is an instance method, not a class method.","title":"batch_tensors"},{"location":"api/data/fields/field/#duplicate","text":"class Field ( Generic [ DataArray ]): | ... | def duplicate ( self )","title":"duplicate"},{"location":"api/data/fields/flag_field/","text":"[ allennlp .data .fields .flag_field ] FlagField # class FlagField ( Field [ Any ]): | def __init__ ( self , flag_value : Any ) -> None A class representing a flag, which must be constant across all instances in a batch. This will be passed to a forward method as a single value of whatever type you pass in. get_padding_lengths # class FlagField ( Field [ Any ]): | ... | @overrides | def get_padding_lengths ( self ) -> Dict [ str , int ] as_tensor # class FlagField ( Field [ Any ]): | ... | @overrides | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> Any empty_field # class FlagField ( Field [ Any ]): | ... | @overrides | def empty_field ( self ) Because this has to be constant across all instances in a batch, we need to keep the same value. batch_tensors # class FlagField ( Field [ Any ]): | ... | @overrides | def batch_tensors ( self , tensor_list : List [ Any ]) -> Any","title":"flag_field"},{"location":"api/data/fields/flag_field/#flagfield","text":"class FlagField ( Field [ Any ]): | def __init__ ( self , flag_value : Any ) -> None A class representing a flag, which must be constant across all instances in a batch. This will be passed to a forward method as a single value of whatever type you pass in.","title":"FlagField"},{"location":"api/data/fields/flag_field/#get_padding_lengths","text":"class FlagField ( Field [ Any ]): | ... | @overrides | def get_padding_lengths ( self ) -> Dict [ str , int ]","title":"get_padding_lengths"},{"location":"api/data/fields/flag_field/#as_tensor","text":"class FlagField ( Field [ Any ]): | ... | @overrides | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> Any","title":"as_tensor"},{"location":"api/data/fields/flag_field/#empty_field","text":"class FlagField ( Field [ Any ]): | ... | @overrides | def empty_field ( self ) Because this has to be constant across all instances in a batch, we need to keep the same value.","title":"empty_field"},{"location":"api/data/fields/flag_field/#batch_tensors","text":"class FlagField ( Field [ Any ]): | ... | @overrides | def batch_tensors ( self , tensor_list : List [ Any ]) -> Any","title":"batch_tensors"},{"location":"api/data/fields/index_field/","text":"[ allennlp .data .fields .index_field ] IndexField # class IndexField ( Field [ torch . Tensor ]): | def __init__ ( self , index : int , sequence_field : SequenceField ) -> None An IndexField is an index into a SequenceField , as might be used for representing a correct answer option in a list, or a span begin and span end position in a passage, for example. Because it's an index into a SequenceField , we take one of those as input and use it to compute padding lengths. Parameters index : int The index of the answer in the SequenceField . This is typically the \"correct answer\" in some classification decision over the sequence, like where an answer span starts in SQuAD, or which answer option is correct in a multiple choice question. A value of -1 means there is no label, which can be used for padding or other purposes. sequence_field : SequenceField A field containing the sequence that this IndexField is a pointer into. get_padding_lengths # class IndexField ( Field [ torch . Tensor ]): | ... | @overrides | def get_padding_lengths ( self ) -> Dict [ str , int ] as_tensor # class IndexField ( Field [ torch . Tensor ]): | ... | @overrides | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor empty_field # class IndexField ( Field [ torch . Tensor ]): | ... | @overrides | def empty_field ( self )","title":"index_field"},{"location":"api/data/fields/index_field/#indexfield","text":"class IndexField ( Field [ torch . Tensor ]): | def __init__ ( self , index : int , sequence_field : SequenceField ) -> None An IndexField is an index into a SequenceField , as might be used for representing a correct answer option in a list, or a span begin and span end position in a passage, for example. Because it's an index into a SequenceField , we take one of those as input and use it to compute padding lengths. Parameters index : int The index of the answer in the SequenceField . This is typically the \"correct answer\" in some classification decision over the sequence, like where an answer span starts in SQuAD, or which answer option is correct in a multiple choice question. A value of -1 means there is no label, which can be used for padding or other purposes. sequence_field : SequenceField A field containing the sequence that this IndexField is a pointer into.","title":"IndexField"},{"location":"api/data/fields/index_field/#get_padding_lengths","text":"class IndexField ( Field [ torch . Tensor ]): | ... | @overrides | def get_padding_lengths ( self ) -> Dict [ str , int ]","title":"get_padding_lengths"},{"location":"api/data/fields/index_field/#as_tensor","text":"class IndexField ( Field [ torch . Tensor ]): | ... | @overrides | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor","title":"as_tensor"},{"location":"api/data/fields/index_field/#empty_field","text":"class IndexField ( Field [ torch . Tensor ]): | ... | @overrides | def empty_field ( self )","title":"empty_field"},{"location":"api/data/fields/label_field/","text":"[ allennlp .data .fields .label_field ] LabelField # class LabelField ( Field [ torch . Tensor ]): | def __init__ ( | self , | label : Union [ str , int ], | label_namespace : str = \"labels\" , | skip_indexing : bool = False | ) -> None A LabelField is a categorical label of some kind, where the labels are either strings of text or 0-indexed integers (if you wish to skip indexing by passing skip_indexing=True). If the labels need indexing, we will use a Vocabulary to convert the string labels into integers. This field will get converted into an integer index representing the class label. Parameters label : Union[str, int] label_namespace : str , optional (default = \"labels\" ) The namespace to use for converting label strings into integers. We map label strings to integers for you (e.g., \"entailment\" and \"contradiction\" get converted to 0, 1, ...), and this namespace tells the Vocabulary object which mapping from strings to integers to use (so \"entailment\" as a label doesn't get the same integer id as \"entailment\" as a word). If you have multiple different label fields in your data, you should make sure you use different namespaces for each one, always using the suffix \"labels\" (e.g., \"passage_labels\" and \"question_labels\"). skip_indexing : bool , optional (default = False ) If your labels are 0-indexed integers, you can pass in this flag, and we'll skip the indexing step. If this is False and your labels are not strings, this throws a ConfigurationError . count_vocab_items # class LabelField ( Field [ torch . Tensor ]): | ... | @overrides | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]]) index # class LabelField ( Field [ torch . Tensor ]): | ... | @overrides | def index ( self , vocab : Vocabulary ) get_padding_lengths # class LabelField ( Field [ torch . Tensor ]): | ... | @overrides | def get_padding_lengths ( self ) -> Dict [ str , int ] as_tensor # class LabelField ( Field [ torch . Tensor ]): | ... | @overrides | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor empty_field # class LabelField ( Field [ torch . Tensor ]): | ... | @overrides | def empty_field ( self )","title":"label_field"},{"location":"api/data/fields/label_field/#labelfield","text":"class LabelField ( Field [ torch . Tensor ]): | def __init__ ( | self , | label : Union [ str , int ], | label_namespace : str = \"labels\" , | skip_indexing : bool = False | ) -> None A LabelField is a categorical label of some kind, where the labels are either strings of text or 0-indexed integers (if you wish to skip indexing by passing skip_indexing=True). If the labels need indexing, we will use a Vocabulary to convert the string labels into integers. This field will get converted into an integer index representing the class label. Parameters label : Union[str, int] label_namespace : str , optional (default = \"labels\" ) The namespace to use for converting label strings into integers. We map label strings to integers for you (e.g., \"entailment\" and \"contradiction\" get converted to 0, 1, ...), and this namespace tells the Vocabulary object which mapping from strings to integers to use (so \"entailment\" as a label doesn't get the same integer id as \"entailment\" as a word). If you have multiple different label fields in your data, you should make sure you use different namespaces for each one, always using the suffix \"labels\" (e.g., \"passage_labels\" and \"question_labels\"). skip_indexing : bool , optional (default = False ) If your labels are 0-indexed integers, you can pass in this flag, and we'll skip the indexing step. If this is False and your labels are not strings, this throws a ConfigurationError .","title":"LabelField"},{"location":"api/data/fields/label_field/#count_vocab_items","text":"class LabelField ( Field [ torch . Tensor ]): | ... | @overrides | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]])","title":"count_vocab_items"},{"location":"api/data/fields/label_field/#index","text":"class LabelField ( Field [ torch . Tensor ]): | ... | @overrides | def index ( self , vocab : Vocabulary )","title":"index"},{"location":"api/data/fields/label_field/#get_padding_lengths","text":"class LabelField ( Field [ torch . Tensor ]): | ... | @overrides | def get_padding_lengths ( self ) -> Dict [ str , int ]","title":"get_padding_lengths"},{"location":"api/data/fields/label_field/#as_tensor","text":"class LabelField ( Field [ torch . Tensor ]): | ... | @overrides | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor","title":"as_tensor"},{"location":"api/data/fields/label_field/#empty_field","text":"class LabelField ( Field [ torch . Tensor ]): | ... | @overrides | def empty_field ( self )","title":"empty_field"},{"location":"api/data/fields/list_field/","text":"[ allennlp .data .fields .list_field ] ListField # class ListField ( SequenceField [ DataArray ]): | def __init__ ( self , field_list : List [ Field ]) -> None A ListField is a list of other fields. You would use this to represent, e.g., a list of answer options that are themselves TextFields . This field will get converted into a tensor that has one more mode than the items in the list. If this is a list of TextFields that have shape (num_words, num_characters), this ListField will output a tensor of shape (num_sentences, num_words, num_characters). Parameters field_list : List[Field] A list of Field objects to be concatenated into a single input tensor. All of the contained Field objects must be of the same type. count_vocab_items # class ListField ( SequenceField [ DataArray ]): | ... | @overrides | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]]) index # class ListField ( SequenceField [ DataArray ]): | ... | @overrides | def index ( self , vocab : Vocabulary ) get_padding_lengths # class ListField ( SequenceField [ DataArray ]): | ... | @overrides | def get_padding_lengths ( self ) -> Dict [ str , int ] sequence_length # class ListField ( SequenceField [ DataArray ]): | ... | @overrides | def sequence_length ( self ) -> int as_tensor # class ListField ( SequenceField [ DataArray ]): | ... | @overrides | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> DataArray empty_field # class ListField ( SequenceField [ DataArray ]): | ... | @overrides | def empty_field ( self ) Our \"empty\" list field will actually have a single field in the list, so that we can correctly construct nested lists. For example, if we have a type that is ListField[ListField[LabelField]] , we need the top-level ListField to know to construct a ListField[LabelField] when it's padding, and the nested ListField needs to know that it's empty objects are LabelFields . Having an \"empty\" list actually have length one makes this all work out, and we'll always be padding to at least length 1, anyway. batch_tensors # class ListField ( SequenceField [ DataArray ]): | ... | @overrides | def batch_tensors ( self , tensor_list : List [ DataArray ]) -> DataArray We defer to the class we're wrapping in a list to handle the batching.","title":"list_field"},{"location":"api/data/fields/list_field/#listfield","text":"class ListField ( SequenceField [ DataArray ]): | def __init__ ( self , field_list : List [ Field ]) -> None A ListField is a list of other fields. You would use this to represent, e.g., a list of answer options that are themselves TextFields . This field will get converted into a tensor that has one more mode than the items in the list. If this is a list of TextFields that have shape (num_words, num_characters), this ListField will output a tensor of shape (num_sentences, num_words, num_characters). Parameters field_list : List[Field] A list of Field objects to be concatenated into a single input tensor. All of the contained Field objects must be of the same type.","title":"ListField"},{"location":"api/data/fields/list_field/#count_vocab_items","text":"class ListField ( SequenceField [ DataArray ]): | ... | @overrides | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]])","title":"count_vocab_items"},{"location":"api/data/fields/list_field/#index","text":"class ListField ( SequenceField [ DataArray ]): | ... | @overrides | def index ( self , vocab : Vocabulary )","title":"index"},{"location":"api/data/fields/list_field/#get_padding_lengths","text":"class ListField ( SequenceField [ DataArray ]): | ... | @overrides | def get_padding_lengths ( self ) -> Dict [ str , int ]","title":"get_padding_lengths"},{"location":"api/data/fields/list_field/#sequence_length","text":"class ListField ( SequenceField [ DataArray ]): | ... | @overrides | def sequence_length ( self ) -> int","title":"sequence_length"},{"location":"api/data/fields/list_field/#as_tensor","text":"class ListField ( SequenceField [ DataArray ]): | ... | @overrides | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> DataArray","title":"as_tensor"},{"location":"api/data/fields/list_field/#empty_field","text":"class ListField ( SequenceField [ DataArray ]): | ... | @overrides | def empty_field ( self ) Our \"empty\" list field will actually have a single field in the list, so that we can correctly construct nested lists. For example, if we have a type that is ListField[ListField[LabelField]] , we need the top-level ListField to know to construct a ListField[LabelField] when it's padding, and the nested ListField needs to know that it's empty objects are LabelFields . Having an \"empty\" list actually have length one makes this all work out, and we'll always be padding to at least length 1, anyway.","title":"empty_field"},{"location":"api/data/fields/list_field/#batch_tensors","text":"class ListField ( SequenceField [ DataArray ]): | ... | @overrides | def batch_tensors ( self , tensor_list : List [ DataArray ]) -> DataArray We defer to the class we're wrapping in a list to handle the batching.","title":"batch_tensors"},{"location":"api/data/fields/metadata_field/","text":"[ allennlp .data .fields .metadata_field ] MetadataField # class MetadataField ( Field [ DataArray ], Mapping [ str , Any ]): | def __init__ ( self , metadata : Any ) -> None A MetadataField is a Field that does not get converted into tensors. It just carries side information that might be needed later on, for computing some third-party metric, or outputting debugging information, or whatever else you need. We use this in the BiDAF model, for instance, to keep track of question IDs and passage token offsets, so we can more easily use the official evaluation script to compute metrics. We don't try to do any kind of smart combination of this field for batched input - when you use this Field in a model, you'll get a list of metadata objects, one for each instance in the batch. Parameters metadata : Any Some object containing the metadata that you want to store. It's likely that you'll want this to be a dictionary, but it could be anything you want. get_padding_lengths # class MetadataField ( Field [ DataArray ], Mapping [ str , Any ]): | ... | @overrides | def get_padding_lengths ( self ) -> Dict [ str , int ] as_tensor # class MetadataField ( Field [ DataArray ], Mapping [ str , Any ]): | ... | @overrides | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> DataArray empty_field # class MetadataField ( Field [ DataArray ], Mapping [ str , Any ]): | ... | @overrides | def empty_field ( self ) -> \"MetadataField\" batch_tensors # class MetadataField ( Field [ DataArray ], Mapping [ str , Any ]): | ... | @overrides | def batch_tensors ( | self , | tensor_list : List [ DataArray ] | ) -> List [ DataArray ]","title":"metadata_field"},{"location":"api/data/fields/metadata_field/#metadatafield","text":"class MetadataField ( Field [ DataArray ], Mapping [ str , Any ]): | def __init__ ( self , metadata : Any ) -> None A MetadataField is a Field that does not get converted into tensors. It just carries side information that might be needed later on, for computing some third-party metric, or outputting debugging information, or whatever else you need. We use this in the BiDAF model, for instance, to keep track of question IDs and passage token offsets, so we can more easily use the official evaluation script to compute metrics. We don't try to do any kind of smart combination of this field for batched input - when you use this Field in a model, you'll get a list of metadata objects, one for each instance in the batch. Parameters metadata : Any Some object containing the metadata that you want to store. It's likely that you'll want this to be a dictionary, but it could be anything you want.","title":"MetadataField"},{"location":"api/data/fields/metadata_field/#get_padding_lengths","text":"class MetadataField ( Field [ DataArray ], Mapping [ str , Any ]): | ... | @overrides | def get_padding_lengths ( self ) -> Dict [ str , int ]","title":"get_padding_lengths"},{"location":"api/data/fields/metadata_field/#as_tensor","text":"class MetadataField ( Field [ DataArray ], Mapping [ str , Any ]): | ... | @overrides | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> DataArray","title":"as_tensor"},{"location":"api/data/fields/metadata_field/#empty_field","text":"class MetadataField ( Field [ DataArray ], Mapping [ str , Any ]): | ... | @overrides | def empty_field ( self ) -> \"MetadataField\"","title":"empty_field"},{"location":"api/data/fields/metadata_field/#batch_tensors","text":"class MetadataField ( Field [ DataArray ], Mapping [ str , Any ]): | ... | @overrides | def batch_tensors ( | self , | tensor_list : List [ DataArray ] | ) -> List [ DataArray ]","title":"batch_tensors"},{"location":"api/data/fields/multilabel_field/","text":"[ allennlp .data .fields .multilabel_field ] MultiLabelField # class MultiLabelField ( Field [ torch . Tensor ]): | def __init__ ( | self , | labels : Sequence [ Union [ str , int ]], | label_namespace : str = \"labels\" , | skip_indexing : bool = False , | num_labels : Optional [ int ] = None | ) -> None A MultiLabelField is an extension of the LabelField that allows for multiple labels. It is particularly useful in multi-label classification where more than one label can be correct. As with the LabelField , labels are either strings of text or 0-indexed integers (if you wish to skip indexing by passing skip_indexing=True). If the labels need indexing, we will use a Vocabulary to convert the string labels into integers. This field will get converted into a vector of length equal to the vocabulary size with one hot encoding for the labels (all zeros, and ones for the labels). Parameters labels : Sequence[Union[str, int]] label_namespace : str , optional (default = \"labels\" ) The namespace to use for converting label strings into integers. We map label strings to integers for you (e.g., \"entailment\" and \"contradiction\" get converted to 0, 1, ...), and this namespace tells the Vocabulary object which mapping from strings to integers to use (so \"entailment\" as a label doesn't get the same integer id as \"entailment\" as a word). If you have multiple different label fields in your data, you should make sure you use different namespaces for each one, always using the suffix \"labels\" (e.g., \"passage_labels\" and \"question_labels\"). skip_indexing : bool , optional (default = False ) If your labels are 0-indexed integers, you can pass in this flag, and we'll skip the indexing step. If this is False and your labels are not strings, this throws a ConfigurationError . num_labels : int , optional (default = None ) If skip_indexing=True , the total number of possible labels should be provided, which is required to decide the size of the output tensor. num_labels should equal largest label id + 1. If skip_indexing=False , num_labels is not required. count_vocab_items # class MultiLabelField ( Field [ torch . Tensor ]): | ... | @overrides | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]]) index # class MultiLabelField ( Field [ torch . Tensor ]): | ... | @overrides | def index ( self , vocab : Vocabulary ) get_padding_lengths # class MultiLabelField ( Field [ torch . Tensor ]): | ... | @overrides | def get_padding_lengths ( self ) -> Dict [ str , int ] as_tensor # class MultiLabelField ( Field [ torch . Tensor ]): | ... | @overrides | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor empty_field # class MultiLabelField ( Field [ torch . Tensor ]): | ... | @overrides | def empty_field ( self )","title":"multilabel_field"},{"location":"api/data/fields/multilabel_field/#multilabelfield","text":"class MultiLabelField ( Field [ torch . Tensor ]): | def __init__ ( | self , | labels : Sequence [ Union [ str , int ]], | label_namespace : str = \"labels\" , | skip_indexing : bool = False , | num_labels : Optional [ int ] = None | ) -> None A MultiLabelField is an extension of the LabelField that allows for multiple labels. It is particularly useful in multi-label classification where more than one label can be correct. As with the LabelField , labels are either strings of text or 0-indexed integers (if you wish to skip indexing by passing skip_indexing=True). If the labels need indexing, we will use a Vocabulary to convert the string labels into integers. This field will get converted into a vector of length equal to the vocabulary size with one hot encoding for the labels (all zeros, and ones for the labels). Parameters labels : Sequence[Union[str, int]] label_namespace : str , optional (default = \"labels\" ) The namespace to use for converting label strings into integers. We map label strings to integers for you (e.g., \"entailment\" and \"contradiction\" get converted to 0, 1, ...), and this namespace tells the Vocabulary object which mapping from strings to integers to use (so \"entailment\" as a label doesn't get the same integer id as \"entailment\" as a word). If you have multiple different label fields in your data, you should make sure you use different namespaces for each one, always using the suffix \"labels\" (e.g., \"passage_labels\" and \"question_labels\"). skip_indexing : bool , optional (default = False ) If your labels are 0-indexed integers, you can pass in this flag, and we'll skip the indexing step. If this is False and your labels are not strings, this throws a ConfigurationError . num_labels : int , optional (default = None ) If skip_indexing=True , the total number of possible labels should be provided, which is required to decide the size of the output tensor. num_labels should equal largest label id + 1. If skip_indexing=False , num_labels is not required.","title":"MultiLabelField"},{"location":"api/data/fields/multilabel_field/#count_vocab_items","text":"class MultiLabelField ( Field [ torch . Tensor ]): | ... | @overrides | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]])","title":"count_vocab_items"},{"location":"api/data/fields/multilabel_field/#index","text":"class MultiLabelField ( Field [ torch . Tensor ]): | ... | @overrides | def index ( self , vocab : Vocabulary )","title":"index"},{"location":"api/data/fields/multilabel_field/#get_padding_lengths","text":"class MultiLabelField ( Field [ torch . Tensor ]): | ... | @overrides | def get_padding_lengths ( self ) -> Dict [ str , int ]","title":"get_padding_lengths"},{"location":"api/data/fields/multilabel_field/#as_tensor","text":"class MultiLabelField ( Field [ torch . Tensor ]): | ... | @overrides | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor","title":"as_tensor"},{"location":"api/data/fields/multilabel_field/#empty_field","text":"class MultiLabelField ( Field [ torch . Tensor ]): | ... | @overrides | def empty_field ( self )","title":"empty_field"},{"location":"api/data/fields/namespace_swapping_field/","text":"[ allennlp .data .fields .namespace_swapping_field ] NamespaceSwappingField # class NamespaceSwappingField ( Field [ torch . Tensor ]): | def __init__ ( | self , | source_tokens : List [ Token ], | target_namespace : str | ) -> None A NamespaceSwappingField is used to map tokens in one namespace to tokens in another namespace. It is used by seq2seq models with a copy mechanism that copies tokens from the source sentence into the target sentence. Parameters source_tokens : List[Token] The tokens from the source sentence. target_namespace : str The namespace that the tokens from the source sentence will be mapped to. index # class NamespaceSwappingField ( Field [ torch . Tensor ]): | ... | @overrides | def index ( self , vocab : Vocabulary ) get_padding_lengths # class NamespaceSwappingField ( Field [ torch . Tensor ]): | ... | @overrides | def get_padding_lengths ( self ) -> Dict [ str , int ] as_tensor # class NamespaceSwappingField ( Field [ torch . Tensor ]): | ... | @overrides | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor empty_field # class NamespaceSwappingField ( Field [ torch . Tensor ]): | ... | @overrides | def empty_field ( self ) -> \"NamespaceSwappingField\"","title":"namespace_swapping_field"},{"location":"api/data/fields/namespace_swapping_field/#namespaceswappingfield","text":"class NamespaceSwappingField ( Field [ torch . Tensor ]): | def __init__ ( | self , | source_tokens : List [ Token ], | target_namespace : str | ) -> None A NamespaceSwappingField is used to map tokens in one namespace to tokens in another namespace. It is used by seq2seq models with a copy mechanism that copies tokens from the source sentence into the target sentence. Parameters source_tokens : List[Token] The tokens from the source sentence. target_namespace : str The namespace that the tokens from the source sentence will be mapped to.","title":"NamespaceSwappingField"},{"location":"api/data/fields/namespace_swapping_field/#index","text":"class NamespaceSwappingField ( Field [ torch . Tensor ]): | ... | @overrides | def index ( self , vocab : Vocabulary )","title":"index"},{"location":"api/data/fields/namespace_swapping_field/#get_padding_lengths","text":"class NamespaceSwappingField ( Field [ torch . Tensor ]): | ... | @overrides | def get_padding_lengths ( self ) -> Dict [ str , int ]","title":"get_padding_lengths"},{"location":"api/data/fields/namespace_swapping_field/#as_tensor","text":"class NamespaceSwappingField ( Field [ torch . Tensor ]): | ... | @overrides | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor","title":"as_tensor"},{"location":"api/data/fields/namespace_swapping_field/#empty_field","text":"class NamespaceSwappingField ( Field [ torch . Tensor ]): | ... | @overrides | def empty_field ( self ) -> \"NamespaceSwappingField\"","title":"empty_field"},{"location":"api/data/fields/sequence_field/","text":"[ allennlp .data .fields .sequence_field ] SequenceField # class SequenceField ( Field [ DataArray ]) A SequenceField represents a sequence of things. This class just adds a method onto Field : sequence_length . It exists so that SequenceLabelField , IndexField and other similar Fields can have a single type to require, with a consistent API, whether they are pointing to words in a TextField , items in a ListField , or something else. sequence_length # class SequenceField ( Field [ DataArray ]): | ... | def sequence_length ( self ) -> int How many elements are there in this sequence? empty_field # class SequenceField ( Field [ DataArray ]): | ... | def empty_field ( self ) -> \"SequenceField\"","title":"sequence_field"},{"location":"api/data/fields/sequence_field/#sequencefield","text":"class SequenceField ( Field [ DataArray ]) A SequenceField represents a sequence of things. This class just adds a method onto Field : sequence_length . It exists so that SequenceLabelField , IndexField and other similar Fields can have a single type to require, with a consistent API, whether they are pointing to words in a TextField , items in a ListField , or something else.","title":"SequenceField"},{"location":"api/data/fields/sequence_field/#sequence_length","text":"class SequenceField ( Field [ DataArray ]): | ... | def sequence_length ( self ) -> int How many elements are there in this sequence?","title":"sequence_length"},{"location":"api/data/fields/sequence_field/#empty_field","text":"class SequenceField ( Field [ DataArray ]): | ... | def empty_field ( self ) -> \"SequenceField\"","title":"empty_field"},{"location":"api/data/fields/sequence_label_field/","text":"[ allennlp .data .fields .sequence_label_field ] SequenceLabelField # class SequenceLabelField ( Field [ torch . Tensor ]): | def __init__ ( | self , | labels : Union [ List [ str ], List [ int ]], | sequence_field : SequenceField , | label_namespace : str = \"labels\" | ) -> None A SequenceLabelField assigns a categorical label to each element in a SequenceField . Because it's a labeling of some other field, we take that field as input here, and we use it to determine our padding and other things. This field will get converted into a list of integer class ids, representing the correct class for each element in the sequence. Parameters labels : Union[List[str], List[int]] A sequence of categorical labels, encoded as strings or integers. These could be POS tags like [NN, JJ, ...], BIO tags like [B-PERS, I-PERS, O, O, ...], or any other categorical tag sequence. If the labels are encoded as integers, they will not be indexed using a vocab. sequence_field : SequenceField A field containing the sequence that this SequenceLabelField is labeling. Most often, this is a TextField , for tagging individual tokens in a sentence. label_namespace : str , optional (default = 'labels' ) The namespace to use for converting tag strings into integers. We convert tag strings to integers for you, and this parameter tells the Vocabulary object which mapping from strings to integers to use (so that \"O\" as a tag doesn't get the same id as \"O\" as a word). count_vocab_items # class SequenceLabelField ( Field [ torch . Tensor ]): | ... | @overrides | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]]) index # class SequenceLabelField ( Field [ torch . Tensor ]): | ... | @overrides | def index ( self , vocab : Vocabulary ) get_padding_lengths # class SequenceLabelField ( Field [ torch . Tensor ]): | ... | @overrides | def get_padding_lengths ( self ) -> Dict [ str , int ] as_tensor # class SequenceLabelField ( Field [ torch . Tensor ]): | ... | @overrides | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor empty_field # class SequenceLabelField ( Field [ torch . Tensor ]): | ... | @overrides | def empty_field ( self ) -> \"SequenceLabelField\" The empty_list here is needed for mypy","title":"sequence_label_field"},{"location":"api/data/fields/sequence_label_field/#sequencelabelfield","text":"class SequenceLabelField ( Field [ torch . Tensor ]): | def __init__ ( | self , | labels : Union [ List [ str ], List [ int ]], | sequence_field : SequenceField , | label_namespace : str = \"labels\" | ) -> None A SequenceLabelField assigns a categorical label to each element in a SequenceField . Because it's a labeling of some other field, we take that field as input here, and we use it to determine our padding and other things. This field will get converted into a list of integer class ids, representing the correct class for each element in the sequence. Parameters labels : Union[List[str], List[int]] A sequence of categorical labels, encoded as strings or integers. These could be POS tags like [NN, JJ, ...], BIO tags like [B-PERS, I-PERS, O, O, ...], or any other categorical tag sequence. If the labels are encoded as integers, they will not be indexed using a vocab. sequence_field : SequenceField A field containing the sequence that this SequenceLabelField is labeling. Most often, this is a TextField , for tagging individual tokens in a sentence. label_namespace : str , optional (default = 'labels' ) The namespace to use for converting tag strings into integers. We convert tag strings to integers for you, and this parameter tells the Vocabulary object which mapping from strings to integers to use (so that \"O\" as a tag doesn't get the same id as \"O\" as a word).","title":"SequenceLabelField"},{"location":"api/data/fields/sequence_label_field/#count_vocab_items","text":"class SequenceLabelField ( Field [ torch . Tensor ]): | ... | @overrides | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]])","title":"count_vocab_items"},{"location":"api/data/fields/sequence_label_field/#index","text":"class SequenceLabelField ( Field [ torch . Tensor ]): | ... | @overrides | def index ( self , vocab : Vocabulary )","title":"index"},{"location":"api/data/fields/sequence_label_field/#get_padding_lengths","text":"class SequenceLabelField ( Field [ torch . Tensor ]): | ... | @overrides | def get_padding_lengths ( self ) -> Dict [ str , int ]","title":"get_padding_lengths"},{"location":"api/data/fields/sequence_label_field/#as_tensor","text":"class SequenceLabelField ( Field [ torch . Tensor ]): | ... | @overrides | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor","title":"as_tensor"},{"location":"api/data/fields/sequence_label_field/#empty_field","text":"class SequenceLabelField ( Field [ torch . Tensor ]): | ... | @overrides | def empty_field ( self ) -> \"SequenceLabelField\" The empty_list here is needed for mypy","title":"empty_field"},{"location":"api/data/fields/span_field/","text":"[ allennlp .data .fields .span_field ] SpanField # class SpanField ( Field [ torch . Tensor ]): | def __init__ ( | self , | span_start : int , | span_end : int , | sequence_field : SequenceField | ) -> None A SpanField is a pair of inclusive, zero-indexed (start, end) indices into a SequenceField , used to represent a span of text. Because it's a pair of indices into a SequenceField , we take one of those as input to make the span's dependence explicit and to validate that the span is well defined. Parameters span_start : int The index of the start of the span in the SequenceField . span_end : int The inclusive index of the end of the span in the SequenceField . sequence_field : SequenceField A field containing the sequence that this SpanField is a span inside. get_padding_lengths # class SpanField ( Field [ torch . Tensor ]): | ... | @overrides | def get_padding_lengths ( self ) -> Dict [ str , int ] as_tensor # class SpanField ( Field [ torch . Tensor ]): | ... | @overrides | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor empty_field # class SpanField ( Field [ torch . Tensor ]): | ... | @overrides | def empty_field ( self )","title":"span_field"},{"location":"api/data/fields/span_field/#spanfield","text":"class SpanField ( Field [ torch . Tensor ]): | def __init__ ( | self , | span_start : int , | span_end : int , | sequence_field : SequenceField | ) -> None A SpanField is a pair of inclusive, zero-indexed (start, end) indices into a SequenceField , used to represent a span of text. Because it's a pair of indices into a SequenceField , we take one of those as input to make the span's dependence explicit and to validate that the span is well defined. Parameters span_start : int The index of the start of the span in the SequenceField . span_end : int The inclusive index of the end of the span in the SequenceField . sequence_field : SequenceField A field containing the sequence that this SpanField is a span inside.","title":"SpanField"},{"location":"api/data/fields/span_field/#get_padding_lengths","text":"class SpanField ( Field [ torch . Tensor ]): | ... | @overrides | def get_padding_lengths ( self ) -> Dict [ str , int ]","title":"get_padding_lengths"},{"location":"api/data/fields/span_field/#as_tensor","text":"class SpanField ( Field [ torch . Tensor ]): | ... | @overrides | def as_tensor ( self , padding_lengths : Dict [ str , int ]) -> torch . Tensor","title":"as_tensor"},{"location":"api/data/fields/span_field/#empty_field","text":"class SpanField ( Field [ torch . Tensor ]): | ... | @overrides | def empty_field ( self )","title":"empty_field"},{"location":"api/data/fields/text_field/","text":"[ allennlp .data .fields .text_field ] A TextField represents a string of text, the kind that you might want to represent with standard word vectors, or pass through an LSTM. TextFieldTensors # TextFieldTensors = Dict [ str , Dict [ str , torch . Tensor ]] TextField # class TextField ( SequenceField [ TextFieldTensors ]): | def __init__ ( | self , | tokens : List [ Token ], | token_indexers : Dict [ str , TokenIndexer ] | ) -> None This Field represents a list of string tokens. Before constructing this object, you need to tokenize raw strings using a Tokenizer . Because string tokens can be represented as indexed arrays in a number of ways, we also take a dictionary of TokenIndexer objects that will be used to convert the tokens into indices. Each TokenIndexer could represent each token as a single ID, or a list of character IDs, or something else. This field will get converted into a dictionary of arrays, one for each TokenIndexer . A SingleIdTokenIndexer produces an array of shape (num_tokens,), while a TokenCharactersIndexer produces an array of shape (num_tokens, num_characters). count_vocab_items # class TextField ( SequenceField [ TextFieldTensors ]): | ... | @overrides | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]]) index # class TextField ( SequenceField [ TextFieldTensors ]): | ... | @overrides | def index ( self , vocab : Vocabulary ) get_padding_lengths # class TextField ( SequenceField [ TextFieldTensors ]): | ... | @overrides | def get_padding_lengths ( self ) -> Dict [ str , int ] The TextField has a list of Tokens , and each Token gets converted into arrays by (potentially) several TokenIndexers . This method gets the max length (over tokens) associated with each of these arrays. sequence_length # class TextField ( SequenceField [ TextFieldTensors ]): | ... | @overrides | def sequence_length ( self ) -> int as_tensor # class TextField ( SequenceField [ TextFieldTensors ]): | ... | @overrides | def as_tensor ( | self , | padding_lengths : Dict [ str , int ] | ) -> TextFieldTensors empty_field # class TextField ( SequenceField [ TextFieldTensors ]): | ... | @overrides | def empty_field ( self ) batch_tensors # class TextField ( SequenceField [ TextFieldTensors ]): | ... | @overrides | def batch_tensors ( | self , | tensor_list : List [ TextFieldTensors ] | ) -> TextFieldTensors This is creating a dict of {token_indexer_name: {token_indexer_outputs: batched_tensor}} for each token indexer used to index this field. duplicate # class TextField ( SequenceField [ TextFieldTensors ]): | ... | @overrides | def duplicate ( self ) Overrides the behavior of duplicate so that self._token_indexers won't actually be deep-copied. Not only would it be extremely inefficient to deep-copy the token indexers, but it also fails in many cases since some tokenizers (like those used in the 'transformers' lib) cannot actually be deep-copied.","title":"text_field"},{"location":"api/data/fields/text_field/#textfieldtensors","text":"TextFieldTensors = Dict [ str , Dict [ str , torch . Tensor ]]","title":"TextFieldTensors"},{"location":"api/data/fields/text_field/#textfield","text":"class TextField ( SequenceField [ TextFieldTensors ]): | def __init__ ( | self , | tokens : List [ Token ], | token_indexers : Dict [ str , TokenIndexer ] | ) -> None This Field represents a list of string tokens. Before constructing this object, you need to tokenize raw strings using a Tokenizer . Because string tokens can be represented as indexed arrays in a number of ways, we also take a dictionary of TokenIndexer objects that will be used to convert the tokens into indices. Each TokenIndexer could represent each token as a single ID, or a list of character IDs, or something else. This field will get converted into a dictionary of arrays, one for each TokenIndexer . A SingleIdTokenIndexer produces an array of shape (num_tokens,), while a TokenCharactersIndexer produces an array of shape (num_tokens, num_characters).","title":"TextField"},{"location":"api/data/fields/text_field/#count_vocab_items","text":"class TextField ( SequenceField [ TextFieldTensors ]): | ... | @overrides | def count_vocab_items ( self , counter : Dict [ str , Dict [ str , int ]])","title":"count_vocab_items"},{"location":"api/data/fields/text_field/#index","text":"class TextField ( SequenceField [ TextFieldTensors ]): | ... | @overrides | def index ( self , vocab : Vocabulary )","title":"index"},{"location":"api/data/fields/text_field/#get_padding_lengths","text":"class TextField ( SequenceField [ TextFieldTensors ]): | ... | @overrides | def get_padding_lengths ( self ) -> Dict [ str , int ] The TextField has a list of Tokens , and each Token gets converted into arrays by (potentially) several TokenIndexers . This method gets the max length (over tokens) associated with each of these arrays.","title":"get_padding_lengths"},{"location":"api/data/fields/text_field/#sequence_length","text":"class TextField ( SequenceField [ TextFieldTensors ]): | ... | @overrides | def sequence_length ( self ) -> int","title":"sequence_length"},{"location":"api/data/fields/text_field/#as_tensor","text":"class TextField ( SequenceField [ TextFieldTensors ]): | ... | @overrides | def as_tensor ( | self , | padding_lengths : Dict [ str , int ] | ) -> TextFieldTensors","title":"as_tensor"},{"location":"api/data/fields/text_field/#empty_field","text":"class TextField ( SequenceField [ TextFieldTensors ]): | ... | @overrides | def empty_field ( self )","title":"empty_field"},{"location":"api/data/fields/text_field/#batch_tensors","text":"class TextField ( SequenceField [ TextFieldTensors ]): | ... | @overrides | def batch_tensors ( | self , | tensor_list : List [ TextFieldTensors ] | ) -> TextFieldTensors This is creating a dict of {token_indexer_name: {token_indexer_outputs: batched_tensor}} for each token indexer used to index this field.","title":"batch_tensors"},{"location":"api/data/fields/text_field/#duplicate","text":"class TextField ( SequenceField [ TextFieldTensors ]): | ... | @overrides | def duplicate ( self ) Overrides the behavior of duplicate so that self._token_indexers won't actually be deep-copied. Not only would it be extremely inefficient to deep-copy the token indexers, but it also fails in many cases since some tokenizers (like those used in the 'transformers' lib) cannot actually be deep-copied.","title":"duplicate"},{"location":"api/data/samplers/bucket_batch_sampler/","text":"[ allennlp .data .samplers .bucket_batch_sampler ] add_noise_to_value # def add_noise_to_value ( value : int , noise_param : float ) BucketBatchSampler # class BucketBatchSampler ( BatchSampler ): | def __init__ ( | self , | data_source : data . Dataset , | batch_size : int , | sorting_keys : List [ str ] = None , | padding_noise : float = 0.1 , | drop_last : bool = False | ) An sampler which by default, argsorts batches with respect to the maximum input lengths per batch . You can provide a list of field names and padding keys (or pass none, in which case they will be inferred) which the dataset will be sorted by before doing this batching, causing inputs with similar length to be batched together, making computation more efficient (as less time is wasted on padded elements of the batch). Parameters data_source : data.Dataset The pytorch Dataset of allennlp Instances to bucket. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"batch_sampler\", it gets constructed separately. - batch_size : int The size of each batch of instances yielded when calling the dataloader. sorting_keys : List[str] , optional To bucket inputs into batches, we want to group the instances by padding length, so that we minimize the amount of padding necessary per batch. In order to do this, we need to know which fields need what type of padding, and in what order. Specifying the right keys for this is a bit cryptic, so if this is not given we try to auto-detect the right keys by iterating through a few instances upfront, reading all of the padding keys and seeing which one has the longest length. We use that one for padding. This should give reasonable results in most cases. Some cases where it might not be the right thing to do are when you have a ListField[TextField] , or when you have a really long, constant length ArrayField . When you need to specify this yourself, you can create an instance from your dataset and call Instance.get_padding_lengths() to see a list of all keys used in your data. You should give one or more of those as the sorting keys here. padding_noise : float , optional (default = .1 ) When sorting by padding length, we add a bit of noise to the lengths, so that the sorting isn't deterministic. This parameter determines how much noise we add, as a percentage of the actual padding value for each instance. drop_last : bool , optional (default = False ) If True , the sampler will drop the last batch if its size would be less than batch_size`.","title":"bucket_batch_sampler"},{"location":"api/data/samplers/bucket_batch_sampler/#add_noise_to_value","text":"def add_noise_to_value ( value : int , noise_param : float )","title":"add_noise_to_value"},{"location":"api/data/samplers/bucket_batch_sampler/#bucketbatchsampler","text":"class BucketBatchSampler ( BatchSampler ): | def __init__ ( | self , | data_source : data . Dataset , | batch_size : int , | sorting_keys : List [ str ] = None , | padding_noise : float = 0.1 , | drop_last : bool = False | ) An sampler which by default, argsorts batches with respect to the maximum input lengths per batch . You can provide a list of field names and padding keys (or pass none, in which case they will be inferred) which the dataset will be sorted by before doing this batching, causing inputs with similar length to be batched together, making computation more efficient (as less time is wasted on padded elements of the batch). Parameters data_source : data.Dataset The pytorch Dataset of allennlp Instances to bucket. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"batch_sampler\", it gets constructed separately. - batch_size : int The size of each batch of instances yielded when calling the dataloader. sorting_keys : List[str] , optional To bucket inputs into batches, we want to group the instances by padding length, so that we minimize the amount of padding necessary per batch. In order to do this, we need to know which fields need what type of padding, and in what order. Specifying the right keys for this is a bit cryptic, so if this is not given we try to auto-detect the right keys by iterating through a few instances upfront, reading all of the padding keys and seeing which one has the longest length. We use that one for padding. This should give reasonable results in most cases. Some cases where it might not be the right thing to do are when you have a ListField[TextField] , or when you have a really long, constant length ArrayField . When you need to specify this yourself, you can create an instance from your dataset and call Instance.get_padding_lengths() to see a list of all keys used in your data. You should give one or more of those as the sorting keys here. padding_noise : float , optional (default = .1 ) When sorting by padding length, we add a bit of noise to the lengths, so that the sorting isn't deterministic. This parameter determines how much noise we add, as a percentage of the actual padding value for each instance. drop_last : bool , optional (default = False ) If True , the sampler will drop the last batch if its size would be less than batch_size`.","title":"BucketBatchSampler"},{"location":"api/data/samplers/max_tokens_batch_sampler/","text":"[ allennlp .data .samplers .max_tokens_batch_sampler ] A # A = TypeVar ( \"A\" ) MaxTokensBatchSampler # class MaxTokensBatchSampler ( BucketBatchSampler ): | def __init__ ( | self , | data_source : data . Dataset , | max_tokens : Optional [ int ] = None , | sorting_keys : List [ str ] = None , | padding_noise : float = 0.1 | ) An sampler which by default, argsorts batches with respect to the maximum input lengths per batch . Batches are then created such that the number of tokens in a batch does not exceed the given maximum number of tokens. You can provide a list of field names and padding keys (or pass none, in which case they will be inferred) which the dataset will be sorted by before doing this batching, causing inputs with similar length to be batched together, making computation more efficient (as less time is wasted on padded elements of the batch). Parameters data_source : data.Dataset The pytorch Dataset of allennlp Instances to bucket. max_tokens : int The maximum number of tokens to include in a batch. sorting_keys : List[str] , optional To bucket inputs into batches, we want to group the instances by padding length, so that we minimize the amount of padding necessary per batch. In order to do this, we need to know which fields need what type of padding, and in what order. Specifying the right keys for this is a bit cryptic, so if this is not given we try to auto-detect the right keys by iterating through a few instances upfront, reading all of the padding keys and seeing which one has the longest length. We use that one for padding. This should give reasonable results in most cases. Some cases where it might not be the right thing to do are when you have a ListField[TextField] , or when you have a really long, constant length ArrayField . When you need to specify this yourself, you can create an instance from your dataset and call Instance.get_padding_lengths() to see a list of all keys used in your data. You should give one or more of those as the sorting keys here. padding_noise : float , optional (default = 0.1 ) When sorting by padding length, we add a bit of noise to the lengths, so that the sorting isn't deterministic. This parameter determines how much noise we add, as a percentage of the actual padding value for each instance.","title":"max_tokens_batch_sampler"},{"location":"api/data/samplers/max_tokens_batch_sampler/#a","text":"A = TypeVar ( \"A\" )","title":"A"},{"location":"api/data/samplers/max_tokens_batch_sampler/#maxtokensbatchsampler","text":"class MaxTokensBatchSampler ( BucketBatchSampler ): | def __init__ ( | self , | data_source : data . Dataset , | max_tokens : Optional [ int ] = None , | sorting_keys : List [ str ] = None , | padding_noise : float = 0.1 | ) An sampler which by default, argsorts batches with respect to the maximum input lengths per batch . Batches are then created such that the number of tokens in a batch does not exceed the given maximum number of tokens. You can provide a list of field names and padding keys (or pass none, in which case they will be inferred) which the dataset will be sorted by before doing this batching, causing inputs with similar length to be batched together, making computation more efficient (as less time is wasted on padded elements of the batch). Parameters data_source : data.Dataset The pytorch Dataset of allennlp Instances to bucket. max_tokens : int The maximum number of tokens to include in a batch. sorting_keys : List[str] , optional To bucket inputs into batches, we want to group the instances by padding length, so that we minimize the amount of padding necessary per batch. In order to do this, we need to know which fields need what type of padding, and in what order. Specifying the right keys for this is a bit cryptic, so if this is not given we try to auto-detect the right keys by iterating through a few instances upfront, reading all of the padding keys and seeing which one has the longest length. We use that one for padding. This should give reasonable results in most cases. Some cases where it might not be the right thing to do are when you have a ListField[TextField] , or when you have a really long, constant length ArrayField . When you need to specify this yourself, you can create an instance from your dataset and call Instance.get_padding_lengths() to see a list of all keys used in your data. You should give one or more of those as the sorting keys here. padding_noise : float , optional (default = 0.1 ) When sorting by padding length, we add a bit of noise to the lengths, so that the sorting isn't deterministic. This parameter determines how much noise we add, as a percentage of the actual padding value for each instance.","title":"MaxTokensBatchSampler"},{"location":"api/data/samplers/samplers/","text":"[ allennlp .data .samplers .samplers ] Sampler # class Sampler ( Registrable ) A copy of the pytorch Sampler which allows us to register it with Registrable. BatchSampler # class BatchSampler ( Registrable ) A copy of the pytorch BatchSampler which allows us to register it with Registrable. SequentialSampler # class SequentialSampler ( data . SequentialSampler , Sampler ): | def __init__ ( self , data_source : data . Dataset ) A registrable version of pytorch's SequentialSampler . Registered as a Sampler with name \"sequential\". In a typical AllenNLP configuration file, data_source parameter does not get an entry under the \"sampler\", it gets constructed separately. RandomSampler # class RandomSampler ( data . RandomSampler , Sampler ): | def __init__ ( | self , | data_source : data . Dataset , | replacement : bool = False , | num_samples : int = None | ) A registrable version of pytorch's RandomSampler . Samples elements randomly. If without replacement, then sample from a shuffled dataset. If with replacement, then user can specify num_samples to draw. Registered as a Sampler with name \"random\". Parameters data_source : Dataset The dataset to sample from. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"sampler\", it gets constructed separately. - replacement : bool , optional (default = False ) Samples are drawn with replacement if True . - num_samples : int , optional (default = len(dataset) ) The number of samples to draw. This argument is supposed to be specified only when replacement is True . SubsetRandomSampler # class SubsetRandomSampler ( data . SubsetRandomSampler , Sampler ): | def __init__ ( self , indices : List [ int ]) A registrable version of pytorch's SubsetRandomSampler . Samples elements randomly from a given list of indices, without replacement. Registered as a Sampler with name \"subset_random\". Parameters indices : List[int] a sequence of indices to sample from. WeightedRandomSampler # class WeightedRandomSampler ( data . WeightedRandomSampler , Sampler ): | def __init__ ( | self , | weights : List [ float ], | num_samples : int , | replacement : bool = True | ) A registrable version of pytorch's WeightedRandomSampler . Samples elements from [0,...,len(weights)-1] with given probabilities (weights). Registered as a Sampler with name \"weighted_random\". Parameters: weights : List[float] A sequence of weights, not necessary summing up to one. num_samples : int The number of samples to draw. replacement : bool If True , samples are drawn with replacement. If not, they are drawn without replacement, which means that when a sample index is drawn for a row, it cannot be drawn again for that row. Examples >>> list ( WeightedRandomSampler ([ 0.1 , 0.9 , 0.4 , 0.7 , 3.0 , 0.6 ], 5 , replacement = True )) [ 0 , 0 , 0 , 1 , 0 ] >>> list ( WeightedRandomSampler ([ 0.9 , 0.4 , 0.05 , 0.2 , 0.3 , 0.1 ], 5 , replacement = False )) [ 0 , 1 , 4 , 3 , 2 ] BasicBatchSampler # class BasicBatchSampler ( data . BatchSampler , BatchSampler ): | def __init__ ( | self , | sampler : Sampler , | batch_size : int , | drop_last : bool | ) A registrable version of pytorch's BatchSampler . Wraps another sampler to yield a mini-batch of indices. Registered as a BatchSampler with name \"basic\". Parameters sampler : Sampler The base sampler. batch_size : int The size of the batch. drop_last : bool If True , the sampler will drop the last batch if its size would be less than batch_size`. Examples >>> list ( BatchSampler ( SequentialSampler ( range ( 10 )), batch_size = 3 , drop_last = False )) [[ 0 , 1 , 2 ], [ 3 , 4 , 5 ], [ 6 , 7 , 8 ], [ 9 ]] >>> list ( BatchSampler ( SequentialSampler ( range ( 10 )), batch_size = 3 , drop_last = True )) [[ 0 , 1 , 2 ], [ 3 , 4 , 5 ], [ 6 , 7 , 8 ]]","title":"samplers"},{"location":"api/data/samplers/samplers/#sampler","text":"class Sampler ( Registrable ) A copy of the pytorch Sampler which allows us to register it with Registrable.","title":"Sampler"},{"location":"api/data/samplers/samplers/#batchsampler","text":"class BatchSampler ( Registrable ) A copy of the pytorch BatchSampler which allows us to register it with Registrable.","title":"BatchSampler"},{"location":"api/data/samplers/samplers/#sequentialsampler","text":"class SequentialSampler ( data . SequentialSampler , Sampler ): | def __init__ ( self , data_source : data . Dataset ) A registrable version of pytorch's SequentialSampler . Registered as a Sampler with name \"sequential\". In a typical AllenNLP configuration file, data_source parameter does not get an entry under the \"sampler\", it gets constructed separately.","title":"SequentialSampler"},{"location":"api/data/samplers/samplers/#randomsampler","text":"class RandomSampler ( data . RandomSampler , Sampler ): | def __init__ ( | self , | data_source : data . Dataset , | replacement : bool = False , | num_samples : int = None | ) A registrable version of pytorch's RandomSampler . Samples elements randomly. If without replacement, then sample from a shuffled dataset. If with replacement, then user can specify num_samples to draw. Registered as a Sampler with name \"random\". Parameters data_source : Dataset The dataset to sample from. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"sampler\", it gets constructed separately. - replacement : bool , optional (default = False ) Samples are drawn with replacement if True . - num_samples : int , optional (default = len(dataset) ) The number of samples to draw. This argument is supposed to be specified only when replacement is True .","title":"RandomSampler"},{"location":"api/data/samplers/samplers/#subsetrandomsampler","text":"class SubsetRandomSampler ( data . SubsetRandomSampler , Sampler ): | def __init__ ( self , indices : List [ int ]) A registrable version of pytorch's SubsetRandomSampler . Samples elements randomly from a given list of indices, without replacement. Registered as a Sampler with name \"subset_random\". Parameters indices : List[int] a sequence of indices to sample from.","title":"SubsetRandomSampler"},{"location":"api/data/samplers/samplers/#weightedrandomsampler","text":"class WeightedRandomSampler ( data . WeightedRandomSampler , Sampler ): | def __init__ ( | self , | weights : List [ float ], | num_samples : int , | replacement : bool = True | ) A registrable version of pytorch's WeightedRandomSampler . Samples elements from [0,...,len(weights)-1] with given probabilities (weights). Registered as a Sampler with name \"weighted_random\". Parameters: weights : List[float] A sequence of weights, not necessary summing up to one. num_samples : int The number of samples to draw. replacement : bool If True , samples are drawn with replacement. If not, they are drawn without replacement, which means that when a sample index is drawn for a row, it cannot be drawn again for that row. Examples >>> list ( WeightedRandomSampler ([ 0.1 , 0.9 , 0.4 , 0.7 , 3.0 , 0.6 ], 5 , replacement = True )) [ 0 , 0 , 0 , 1 , 0 ] >>> list ( WeightedRandomSampler ([ 0.9 , 0.4 , 0.05 , 0.2 , 0.3 , 0.1 ], 5 , replacement = False )) [ 0 , 1 , 4 , 3 , 2 ]","title":"WeightedRandomSampler"},{"location":"api/data/samplers/samplers/#basicbatchsampler","text":"class BasicBatchSampler ( data . BatchSampler , BatchSampler ): | def __init__ ( | self , | sampler : Sampler , | batch_size : int , | drop_last : bool | ) A registrable version of pytorch's BatchSampler . Wraps another sampler to yield a mini-batch of indices. Registered as a BatchSampler with name \"basic\". Parameters sampler : Sampler The base sampler. batch_size : int The size of the batch. drop_last : bool If True , the sampler will drop the last batch if its size would be less than batch_size`. Examples >>> list ( BatchSampler ( SequentialSampler ( range ( 10 )), batch_size = 3 , drop_last = False )) [[ 0 , 1 , 2 ], [ 3 , 4 , 5 ], [ 6 , 7 , 8 ], [ 9 ]] >>> list ( BatchSampler ( SequentialSampler ( range ( 10 )), batch_size = 3 , drop_last = True )) [[ 0 , 1 , 2 ], [ 3 , 4 , 5 ], [ 6 , 7 , 8 ]]","title":"BasicBatchSampler"},{"location":"api/data/token_indexers/elmo_indexer/","text":"[ allennlp .data .token_indexers .elmo_indexer ] ELMoCharacterMapper # class ELMoCharacterMapper : | def __init__ ( self , tokens_to_add : Dict [ str , int ] = None ) -> None Maps individual tokens to sequences of character ids, compatible with ELMo. To be consistent with previously trained models, we include it here as special of existing character indexers. We allow to add optional additional special tokens with designated character ids with tokens_to_add . max_word_length # class ELMoCharacterMapper : | ... | max_word_length = 50 beginning_of_sentence_character # class ELMoCharacterMapper : | ... | beginning_of_sentence_character = 256 end_of_sentence_character # class ELMoCharacterMapper : | ... | end_of_sentence_character = 257 beginning_of_word_character # class ELMoCharacterMapper : | ... | beginning_of_word_character = 258 end_of_word_character # class ELMoCharacterMapper : | ... | end_of_word_character = 259 padding_character # class ELMoCharacterMapper : | ... | padding_character = 260 beginning_of_sentence_characters # class ELMoCharacterMapper : | ... | beginning_of_sentence_characters = _make_bos_eos ( beginning_of_sentence_character , padding_character , beginning ... end_of_sentence_characters # class ELMoCharacterMapper : | ... | end_of_sentence_characters = _make_bos_eos ( end_of_sentence_character , padding_character , beginning_of_wo ... bos_token # class ELMoCharacterMapper : | ... | bos_token = \"<S>\" eos_token # class ELMoCharacterMapper : | ... | eos_token = \"</S>\" convert_word_to_char_ids # class ELMoCharacterMapper : | ... | def convert_word_to_char_ids ( self , word : str ) -> List [ int ] ELMoTokenCharactersIndexer # class ELMoTokenCharactersIndexer ( TokenIndexer ): | def __init__ ( | self , | namespace : str = \"elmo_characters\" , | tokens_to_add : Dict [ str , int ] = None , | token_min_padding_length : int = 0 | ) -> None Convert a token to an array of character ids to compute ELMo representations. Registered as a TokenIndexer with name \"elmo_characters\". Parameters namespace : str , optional (default = elmo_characters ) tokens_to_add : Dict[str, int] , optional (default = None ) If not None, then provides a mapping of special tokens to character ids. When using pre-trained models, then the character id must be less then 261, and we recommend using un-used ids (e.g. 1-32). token_min_padding_length : int , optional (default = 0 ) See TokenIndexer . count_vocab_items # class ELMoTokenCharactersIndexer ( TokenIndexer ): | ... | @overrides | def count_vocab_items ( | self , | token : Token , | counter : Dict [ str , Dict [ str , int ]] | ) get_empty_token_list # class ELMoTokenCharactersIndexer ( TokenIndexer ): | ... | @overrides | def get_empty_token_list ( self ) -> IndexedTokenList tokens_to_indices # class ELMoTokenCharactersIndexer ( TokenIndexer ): | ... | @overrides | def tokens_to_indices ( | self , | tokens : List [ Token ], | vocabulary : Vocabulary | ) -> Dict [ str , List [ List [ int ]]] https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/wordpiece_indexer.py#L113 as_padded_tensor_dict # class ELMoTokenCharactersIndexer ( TokenIndexer ): | ... | @overrides | def as_padded_tensor_dict ( | self , | tokens : IndexedTokenList , | padding_lengths : Dict [ str , int ] | ) -> Dict [ str , torch . Tensor ] Overriding this method only because we need a different padding token than the default.","title":"elmo_indexer"},{"location":"api/data/token_indexers/elmo_indexer/#elmocharactermapper","text":"class ELMoCharacterMapper : | def __init__ ( self , tokens_to_add : Dict [ str , int ] = None ) -> None Maps individual tokens to sequences of character ids, compatible with ELMo. To be consistent with previously trained models, we include it here as special of existing character indexers. We allow to add optional additional special tokens with designated character ids with tokens_to_add .","title":"ELMoCharacterMapper"},{"location":"api/data/token_indexers/elmo_indexer/#max_word_length","text":"class ELMoCharacterMapper : | ... | max_word_length = 50","title":"max_word_length"},{"location":"api/data/token_indexers/elmo_indexer/#beginning_of_sentence_character","text":"class ELMoCharacterMapper : | ... | beginning_of_sentence_character = 256","title":"beginning_of_sentence_character"},{"location":"api/data/token_indexers/elmo_indexer/#end_of_sentence_character","text":"class ELMoCharacterMapper : | ... | end_of_sentence_character = 257","title":"end_of_sentence_character"},{"location":"api/data/token_indexers/elmo_indexer/#beginning_of_word_character","text":"class ELMoCharacterMapper : | ... | beginning_of_word_character = 258","title":"beginning_of_word_character"},{"location":"api/data/token_indexers/elmo_indexer/#end_of_word_character","text":"class ELMoCharacterMapper : | ... | end_of_word_character = 259","title":"end_of_word_character"},{"location":"api/data/token_indexers/elmo_indexer/#padding_character","text":"class ELMoCharacterMapper : | ... | padding_character = 260","title":"padding_character"},{"location":"api/data/token_indexers/elmo_indexer/#beginning_of_sentence_characters","text":"class ELMoCharacterMapper : | ... | beginning_of_sentence_characters = _make_bos_eos ( beginning_of_sentence_character , padding_character , beginning ...","title":"beginning_of_sentence_characters"},{"location":"api/data/token_indexers/elmo_indexer/#end_of_sentence_characters","text":"class ELMoCharacterMapper : | ... | end_of_sentence_characters = _make_bos_eos ( end_of_sentence_character , padding_character , beginning_of_wo ...","title":"end_of_sentence_characters"},{"location":"api/data/token_indexers/elmo_indexer/#bos_token","text":"class ELMoCharacterMapper : | ... | bos_token = \"<S>\"","title":"bos_token"},{"location":"api/data/token_indexers/elmo_indexer/#eos_token","text":"class ELMoCharacterMapper : | ... | eos_token = \"</S>\"","title":"eos_token"},{"location":"api/data/token_indexers/elmo_indexer/#convert_word_to_char_ids","text":"class ELMoCharacterMapper : | ... | def convert_word_to_char_ids ( self , word : str ) -> List [ int ]","title":"convert_word_to_char_ids"},{"location":"api/data/token_indexers/elmo_indexer/#elmotokencharactersindexer","text":"class ELMoTokenCharactersIndexer ( TokenIndexer ): | def __init__ ( | self , | namespace : str = \"elmo_characters\" , | tokens_to_add : Dict [ str , int ] = None , | token_min_padding_length : int = 0 | ) -> None Convert a token to an array of character ids to compute ELMo representations. Registered as a TokenIndexer with name \"elmo_characters\". Parameters namespace : str , optional (default = elmo_characters ) tokens_to_add : Dict[str, int] , optional (default = None ) If not None, then provides a mapping of special tokens to character ids. When using pre-trained models, then the character id must be less then 261, and we recommend using un-used ids (e.g. 1-32). token_min_padding_length : int , optional (default = 0 ) See TokenIndexer .","title":"ELMoTokenCharactersIndexer"},{"location":"api/data/token_indexers/elmo_indexer/#count_vocab_items","text":"class ELMoTokenCharactersIndexer ( TokenIndexer ): | ... | @overrides | def count_vocab_items ( | self , | token : Token , | counter : Dict [ str , Dict [ str , int ]] | )","title":"count_vocab_items"},{"location":"api/data/token_indexers/elmo_indexer/#get_empty_token_list","text":"class ELMoTokenCharactersIndexer ( TokenIndexer ): | ... | @overrides | def get_empty_token_list ( self ) -> IndexedTokenList","title":"get_empty_token_list"},{"location":"api/data/token_indexers/elmo_indexer/#tokens_to_indices","text":"class ELMoTokenCharactersIndexer ( TokenIndexer ): | ... | @overrides | def tokens_to_indices ( | self , | tokens : List [ Token ], | vocabulary : Vocabulary | ) -> Dict [ str , List [ List [ int ]]] https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/wordpiece_indexer.py#L113","title":"tokens_to_indices"},{"location":"api/data/token_indexers/elmo_indexer/#as_padded_tensor_dict","text":"class ELMoTokenCharactersIndexer ( TokenIndexer ): | ... | @overrides | def as_padded_tensor_dict ( | self , | tokens : IndexedTokenList , | padding_lengths : Dict [ str , int ] | ) -> Dict [ str , torch . Tensor ] Overriding this method only because we need a different padding token than the default.","title":"as_padded_tensor_dict"},{"location":"api/data/token_indexers/pretrained_transformer_indexer/","text":"[ allennlp .data .token_indexers .pretrained_transformer_indexer ] PretrainedTransformerIndexer # class PretrainedTransformerIndexer ( TokenIndexer ): | def __init__ ( | self , | model_name : str , | namespace : str = \"tags\" , | max_length : int = None , | ** kwargs | ) -> None This TokenIndexer assumes that Tokens already have their indexes in them (see text_id field). We still require model_name because we want to form allennlp vocabulary from pretrained one. This Indexer is only really appropriate to use if you've also used a corresponding PretrainedTransformerTokenizer to tokenize your input. Otherwise you'll have a mismatch between your tokens and your vocabulary, and you'll get a lot of UNK tokens. Registered as a TokenIndexer with name \"pretrained_transformer\". Parameters model_name : str The name of the transformers model to use. namespace : str , optional (default = tags ) We will add the tokens in the pytorch_transformer vocabulary to this vocabulary namespace. We use a somewhat confusing default value of tags so that we do not add padding or UNK tokens to this namespace, which would break on loading because we wouldn't find our default OOV token. max_length : int , optional (default = None ) If not None, split the document into segments of this many tokens (including special tokens) before feeding into the embedder. The embedder embeds these segments independently and concatenate the results to get the original document representation. Should be set to the same value as the max_length option on the PretrainedTransformerEmbedder . count_vocab_items # class PretrainedTransformerIndexer ( TokenIndexer ): | ... | @overrides | def count_vocab_items ( | self , | token : Token , | counter : Dict [ str , Dict [ str , int ]] | ) If we only use pretrained models, we don't need to do anything here. tokens_to_indices # class PretrainedTransformerIndexer ( TokenIndexer ): | ... | @overrides | def tokens_to_indices ( | self , | tokens : List [ Token ], | vocabulary : Vocabulary | ) -> IndexedTokenList indices_to_tokens # class PretrainedTransformerIndexer ( TokenIndexer ): | ... | @overrides | def indices_to_tokens ( | self , | indexed_tokens : IndexedTokenList , | vocabulary : Vocabulary | ) -> List [ Token ] get_empty_token_list # class PretrainedTransformerIndexer ( TokenIndexer ): | ... | @overrides | def get_empty_token_list ( self ) -> IndexedTokenList as_padded_tensor_dict # class PretrainedTransformerIndexer ( TokenIndexer ): | ... | @overrides | def as_padded_tensor_dict ( | self , | tokens : IndexedTokenList , | padding_lengths : Dict [ str , int ] | ) -> Dict [ str , torch . Tensor ]","title":"pretrained_transformer_indexer"},{"location":"api/data/token_indexers/pretrained_transformer_indexer/#pretrainedtransformerindexer","text":"class PretrainedTransformerIndexer ( TokenIndexer ): | def __init__ ( | self , | model_name : str , | namespace : str = \"tags\" , | max_length : int = None , | ** kwargs | ) -> None This TokenIndexer assumes that Tokens already have their indexes in them (see text_id field). We still require model_name because we want to form allennlp vocabulary from pretrained one. This Indexer is only really appropriate to use if you've also used a corresponding PretrainedTransformerTokenizer to tokenize your input. Otherwise you'll have a mismatch between your tokens and your vocabulary, and you'll get a lot of UNK tokens. Registered as a TokenIndexer with name \"pretrained_transformer\". Parameters model_name : str The name of the transformers model to use. namespace : str , optional (default = tags ) We will add the tokens in the pytorch_transformer vocabulary to this vocabulary namespace. We use a somewhat confusing default value of tags so that we do not add padding or UNK tokens to this namespace, which would break on loading because we wouldn't find our default OOV token. max_length : int , optional (default = None ) If not None, split the document into segments of this many tokens (including special tokens) before feeding into the embedder. The embedder embeds these segments independently and concatenate the results to get the original document representation. Should be set to the same value as the max_length option on the PretrainedTransformerEmbedder .","title":"PretrainedTransformerIndexer"},{"location":"api/data/token_indexers/pretrained_transformer_indexer/#count_vocab_items","text":"class PretrainedTransformerIndexer ( TokenIndexer ): | ... | @overrides | def count_vocab_items ( | self , | token : Token , | counter : Dict [ str , Dict [ str , int ]] | ) If we only use pretrained models, we don't need to do anything here.","title":"count_vocab_items"},{"location":"api/data/token_indexers/pretrained_transformer_indexer/#tokens_to_indices","text":"class PretrainedTransformerIndexer ( TokenIndexer ): | ... | @overrides | def tokens_to_indices ( | self , | tokens : List [ Token ], | vocabulary : Vocabulary | ) -> IndexedTokenList","title":"tokens_to_indices"},{"location":"api/data/token_indexers/pretrained_transformer_indexer/#indices_to_tokens","text":"class PretrainedTransformerIndexer ( TokenIndexer ): | ... | @overrides | def indices_to_tokens ( | self , | indexed_tokens : IndexedTokenList , | vocabulary : Vocabulary | ) -> List [ Token ]","title":"indices_to_tokens"},{"location":"api/data/token_indexers/pretrained_transformer_indexer/#get_empty_token_list","text":"class PretrainedTransformerIndexer ( TokenIndexer ): | ... | @overrides | def get_empty_token_list ( self ) -> IndexedTokenList","title":"get_empty_token_list"},{"location":"api/data/token_indexers/pretrained_transformer_indexer/#as_padded_tensor_dict","text":"class PretrainedTransformerIndexer ( TokenIndexer ): | ... | @overrides | def as_padded_tensor_dict ( | self , | tokens : IndexedTokenList , | padding_lengths : Dict [ str , int ] | ) -> Dict [ str , torch . Tensor ]","title":"as_padded_tensor_dict"},{"location":"api/data/token_indexers/pretrained_transformer_mismatched_indexer/","text":"[ allennlp .data .token_indexers .pretrained_transformer_mismatched_indexer ] PretrainedTransformerMismatchedIndexer # class PretrainedTransformerMismatchedIndexer ( TokenIndexer ): | def __init__ ( | self , | model_name : str , | namespace : str = \"tags\" , | max_length : int = None , | ** kwargs | ) -> None Use this indexer when (for whatever reason) you are not using a corresponding PretrainedTransformerTokenizer on your input. We assume that you used a tokenizer that splits strings into words, while the transformer expects wordpieces as input. This indexer splits the words into wordpieces and flattens them out. You should use the corresponding PretrainedTransformerMismatchedEmbedder to embed these wordpieces and then pull out a single vector for each original word. Registered as a TokenIndexer with name \"pretrained_transformer_mismatched\". Parameters model_name : str The name of the transformers model to use. namespace : str , optional (default = tags ) We will add the tokens in the pytorch_transformer vocabulary to this vocabulary namespace. We use a somewhat confusing default value of tags so that we do not add padding or UNK tokens to this namespace, which would break on loading because we wouldn't find our default OOV token. max_length : int , optional (default = None ) If positive, split the document into segments of this many tokens (including special tokens) before feeding into the embedder. The embedder embeds these segments independently and concatenate the results to get the original document representation. Should be set to the same value as the max_length option on the PretrainedTransformerMismatchedEmbedder . count_vocab_items # class PretrainedTransformerMismatchedIndexer ( TokenIndexer ): | ... | @overrides | def count_vocab_items ( | self , | token : Token , | counter : Dict [ str , Dict [ str , int ]] | ) tokens_to_indices # class PretrainedTransformerMismatchedIndexer ( TokenIndexer ): | ... | @overrides | def tokens_to_indices ( | self , | tokens : List [ Token ], | vocabulary : Vocabulary | ) -> IndexedTokenList get_empty_token_list # class PretrainedTransformerMismatchedIndexer ( TokenIndexer ): | ... | @overrides | def get_empty_token_list ( self ) -> IndexedTokenList as_padded_tensor_dict # class PretrainedTransformerMismatchedIndexer ( TokenIndexer ): | ... | @overrides | def as_padded_tensor_dict ( | self , | tokens : IndexedTokenList , | padding_lengths : Dict [ str , int ] | ) -> Dict [ str , torch . Tensor ]","title":"pretrained_transformer_mismatched_indexer"},{"location":"api/data/token_indexers/pretrained_transformer_mismatched_indexer/#pretrainedtransformermismatchedindexer","text":"class PretrainedTransformerMismatchedIndexer ( TokenIndexer ): | def __init__ ( | self , | model_name : str , | namespace : str = \"tags\" , | max_length : int = None , | ** kwargs | ) -> None Use this indexer when (for whatever reason) you are not using a corresponding PretrainedTransformerTokenizer on your input. We assume that you used a tokenizer that splits strings into words, while the transformer expects wordpieces as input. This indexer splits the words into wordpieces and flattens them out. You should use the corresponding PretrainedTransformerMismatchedEmbedder to embed these wordpieces and then pull out a single vector for each original word. Registered as a TokenIndexer with name \"pretrained_transformer_mismatched\". Parameters model_name : str The name of the transformers model to use. namespace : str , optional (default = tags ) We will add the tokens in the pytorch_transformer vocabulary to this vocabulary namespace. We use a somewhat confusing default value of tags so that we do not add padding or UNK tokens to this namespace, which would break on loading because we wouldn't find our default OOV token. max_length : int , optional (default = None ) If positive, split the document into segments of this many tokens (including special tokens) before feeding into the embedder. The embedder embeds these segments independently and concatenate the results to get the original document representation. Should be set to the same value as the max_length option on the PretrainedTransformerMismatchedEmbedder .","title":"PretrainedTransformerMismatchedIndexer"},{"location":"api/data/token_indexers/pretrained_transformer_mismatched_indexer/#count_vocab_items","text":"class PretrainedTransformerMismatchedIndexer ( TokenIndexer ): | ... | @overrides | def count_vocab_items ( | self , | token : Token , | counter : Dict [ str , Dict [ str , int ]] | )","title":"count_vocab_items"},{"location":"api/data/token_indexers/pretrained_transformer_mismatched_indexer/#tokens_to_indices","text":"class PretrainedTransformerMismatchedIndexer ( TokenIndexer ): | ... | @overrides | def tokens_to_indices ( | self , | tokens : List [ Token ], | vocabulary : Vocabulary | ) -> IndexedTokenList","title":"tokens_to_indices"},{"location":"api/data/token_indexers/pretrained_transformer_mismatched_indexer/#get_empty_token_list","text":"class PretrainedTransformerMismatchedIndexer ( TokenIndexer ): | ... | @overrides | def get_empty_token_list ( self ) -> IndexedTokenList","title":"get_empty_token_list"},{"location":"api/data/token_indexers/pretrained_transformer_mismatched_indexer/#as_padded_tensor_dict","text":"class PretrainedTransformerMismatchedIndexer ( TokenIndexer ): | ... | @overrides | def as_padded_tensor_dict ( | self , | tokens : IndexedTokenList , | padding_lengths : Dict [ str , int ] | ) -> Dict [ str , torch . Tensor ]","title":"as_padded_tensor_dict"},{"location":"api/data/token_indexers/single_id_token_indexer/","text":"[ allennlp .data .token_indexers .single_id_token_indexer ] SingleIdTokenIndexer # class SingleIdTokenIndexer ( TokenIndexer ): | def __init__ ( | self , | namespace : Optional [ str ] = \"tokens\" , | lowercase_tokens : bool = False , | start_tokens : List [ str ] = None , | end_tokens : List [ str ] = None , | feature_name : str = \"text\" , | default_value : str = _DEFAULT_VALUE , | token_min_padding_length : int = 0 | ) -> None This TokenIndexer represents tokens as single integers. Registered as a TokenIndexer with name \"single_id\". Parameters namespace : Optional[str] , optional (default = \"tokens\" ) We will use this namespace in the Vocabulary to map strings to indices. If you explicitly pass in None here, we will skip indexing and vocabulary lookups. This means that the feature_name you use must correspond to an integer value (like text_id , for instance, which gets set by some tokenizers, such as when using byte encoding). lowercase_tokens : bool , optional (default = False ) If True , we will call token.lower() before getting an index for the token from the vocabulary. start_tokens : List[str] , optional (default = None ) These are prepended to the tokens provided to tokens_to_indices . end_tokens : List[str] , optional (default = None ) These are appended to the tokens provided to tokens_to_indices . feature_name : str , optional (default = \"text\" ) We will use the Token attribute with this name as input. This is potentially useful, e.g., for using NER tags instead of (or in addition to) surface forms as your inputs (passing ent_type_ here would do that). If you use a non-default value here, you almost certainly want to also change the namespace parameter, and you might want to give a default_value . default_value : str , optional When you want to use a non-default feature_name , you sometimes want to have a default value to go with it, e.g., in case you don't have an NER tag for a particular token, for some reason. This value will get used if we don't find a value in feature_name . If this is not given, we will crash if a token doesn't have a value for the given feature_name , so that you don't get weird, silent errors by default. token_min_padding_length : int , optional (default = 0 ) See TokenIndexer . count_vocab_items # class SingleIdTokenIndexer ( TokenIndexer ): | ... | @overrides | def count_vocab_items ( | self , | token : Token , | counter : Dict [ str , Dict [ str , int ]] | ) tokens_to_indices # class SingleIdTokenIndexer ( TokenIndexer ): | ... | @overrides | def tokens_to_indices ( | self , | tokens : List [ Token ], | vocabulary : Vocabulary | ) -> Dict [ str , List [ int ]] get_empty_token_list # class SingleIdTokenIndexer ( TokenIndexer ): | ... | @overrides | def get_empty_token_list ( self ) -> IndexedTokenList","title":"single_id_token_indexer"},{"location":"api/data/token_indexers/single_id_token_indexer/#singleidtokenindexer","text":"class SingleIdTokenIndexer ( TokenIndexer ): | def __init__ ( | self , | namespace : Optional [ str ] = \"tokens\" , | lowercase_tokens : bool = False , | start_tokens : List [ str ] = None , | end_tokens : List [ str ] = None , | feature_name : str = \"text\" , | default_value : str = _DEFAULT_VALUE , | token_min_padding_length : int = 0 | ) -> None This TokenIndexer represents tokens as single integers. Registered as a TokenIndexer with name \"single_id\". Parameters namespace : Optional[str] , optional (default = \"tokens\" ) We will use this namespace in the Vocabulary to map strings to indices. If you explicitly pass in None here, we will skip indexing and vocabulary lookups. This means that the feature_name you use must correspond to an integer value (like text_id , for instance, which gets set by some tokenizers, such as when using byte encoding). lowercase_tokens : bool , optional (default = False ) If True , we will call token.lower() before getting an index for the token from the vocabulary. start_tokens : List[str] , optional (default = None ) These are prepended to the tokens provided to tokens_to_indices . end_tokens : List[str] , optional (default = None ) These are appended to the tokens provided to tokens_to_indices . feature_name : str , optional (default = \"text\" ) We will use the Token attribute with this name as input. This is potentially useful, e.g., for using NER tags instead of (or in addition to) surface forms as your inputs (passing ent_type_ here would do that). If you use a non-default value here, you almost certainly want to also change the namespace parameter, and you might want to give a default_value . default_value : str , optional When you want to use a non-default feature_name , you sometimes want to have a default value to go with it, e.g., in case you don't have an NER tag for a particular token, for some reason. This value will get used if we don't find a value in feature_name . If this is not given, we will crash if a token doesn't have a value for the given feature_name , so that you don't get weird, silent errors by default. token_min_padding_length : int , optional (default = 0 ) See TokenIndexer .","title":"SingleIdTokenIndexer"},{"location":"api/data/token_indexers/single_id_token_indexer/#count_vocab_items","text":"class SingleIdTokenIndexer ( TokenIndexer ): | ... | @overrides | def count_vocab_items ( | self , | token : Token , | counter : Dict [ str , Dict [ str , int ]] | )","title":"count_vocab_items"},{"location":"api/data/token_indexers/single_id_token_indexer/#tokens_to_indices","text":"class SingleIdTokenIndexer ( TokenIndexer ): | ... | @overrides | def tokens_to_indices ( | self , | tokens : List [ Token ], | vocabulary : Vocabulary | ) -> Dict [ str , List [ int ]]","title":"tokens_to_indices"},{"location":"api/data/token_indexers/single_id_token_indexer/#get_empty_token_list","text":"class SingleIdTokenIndexer ( TokenIndexer ): | ... | @overrides | def get_empty_token_list ( self ) -> IndexedTokenList","title":"get_empty_token_list"},{"location":"api/data/token_indexers/spacy_indexer/","text":"[ allennlp .data .token_indexers .spacy_indexer ] SpacyTokenIndexer # class SpacyTokenIndexer ( TokenIndexer ): | def __init__ ( | self , | hidden_dim : int = 96 , | token_min_padding_length : int = 0 | ) -> None This SpacyTokenIndexer represents tokens as word vectors from a spacy model. You might want to do this for two main reasons; easier integration with a spacy pipeline and no out of vocabulary tokens. Registered as a TokenIndexer with name \"spacy\". Parameters hidden_dim : int , optional (default = 96 ) The dimension of the vectors that spacy generates for representing words. token_min_padding_length : int , optional (default = 0 ) See TokenIndexer . count_vocab_items # class SpacyTokenIndexer ( TokenIndexer ): | ... | @overrides | def count_vocab_items ( | self , | token : Token , | counter : Dict [ str , Dict [ str , int ]] | ) We are using spacy to generate embeddings directly for our model, so we don't need to capture the vocab - it is defined by the spacy model we are using instead. tokens_to_indices # class SpacyTokenIndexer ( TokenIndexer ): | ... | @overrides | def tokens_to_indices ( | self , | tokens : List [ SpacyToken ], | vocabulary : Vocabulary | ) -> Dict [ str , List [ numpy . ndarray ]] as_padded_tensor_dict # class SpacyTokenIndexer ( TokenIndexer ): | ... | @overrides | def as_padded_tensor_dict ( | self , | tokens : IndexedTokenList , | padding_lengths : Dict [ str , int ] | ) -> Dict [ str , torch . Tensor ]","title":"spacy_indexer"},{"location":"api/data/token_indexers/spacy_indexer/#spacytokenindexer","text":"class SpacyTokenIndexer ( TokenIndexer ): | def __init__ ( | self , | hidden_dim : int = 96 , | token_min_padding_length : int = 0 | ) -> None This SpacyTokenIndexer represents tokens as word vectors from a spacy model. You might want to do this for two main reasons; easier integration with a spacy pipeline and no out of vocabulary tokens. Registered as a TokenIndexer with name \"spacy\". Parameters hidden_dim : int , optional (default = 96 ) The dimension of the vectors that spacy generates for representing words. token_min_padding_length : int , optional (default = 0 ) See TokenIndexer .","title":"SpacyTokenIndexer"},{"location":"api/data/token_indexers/spacy_indexer/#count_vocab_items","text":"class SpacyTokenIndexer ( TokenIndexer ): | ... | @overrides | def count_vocab_items ( | self , | token : Token , | counter : Dict [ str , Dict [ str , int ]] | ) We are using spacy to generate embeddings directly for our model, so we don't need to capture the vocab - it is defined by the spacy model we are using instead.","title":"count_vocab_items"},{"location":"api/data/token_indexers/spacy_indexer/#tokens_to_indices","text":"class SpacyTokenIndexer ( TokenIndexer ): | ... | @overrides | def tokens_to_indices ( | self , | tokens : List [ SpacyToken ], | vocabulary : Vocabulary | ) -> Dict [ str , List [ numpy . ndarray ]]","title":"tokens_to_indices"},{"location":"api/data/token_indexers/spacy_indexer/#as_padded_tensor_dict","text":"class SpacyTokenIndexer ( TokenIndexer ): | ... | @overrides | def as_padded_tensor_dict ( | self , | tokens : IndexedTokenList , | padding_lengths : Dict [ str , int ] | ) -> Dict [ str , torch . Tensor ]","title":"as_padded_tensor_dict"},{"location":"api/data/token_indexers/token_characters_indexer/","text":"[ allennlp .data .token_indexers .token_characters_indexer ] TokenCharactersIndexer # class TokenCharactersIndexer ( TokenIndexer ): | def __init__ ( | self , | namespace : str = \"token_characters\" , | character_tokenizer : CharacterTokenizer = CharacterTokenizer (), | start_tokens : List [ str ] = None , | end_tokens : List [ str ] = None , | min_padding_length : int = 0 , | token_min_padding_length : int = 0 | ) -> None This TokenIndexer represents tokens as lists of character indices. Registered as a TokenIndexer with name \"characters\". Parameters namespace : str , optional (default = token_characters ) We will use this namespace in the Vocabulary to map the characters in each token to indices. character_tokenizer : CharacterTokenizer , optional (default = CharacterTokenizer() ) We use a CharacterTokenizer to handle splitting tokens into characters, as it has options for byte encoding and other things. The default here is to instantiate a CharacterTokenizer with its default parameters, which uses unicode characters and retains casing. start_tokens : List[str] , optional (default = None ) These are prepended to the tokens provided to tokens_to_indices . end_tokens : List[str] , optional (default = None ) These are appended to the tokens provided to tokens_to_indices . min_padding_length : int , optional (default = 0 ) We use this value as the minimum length of padding. Usually used with CnnEncoder , its value should be set to the maximum value of ngram_filter_sizes correspondingly. token_min_padding_length : int , optional (default = 0 ) See TokenIndexer . count_vocab_items # class TokenCharactersIndexer ( TokenIndexer ): | ... | @overrides | def count_vocab_items ( | self , | token : Token , | counter : Dict [ str , Dict [ str , int ]] | ) tokens_to_indices # class TokenCharactersIndexer ( TokenIndexer ): | ... | @overrides | def tokens_to_indices ( | self , | tokens : List [ Token ], | vocabulary : Vocabulary | ) -> Dict [ str , List [ List [ int ]]] get_padding_lengths # class TokenCharactersIndexer ( TokenIndexer ): | ... | @overrides | def get_padding_lengths ( | self , | indexed_tokens : IndexedTokenList | ) -> Dict [ str , int ] as_padded_tensor_dict # class TokenCharactersIndexer ( TokenIndexer ): | ... | @overrides | def as_padded_tensor_dict ( | self , | tokens : IndexedTokenList , | padding_lengths : Dict [ str , int ] | ) -> Dict [ str , torch . Tensor ] Pad the tokens. get_empty_token_list # class TokenCharactersIndexer ( TokenIndexer ): | ... | @overrides | def get_empty_token_list ( self ) -> IndexedTokenList","title":"token_characters_indexer"},{"location":"api/data/token_indexers/token_characters_indexer/#tokencharactersindexer","text":"class TokenCharactersIndexer ( TokenIndexer ): | def __init__ ( | self , | namespace : str = \"token_characters\" , | character_tokenizer : CharacterTokenizer = CharacterTokenizer (), | start_tokens : List [ str ] = None , | end_tokens : List [ str ] = None , | min_padding_length : int = 0 , | token_min_padding_length : int = 0 | ) -> None This TokenIndexer represents tokens as lists of character indices. Registered as a TokenIndexer with name \"characters\". Parameters namespace : str , optional (default = token_characters ) We will use this namespace in the Vocabulary to map the characters in each token to indices. character_tokenizer : CharacterTokenizer , optional (default = CharacterTokenizer() ) We use a CharacterTokenizer to handle splitting tokens into characters, as it has options for byte encoding and other things. The default here is to instantiate a CharacterTokenizer with its default parameters, which uses unicode characters and retains casing. start_tokens : List[str] , optional (default = None ) These are prepended to the tokens provided to tokens_to_indices . end_tokens : List[str] , optional (default = None ) These are appended to the tokens provided to tokens_to_indices . min_padding_length : int , optional (default = 0 ) We use this value as the minimum length of padding. Usually used with CnnEncoder , its value should be set to the maximum value of ngram_filter_sizes correspondingly. token_min_padding_length : int , optional (default = 0 ) See TokenIndexer .","title":"TokenCharactersIndexer"},{"location":"api/data/token_indexers/token_characters_indexer/#count_vocab_items","text":"class TokenCharactersIndexer ( TokenIndexer ): | ... | @overrides | def count_vocab_items ( | self , | token : Token , | counter : Dict [ str , Dict [ str , int ]] | )","title":"count_vocab_items"},{"location":"api/data/token_indexers/token_characters_indexer/#tokens_to_indices","text":"class TokenCharactersIndexer ( TokenIndexer ): | ... | @overrides | def tokens_to_indices ( | self , | tokens : List [ Token ], | vocabulary : Vocabulary | ) -> Dict [ str , List [ List [ int ]]]","title":"tokens_to_indices"},{"location":"api/data/token_indexers/token_characters_indexer/#get_padding_lengths","text":"class TokenCharactersIndexer ( TokenIndexer ): | ... | @overrides | def get_padding_lengths ( | self , | indexed_tokens : IndexedTokenList | ) -> Dict [ str , int ]","title":"get_padding_lengths"},{"location":"api/data/token_indexers/token_characters_indexer/#as_padded_tensor_dict","text":"class TokenCharactersIndexer ( TokenIndexer ): | ... | @overrides | def as_padded_tensor_dict ( | self , | tokens : IndexedTokenList , | padding_lengths : Dict [ str , int ] | ) -> Dict [ str , torch . Tensor ] Pad the tokens.","title":"as_padded_tensor_dict"},{"location":"api/data/token_indexers/token_characters_indexer/#get_empty_token_list","text":"class TokenCharactersIndexer ( TokenIndexer ): | ... | @overrides | def get_empty_token_list ( self ) -> IndexedTokenList","title":"get_empty_token_list"},{"location":"api/data/token_indexers/token_indexer/","text":"[ allennlp .data .token_indexers .token_indexer ] IndexedTokenList # IndexedTokenList = Dict [ str , List [ Any ]] TokenIndexer # class TokenIndexer ( Registrable ): | def __init__ ( self , token_min_padding_length : int = 0 ) -> None A TokenIndexer determines how string tokens get represented as arrays of indices in a model. This class both converts strings into numerical values, with the help of a Vocabulary , and it produces actual arrays. Tokens can be represented as single IDs (e.g., the word \"cat\" gets represented by the number 34), or as lists of character IDs (e.g., \"cat\" gets represented by the numbers [23, 10, 18]), or in some other way that you can come up with (e.g., if you have some structured input you want to represent in a special way in your data arrays, you can do that here). Parameters token_min_padding_length : int , optional (default = 0 ) The minimum padding length required for the TokenIndexer . For example, the minimum padding length of SingleIdTokenIndexer is the largest size of filter when using CnnEncoder . Note that if you set this for one TokenIndexer, you likely have to set it for all TokenIndexer for the same field, otherwise you'll get mismatched tensor sizes. default_implementation # class TokenIndexer ( Registrable ): | ... | default_implementation = \"single_id\" has_warned_for_as_padded_tensor # class TokenIndexer ( Registrable ): | ... | has_warned_for_as_padded_tensor = False count_vocab_items # class TokenIndexer ( Registrable ): | ... | def count_vocab_items ( | self , | token : Token , | counter : Dict [ str , Dict [ str , int ]] | ) The Vocabulary needs to assign indices to whatever strings we see in the training data (possibly doing some frequency filtering and using an OOV, or out of vocabulary, token). This method takes a token and a dictionary of counts and increments counts for whatever vocabulary items are present in the token. If this is a single token ID representation, the vocabulary item is likely the token itself. If this is a token characters representation, the vocabulary items are all of the characters in the token. tokens_to_indices # class TokenIndexer ( Registrable ): | ... | def tokens_to_indices ( | self , | tokens : List [ Token ], | vocabulary : Vocabulary | ) -> IndexedTokenList Takes a list of tokens and converts them to an IndexedTokenList . This could be just an ID for each token from the vocabulary. Or it could split each token into characters and return one ID per character. Or (for instance, in the case of byte-pair encoding) there might not be a clean mapping from individual tokens to indices, and the IndexedTokenList could be a complex data structure. indices_to_tokens # class TokenIndexer ( Registrable ): | ... | def indices_to_tokens ( | self , | indexed_tokens : IndexedTokenList , | vocabulary : Vocabulary | ) -> List [ Token ] Inverse operations of tokens_to_indices. Takes an IndexedTokenList and converts it back into a list of tokens. get_empty_token_list # class TokenIndexer ( Registrable ): | ... | def get_empty_token_list ( self ) -> IndexedTokenList Returns an already indexed version of an empty token list. This is typically just an empty list for whatever keys are used in the indexer. get_padding_lengths # class TokenIndexer ( Registrable ): | ... | def get_padding_lengths ( | self , | indexed_tokens : IndexedTokenList | ) -> Dict [ str , int ] This method returns a padding dictionary for the given indexed_tokens specifying all lengths that need padding. If all you have is a list of single ID tokens, this is just the length of the list, and that's what the default implementation will give you. If you have something more complicated, like a list of character ids for token, you'll need to override this. as_padded_tensor_dict # class TokenIndexer ( Registrable ): | ... | def as_padded_tensor_dict ( | self , | tokens : IndexedTokenList , | padding_lengths : Dict [ str , int ] | ) -> Dict [ str , torch . Tensor ] This method pads a list of tokens given the input padding lengths (which could actually truncate things, depending on settings) and returns that padded list of input tokens as a Dict[str, torch.Tensor] . This is a dictionary because there should be one key per argument that the TokenEmbedder corresponding to this class expects in its forward() method (where the argument name in the TokenEmbedder needs to make the key in this dictionary). The base class implements the case when all you want to do is create a padded LongTensor for every list in the tokens dictionary. If your TokenIndexer needs more complex logic than that, you need to override this method.","title":"token_indexer"},{"location":"api/data/token_indexers/token_indexer/#indexedtokenlist","text":"IndexedTokenList = Dict [ str , List [ Any ]]","title":"IndexedTokenList"},{"location":"api/data/token_indexers/token_indexer/#tokenindexer","text":"class TokenIndexer ( Registrable ): | def __init__ ( self , token_min_padding_length : int = 0 ) -> None A TokenIndexer determines how string tokens get represented as arrays of indices in a model. This class both converts strings into numerical values, with the help of a Vocabulary , and it produces actual arrays. Tokens can be represented as single IDs (e.g., the word \"cat\" gets represented by the number 34), or as lists of character IDs (e.g., \"cat\" gets represented by the numbers [23, 10, 18]), or in some other way that you can come up with (e.g., if you have some structured input you want to represent in a special way in your data arrays, you can do that here). Parameters token_min_padding_length : int , optional (default = 0 ) The minimum padding length required for the TokenIndexer . For example, the minimum padding length of SingleIdTokenIndexer is the largest size of filter when using CnnEncoder . Note that if you set this for one TokenIndexer, you likely have to set it for all TokenIndexer for the same field, otherwise you'll get mismatched tensor sizes.","title":"TokenIndexer"},{"location":"api/data/token_indexers/token_indexer/#default_implementation","text":"class TokenIndexer ( Registrable ): | ... | default_implementation = \"single_id\"","title":"default_implementation"},{"location":"api/data/token_indexers/token_indexer/#has_warned_for_as_padded_tensor","text":"class TokenIndexer ( Registrable ): | ... | has_warned_for_as_padded_tensor = False","title":"has_warned_for_as_padded_tensor"},{"location":"api/data/token_indexers/token_indexer/#count_vocab_items","text":"class TokenIndexer ( Registrable ): | ... | def count_vocab_items ( | self , | token : Token , | counter : Dict [ str , Dict [ str , int ]] | ) The Vocabulary needs to assign indices to whatever strings we see in the training data (possibly doing some frequency filtering and using an OOV, or out of vocabulary, token). This method takes a token and a dictionary of counts and increments counts for whatever vocabulary items are present in the token. If this is a single token ID representation, the vocabulary item is likely the token itself. If this is a token characters representation, the vocabulary items are all of the characters in the token.","title":"count_vocab_items"},{"location":"api/data/token_indexers/token_indexer/#tokens_to_indices","text":"class TokenIndexer ( Registrable ): | ... | def tokens_to_indices ( | self , | tokens : List [ Token ], | vocabulary : Vocabulary | ) -> IndexedTokenList Takes a list of tokens and converts them to an IndexedTokenList . This could be just an ID for each token from the vocabulary. Or it could split each token into characters and return one ID per character. Or (for instance, in the case of byte-pair encoding) there might not be a clean mapping from individual tokens to indices, and the IndexedTokenList could be a complex data structure.","title":"tokens_to_indices"},{"location":"api/data/token_indexers/token_indexer/#indices_to_tokens","text":"class TokenIndexer ( Registrable ): | ... | def indices_to_tokens ( | self , | indexed_tokens : IndexedTokenList , | vocabulary : Vocabulary | ) -> List [ Token ] Inverse operations of tokens_to_indices. Takes an IndexedTokenList and converts it back into a list of tokens.","title":"indices_to_tokens"},{"location":"api/data/token_indexers/token_indexer/#get_empty_token_list","text":"class TokenIndexer ( Registrable ): | ... | def get_empty_token_list ( self ) -> IndexedTokenList Returns an already indexed version of an empty token list. This is typically just an empty list for whatever keys are used in the indexer.","title":"get_empty_token_list"},{"location":"api/data/token_indexers/token_indexer/#get_padding_lengths","text":"class TokenIndexer ( Registrable ): | ... | def get_padding_lengths ( | self , | indexed_tokens : IndexedTokenList | ) -> Dict [ str , int ] This method returns a padding dictionary for the given indexed_tokens specifying all lengths that need padding. If all you have is a list of single ID tokens, this is just the length of the list, and that's what the default implementation will give you. If you have something more complicated, like a list of character ids for token, you'll need to override this.","title":"get_padding_lengths"},{"location":"api/data/token_indexers/token_indexer/#as_padded_tensor_dict","text":"class TokenIndexer ( Registrable ): | ... | def as_padded_tensor_dict ( | self , | tokens : IndexedTokenList , | padding_lengths : Dict [ str , int ] | ) -> Dict [ str , torch . Tensor ] This method pads a list of tokens given the input padding lengths (which could actually truncate things, depending on settings) and returns that padded list of input tokens as a Dict[str, torch.Tensor] . This is a dictionary because there should be one key per argument that the TokenEmbedder corresponding to this class expects in its forward() method (where the argument name in the TokenEmbedder needs to make the key in this dictionary). The base class implements the case when all you want to do is create a padded LongTensor for every list in the tokens dictionary. If your TokenIndexer needs more complex logic than that, you need to override this method.","title":"as_padded_tensor_dict"},{"location":"api/data/tokenizers/character_tokenizer/","text":"[ allennlp .data .tokenizers .character_tokenizer ] CharacterTokenizer # class CharacterTokenizer ( Tokenizer ): | def __init__ ( | self , | byte_encoding : str = None , | lowercase_characters : bool = False , | start_tokens : List [ Union [ str , int ]] = None , | end_tokens : List [ Union [ str , int ]] = None | ) -> None A CharacterTokenizer splits strings into character tokens. Registered as a Tokenizer with name \"character\". Parameters byte_encoding : str , optional (default = None ) If not None , we will use this encoding to encode the string as bytes, and use the byte sequence as characters, instead of the unicode characters in the python string. E.g., the character '\u00e1' would be a single token if this option is None , but it would be two tokens if this option is set to \"utf-8\" . If this is not None , tokenize will return a List[int] instead of a List[str] , and we will bypass the vocabulary in the TokenIndexer . lowercase_characters : bool , optional (default = False ) If True , we will lowercase all of the characters in the text before doing any other operation. You probably do not want to do this, as character vocabularies are generally not very large to begin with, but it's an option if you really want it. start_tokens : List[str] , optional If given, these tokens will be added to the beginning of every string we tokenize. If using byte encoding, this should actually be a List[int] , not a List[str] . end_tokens : List[str] , optional If given, these tokens will be added to the end of every string we tokenize. If using byte encoding, this should actually be a List[int] , not a List[str] . tokenize # class CharacterTokenizer ( Tokenizer ): | ... | @overrides | def tokenize ( self , text : str ) -> List [ Token ]","title":"character_tokenizer"},{"location":"api/data/tokenizers/character_tokenizer/#charactertokenizer","text":"class CharacterTokenizer ( Tokenizer ): | def __init__ ( | self , | byte_encoding : str = None , | lowercase_characters : bool = False , | start_tokens : List [ Union [ str , int ]] = None , | end_tokens : List [ Union [ str , int ]] = None | ) -> None A CharacterTokenizer splits strings into character tokens. Registered as a Tokenizer with name \"character\". Parameters byte_encoding : str , optional (default = None ) If not None , we will use this encoding to encode the string as bytes, and use the byte sequence as characters, instead of the unicode characters in the python string. E.g., the character '\u00e1' would be a single token if this option is None , but it would be two tokens if this option is set to \"utf-8\" . If this is not None , tokenize will return a List[int] instead of a List[str] , and we will bypass the vocabulary in the TokenIndexer . lowercase_characters : bool , optional (default = False ) If True , we will lowercase all of the characters in the text before doing any other operation. You probably do not want to do this, as character vocabularies are generally not very large to begin with, but it's an option if you really want it. start_tokens : List[str] , optional If given, these tokens will be added to the beginning of every string we tokenize. If using byte encoding, this should actually be a List[int] , not a List[str] . end_tokens : List[str] , optional If given, these tokens will be added to the end of every string we tokenize. If using byte encoding, this should actually be a List[int] , not a List[str] .","title":"CharacterTokenizer"},{"location":"api/data/tokenizers/character_tokenizer/#tokenize","text":"class CharacterTokenizer ( Tokenizer ): | ... | @overrides | def tokenize ( self , text : str ) -> List [ Token ]","title":"tokenize"},{"location":"api/data/tokenizers/letters_digits_tokenizer/","text":"[ allennlp .data .tokenizers .letters_digits_tokenizer ] LettersDigitsTokenizer # class LettersDigitsTokenizer ( Tokenizer ) A Tokenizer which keeps runs of (unicode) letters and runs of digits together, while every other non-whitespace character becomes a separate word. Registered as a Tokenizer with name \"letters_digits\". tokenize # class LettersDigitsTokenizer ( Tokenizer ): | ... | @overrides | def tokenize ( self , text : str ) -> List [ Token ] We use the [^\\W\\d_] pattern as a trick to match unicode letters","title":"letters_digits_tokenizer"},{"location":"api/data/tokenizers/letters_digits_tokenizer/#lettersdigitstokenizer","text":"class LettersDigitsTokenizer ( Tokenizer ) A Tokenizer which keeps runs of (unicode) letters and runs of digits together, while every other non-whitespace character becomes a separate word. Registered as a Tokenizer with name \"letters_digits\".","title":"LettersDigitsTokenizer"},{"location":"api/data/tokenizers/letters_digits_tokenizer/#tokenize","text":"class LettersDigitsTokenizer ( Tokenizer ): | ... | @overrides | def tokenize ( self , text : str ) -> List [ Token ] We use the [^\\W\\d_] pattern as a trick to match unicode letters","title":"tokenize"},{"location":"api/data/tokenizers/pretrained_transformer_tokenizer/","text":"[ allennlp .data .tokenizers .pretrained_transformer_tokenizer ] PretrainedTransformerTokenizer # class PretrainedTransformerTokenizer ( Tokenizer ): | def __init__ ( | self , | model_name : str , | add_special_tokens : bool = True , | max_length : Optional [ int ] = None , | stride : int = 0 , | truncation_strategy : str = \"longest_first\" , | tokenizer_kwargs : Optional [ Dict [ str , Any ]] = None | ) -> None A PretrainedTransformerTokenizer uses a model from HuggingFace's transformers library to tokenize some input text. This often means wordpieces (where 'AllenNLP is awesome' might get split into ['Allen', '##NL', '##P', 'is', 'awesome'] ), but it could also use byte-pair encoding, or some other tokenization, depending on the pretrained model that you're using. We take a model name as an input parameter, which we will pass to AutoTokenizer.from_pretrained . We also add special tokens relative to the pretrained model and truncate the sequences. This tokenizer also indexes tokens and adds the indexes to the Token fields so that they can be picked up by PretrainedTransformerIndexer . Registered as a Tokenizer with name \"pretrained_transformer\". Parameters model_name : str The name of the pretrained wordpiece tokenizer to use. add_special_tokens : bool , optional (default = True ) If set to True , the sequences will be encoded with the special tokens relative to their model. max_length : int , optional (default = None ) If set to a number, will limit the total sequence returned so that it has a maximum length. If there are overflowing tokens, those will be added to the returned dictionary stride : int , optional (default = 0 ) If set to a number along with max_length, the overflowing tokens returned will contain some tokens from the main sequence returned. The value of this argument defines the number of additional tokens. truncation_strategy : str , optional (default = 'longest_first' ) String selected in the following options: 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length starting from the longest one at each token (when there is a pair of input sequences) 'only_first': Only truncate the first sequence 'only_second': Only truncate the second sequence 'do_not_truncate': Do not truncate (raise an error if the input sequence is longer than max_length) tokenizer_kwargs : Dict[str, Any] Dictionary with additional arguments for AutoTokenizer.from_pretrained . tokenizer_lowercases # class PretrainedTransformerTokenizer ( Tokenizer ): | ... | @staticmethod | def tokenizer_lowercases ( tokenizer : PreTrainedTokenizer ) -> bool Huggingface tokenizers have different ways of remembering whether they lowercase or not. Detecting it this way seems like the least brittle way to do it. tokenize # class PretrainedTransformerTokenizer ( Tokenizer ): | ... | @overrides | def tokenize ( self , text : str ) -> List [ Token ] This method only handles a single sentence (or sequence) of text. intra_word_tokenize # class PretrainedTransformerTokenizer ( Tokenizer ): | ... | def intra_word_tokenize ( | self , | string_tokens : List [ str ] | ) -> Tuple [ List [ Token ], List [ Optional [ Tuple [ int , int ]]]] Tokenizes each word into wordpieces separately and returns the wordpiece IDs. Also calculates offsets such that tokens[offsets[i][0]:offsets[i][1] + 1] corresponds to the original i-th token. This function inserts special tokens. intra_word_tokenize_sentence_pair # class PretrainedTransformerTokenizer ( Tokenizer ): | ... | def intra_word_tokenize_sentence_pair ( | self , | string_tokens_a : List [ str ], | string_tokens_b : List [ str ] | ) -> Tuple [ List [ Token ], List [ Tuple [ int , int ]], List [ Tuple [ int , int ]]] Tokenizes each word into wordpieces separately and returns the wordpiece IDs. Also calculates offsets such that wordpieces[offsets[i][0]:offsets[i][1] + 1] corresponds to the original i-th token. This function inserts special tokens. add_special_tokens # class PretrainedTransformerTokenizer ( Tokenizer ): | ... | def add_special_tokens ( | self , | tokens1 : List [ Token ], | tokens2 : Optional [ List [ Token ]] = None | ) -> List [ Token ] num_special_tokens_for_sequence # class PretrainedTransformerTokenizer ( Tokenizer ): | ... | def num_special_tokens_for_sequence ( self ) -> int num_special_tokens_for_pair # class PretrainedTransformerTokenizer ( Tokenizer ): | ... | def num_special_tokens_for_pair ( self ) -> int","title":"pretrained_transformer_tokenizer"},{"location":"api/data/tokenizers/pretrained_transformer_tokenizer/#pretrainedtransformertokenizer","text":"class PretrainedTransformerTokenizer ( Tokenizer ): | def __init__ ( | self , | model_name : str , | add_special_tokens : bool = True , | max_length : Optional [ int ] = None , | stride : int = 0 , | truncation_strategy : str = \"longest_first\" , | tokenizer_kwargs : Optional [ Dict [ str , Any ]] = None | ) -> None A PretrainedTransformerTokenizer uses a model from HuggingFace's transformers library to tokenize some input text. This often means wordpieces (where 'AllenNLP is awesome' might get split into ['Allen', '##NL', '##P', 'is', 'awesome'] ), but it could also use byte-pair encoding, or some other tokenization, depending on the pretrained model that you're using. We take a model name as an input parameter, which we will pass to AutoTokenizer.from_pretrained . We also add special tokens relative to the pretrained model and truncate the sequences. This tokenizer also indexes tokens and adds the indexes to the Token fields so that they can be picked up by PretrainedTransformerIndexer . Registered as a Tokenizer with name \"pretrained_transformer\". Parameters model_name : str The name of the pretrained wordpiece tokenizer to use. add_special_tokens : bool , optional (default = True ) If set to True , the sequences will be encoded with the special tokens relative to their model. max_length : int , optional (default = None ) If set to a number, will limit the total sequence returned so that it has a maximum length. If there are overflowing tokens, those will be added to the returned dictionary stride : int , optional (default = 0 ) If set to a number along with max_length, the overflowing tokens returned will contain some tokens from the main sequence returned. The value of this argument defines the number of additional tokens. truncation_strategy : str , optional (default = 'longest_first' ) String selected in the following options: 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length starting from the longest one at each token (when there is a pair of input sequences) 'only_first': Only truncate the first sequence 'only_second': Only truncate the second sequence 'do_not_truncate': Do not truncate (raise an error if the input sequence is longer than max_length) tokenizer_kwargs : Dict[str, Any] Dictionary with additional arguments for AutoTokenizer.from_pretrained .","title":"PretrainedTransformerTokenizer"},{"location":"api/data/tokenizers/pretrained_transformer_tokenizer/#tokenizer_lowercases","text":"class PretrainedTransformerTokenizer ( Tokenizer ): | ... | @staticmethod | def tokenizer_lowercases ( tokenizer : PreTrainedTokenizer ) -> bool Huggingface tokenizers have different ways of remembering whether they lowercase or not. Detecting it this way seems like the least brittle way to do it.","title":"tokenizer_lowercases"},{"location":"api/data/tokenizers/pretrained_transformer_tokenizer/#tokenize","text":"class PretrainedTransformerTokenizer ( Tokenizer ): | ... | @overrides | def tokenize ( self , text : str ) -> List [ Token ] This method only handles a single sentence (or sequence) of text.","title":"tokenize"},{"location":"api/data/tokenizers/pretrained_transformer_tokenizer/#intra_word_tokenize","text":"class PretrainedTransformerTokenizer ( Tokenizer ): | ... | def intra_word_tokenize ( | self , | string_tokens : List [ str ] | ) -> Tuple [ List [ Token ], List [ Optional [ Tuple [ int , int ]]]] Tokenizes each word into wordpieces separately and returns the wordpiece IDs. Also calculates offsets such that tokens[offsets[i][0]:offsets[i][1] + 1] corresponds to the original i-th token. This function inserts special tokens.","title":"intra_word_tokenize"},{"location":"api/data/tokenizers/pretrained_transformer_tokenizer/#intra_word_tokenize_sentence_pair","text":"class PretrainedTransformerTokenizer ( Tokenizer ): | ... | def intra_word_tokenize_sentence_pair ( | self , | string_tokens_a : List [ str ], | string_tokens_b : List [ str ] | ) -> Tuple [ List [ Token ], List [ Tuple [ int , int ]], List [ Tuple [ int , int ]]] Tokenizes each word into wordpieces separately and returns the wordpiece IDs. Also calculates offsets such that wordpieces[offsets[i][0]:offsets[i][1] + 1] corresponds to the original i-th token. This function inserts special tokens.","title":"intra_word_tokenize_sentence_pair"},{"location":"api/data/tokenizers/pretrained_transformer_tokenizer/#add_special_tokens","text":"class PretrainedTransformerTokenizer ( Tokenizer ): | ... | def add_special_tokens ( | self , | tokens1 : List [ Token ], | tokens2 : Optional [ List [ Token ]] = None | ) -> List [ Token ]","title":"add_special_tokens"},{"location":"api/data/tokenizers/pretrained_transformer_tokenizer/#num_special_tokens_for_sequence","text":"class PretrainedTransformerTokenizer ( Tokenizer ): | ... | def num_special_tokens_for_sequence ( self ) -> int","title":"num_special_tokens_for_sequence"},{"location":"api/data/tokenizers/pretrained_transformer_tokenizer/#num_special_tokens_for_pair","text":"class PretrainedTransformerTokenizer ( Tokenizer ): | ... | def num_special_tokens_for_pair ( self ) -> int","title":"num_special_tokens_for_pair"},{"location":"api/data/tokenizers/sentence_splitter/","text":"[ allennlp .data .tokenizers .sentence_splitter ] SentenceSplitter # class SentenceSplitter ( Registrable ) A SentenceSplitter splits strings into sentences. default_implementation # class SentenceSplitter ( Registrable ): | ... | default_implementation = \"spacy\" split_sentences # class SentenceSplitter ( Registrable ): | ... | def split_sentences ( self , text : str ) -> List [ str ] Splits a text str paragraph into a list of str , where each is a sentence. batch_split_sentences # class SentenceSplitter ( Registrable ): | ... | def batch_split_sentences ( self , texts : List [ str ]) -> List [ List [ str ]] Default implementation is to just iterate over the texts and call split_sentences . SpacySentenceSplitter # class SpacySentenceSplitter ( SentenceSplitter ): | def __init__ ( | self , | language : str = \"en_core_web_sm\" , | rule_based : bool = False | ) -> None A SentenceSplitter that uses spaCy's built-in sentence boundary detection. Spacy's default sentence splitter uses a dependency parse to detect sentence boundaries, so it is slow, but accurate. Another option is to use rule-based sentence boundary detection. It's fast and has a small memory footprint, since it uses punctuation to detect sentence boundaries. This can be activated with the rule_based flag. By default, SpacySentenceSplitter calls the default spacy boundary detector. Registered as a SentenceSplitter with name \"spacy\". split_sentences # class SpacySentenceSplitter ( SentenceSplitter ): | ... | @overrides | def split_sentences ( self , text : str ) -> List [ str ] batch_split_sentences # class SpacySentenceSplitter ( SentenceSplitter ): | ... | @overrides | def batch_split_sentences ( self , texts : List [ str ]) -> List [ List [ str ]] This method lets you take advantage of spacy's batch processing.","title":"sentence_splitter"},{"location":"api/data/tokenizers/sentence_splitter/#sentencesplitter","text":"class SentenceSplitter ( Registrable ) A SentenceSplitter splits strings into sentences.","title":"SentenceSplitter"},{"location":"api/data/tokenizers/sentence_splitter/#default_implementation","text":"class SentenceSplitter ( Registrable ): | ... | default_implementation = \"spacy\"","title":"default_implementation"},{"location":"api/data/tokenizers/sentence_splitter/#split_sentences","text":"class SentenceSplitter ( Registrable ): | ... | def split_sentences ( self , text : str ) -> List [ str ] Splits a text str paragraph into a list of str , where each is a sentence.","title":"split_sentences"},{"location":"api/data/tokenizers/sentence_splitter/#batch_split_sentences","text":"class SentenceSplitter ( Registrable ): | ... | def batch_split_sentences ( self , texts : List [ str ]) -> List [ List [ str ]] Default implementation is to just iterate over the texts and call split_sentences .","title":"batch_split_sentences"},{"location":"api/data/tokenizers/sentence_splitter/#spacysentencesplitter","text":"class SpacySentenceSplitter ( SentenceSplitter ): | def __init__ ( | self , | language : str = \"en_core_web_sm\" , | rule_based : bool = False | ) -> None A SentenceSplitter that uses spaCy's built-in sentence boundary detection. Spacy's default sentence splitter uses a dependency parse to detect sentence boundaries, so it is slow, but accurate. Another option is to use rule-based sentence boundary detection. It's fast and has a small memory footprint, since it uses punctuation to detect sentence boundaries. This can be activated with the rule_based flag. By default, SpacySentenceSplitter calls the default spacy boundary detector. Registered as a SentenceSplitter with name \"spacy\".","title":"SpacySentenceSplitter"},{"location":"api/data/tokenizers/sentence_splitter/#split_sentences_1","text":"class SpacySentenceSplitter ( SentenceSplitter ): | ... | @overrides | def split_sentences ( self , text : str ) -> List [ str ]","title":"split_sentences"},{"location":"api/data/tokenizers/sentence_splitter/#batch_split_sentences_1","text":"class SpacySentenceSplitter ( SentenceSplitter ): | ... | @overrides | def batch_split_sentences ( self , texts : List [ str ]) -> List [ List [ str ]] This method lets you take advantage of spacy's batch processing.","title":"batch_split_sentences"},{"location":"api/data/tokenizers/spacy_tokenizer/","text":"[ allennlp .data .tokenizers .spacy_tokenizer ] SpacyTokenizer # class SpacyTokenizer ( Tokenizer ): | def __init__ ( | self , | language : str = \"en_core_web_sm\" , | pos_tags : bool = False , | parse : bool = False , | ner : bool = False , | keep_spacy_tokens : bool = False , | split_on_spaces : bool = False , | start_tokens : Optional [ List [ str ]] = None , | end_tokens : Optional [ List [ str ]] = None | ) -> None A Tokenizer that uses spaCy's tokenizer. It's fast and reasonable - this is the recommended Tokenizer . By default it will return allennlp Tokens, which are small, efficient NamedTuples (and are serializable). If you want to keep the original spaCy tokens, pass keep_spacy_tokens=True. Note that we leave one particular piece of post-processing for later: the decision of whether or not to lowercase the token. This is for two reasons: (1) if you want to make two different casing decisions for whatever reason, you won't have to run the tokenizer twice, and more importantly (2) if you want to lowercase words for your word embedding, but retain capitalization in a character-level representation, we need to retain the capitalization here. Registered as a Tokenizer with name \"spacy\", which is currently the default. Parameters language : str , optional (default = \"en_core_web_sm\" ) Spacy model name. pos_tags : bool , optional (default = False ) If True , performs POS tagging with spacy model on the tokens. Generally used in conjunction with PosTagIndexer . parse : bool , optional (default = False ) If True , performs dependency parsing with spacy model on the tokens. Generally used in conjunction with DepLabelIndexer . ner : bool , optional (default = False ) If True , performs dependency parsing with spacy model on the tokens. Generally used in conjunction with NerTagIndexer . keep_spacy_tokens : bool , optional (default = False ) If True , will preserve spacy token objects, We copy spacy tokens into our own class by default instead because spacy Cython Tokens can't be pickled. split_on_spaces : bool , optional (default = False ) If True , will split by spaces without performing tokenization. Used when your data is already tokenized, but you want to perform pos, ner or parsing on the tokens. start_tokens : Optional[List[str]] , optional (default = None ) If given, these tokens will be added to the beginning of every string we tokenize. end_tokens : Optional[List[str]] , optional (default = None ) If given, these tokens will be added to the end of every string we tokenize. batch_tokenize # class SpacyTokenizer ( Tokenizer ): | ... | @overrides | def batch_tokenize ( self , texts : List [ str ]) -> List [ List [ Token ]] tokenize # class SpacyTokenizer ( Tokenizer ): | ... | @overrides | def tokenize ( self , text : str ) -> List [ Token ] This works because our Token class matches spacy's. __call__ # class _WhitespaceSpacyTokenizer : | ... | def __call__ ( self , text )","title":"spacy_tokenizer"},{"location":"api/data/tokenizers/spacy_tokenizer/#spacytokenizer","text":"class SpacyTokenizer ( Tokenizer ): | def __init__ ( | self , | language : str = \"en_core_web_sm\" , | pos_tags : bool = False , | parse : bool = False , | ner : bool = False , | keep_spacy_tokens : bool = False , | split_on_spaces : bool = False , | start_tokens : Optional [ List [ str ]] = None , | end_tokens : Optional [ List [ str ]] = None | ) -> None A Tokenizer that uses spaCy's tokenizer. It's fast and reasonable - this is the recommended Tokenizer . By default it will return allennlp Tokens, which are small, efficient NamedTuples (and are serializable). If you want to keep the original spaCy tokens, pass keep_spacy_tokens=True. Note that we leave one particular piece of post-processing for later: the decision of whether or not to lowercase the token. This is for two reasons: (1) if you want to make two different casing decisions for whatever reason, you won't have to run the tokenizer twice, and more importantly (2) if you want to lowercase words for your word embedding, but retain capitalization in a character-level representation, we need to retain the capitalization here. Registered as a Tokenizer with name \"spacy\", which is currently the default. Parameters language : str , optional (default = \"en_core_web_sm\" ) Spacy model name. pos_tags : bool , optional (default = False ) If True , performs POS tagging with spacy model on the tokens. Generally used in conjunction with PosTagIndexer . parse : bool , optional (default = False ) If True , performs dependency parsing with spacy model on the tokens. Generally used in conjunction with DepLabelIndexer . ner : bool , optional (default = False ) If True , performs dependency parsing with spacy model on the tokens. Generally used in conjunction with NerTagIndexer . keep_spacy_tokens : bool , optional (default = False ) If True , will preserve spacy token objects, We copy spacy tokens into our own class by default instead because spacy Cython Tokens can't be pickled. split_on_spaces : bool , optional (default = False ) If True , will split by spaces without performing tokenization. Used when your data is already tokenized, but you want to perform pos, ner or parsing on the tokens. start_tokens : Optional[List[str]] , optional (default = None ) If given, these tokens will be added to the beginning of every string we tokenize. end_tokens : Optional[List[str]] , optional (default = None ) If given, these tokens will be added to the end of every string we tokenize.","title":"SpacyTokenizer"},{"location":"api/data/tokenizers/spacy_tokenizer/#batch_tokenize","text":"class SpacyTokenizer ( Tokenizer ): | ... | @overrides | def batch_tokenize ( self , texts : List [ str ]) -> List [ List [ Token ]]","title":"batch_tokenize"},{"location":"api/data/tokenizers/spacy_tokenizer/#tokenize","text":"class SpacyTokenizer ( Tokenizer ): | ... | @overrides | def tokenize ( self , text : str ) -> List [ Token ] This works because our Token class matches spacy's.","title":"tokenize"},{"location":"api/data/tokenizers/spacy_tokenizer/#__call__","text":"class _WhitespaceSpacyTokenizer : | ... | def __call__ ( self , text )","title":"__call__"},{"location":"api/data/tokenizers/token/","text":"[ allennlp .data .tokenizers .token ] Token # class Token : | def __init__ ( | self , | text : str = None , | idx : int = None , | idx_end : int = None , | lemma_ : str = None , | pos_ : str = None , | tag_ : str = None , | dep_ : str = None , | ent_type_ : str = None , | text_id : int = None , | type_id : int = None | ) -> None A simple token representation, keeping track of the token's text, offset in the passage it was taken from, POS tag, dependency relation, and similar information. These fields match spacy's exactly, so we can just use a spacy token for this. Parameters text : str , optional The original text represented by this token. idx : int , optional The character offset of this token into the tokenized passage. idx_end : int , optional The character offset one past the last character in the tokenized passage. lemma_ : str , optional The lemma of this token. pos_ : str , optional The coarse-grained part of speech of this token. tag_ : str , optional The fine-grained part of speech of this token. dep_ : str , optional The dependency relation for this token. ent_type_ : str , optional The entity type (i.e., the NER tag) for this token. text_id : int , optional If your tokenizer returns integers instead of strings (e.g., because you're doing byte encoding, or some hash-based embedding), set this with the integer. If this is set, we will bypass the vocabulary when indexing this token, regardless of whether text is also set. You can also set text with the original text, if you want, so that you can still use a character-level representation in addition to a hash-based word embedding. type_id : int , optional Token type id used by some pretrained language models like original BERT The other fields on Token follow the fields on spacy's Token object; this is one we added, similar to spacy's lex_id . text # class Token : | ... | text : Optional [ str ] = None idx # class Token : | ... | idx : Optional [ int ] = None idx_end # class Token : | ... | idx_end : Optional [ int ] = None lemma_ # class Token : | ... | lemma_ : Optional [ str ] = None pos_ # class Token : | ... | pos_ : Optional [ str ] = None tag_ # class Token : | ... | tag_ : Optional [ str ] = None dep_ # class Token : | ... | dep_ : Optional [ str ] = None ent_type_ # class Token : | ... | ent_type_ : Optional [ str ] = None text_id # class Token : | ... | text_id : Optional [ int ] = None type_id # class Token : | ... | type_id : Optional [ int ] = None show_token # def show_token ( token : Token ) -> str","title":"token"},{"location":"api/data/tokenizers/token/#token","text":"class Token : | def __init__ ( | self , | text : str = None , | idx : int = None , | idx_end : int = None , | lemma_ : str = None , | pos_ : str = None , | tag_ : str = None , | dep_ : str = None , | ent_type_ : str = None , | text_id : int = None , | type_id : int = None | ) -> None A simple token representation, keeping track of the token's text, offset in the passage it was taken from, POS tag, dependency relation, and similar information. These fields match spacy's exactly, so we can just use a spacy token for this. Parameters text : str , optional The original text represented by this token. idx : int , optional The character offset of this token into the tokenized passage. idx_end : int , optional The character offset one past the last character in the tokenized passage. lemma_ : str , optional The lemma of this token. pos_ : str , optional The coarse-grained part of speech of this token. tag_ : str , optional The fine-grained part of speech of this token. dep_ : str , optional The dependency relation for this token. ent_type_ : str , optional The entity type (i.e., the NER tag) for this token. text_id : int , optional If your tokenizer returns integers instead of strings (e.g., because you're doing byte encoding, or some hash-based embedding), set this with the integer. If this is set, we will bypass the vocabulary when indexing this token, regardless of whether text is also set. You can also set text with the original text, if you want, so that you can still use a character-level representation in addition to a hash-based word embedding. type_id : int , optional Token type id used by some pretrained language models like original BERT The other fields on Token follow the fields on spacy's Token object; this is one we added, similar to spacy's lex_id .","title":"Token"},{"location":"api/data/tokenizers/token/#text","text":"class Token : | ... | text : Optional [ str ] = None","title":"text"},{"location":"api/data/tokenizers/token/#idx","text":"class Token : | ... | idx : Optional [ int ] = None","title":"idx"},{"location":"api/data/tokenizers/token/#idx_end","text":"class Token : | ... | idx_end : Optional [ int ] = None","title":"idx_end"},{"location":"api/data/tokenizers/token/#lemma_","text":"class Token : | ... | lemma_ : Optional [ str ] = None","title":"lemma_"},{"location":"api/data/tokenizers/token/#pos_","text":"class Token : | ... | pos_ : Optional [ str ] = None","title":"pos_"},{"location":"api/data/tokenizers/token/#tag_","text":"class Token : | ... | tag_ : Optional [ str ] = None","title":"tag_"},{"location":"api/data/tokenizers/token/#dep_","text":"class Token : | ... | dep_ : Optional [ str ] = None","title":"dep_"},{"location":"api/data/tokenizers/token/#ent_type_","text":"class Token : | ... | ent_type_ : Optional [ str ] = None","title":"ent_type_"},{"location":"api/data/tokenizers/token/#text_id","text":"class Token : | ... | text_id : Optional [ int ] = None","title":"text_id"},{"location":"api/data/tokenizers/token/#type_id","text":"class Token : | ... | type_id : Optional [ int ] = None","title":"type_id"},{"location":"api/data/tokenizers/token/#show_token","text":"def show_token ( token : Token ) -> str","title":"show_token"},{"location":"api/data/tokenizers/tokenizer/","text":"[ allennlp .data .tokenizers .tokenizer ] Tokenizer # class Tokenizer ( Registrable ) A Tokenizer splits strings of text into tokens. Typically, this either splits text into word tokens or character tokens, and those are the two tokenizer subclasses we have implemented here, though you could imagine wanting to do other kinds of tokenization for structured or other inputs. See the parameters to, e.g., .SpacyTokenizer , or whichever tokenizer you want to use. If the base input to your model is words, you should use a .SpacyTokenizer , even if you also want to have a character-level encoder to get an additional vector for each word token. Splitting word tokens into character arrays is handled separately, in the ..token_representations.TokenRepresentation class. default_implementation # class Tokenizer ( Registrable ): | ... | default_implementation = \"spacy\" batch_tokenize # class Tokenizer ( Registrable ): | ... | def batch_tokenize ( self , texts : List [ str ]) -> List [ List [ Token ]] Batches together tokenization of several texts, in case that is faster for particular tokenizers. By default we just do this without batching. Override this in your tokenizer if you have a good way of doing batched computation. tokenize # class Tokenizer ( Registrable ): | ... | def tokenize ( self , text : str ) -> List [ Token ] Actually implements splitting words into tokens. Returns tokens : List[Token] add_special_tokens # class Tokenizer ( Registrable ): | ... | def add_special_tokens ( | self , | tokens1 : List [ Token ], | tokens2 : Optional [ List [ Token ]] = None | ) -> List [ Token ] Adds special tokens to tokenized text. These are tokens like [CLS] or [SEP]. Not all tokenizers do this. The default is to just return the tokens unchanged. Parameters tokens1 : List[Token] The list of tokens to add special tokens to. tokens2 : Optional[List[Token]] An optional second list of tokens. This will be concatenated with tokens1 . Special tokens will be added as appropriate. Returns tokens : List[Token] The combined list of tokens, with special tokens added. num_special_tokens_for_sequence # class Tokenizer ( Registrable ): | ... | def num_special_tokens_for_sequence ( self ) -> int Returns the number of special tokens added for a single sequence. num_special_tokens_for_pair # class Tokenizer ( Registrable ): | ... | def num_special_tokens_for_pair ( self ) -> int Returns the number of special tokens added for a pair of sequences.","title":"tokenizer"},{"location":"api/data/tokenizers/tokenizer/#tokenizer","text":"class Tokenizer ( Registrable ) A Tokenizer splits strings of text into tokens. Typically, this either splits text into word tokens or character tokens, and those are the two tokenizer subclasses we have implemented here, though you could imagine wanting to do other kinds of tokenization for structured or other inputs. See the parameters to, e.g., .SpacyTokenizer , or whichever tokenizer you want to use. If the base input to your model is words, you should use a .SpacyTokenizer , even if you also want to have a character-level encoder to get an additional vector for each word token. Splitting word tokens into character arrays is handled separately, in the ..token_representations.TokenRepresentation class.","title":"Tokenizer"},{"location":"api/data/tokenizers/tokenizer/#default_implementation","text":"class Tokenizer ( Registrable ): | ... | default_implementation = \"spacy\"","title":"default_implementation"},{"location":"api/data/tokenizers/tokenizer/#batch_tokenize","text":"class Tokenizer ( Registrable ): | ... | def batch_tokenize ( self , texts : List [ str ]) -> List [ List [ Token ]] Batches together tokenization of several texts, in case that is faster for particular tokenizers. By default we just do this without batching. Override this in your tokenizer if you have a good way of doing batched computation.","title":"batch_tokenize"},{"location":"api/data/tokenizers/tokenizer/#tokenize","text":"class Tokenizer ( Registrable ): | ... | def tokenize ( self , text : str ) -> List [ Token ] Actually implements splitting words into tokens. Returns tokens : List[Token]","title":"tokenize"},{"location":"api/data/tokenizers/tokenizer/#add_special_tokens","text":"class Tokenizer ( Registrable ): | ... | def add_special_tokens ( | self , | tokens1 : List [ Token ], | tokens2 : Optional [ List [ Token ]] = None | ) -> List [ Token ] Adds special tokens to tokenized text. These are tokens like [CLS] or [SEP]. Not all tokenizers do this. The default is to just return the tokens unchanged. Parameters tokens1 : List[Token] The list of tokens to add special tokens to. tokens2 : Optional[List[Token]] An optional second list of tokens. This will be concatenated with tokens1 . Special tokens will be added as appropriate. Returns tokens : List[Token] The combined list of tokens, with special tokens added.","title":"add_special_tokens"},{"location":"api/data/tokenizers/tokenizer/#num_special_tokens_for_sequence","text":"class Tokenizer ( Registrable ): | ... | def num_special_tokens_for_sequence ( self ) -> int Returns the number of special tokens added for a single sequence.","title":"num_special_tokens_for_sequence"},{"location":"api/data/tokenizers/tokenizer/#num_special_tokens_for_pair","text":"class Tokenizer ( Registrable ): | ... | def num_special_tokens_for_pair ( self ) -> int Returns the number of special tokens added for a pair of sequences.","title":"num_special_tokens_for_pair"},{"location":"api/data/tokenizers/whitespace_tokenizer/","text":"[ allennlp .data .tokenizers .whitespace_tokenizer ] WhitespaceTokenizer # class WhitespaceTokenizer ( Tokenizer ) A Tokenizer that assumes you've already done your own tokenization somehow and have separated the tokens by spaces. We just split the input string on whitespace and return the resulting list. Note that we use text.split() , which means that the amount of whitespace between the tokens does not matter. This will never result in spaces being included as tokens. Registered as a Tokenizer with name \"whitespace\" and \"just_spaces\". tokenize # class WhitespaceTokenizer ( Tokenizer ): | ... | @overrides | def tokenize ( self , text : str ) -> List [ Token ]","title":"whitespace_tokenizer"},{"location":"api/data/tokenizers/whitespace_tokenizer/#whitespacetokenizer","text":"class WhitespaceTokenizer ( Tokenizer ) A Tokenizer that assumes you've already done your own tokenization somehow and have separated the tokens by spaces. We just split the input string on whitespace and return the resulting list. Note that we use text.split() , which means that the amount of whitespace between the tokens does not matter. This will never result in spaces being included as tokens. Registered as a Tokenizer with name \"whitespace\" and \"just_spaces\".","title":"WhitespaceTokenizer"},{"location":"api/data/tokenizers/whitespace_tokenizer/#tokenize","text":"class WhitespaceTokenizer ( Tokenizer ): | ... | @overrides | def tokenize ( self , text : str ) -> List [ Token ]","title":"tokenize"},{"location":"api/interpret/attackers/attacker/","text":"[ allennlp .interpret .attackers .attacker ] Attacker # class Attacker ( Registrable ): | def __init__ ( self , predictor : Predictor ) -> None An Attacker will modify an input (e.g., add or delete tokens) to try to change an AllenNLP Predictor's output in a desired manner (e.g., make it incorrect). initialize # class Attacker ( Registrable ): | ... | def initialize ( self ) Initializes any components of the Attacker that are expensive to compute, so that they are not created on init (). Default implementation is pass . attack_from_json # class Attacker ( Registrable ): | ... | def attack_from_json ( | self , | inputs : JsonDict , | input_field_to_attack : str , | grad_input_field : str , | ignore_tokens : List [ str ], | target : JsonDict | ) -> JsonDict This function finds a modification to the input text that would change the model's prediction in some desired manner (e.g., an adversarial attack). Parameters inputs : JsonDict The input you want to attack (the same as the argument to a Predictor, e.g., predict_json()). input_field_to_attack : str The key in the inputs JsonDict you want to attack, e.g., tokens . grad_input_field : str The field in the gradients dictionary that contains the input gradients. For example, grad_input_1 will be the field for single input tasks. See get_gradients() in Predictor for more information on field names. target : JsonDict If given, this is a targeted attack, trying to change the prediction to a particular value, instead of just changing it from its original prediction. Subclasses are not required to accept this argument, as not all attacks make sense as targeted attacks. Perhaps that means we should make the API more crisp, but adding another class is not worth it. Returns reduced_input : JsonDict Contains the final, sanitized input after adversarial modification.","title":"attacker"},{"location":"api/interpret/attackers/attacker/#attacker","text":"class Attacker ( Registrable ): | def __init__ ( self , predictor : Predictor ) -> None An Attacker will modify an input (e.g., add or delete tokens) to try to change an AllenNLP Predictor's output in a desired manner (e.g., make it incorrect).","title":"Attacker"},{"location":"api/interpret/attackers/attacker/#initialize","text":"class Attacker ( Registrable ): | ... | def initialize ( self ) Initializes any components of the Attacker that are expensive to compute, so that they are not created on init (). Default implementation is pass .","title":"initialize"},{"location":"api/interpret/attackers/attacker/#attack_from_json","text":"class Attacker ( Registrable ): | ... | def attack_from_json ( | self , | inputs : JsonDict , | input_field_to_attack : str , | grad_input_field : str , | ignore_tokens : List [ str ], | target : JsonDict | ) -> JsonDict This function finds a modification to the input text that would change the model's prediction in some desired manner (e.g., an adversarial attack). Parameters inputs : JsonDict The input you want to attack (the same as the argument to a Predictor, e.g., predict_json()). input_field_to_attack : str The key in the inputs JsonDict you want to attack, e.g., tokens . grad_input_field : str The field in the gradients dictionary that contains the input gradients. For example, grad_input_1 will be the field for single input tasks. See get_gradients() in Predictor for more information on field names. target : JsonDict If given, this is a targeted attack, trying to change the prediction to a particular value, instead of just changing it from its original prediction. Subclasses are not required to accept this argument, as not all attacks make sense as targeted attacks. Perhaps that means we should make the API more crisp, but adding another class is not worth it. Returns reduced_input : JsonDict Contains the final, sanitized input after adversarial modification.","title":"attack_from_json"},{"location":"api/interpret/attackers/hotflip/","text":"[ allennlp .interpret .attackers .hotflip ] DEFAULT_IGNORE_TOKENS # DEFAULT_IGNORE_TOKENS = [ \"@@NULL@@\" , \".\" , \",\" , \";\" , \"!\" , \"?\" , \"[MASK]\" , \"[SEP]\" , \"[CLS]\" ] Hotflip # class Hotflip ( Attacker ): | def __init__ ( | self , | predictor : Predictor , | vocab_namespace : str = \"tokens\" , | max_tokens : int = 5000 | ) -> None Runs the HotFlip style attack at the word-level https://arxiv.org/abs/1712.06751. We use the first-order taylor approximation described in https://arxiv.org/abs/1903.06620, in the function _first_order_taylor() . We try to re-use the embedding matrix from the model when deciding what other words to flip a token to. For a large class of models, this is straightforward. When there is a character-level encoder, however (e.g., with ELMo, any char-CNN, etc.), or a combination of encoders (e.g., ELMo + glove), we need to construct a fake embedding matrix that we can use in _first_order_taylor() . We do this by getting a list of words from the model's vocabulary and embedding them using the encoder. This can be expensive, both in terms of time and memory usage, so we take a max_tokens parameter to limit the size of this fake embedding matrix. This also requires a model to have a token vocabulary in the first place, which can be problematic for models that only have character vocabularies. Registered as an Attacker with name \"hotflip\". Parameters predictor : Predictor The model (inside a Predictor) that we're attacking. We use this to get gradients and predictions. vocab_namespace : str , optional (default = 'tokens' ) We use this to know three things: (1) which tokens we should ignore when producing flips (we don't consider non-alphanumeric tokens); (2) what the string value is of the token that we produced, so we can show something human-readable to the user; and (3) if we need to construct a fake embedding matrix, we use the tokens in the vocabulary as flip candidates. max_tokens : int , optional (default = 5000 ) This is only used when we need to construct a fake embedding matrix. That matrix can take a lot of memory when the vocab size is large. This parameter puts a cap on the number of tokens to use, so the fake embedding matrix doesn't take as much memory. initialize # class Hotflip ( Attacker ): | ... | def initialize ( self ) Call this function before running attack_from_json(). We put the call to _construct_embedding_matrix() in this function to prevent a large amount of compute being done when init () is called. attack_from_json # class Hotflip ( Attacker ): | ... | def attack_from_json ( | self , | inputs : JsonDict , | input_field_to_attack : str = \"tokens\" , | grad_input_field : str = \"grad_input_1\" , | ignore_tokens : List [ str ] = None , | target : JsonDict = None | ) -> JsonDict Replaces one token at a time from the input until the model's prediction changes. input_field_to_attack is for example tokens , it says what the input field is called. grad_input_field is for example grad_input_1 , which is a key into a grads dictionary. The method computes the gradient w.r.t. the tokens, finds the token with the maximum gradient (by L2 norm), and replaces it with another token based on the first-order Taylor approximation of the loss. This process is iteratively repeated until the prediction changes. Once a token is replaced, it is not flipped again. Parameters inputs : JsonDict The model inputs, the same as what is passed to a Predictor . input_field_to_attack : str , optional (default = 'tokens' ) The field that has the tokens that we're going to be flipping. This must be a TextField . grad_input_field : str , optional (default = 'grad_input_1' ) If there is more than one field that gets embedded in your model (e.g., a question and a passage, or a premise and a hypothesis), this tells us the key to use to get the correct gradients. This selects from the output of Predictor.get_gradients . ignore_tokens : List[str] , optional (default = DEFAULT_IGNORE_TOKENS ) These tokens will not be flipped. The default list includes some simple punctuation, OOV and padding tokens, and common control tokens for BERT, etc. target : JsonDict , optional (default = None ) If given, this will be a targeted hotflip attack, where instead of just trying to change a model's prediction from what it current is predicting, we try to change it to a specific target value. This is a JsonDict because it needs to specify the field name and target value. For example, for a masked LM, this would be something like {\"words\": [\"she\"]} , because \"words\" is the field name, there is one mask token (hence the list of length one), and we want to change the prediction from whatever it was to \"she\" . attack_instance # class Hotflip ( Attacker ): | ... | def attack_instance ( | self , | instance : Instance , | inputs : JsonDict , | input_field_to_attack : str = \"tokens\" , | grad_input_field : str = \"grad_input_1\" , | ignore_tokens : List [ str ] = None , | target : JsonDict = None | ) -> Tuple [ List [ Token ], JsonDict ]","title":"hotflip"},{"location":"api/interpret/attackers/hotflip/#default_ignore_tokens","text":"DEFAULT_IGNORE_TOKENS = [ \"@@NULL@@\" , \".\" , \",\" , \";\" , \"!\" , \"?\" , \"[MASK]\" , \"[SEP]\" , \"[CLS]\" ]","title":"DEFAULT_IGNORE_TOKENS"},{"location":"api/interpret/attackers/hotflip/#hotflip","text":"class Hotflip ( Attacker ): | def __init__ ( | self , | predictor : Predictor , | vocab_namespace : str = \"tokens\" , | max_tokens : int = 5000 | ) -> None Runs the HotFlip style attack at the word-level https://arxiv.org/abs/1712.06751. We use the first-order taylor approximation described in https://arxiv.org/abs/1903.06620, in the function _first_order_taylor() . We try to re-use the embedding matrix from the model when deciding what other words to flip a token to. For a large class of models, this is straightforward. When there is a character-level encoder, however (e.g., with ELMo, any char-CNN, etc.), or a combination of encoders (e.g., ELMo + glove), we need to construct a fake embedding matrix that we can use in _first_order_taylor() . We do this by getting a list of words from the model's vocabulary and embedding them using the encoder. This can be expensive, both in terms of time and memory usage, so we take a max_tokens parameter to limit the size of this fake embedding matrix. This also requires a model to have a token vocabulary in the first place, which can be problematic for models that only have character vocabularies. Registered as an Attacker with name \"hotflip\". Parameters predictor : Predictor The model (inside a Predictor) that we're attacking. We use this to get gradients and predictions. vocab_namespace : str , optional (default = 'tokens' ) We use this to know three things: (1) which tokens we should ignore when producing flips (we don't consider non-alphanumeric tokens); (2) what the string value is of the token that we produced, so we can show something human-readable to the user; and (3) if we need to construct a fake embedding matrix, we use the tokens in the vocabulary as flip candidates. max_tokens : int , optional (default = 5000 ) This is only used when we need to construct a fake embedding matrix. That matrix can take a lot of memory when the vocab size is large. This parameter puts a cap on the number of tokens to use, so the fake embedding matrix doesn't take as much memory.","title":"Hotflip"},{"location":"api/interpret/attackers/hotflip/#initialize","text":"class Hotflip ( Attacker ): | ... | def initialize ( self ) Call this function before running attack_from_json(). We put the call to _construct_embedding_matrix() in this function to prevent a large amount of compute being done when init () is called.","title":"initialize"},{"location":"api/interpret/attackers/hotflip/#attack_from_json","text":"class Hotflip ( Attacker ): | ... | def attack_from_json ( | self , | inputs : JsonDict , | input_field_to_attack : str = \"tokens\" , | grad_input_field : str = \"grad_input_1\" , | ignore_tokens : List [ str ] = None , | target : JsonDict = None | ) -> JsonDict Replaces one token at a time from the input until the model's prediction changes. input_field_to_attack is for example tokens , it says what the input field is called. grad_input_field is for example grad_input_1 , which is a key into a grads dictionary. The method computes the gradient w.r.t. the tokens, finds the token with the maximum gradient (by L2 norm), and replaces it with another token based on the first-order Taylor approximation of the loss. This process is iteratively repeated until the prediction changes. Once a token is replaced, it is not flipped again. Parameters inputs : JsonDict The model inputs, the same as what is passed to a Predictor . input_field_to_attack : str , optional (default = 'tokens' ) The field that has the tokens that we're going to be flipping. This must be a TextField . grad_input_field : str , optional (default = 'grad_input_1' ) If there is more than one field that gets embedded in your model (e.g., a question and a passage, or a premise and a hypothesis), this tells us the key to use to get the correct gradients. This selects from the output of Predictor.get_gradients . ignore_tokens : List[str] , optional (default = DEFAULT_IGNORE_TOKENS ) These tokens will not be flipped. The default list includes some simple punctuation, OOV and padding tokens, and common control tokens for BERT, etc. target : JsonDict , optional (default = None ) If given, this will be a targeted hotflip attack, where instead of just trying to change a model's prediction from what it current is predicting, we try to change it to a specific target value. This is a JsonDict because it needs to specify the field name and target value. For example, for a masked LM, this would be something like {\"words\": [\"she\"]} , because \"words\" is the field name, there is one mask token (hence the list of length one), and we want to change the prediction from whatever it was to \"she\" .","title":"attack_from_json"},{"location":"api/interpret/attackers/hotflip/#attack_instance","text":"class Hotflip ( Attacker ): | ... | def attack_instance ( | self , | instance : Instance , | inputs : JsonDict , | input_field_to_attack : str = \"tokens\" , | grad_input_field : str = \"grad_input_1\" , | ignore_tokens : List [ str ] = None , | target : JsonDict = None | ) -> Tuple [ List [ Token ], JsonDict ]","title":"attack_instance"},{"location":"api/interpret/attackers/input_reduction/","text":"[ allennlp .interpret .attackers .input_reduction ] InputReduction # class InputReduction ( Attacker ): | def __init__ ( self , predictor : Predictor , beam_size : int = 3 ) -> None Runs the input reduction method from Pathologies of Neural Models Make Interpretations Difficult , which removes as many words as possible from the input without changing the model's prediction. The functions on this class handle a special case for NER by looking for a field called \"tags\" This check is brittle, i.e., the code could break if the name of this field has changed, or if a non-NER model has a field called \"tags\". Registered as an Attacker with name \"input-reduction\". attack_from_json # class InputReduction ( Attacker ): | ... | def attack_from_json ( | self , | inputs : JsonDict = None , | input_field_to_attack : str = \"tokens\" , | grad_input_field : str = \"grad_input_1\" , | ignore_tokens : List [ str ] = None , | target : JsonDict = None | )","title":"input_reduction"},{"location":"api/interpret/attackers/input_reduction/#inputreduction","text":"class InputReduction ( Attacker ): | def __init__ ( self , predictor : Predictor , beam_size : int = 3 ) -> None Runs the input reduction method from Pathologies of Neural Models Make Interpretations Difficult , which removes as many words as possible from the input without changing the model's prediction. The functions on this class handle a special case for NER by looking for a field called \"tags\" This check is brittle, i.e., the code could break if the name of this field has changed, or if a non-NER model has a field called \"tags\". Registered as an Attacker with name \"input-reduction\".","title":"InputReduction"},{"location":"api/interpret/attackers/input_reduction/#attack_from_json","text":"class InputReduction ( Attacker ): | ... | def attack_from_json ( | self , | inputs : JsonDict = None , | input_field_to_attack : str = \"tokens\" , | grad_input_field : str = \"grad_input_1\" , | ignore_tokens : List [ str ] = None , | target : JsonDict = None | )","title":"attack_from_json"},{"location":"api/interpret/attackers/utils/","text":"[ allennlp .interpret .attackers .utils ] get_fields_to_compare # def get_fields_to_compare ( inputs : JsonDict , instance : Instance , input_field_to_attack : str ) -> JsonDict Gets a list of the fields that should be checked for equality after an attack is performed. Parameters inputs : JsonDict The input you want to attack, similar to the argument to a Predictor, e.g., predict_json(). instance : Instance A labeled instance that is output from json_to_labeled_instances(). input_field_to_attack : str The key in the inputs JsonDict you want to attack, e.g., tokens. Returns fields : JsonDict The fields that must be compared for equality. instance_has_changed # def instance_has_changed ( instance : Instance , fields_to_compare : JsonDict )","title":"utils"},{"location":"api/interpret/attackers/utils/#get_fields_to_compare","text":"def get_fields_to_compare ( inputs : JsonDict , instance : Instance , input_field_to_attack : str ) -> JsonDict Gets a list of the fields that should be checked for equality after an attack is performed. Parameters inputs : JsonDict The input you want to attack, similar to the argument to a Predictor, e.g., predict_json(). instance : Instance A labeled instance that is output from json_to_labeled_instances(). input_field_to_attack : str The key in the inputs JsonDict you want to attack, e.g., tokens. Returns fields : JsonDict The fields that must be compared for equality.","title":"get_fields_to_compare"},{"location":"api/interpret/attackers/utils/#instance_has_changed","text":"def instance_has_changed ( instance : Instance , fields_to_compare : JsonDict )","title":"instance_has_changed"},{"location":"api/interpret/saliency_interpreters/integrated_gradient/","text":"[ allennlp .interpret .saliency_interpreters .integrated_gradient ] IntegratedGradient # class IntegratedGradient ( SaliencyInterpreter ) Interprets the prediction using Integrated Gradients (https://arxiv.org/abs/1703.01365) Registered as a SaliencyInterpreter with name \"integrated-gradient\". saliency_interpret_from_json # class IntegratedGradient ( SaliencyInterpreter ): | ... | def saliency_interpret_from_json ( self , inputs : JsonDict ) -> JsonDict Convert inputs to labeled instances","title":"integrated_gradient"},{"location":"api/interpret/saliency_interpreters/integrated_gradient/#integratedgradient","text":"class IntegratedGradient ( SaliencyInterpreter ) Interprets the prediction using Integrated Gradients (https://arxiv.org/abs/1703.01365) Registered as a SaliencyInterpreter with name \"integrated-gradient\".","title":"IntegratedGradient"},{"location":"api/interpret/saliency_interpreters/integrated_gradient/#saliency_interpret_from_json","text":"class IntegratedGradient ( SaliencyInterpreter ): | ... | def saliency_interpret_from_json ( self , inputs : JsonDict ) -> JsonDict Convert inputs to labeled instances","title":"saliency_interpret_from_json"},{"location":"api/interpret/saliency_interpreters/saliency_interpreter/","text":"[ allennlp .interpret .saliency_interpreters .saliency_interpreter ] SaliencyInterpreter # class SaliencyInterpreter ( Registrable ): | def __init__ ( self , predictor : Predictor ) -> None A SaliencyInterpreter interprets an AllenNLP Predictor's outputs by assigning a saliency score to each input token. saliency_interpret_from_json # class SaliencyInterpreter ( Registrable ): | ... | def saliency_interpret_from_json ( self , inputs : JsonDict ) -> JsonDict This function finds saliency values for each input token. Parameters inputs : JsonDict The input you want to interpret (the same as the argument to a Predictor, e.g., predict_json()). Returns interpretation : JsonDict Contains the normalized saliency values for each input token. The dict has entries for each instance in the inputs JsonDict, e.g., {instance_1: ..., instance_2:, ... } . Each one of those entries has entries for the saliency of the inputs, e.g., {grad_input_1: ..., grad_input_2: ... } .","title":"saliency_interpreter"},{"location":"api/interpret/saliency_interpreters/saliency_interpreter/#saliencyinterpreter","text":"class SaliencyInterpreter ( Registrable ): | def __init__ ( self , predictor : Predictor ) -> None A SaliencyInterpreter interprets an AllenNLP Predictor's outputs by assigning a saliency score to each input token.","title":"SaliencyInterpreter"},{"location":"api/interpret/saliency_interpreters/saliency_interpreter/#saliency_interpret_from_json","text":"class SaliencyInterpreter ( Registrable ): | ... | def saliency_interpret_from_json ( self , inputs : JsonDict ) -> JsonDict This function finds saliency values for each input token. Parameters inputs : JsonDict The input you want to interpret (the same as the argument to a Predictor, e.g., predict_json()). Returns interpretation : JsonDict Contains the normalized saliency values for each input token. The dict has entries for each instance in the inputs JsonDict, e.g., {instance_1: ..., instance_2:, ... } . Each one of those entries has entries for the saliency of the inputs, e.g., {grad_input_1: ..., grad_input_2: ... } .","title":"saliency_interpret_from_json"},{"location":"api/interpret/saliency_interpreters/simple_gradient/","text":"[ allennlp .interpret .saliency_interpreters .simple_gradient ] SimpleGradient # class SimpleGradient ( SaliencyInterpreter ) Registered as a SaliencyInterpreter with name \"simple-gradient\". saliency_interpret_from_json # class SimpleGradient ( SaliencyInterpreter ): | ... | def saliency_interpret_from_json ( self , inputs : JsonDict ) -> JsonDict Interprets the model's prediction for inputs. Gets the gradients of the loss with respect to the input and returns those gradients normalized and sanitized.","title":"simple_gradient"},{"location":"api/interpret/saliency_interpreters/simple_gradient/#simplegradient","text":"class SimpleGradient ( SaliencyInterpreter ) Registered as a SaliencyInterpreter with name \"simple-gradient\".","title":"SimpleGradient"},{"location":"api/interpret/saliency_interpreters/simple_gradient/#saliency_interpret_from_json","text":"class SimpleGradient ( SaliencyInterpreter ): | ... | def saliency_interpret_from_json ( self , inputs : JsonDict ) -> JsonDict Interprets the model's prediction for inputs. Gets the gradients of the loss with respect to the input and returns those gradients normalized and sanitized.","title":"saliency_interpret_from_json"},{"location":"api/interpret/saliency_interpreters/smooth_gradient/","text":"[ allennlp .interpret .saliency_interpreters .smooth_gradient ] SmoothGradient # class SmoothGradient ( SaliencyInterpreter ): | def __init__ ( self , predictor : Predictor ) -> None Interprets the prediction using SmoothGrad (https://arxiv.org/abs/1706.03825) Registered as a SaliencyInterpreter with name \"smooth-gradient\". saliency_interpret_from_json # class SmoothGradient ( SaliencyInterpreter ): | ... | def saliency_interpret_from_json ( self , inputs : JsonDict ) -> JsonDict Convert inputs to labeled instances","title":"smooth_gradient"},{"location":"api/interpret/saliency_interpreters/smooth_gradient/#smoothgradient","text":"class SmoothGradient ( SaliencyInterpreter ): | def __init__ ( self , predictor : Predictor ) -> None Interprets the prediction using SmoothGrad (https://arxiv.org/abs/1706.03825) Registered as a SaliencyInterpreter with name \"smooth-gradient\".","title":"SmoothGradient"},{"location":"api/interpret/saliency_interpreters/smooth_gradient/#saliency_interpret_from_json","text":"class SmoothGradient ( SaliencyInterpreter ): | ... | def saliency_interpret_from_json ( self , inputs : JsonDict ) -> JsonDict Convert inputs to labeled instances","title":"saliency_interpret_from_json"},{"location":"api/models/archival/","text":"[ allennlp .models .archival ] Helper functions for archiving models and restoring archived models. Archive # class Archive ( NamedTuple ) An archive comprises a Model and its experimental config model # class Archive ( NamedTuple ): | ... | model : Model = None config # class Archive ( NamedTuple ): | ... | config : Params = None extract_module # class Archive ( NamedTuple ): | ... | def extract_module ( self , path : str , freeze : bool = True ) -> Module This method can be used to load a module from the pretrained model archive. It is also used implicitly in FromParams based construction. So instead of using standard params to construct a module, you can instead load a pretrained module from the model archive directly. For eg, instead of using params like {\"type\": \"module_type\", ...}, you can use the following template:: { \"_pretrained\": { \"archive_file\": \"../path/to/model.tar.gz\", \"path\": \"path.to.module.in.model\", \"freeze\": False } } If you use this feature with FromParams, take care of the following caveat: Call to initializer(self) at end of model initializer can potentially wipe the transferred parameters by reinitializing them. This can happen if you have setup initializer regex that also matches parameters of the transferred module. To safe-guard against this, you can either update your initializer regex to prevent conflicting match or add extra initializer:: [ [\".*transferred_module_name.*\", \"prevent\"]] ] Parameters path : str Path of target module to be loaded from the model. Eg. \"_textfield_embedder.token_embedder_tokens\" freeze : bool , optional (default = True ) Whether to freeze the module parameters or not. CONFIG_NAME # CONFIG_NAME = \"config.json\" archive_model # def archive_model ( serialization_dir : Union [ str , PathLike ], weights : str = _DEFAULT_WEIGHTS , archive_path : Union [ str , PathLike ] = None ) -> None Archive the model weights, its training configuration, and its vocabulary to model.tar.gz . Parameters serialization_dir : str The directory where the weights and vocabulary are written out. weights : str , optional (default = _DEFAULT_WEIGHTS ) Which weights file to include in the archive. The default is best.th . archive_path : str , optional (default = None ) A full path to serialize the model to. The default is \"model.tar.gz\" inside the serialization_dir. If you pass a directory here, we'll serialize the model to \"model.tar.gz\" inside the directory. load_archive # def load_archive ( archive_file : str , cuda_device : int = - 1 , overrides : str = \"\" , weights_file : str = None ) -> Archive Instantiates an Archive from an archived tar.gz file. Parameters archive_file : str The archive file to load the model from. cuda_device : int , optional (default = -1 ) If cuda_device is >= 0, the model will be loaded onto the corresponding GPU. Otherwise it will be loaded onto the CPU. overrides : str , optional (default = \"\" ) JSON overrides to apply to the unarchived Params object. weights_file : str , optional (default = None ) The weights file to use. If unspecified, weights.th in the archive_file will be used.","title":"archival"},{"location":"api/models/archival/#archive","text":"class Archive ( NamedTuple ) An archive comprises a Model and its experimental config","title":"Archive"},{"location":"api/models/archival/#model","text":"class Archive ( NamedTuple ): | ... | model : Model = None","title":"model"},{"location":"api/models/archival/#config","text":"class Archive ( NamedTuple ): | ... | config : Params = None","title":"config"},{"location":"api/models/archival/#extract_module","text":"class Archive ( NamedTuple ): | ... | def extract_module ( self , path : str , freeze : bool = True ) -> Module This method can be used to load a module from the pretrained model archive. It is also used implicitly in FromParams based construction. So instead of using standard params to construct a module, you can instead load a pretrained module from the model archive directly. For eg, instead of using params like {\"type\": \"module_type\", ...}, you can use the following template:: { \"_pretrained\": { \"archive_file\": \"../path/to/model.tar.gz\", \"path\": \"path.to.module.in.model\", \"freeze\": False } } If you use this feature with FromParams, take care of the following caveat: Call to initializer(self) at end of model initializer can potentially wipe the transferred parameters by reinitializing them. This can happen if you have setup initializer regex that also matches parameters of the transferred module. To safe-guard against this, you can either update your initializer regex to prevent conflicting match or add extra initializer:: [ [\".*transferred_module_name.*\", \"prevent\"]] ] Parameters path : str Path of target module to be loaded from the model. Eg. \"_textfield_embedder.token_embedder_tokens\" freeze : bool , optional (default = True ) Whether to freeze the module parameters or not.","title":"extract_module"},{"location":"api/models/archival/#config_name","text":"CONFIG_NAME = \"config.json\"","title":"CONFIG_NAME"},{"location":"api/models/archival/#archive_model","text":"def archive_model ( serialization_dir : Union [ str , PathLike ], weights : str = _DEFAULT_WEIGHTS , archive_path : Union [ str , PathLike ] = None ) -> None Archive the model weights, its training configuration, and its vocabulary to model.tar.gz . Parameters serialization_dir : str The directory where the weights and vocabulary are written out. weights : str , optional (default = _DEFAULT_WEIGHTS ) Which weights file to include in the archive. The default is best.th . archive_path : str , optional (default = None ) A full path to serialize the model to. The default is \"model.tar.gz\" inside the serialization_dir. If you pass a directory here, we'll serialize the model to \"model.tar.gz\" inside the directory.","title":"archive_model"},{"location":"api/models/archival/#load_archive","text":"def load_archive ( archive_file : str , cuda_device : int = - 1 , overrides : str = \"\" , weights_file : str = None ) -> Archive Instantiates an Archive from an archived tar.gz file. Parameters archive_file : str The archive file to load the model from. cuda_device : int , optional (default = -1 ) If cuda_device is >= 0, the model will be loaded onto the corresponding GPU. Otherwise it will be loaded onto the CPU. overrides : str , optional (default = \"\" ) JSON overrides to apply to the unarchived Params object. weights_file : str , optional (default = None ) The weights file to use. If unspecified, weights.th in the archive_file will be used.","title":"load_archive"},{"location":"api/models/basic_classifier/","text":"[ allennlp .models .basic_classifier ] BasicClassifier # class BasicClassifier ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | seq2vec_encoder : Seq2VecEncoder , | seq2seq_encoder : Seq2SeqEncoder = None , | feedforward : Optional [ FeedForward ] = None , | dropout : float = None , | num_labels : int = None , | label_namespace : str = \"labels\" , | namespace : str = \"tokens\" , | initializer : InitializerApplicator = InitializerApplicator (), | ** kwargs | ) -> None This Model implements a basic text classifier. After embedding the text into a text field, we will optionally encode the embeddings with a Seq2SeqEncoder . The resulting sequence is pooled using a Seq2VecEncoder and then passed to a linear classification layer, which projects into the label space. If a Seq2SeqEncoder is not provided, we will pass the embedded text directly to the Seq2VecEncoder . Registered as a Model with name \"basic_classifier\". Parameters vocab : Vocabulary text_field_embedder : TextFieldEmbedder Used to embed the input text into a TextField seq2seq_encoder : Seq2SeqEncoder , optional (default = None ) Optional Seq2Seq encoder layer for the input text. seq2vec_encoder : Seq2VecEncoder Required Seq2Vec encoder layer. If seq2seq_encoder is provided, this encoder will pool its output. Otherwise, this encoder will operate directly on the output of the text_field_embedder . feedforward : FeedForward , optional (default = None ) An optional feedforward layer to apply after the seq2vec_encoder. dropout : float , optional (default = None ) Dropout percentage to use. num_labels : int , optional (default = None ) Number of labels to project to in classification layer. By default, the classification layer will project to the size of the vocabulary namespace corresponding to labels. label_namespace : str , optional (default = \"labels\" ) Vocabulary namespace corresponding to labels. By default, we use the \"labels\" namespace. initializer : InitializerApplicator , optional (default = InitializerApplicator() ) If provided, will be used to initialize the model parameters. forward # class BasicClassifier ( Model ): | ... | def forward ( | self , | tokens : TextFieldTensors , | label : torch . IntTensor = None | ) -> Dict [ str , torch . Tensor ] Parameters tokens : TextFieldTensors From a TextField label : torch.IntTensor , optional (default = None ) From a LabelField Returns An output dictionary consisting of: logits ( torch.FloatTensor ) : A tensor of shape (batch_size, num_labels) representing unnormalized log probabilities of the label. probs ( torch.FloatTensor ) : A tensor of shape (batch_size, num_labels) representing probabilities of the label. loss : ( torch.FloatTensor , optional) : A scalar loss to be optimised. make_output_human_readable # class BasicClassifier ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Does a simple argmax over the probabilities, converts index to string label, and add \"label\" key to the dictionary with the result. get_metrics # class BasicClassifier ( Model ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] default_predictor # class BasicClassifier ( Model ): | ... | default_predictor = \"text_classifier\"","title":"basic_classifier"},{"location":"api/models/basic_classifier/#basicclassifier","text":"class BasicClassifier ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | seq2vec_encoder : Seq2VecEncoder , | seq2seq_encoder : Seq2SeqEncoder = None , | feedforward : Optional [ FeedForward ] = None , | dropout : float = None , | num_labels : int = None , | label_namespace : str = \"labels\" , | namespace : str = \"tokens\" , | initializer : InitializerApplicator = InitializerApplicator (), | ** kwargs | ) -> None This Model implements a basic text classifier. After embedding the text into a text field, we will optionally encode the embeddings with a Seq2SeqEncoder . The resulting sequence is pooled using a Seq2VecEncoder and then passed to a linear classification layer, which projects into the label space. If a Seq2SeqEncoder is not provided, we will pass the embedded text directly to the Seq2VecEncoder . Registered as a Model with name \"basic_classifier\". Parameters vocab : Vocabulary text_field_embedder : TextFieldEmbedder Used to embed the input text into a TextField seq2seq_encoder : Seq2SeqEncoder , optional (default = None ) Optional Seq2Seq encoder layer for the input text. seq2vec_encoder : Seq2VecEncoder Required Seq2Vec encoder layer. If seq2seq_encoder is provided, this encoder will pool its output. Otherwise, this encoder will operate directly on the output of the text_field_embedder . feedforward : FeedForward , optional (default = None ) An optional feedforward layer to apply after the seq2vec_encoder. dropout : float , optional (default = None ) Dropout percentage to use. num_labels : int , optional (default = None ) Number of labels to project to in classification layer. By default, the classification layer will project to the size of the vocabulary namespace corresponding to labels. label_namespace : str , optional (default = \"labels\" ) Vocabulary namespace corresponding to labels. By default, we use the \"labels\" namespace. initializer : InitializerApplicator , optional (default = InitializerApplicator() ) If provided, will be used to initialize the model parameters.","title":"BasicClassifier"},{"location":"api/models/basic_classifier/#forward","text":"class BasicClassifier ( Model ): | ... | def forward ( | self , | tokens : TextFieldTensors , | label : torch . IntTensor = None | ) -> Dict [ str , torch . Tensor ] Parameters tokens : TextFieldTensors From a TextField label : torch.IntTensor , optional (default = None ) From a LabelField Returns An output dictionary consisting of: logits ( torch.FloatTensor ) : A tensor of shape (batch_size, num_labels) representing unnormalized log probabilities of the label. probs ( torch.FloatTensor ) : A tensor of shape (batch_size, num_labels) representing probabilities of the label. loss : ( torch.FloatTensor , optional) : A scalar loss to be optimised.","title":"forward"},{"location":"api/models/basic_classifier/#make_output_human_readable","text":"class BasicClassifier ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Does a simple argmax over the probabilities, converts index to string label, and add \"label\" key to the dictionary with the result.","title":"make_output_human_readable"},{"location":"api/models/basic_classifier/#get_metrics","text":"class BasicClassifier ( Model ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"api/models/basic_classifier/#default_predictor","text":"class BasicClassifier ( Model ): | ... | default_predictor = \"text_classifier\"","title":"default_predictor"},{"location":"api/models/model/","text":"[ allennlp .models .model ] Model is an abstract class representing an AllenNLP model. Model # class Model ( torch . nn . Module , Registrable ): | def __init__ ( | self , | vocab : Vocabulary , | regularizer : RegularizerApplicator = None | ) -> None This abstract class represents a model to be trained. Rather than relying completely on the Pytorch Module, we modify the output spec of forward to be a dictionary. Models built using this API are still compatible with other pytorch models and can be used naturally as modules within other models - outputs are dictionaries, which can be unpacked and passed into other layers. One caveat to this is that if you wish to use an AllenNLP model inside a Container (such as nn.Sequential), you must interleave the models with a wrapper module which unpacks the dictionary into a list of tensors. In order for your model to be trained using the Trainer api, the output dictionary of your Model must include a \"loss\" key, which will be optimised during the training process. Finally, you can optionally implement Model.get_metrics in order to make use of early stopping and best-model serialization based on a validation metric in Trainer . Metrics that begin with \"_\" will not be logged to the progress bar by Trainer . The from_archive method on this class is registered as a Model with name \"from_archive\". So, if you are using a configuration file, you can specify a model as {\"type\": \"from_archive\", \"archive_file\": \"/path/to/archive.tar.gz\"} , which will pull out the model from the given location and return it. Parameters vocab : Vocabulary There are two typical use-cases for the Vocabulary in a Model : getting vocabulary sizes when constructing embedding matrices or output classifiers (as the vocabulary holds the number of classes in your output, also), and translating model output into human-readable form. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"model\", it gets specified as a top-level parameter, then is passed in to the model separately. - regularizer : RegularizerApplicator , optional If given, the Trainer will use this to regularize model parameters. default_predictor # class Model ( torch . nn . Module , Registrable ): | ... | default_predictor : Optional [ str ] = None get_regularization_penalty # class Model ( torch . nn . Module , Registrable ): | ... | def get_regularization_penalty ( self ) -> Optional [ torch . Tensor ] Computes the regularization penalty for the model. Returns None if the model was not configured to use regularization. get_parameters_for_histogram_tensorboard_logging # class Model ( torch . nn . Module , Registrable ): | ... | def get_parameters_for_histogram_tensorboard_logging ( | self | ) -> List [ str ] Returns the name of model parameters used for logging histograms to tensorboard. forward # class Model ( torch . nn . Module , Registrable ): | ... | def forward ( self , * inputs ) -> Dict [ str , torch . Tensor ] Defines the forward pass of the model. In addition, to facilitate easy training, this method is designed to compute a loss function defined by a user. The input is comprised of everything required to perform a training update, including labels - you define the signature here! It is down to the user to ensure that inference can be performed without the presence of these labels. Hence, any inputs not available at inference time should only be used inside a conditional block. The intended sketch of this method is as follows:: def forward(self, input1, input2, targets=None): .... .... output1 = self.layer1(input1) output2 = self.layer2(input2) output_dict = {\"output1\": output1, \"output2\": output2} if targets is not None: # Function returning a scalar torch.Tensor, defined by the user. loss = self._compute_loss(output1, output2, targets) output_dict[\"loss\"] = loss return output_dict Parameters *inputs : Any Tensors comprising everything needed to perform a training update, including labels, which should be optional (i.e have a default value of None ). At inference time, simply pass the relevant inputs, not including the labels. Returns output_dict : Dict[str, torch.Tensor] The outputs from the model. In order to train a model using the Trainer api, you must provide a \"loss\" key pointing to a scalar torch.Tensor representing the loss to be optimized. forward_on_instance # class Model ( torch . nn . Module , Registrable ): | ... | def forward_on_instance ( | self , | instance : Instance | ) -> Dict [ str , numpy . ndarray ] Takes an Instance , which typically has raw text in it, converts that text into arrays using this model's Vocabulary , passes those arrays through self.forward() and self.make_output_human_readable() (which by default does nothing) and returns the result. Before returning the result, we convert any torch.Tensors into numpy arrays and remove the batch dimension. forward_on_instances # class Model ( torch . nn . Module , Registrable ): | ... | def forward_on_instances ( | self , | instances : List [ Instance ] | ) -> List [ Dict [ str , numpy . ndarray ]] Takes a list of Instances , converts that text into arrays using this model's Vocabulary , passes those arrays through self.forward() and self.make_output_human_readable() (which by default does nothing) and returns the result. Before returning the result, we convert any torch.Tensors into numpy arrays and separate the batched output into a list of individual dicts per instance. Note that typically this will be faster on a GPU (and conditionally, on a CPU) than repeated calls to forward_on_instance . Parameters instances : List[Instance] The instances to run the model on. Returns A list of the models output for each instance. make_output_human_readable # class Model ( torch . nn . Module , Registrable ): | ... | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Takes the result of forward and makes it human readable. Most of the time, the only thing this method does is convert tokens / predicted labels from tensors to strings that humans might actually understand. Somtimes you'll also do an argmax or something in here, too, but that most often happens in Model.forward , before you compute your metrics. This method modifies the input dictionary, and also returns the same dictionary. By default in the base class we do nothing. get_metrics # class Model ( torch . nn . Module , Registrable ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] Returns a dictionary of metrics. This method will be called by allennlp.training.Trainer in order to compute and use model metrics for early stopping and model serialization. We return an empty dictionary here rather than raising as it is not required to implement metrics for a new model. A boolean reset parameter is passed, as frequently a metric accumulator will have some state which should be reset between epochs. This is also compatible with Metric s . Metrics should be populated during the call to forward , with the Metric handling the accumulation of the metric until this method is called. load # class Model ( torch . nn . Module , Registrable ): | ... | @classmethod | def load ( | cls , | config : Params , | serialization_dir : Union [ str , PathLike ], | weights_file : Optional [ Union [ str , PathLike ]] = None , | cuda_device : int = - 1 | ) -> \"Model\" Instantiates an already-trained model, based on the experiment configuration and some optional overrides. Parameters config : Params The configuration that was used to train the model. It should definitely have a model section, and should probably have a trainer section as well. serialization_dir : str = None The directory containing the serialized weights, parameters, and vocabulary of the model. weights_file : str = None By default we load the weights from best.th in the serialization directory, but you can override that value here. cuda_device : int = -1 By default we load the model on the CPU, but if you want to load it for GPU usage you can specify the id of your GPU here Returns model : Model The model specified in the configuration, loaded with the serialized vocabulary and the trained weights. extend_embedder_vocab # class Model ( torch . nn . Module , Registrable ): | ... | def extend_embedder_vocab ( | self , | embedding_sources_mapping : Dict [ str , str ] = None | ) -> None Iterates through all embedding modules in the model and assures it can embed with the extended vocab. This is required in fine-tuning or transfer learning scenarios where model was trained with original vocabulary but during fine-tuning/transfer-learning, it will have it work with extended vocabulary (original + new-data vocabulary). Parameters embedding_sources_mapping : Dict[str, str] , optional (default = None ) Mapping from model_path to pretrained-file path of the embedding modules. If pretrained-file used at time of embedding initialization isn't available now, user should pass this mapping. Model path is path traversing the model attributes upto this embedding module. Eg. \"_text_field_embedder.token_embedder_tokens\". from_archive # class Model ( torch . nn . Module , Registrable ): | ... | @classmethod | def from_archive ( | cls , | archive_file : str , | vocab : Vocabulary = None | ) -> \"Model\" Loads a model from an archive file. This basically just calls return archival.load_archive(archive_file).model . It exists as a method here for convenience, and so that we can register it for easy use for fine tuning an existing model from a config file. If vocab is given, we will extend the loaded model's vocabulary using the passed vocab object (including calling extend_embedder_vocab , which extends embedding layers). remove_pretrained_embedding_params # def remove_pretrained_embedding_params ( params : Params )","title":"model"},{"location":"api/models/model/#model","text":"class Model ( torch . nn . Module , Registrable ): | def __init__ ( | self , | vocab : Vocabulary , | regularizer : RegularizerApplicator = None | ) -> None This abstract class represents a model to be trained. Rather than relying completely on the Pytorch Module, we modify the output spec of forward to be a dictionary. Models built using this API are still compatible with other pytorch models and can be used naturally as modules within other models - outputs are dictionaries, which can be unpacked and passed into other layers. One caveat to this is that if you wish to use an AllenNLP model inside a Container (such as nn.Sequential), you must interleave the models with a wrapper module which unpacks the dictionary into a list of tensors. In order for your model to be trained using the Trainer api, the output dictionary of your Model must include a \"loss\" key, which will be optimised during the training process. Finally, you can optionally implement Model.get_metrics in order to make use of early stopping and best-model serialization based on a validation metric in Trainer . Metrics that begin with \"_\" will not be logged to the progress bar by Trainer . The from_archive method on this class is registered as a Model with name \"from_archive\". So, if you are using a configuration file, you can specify a model as {\"type\": \"from_archive\", \"archive_file\": \"/path/to/archive.tar.gz\"} , which will pull out the model from the given location and return it. Parameters vocab : Vocabulary There are two typical use-cases for the Vocabulary in a Model : getting vocabulary sizes when constructing embedding matrices or output classifiers (as the vocabulary holds the number of classes in your output, also), and translating model output into human-readable form. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"model\", it gets specified as a top-level parameter, then is passed in to the model separately. - regularizer : RegularizerApplicator , optional If given, the Trainer will use this to regularize model parameters.","title":"Model"},{"location":"api/models/model/#default_predictor","text":"class Model ( torch . nn . Module , Registrable ): | ... | default_predictor : Optional [ str ] = None","title":"default_predictor"},{"location":"api/models/model/#get_regularization_penalty","text":"class Model ( torch . nn . Module , Registrable ): | ... | def get_regularization_penalty ( self ) -> Optional [ torch . Tensor ] Computes the regularization penalty for the model. Returns None if the model was not configured to use regularization.","title":"get_regularization_penalty"},{"location":"api/models/model/#get_parameters_for_histogram_tensorboard_logging","text":"class Model ( torch . nn . Module , Registrable ): | ... | def get_parameters_for_histogram_tensorboard_logging ( | self | ) -> List [ str ] Returns the name of model parameters used for logging histograms to tensorboard.","title":"get_parameters_for_histogram_tensorboard_logging"},{"location":"api/models/model/#forward","text":"class Model ( torch . nn . Module , Registrable ): | ... | def forward ( self , * inputs ) -> Dict [ str , torch . Tensor ] Defines the forward pass of the model. In addition, to facilitate easy training, this method is designed to compute a loss function defined by a user. The input is comprised of everything required to perform a training update, including labels - you define the signature here! It is down to the user to ensure that inference can be performed without the presence of these labels. Hence, any inputs not available at inference time should only be used inside a conditional block. The intended sketch of this method is as follows:: def forward(self, input1, input2, targets=None): .... .... output1 = self.layer1(input1) output2 = self.layer2(input2) output_dict = {\"output1\": output1, \"output2\": output2} if targets is not None: # Function returning a scalar torch.Tensor, defined by the user. loss = self._compute_loss(output1, output2, targets) output_dict[\"loss\"] = loss return output_dict Parameters *inputs : Any Tensors comprising everything needed to perform a training update, including labels, which should be optional (i.e have a default value of None ). At inference time, simply pass the relevant inputs, not including the labels. Returns output_dict : Dict[str, torch.Tensor] The outputs from the model. In order to train a model using the Trainer api, you must provide a \"loss\" key pointing to a scalar torch.Tensor representing the loss to be optimized.","title":"forward"},{"location":"api/models/model/#forward_on_instance","text":"class Model ( torch . nn . Module , Registrable ): | ... | def forward_on_instance ( | self , | instance : Instance | ) -> Dict [ str , numpy . ndarray ] Takes an Instance , which typically has raw text in it, converts that text into arrays using this model's Vocabulary , passes those arrays through self.forward() and self.make_output_human_readable() (which by default does nothing) and returns the result. Before returning the result, we convert any torch.Tensors into numpy arrays and remove the batch dimension.","title":"forward_on_instance"},{"location":"api/models/model/#forward_on_instances","text":"class Model ( torch . nn . Module , Registrable ): | ... | def forward_on_instances ( | self , | instances : List [ Instance ] | ) -> List [ Dict [ str , numpy . ndarray ]] Takes a list of Instances , converts that text into arrays using this model's Vocabulary , passes those arrays through self.forward() and self.make_output_human_readable() (which by default does nothing) and returns the result. Before returning the result, we convert any torch.Tensors into numpy arrays and separate the batched output into a list of individual dicts per instance. Note that typically this will be faster on a GPU (and conditionally, on a CPU) than repeated calls to forward_on_instance . Parameters instances : List[Instance] The instances to run the model on. Returns A list of the models output for each instance.","title":"forward_on_instances"},{"location":"api/models/model/#make_output_human_readable","text":"class Model ( torch . nn . Module , Registrable ): | ... | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Takes the result of forward and makes it human readable. Most of the time, the only thing this method does is convert tokens / predicted labels from tensors to strings that humans might actually understand. Somtimes you'll also do an argmax or something in here, too, but that most often happens in Model.forward , before you compute your metrics. This method modifies the input dictionary, and also returns the same dictionary. By default in the base class we do nothing.","title":"make_output_human_readable"},{"location":"api/models/model/#get_metrics","text":"class Model ( torch . nn . Module , Registrable ): | ... | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] Returns a dictionary of metrics. This method will be called by allennlp.training.Trainer in order to compute and use model metrics for early stopping and model serialization. We return an empty dictionary here rather than raising as it is not required to implement metrics for a new model. A boolean reset parameter is passed, as frequently a metric accumulator will have some state which should be reset between epochs. This is also compatible with Metric s . Metrics should be populated during the call to forward , with the Metric handling the accumulation of the metric until this method is called.","title":"get_metrics"},{"location":"api/models/model/#load","text":"class Model ( torch . nn . Module , Registrable ): | ... | @classmethod | def load ( | cls , | config : Params , | serialization_dir : Union [ str , PathLike ], | weights_file : Optional [ Union [ str , PathLike ]] = None , | cuda_device : int = - 1 | ) -> \"Model\" Instantiates an already-trained model, based on the experiment configuration and some optional overrides. Parameters config : Params The configuration that was used to train the model. It should definitely have a model section, and should probably have a trainer section as well. serialization_dir : str = None The directory containing the serialized weights, parameters, and vocabulary of the model. weights_file : str = None By default we load the weights from best.th in the serialization directory, but you can override that value here. cuda_device : int = -1 By default we load the model on the CPU, but if you want to load it for GPU usage you can specify the id of your GPU here Returns model : Model The model specified in the configuration, loaded with the serialized vocabulary and the trained weights.","title":"load"},{"location":"api/models/model/#extend_embedder_vocab","text":"class Model ( torch . nn . Module , Registrable ): | ... | def extend_embedder_vocab ( | self , | embedding_sources_mapping : Dict [ str , str ] = None | ) -> None Iterates through all embedding modules in the model and assures it can embed with the extended vocab. This is required in fine-tuning or transfer learning scenarios where model was trained with original vocabulary but during fine-tuning/transfer-learning, it will have it work with extended vocabulary (original + new-data vocabulary). Parameters embedding_sources_mapping : Dict[str, str] , optional (default = None ) Mapping from model_path to pretrained-file path of the embedding modules. If pretrained-file used at time of embedding initialization isn't available now, user should pass this mapping. Model path is path traversing the model attributes upto this embedding module. Eg. \"_text_field_embedder.token_embedder_tokens\".","title":"extend_embedder_vocab"},{"location":"api/models/model/#from_archive","text":"class Model ( torch . nn . Module , Registrable ): | ... | @classmethod | def from_archive ( | cls , | archive_file : str , | vocab : Vocabulary = None | ) -> \"Model\" Loads a model from an archive file. This basically just calls return archival.load_archive(archive_file).model . It exists as a method here for convenience, and so that we can register it for easy use for fine tuning an existing model from a config file. If vocab is given, we will extend the loaded model's vocabulary using the passed vocab object (including calling extend_embedder_vocab , which extends embedding layers).","title":"from_archive"},{"location":"api/models/model/#remove_pretrained_embedding_params","text":"def remove_pretrained_embedding_params ( params : Params )","title":"remove_pretrained_embedding_params"},{"location":"api/models/simple_tagger/","text":"[ allennlp .models .simple_tagger ] SimpleTagger # class SimpleTagger ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | encoder : Seq2SeqEncoder , | calculate_span_f1 : bool = None , | label_encoding : Optional [ str ] = None , | label_namespace : str = \"labels\" , | verbose_metrics : bool = False , | initializer : InitializerApplicator = InitializerApplicator (), | ** kwargs | ) -> None This SimpleTagger simply encodes a sequence of text with a stacked Seq2SeqEncoder , then predicts a tag for each token in the sequence. Registered as a Model with name \"simple_tagger\". Parameters vocab : Vocabulary A Vocabulary, required in order to compute sizes for input/output projections. text_field_embedder : TextFieldEmbedder Used to embed the tokens TextField we get as input to the model. encoder : Seq2SeqEncoder The encoder (with its own internal stacking) that we will use in between embedding tokens and predicting output tags. calculate_span_f1 : bool , optional (default = None ) Calculate span-level F1 metrics during training. If this is True , then label_encoding is required. If None and label_encoding is specified, this is set to True . If None and label_encoding is not specified, it defaults to False . label_encoding : str , optional (default = None ) Label encoding to use when calculating span f1. Valid options are \"BIO\", \"BIOUL\", \"IOB1\", \"BMES\". Required if calculate_span_f1 is true. label_namespace : str , optional (default = labels ) This is needed to compute the SpanBasedF1Measure metric, if desired. Unless you did something unusual, the default value should be what you want. verbose_metrics : bool , optional (default = False ) If true, metrics will be returned per label class in addition to the overall statistics. initializer : InitializerApplicator , optional (default = InitializerApplicator() ) Used to initialize the model parameters. forward # class SimpleTagger ( Model ): | ... | @overrides | def forward ( | self , | tokens : TextFieldTensors , | tags : torch . LongTensor = None , | metadata : List [ Dict [ str , Any ]] = None , | ignore_loss_on_o_tags : bool = False | ) -> Dict [ str , torch . Tensor ] Parameters tokens : TextFieldTensors The output of TextField.as_array() , which should typically be passed directly to a TextFieldEmbedder . This output is a dictionary mapping keys to TokenIndexer tensors. At its most basic, using a SingleIdTokenIndexer this is : {\"tokens\": Tensor(batch_size, num_tokens)} . This dictionary will have the same keys as were used for the TokenIndexers when you created the TextField representing your sequence. The dictionary is designed to be passed directly to a TextFieldEmbedder , which knows how to combine different word representations into a single vector per token in your input. tags : torch.LongTensor , optional (default = None ) A torch tensor representing the sequence of integer gold class labels of shape (batch_size, num_tokens) . metadata : List[Dict[str, Any]] , optional (default = None ) metadata containing the original words in the sentence to be tagged under a 'words' key. ignore_loss_on_o_tags : bool , optional (default = False ) If True, we compute the loss only for actual spans in tags , and not on O tokens. This is useful for computing gradients of the loss on a single span , for interpretation / attacking. Returns An output dictionary consisting of: logits ( torch.FloatTensor ) : A tensor of shape (batch_size, num_tokens, tag_vocab_size) representing unnormalised log probabilities of the tag classes. class_probabilities ( torch.FloatTensor ) : A tensor of shape (batch_size, num_tokens, tag_vocab_size) representing a distribution of the tag classes per word. loss ( torch.FloatTensor , optional) : A scalar loss to be optimised. make_output_human_readable # class SimpleTagger ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Does a simple position-wise argmax over each token, converts indices to string labels, and adds a \"tags\" key to the dictionary with the result. get_metrics # class SimpleTagger ( Model ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ] default_predictor # class SimpleTagger ( Model ): | ... | default_predictor = \"sentence_tagger\"","title":"simple_tagger"},{"location":"api/models/simple_tagger/#simpletagger","text":"class SimpleTagger ( Model ): | def __init__ ( | self , | vocab : Vocabulary , | text_field_embedder : TextFieldEmbedder , | encoder : Seq2SeqEncoder , | calculate_span_f1 : bool = None , | label_encoding : Optional [ str ] = None , | label_namespace : str = \"labels\" , | verbose_metrics : bool = False , | initializer : InitializerApplicator = InitializerApplicator (), | ** kwargs | ) -> None This SimpleTagger simply encodes a sequence of text with a stacked Seq2SeqEncoder , then predicts a tag for each token in the sequence. Registered as a Model with name \"simple_tagger\". Parameters vocab : Vocabulary A Vocabulary, required in order to compute sizes for input/output projections. text_field_embedder : TextFieldEmbedder Used to embed the tokens TextField we get as input to the model. encoder : Seq2SeqEncoder The encoder (with its own internal stacking) that we will use in between embedding tokens and predicting output tags. calculate_span_f1 : bool , optional (default = None ) Calculate span-level F1 metrics during training. If this is True , then label_encoding is required. If None and label_encoding is specified, this is set to True . If None and label_encoding is not specified, it defaults to False . label_encoding : str , optional (default = None ) Label encoding to use when calculating span f1. Valid options are \"BIO\", \"BIOUL\", \"IOB1\", \"BMES\". Required if calculate_span_f1 is true. label_namespace : str , optional (default = labels ) This is needed to compute the SpanBasedF1Measure metric, if desired. Unless you did something unusual, the default value should be what you want. verbose_metrics : bool , optional (default = False ) If true, metrics will be returned per label class in addition to the overall statistics. initializer : InitializerApplicator , optional (default = InitializerApplicator() ) Used to initialize the model parameters.","title":"SimpleTagger"},{"location":"api/models/simple_tagger/#forward","text":"class SimpleTagger ( Model ): | ... | @overrides | def forward ( | self , | tokens : TextFieldTensors , | tags : torch . LongTensor = None , | metadata : List [ Dict [ str , Any ]] = None , | ignore_loss_on_o_tags : bool = False | ) -> Dict [ str , torch . Tensor ] Parameters tokens : TextFieldTensors The output of TextField.as_array() , which should typically be passed directly to a TextFieldEmbedder . This output is a dictionary mapping keys to TokenIndexer tensors. At its most basic, using a SingleIdTokenIndexer this is : {\"tokens\": Tensor(batch_size, num_tokens)} . This dictionary will have the same keys as were used for the TokenIndexers when you created the TextField representing your sequence. The dictionary is designed to be passed directly to a TextFieldEmbedder , which knows how to combine different word representations into a single vector per token in your input. tags : torch.LongTensor , optional (default = None ) A torch tensor representing the sequence of integer gold class labels of shape (batch_size, num_tokens) . metadata : List[Dict[str, Any]] , optional (default = None ) metadata containing the original words in the sentence to be tagged under a 'words' key. ignore_loss_on_o_tags : bool , optional (default = False ) If True, we compute the loss only for actual spans in tags , and not on O tokens. This is useful for computing gradients of the loss on a single span , for interpretation / attacking. Returns An output dictionary consisting of: logits ( torch.FloatTensor ) : A tensor of shape (batch_size, num_tokens, tag_vocab_size) representing unnormalised log probabilities of the tag classes. class_probabilities ( torch.FloatTensor ) : A tensor of shape (batch_size, num_tokens, tag_vocab_size) representing a distribution of the tag classes per word. loss ( torch.FloatTensor , optional) : A scalar loss to be optimised.","title":"forward"},{"location":"api/models/simple_tagger/#make_output_human_readable","text":"class SimpleTagger ( Model ): | ... | @overrides | def make_output_human_readable ( | self , | output_dict : Dict [ str , torch . Tensor ] | ) -> Dict [ str , torch . Tensor ] Does a simple position-wise argmax over each token, converts indices to string labels, and adds a \"tags\" key to the dictionary with the result.","title":"make_output_human_readable"},{"location":"api/models/simple_tagger/#get_metrics","text":"class SimpleTagger ( Model ): | ... | @overrides | def get_metrics ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metrics"},{"location":"api/models/simple_tagger/#default_predictor","text":"class SimpleTagger ( Model ): | ... | default_predictor = \"sentence_tagger\"","title":"default_predictor"},{"location":"api/modules/augmented_lstm/","text":"[ allennlp .modules .augmented_lstm ] An LSTM with Recurrent Dropout and the option to use highway connections between layers. Based on PyText version (that was based on a previous AllenNLP version) AugmentedLSTMCell # class AugmentedLSTMCell ( torch . nn . Module ): | def __init__ ( | self , | embed_dim : int , | lstm_dim : int , | use_highway : bool = True , | use_bias : bool = True | ) AugmentedLSTMCell implements a AugmentedLSTM cell. Parameters embed_dim : int The number of expected features in the input. lstm_dim : int Number of features in the hidden state of the LSTM. use_highway : bool , optional (default = True ) If True we append a highway network to the outputs of the LSTM. use_bias : bool , optional (default = True ) If True we use a bias in our LSTM calculations, otherwise we don't. Attributes input_linearity : nn.Module Fused weight matrix which computes a linear function over the input. state_linearity : nn.Module Fused weight matrix which computes a linear function over the states. reset_parameters # class AugmentedLSTMCell ( torch . nn . Module ): | ... | def reset_parameters ( self ) Use sensible default initializations for parameters. forward # class AugmentedLSTMCell ( torch . nn . Module ): | ... | def forward ( | self , | x : torch . Tensor , | states = Tuple [ torch . Tensor , torch . Tensor ], | variational_dropout_mask : Optional [ torch . BoolTensor ] = None | ) -> Tuple [ torch . Tensor , torch . Tensor ] Warning DO NOT USE THIS LAYER DIRECTLY, instead use the AugmentedLSTM class Parameters x : torch.Tensor Input tensor of shape (bsize x input_dim). states : Tuple[torch.Tensor, torch.Tensor] Tuple of tensors containing the hidden state and the cell state of each element in the batch. Each of these tensors have a dimension of (bsize x nhid). Defaults to None . Returns Tuple[torch.Tensor, torch.Tensor] Returned states. Shape of each state is (bsize x nhid). AugmentedLstm # class AugmentedLstm ( torch . nn . Module ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | go_forward : bool = True , | recurrent_dropout_probability : float = 0.0 , | use_highway : bool = True , | use_input_projection_bias : bool = True | ) AugmentedLstm implements a one-layer single directional AugmentedLSTM layer. AugmentedLSTM is an LSTM which optionally appends an optional highway network to the output layer. Furthermore the dropout controls the level of variational dropout done. Parameters input_size : int The number of expected features in the input. hidden_size : int Number of features in the hidden state of the LSTM. Defaults to 32. go_forward : bool Whether to compute features left to right (forward) or right to left (backward). recurrent_dropout_probability : float Variational dropout probability to use. Defaults to 0.0. use_highway : bool If True we append a highway network to the outputs of the LSTM. use_input_projection_bias : bool If True we use a bias in our LSTM calculations, otherwise we don't. Attributes cell : AugmentedLSTMCell AugmentedLSTMCell that is applied at every timestep. forward # class AugmentedLstm ( torch . nn . Module ): | ... | def forward ( | self , | inputs : PackedSequence , | states : Optional [ Tuple [ torch . Tensor , torch . Tensor ]] = None | ) -> Tuple [ PackedSequence , Tuple [ torch . Tensor , torch . Tensor ]] Warning: Would be better to use the BiAugmentedLstm class in a regular model Given an input batch of sequential data such as word embeddings, produces a single layer unidirectional AugmentedLSTM representation of the sequential input and new state tensors. Parameters inputs : PackedSequence bsize sequences of shape (len, input_dim) each, in PackedSequence format states : Tuple[torch.Tensor, torch.Tensor] Tuple of tensors containing the initial hidden state and the cell state of each element in the batch. Each of these tensors have a dimension of (1 x bsize x nhid). Defaults to None . Returns Tuple[PackedSequence, Tuple[torch.Tensor, torch.Tensor]] AugmentedLSTM representation of input and the state of the LSTM t = seq_len . Shape of representation is (bsize x seq_len x representation_dim). Shape of each state is (1 x bsize x nhid). BiAugmentedLstm # class BiAugmentedLstm ( torch . nn . Module ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int = 1 , | bias : bool = True , | recurrent_dropout_probability : float = 0.0 , | bidirectional : bool = False , | padding_value : float = 0.0 , | use_highway : bool = True | ) -> None BiAugmentedLstm implements a generic AugmentedLSTM representation layer. BiAugmentedLstm is an LSTM which optionally appends an optional highway network to the output layer. Furthermore the dropout controls the level of variational dropout done. Parameters input_size : int The dimension of the inputs to the LSTM. hidden_size : int The dimension of the outputs of the LSTM. num_layers : int Number of recurrent layers. Eg. setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in the outputs of the first LSTM and computing the final result. Defaults to 1. bias : bool If True we use a bias in our LSTM calculations, otherwise we don't. recurrent_dropout_probability : float , optional (default = 0.0 ) Variational dropout probability to use. bidirectional : bool If True , becomes a bidirectional LSTM. Defaults to True . padding_value : float , optional (default = 0.0 ) Value for the padded elements. Defaults to 0.0. use_highway : bool , optional (default = True ) Whether or not to use highway connections between layers. This effectively involves reparameterising the normal output of an LSTM as:: gate = sigmoid(W_x1 * x_t + W_h * h_t) output = gate * h_t + (1 - gate) * (W_x2 * x_t) Returns output_accumulator : PackedSequence The outputs of the LSTM for each timestep. A tensor of shape (batch_size, max_timesteps, hidden_size) where for a given batch element, all outputs past the sequence length for that batch are zero tensors. forward # class BiAugmentedLstm ( torch . nn . Module ): | ... | def forward ( | self , | inputs : torch . Tensor , | states : Optional [ Tuple [ torch . Tensor , torch . Tensor ]] = None | ) -> Tuple [ torch . Tensor , Tuple [ torch . Tensor , torch . Tensor ]] Given an input batch of sequential data such as word embeddings, produces a AugmentedLSTM representation of the sequential input and new state tensors. Parameters inputs : PackedSequence A tensor of shape (batch_size, num_timesteps, input_size) to apply the LSTM over. states : Tuple[torch.Tensor, torch.Tensor] Tuple of tensors containing the initial hidden state and the cell state of each element in the batch. Each of these tensors have a dimension of (bsize x num_layers x num_directions * nhid). Defaults to None . Returns Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]] AgumentedLSTM representation of input and the state of the LSTM t = seq_len . Shape of representation is (bsize x seq_len x representation_dim). Shape of each state is (bsize x num_layers * num_directions x nhid).","title":"augmented_lstm"},{"location":"api/modules/augmented_lstm/#augmentedlstmcell","text":"class AugmentedLSTMCell ( torch . nn . Module ): | def __init__ ( | self , | embed_dim : int , | lstm_dim : int , | use_highway : bool = True , | use_bias : bool = True | ) AugmentedLSTMCell implements a AugmentedLSTM cell. Parameters embed_dim : int The number of expected features in the input. lstm_dim : int Number of features in the hidden state of the LSTM. use_highway : bool , optional (default = True ) If True we append a highway network to the outputs of the LSTM. use_bias : bool , optional (default = True ) If True we use a bias in our LSTM calculations, otherwise we don't. Attributes input_linearity : nn.Module Fused weight matrix which computes a linear function over the input. state_linearity : nn.Module Fused weight matrix which computes a linear function over the states.","title":"AugmentedLSTMCell"},{"location":"api/modules/augmented_lstm/#reset_parameters","text":"class AugmentedLSTMCell ( torch . nn . Module ): | ... | def reset_parameters ( self ) Use sensible default initializations for parameters.","title":"reset_parameters"},{"location":"api/modules/augmented_lstm/#forward","text":"class AugmentedLSTMCell ( torch . nn . Module ): | ... | def forward ( | self , | x : torch . Tensor , | states = Tuple [ torch . Tensor , torch . Tensor ], | variational_dropout_mask : Optional [ torch . BoolTensor ] = None | ) -> Tuple [ torch . Tensor , torch . Tensor ] Warning DO NOT USE THIS LAYER DIRECTLY, instead use the AugmentedLSTM class Parameters x : torch.Tensor Input tensor of shape (bsize x input_dim). states : Tuple[torch.Tensor, torch.Tensor] Tuple of tensors containing the hidden state and the cell state of each element in the batch. Each of these tensors have a dimension of (bsize x nhid). Defaults to None . Returns Tuple[torch.Tensor, torch.Tensor] Returned states. Shape of each state is (bsize x nhid).","title":"forward"},{"location":"api/modules/augmented_lstm/#augmentedlstm","text":"class AugmentedLstm ( torch . nn . Module ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | go_forward : bool = True , | recurrent_dropout_probability : float = 0.0 , | use_highway : bool = True , | use_input_projection_bias : bool = True | ) AugmentedLstm implements a one-layer single directional AugmentedLSTM layer. AugmentedLSTM is an LSTM which optionally appends an optional highway network to the output layer. Furthermore the dropout controls the level of variational dropout done. Parameters input_size : int The number of expected features in the input. hidden_size : int Number of features in the hidden state of the LSTM. Defaults to 32. go_forward : bool Whether to compute features left to right (forward) or right to left (backward). recurrent_dropout_probability : float Variational dropout probability to use. Defaults to 0.0. use_highway : bool If True we append a highway network to the outputs of the LSTM. use_input_projection_bias : bool If True we use a bias in our LSTM calculations, otherwise we don't. Attributes cell : AugmentedLSTMCell AugmentedLSTMCell that is applied at every timestep.","title":"AugmentedLstm"},{"location":"api/modules/augmented_lstm/#forward_1","text":"class AugmentedLstm ( torch . nn . Module ): | ... | def forward ( | self , | inputs : PackedSequence , | states : Optional [ Tuple [ torch . Tensor , torch . Tensor ]] = None | ) -> Tuple [ PackedSequence , Tuple [ torch . Tensor , torch . Tensor ]] Warning: Would be better to use the BiAugmentedLstm class in a regular model Given an input batch of sequential data such as word embeddings, produces a single layer unidirectional AugmentedLSTM representation of the sequential input and new state tensors. Parameters inputs : PackedSequence bsize sequences of shape (len, input_dim) each, in PackedSequence format states : Tuple[torch.Tensor, torch.Tensor] Tuple of tensors containing the initial hidden state and the cell state of each element in the batch. Each of these tensors have a dimension of (1 x bsize x nhid). Defaults to None . Returns Tuple[PackedSequence, Tuple[torch.Tensor, torch.Tensor]] AugmentedLSTM representation of input and the state of the LSTM t = seq_len . Shape of representation is (bsize x seq_len x representation_dim). Shape of each state is (1 x bsize x nhid).","title":"forward"},{"location":"api/modules/augmented_lstm/#biaugmentedlstm","text":"class BiAugmentedLstm ( torch . nn . Module ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int = 1 , | bias : bool = True , | recurrent_dropout_probability : float = 0.0 , | bidirectional : bool = False , | padding_value : float = 0.0 , | use_highway : bool = True | ) -> None BiAugmentedLstm implements a generic AugmentedLSTM representation layer. BiAugmentedLstm is an LSTM which optionally appends an optional highway network to the output layer. Furthermore the dropout controls the level of variational dropout done. Parameters input_size : int The dimension of the inputs to the LSTM. hidden_size : int The dimension of the outputs of the LSTM. num_layers : int Number of recurrent layers. Eg. setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in the outputs of the first LSTM and computing the final result. Defaults to 1. bias : bool If True we use a bias in our LSTM calculations, otherwise we don't. recurrent_dropout_probability : float , optional (default = 0.0 ) Variational dropout probability to use. bidirectional : bool If True , becomes a bidirectional LSTM. Defaults to True . padding_value : float , optional (default = 0.0 ) Value for the padded elements. Defaults to 0.0. use_highway : bool , optional (default = True ) Whether or not to use highway connections between layers. This effectively involves reparameterising the normal output of an LSTM as:: gate = sigmoid(W_x1 * x_t + W_h * h_t) output = gate * h_t + (1 - gate) * (W_x2 * x_t) Returns output_accumulator : PackedSequence The outputs of the LSTM for each timestep. A tensor of shape (batch_size, max_timesteps, hidden_size) where for a given batch element, all outputs past the sequence length for that batch are zero tensors.","title":"BiAugmentedLstm"},{"location":"api/modules/augmented_lstm/#forward_2","text":"class BiAugmentedLstm ( torch . nn . Module ): | ... | def forward ( | self , | inputs : torch . Tensor , | states : Optional [ Tuple [ torch . Tensor , torch . Tensor ]] = None | ) -> Tuple [ torch . Tensor , Tuple [ torch . Tensor , torch . Tensor ]] Given an input batch of sequential data such as word embeddings, produces a AugmentedLSTM representation of the sequential input and new state tensors. Parameters inputs : PackedSequence A tensor of shape (batch_size, num_timesteps, input_size) to apply the LSTM over. states : Tuple[torch.Tensor, torch.Tensor] Tuple of tensors containing the initial hidden state and the cell state of each element in the batch. Each of these tensors have a dimension of (bsize x num_layers x num_directions * nhid). Defaults to None . Returns Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]] AgumentedLSTM representation of input and the state of the LSTM t = seq_len . Shape of representation is (bsize x seq_len x representation_dim). Shape of each state is (bsize x num_layers * num_directions x nhid).","title":"forward"},{"location":"api/modules/bimpm_matching/","text":"[ allennlp .modules .bimpm_matching ] Multi-perspective matching layer multi_perspective_match # def multi_perspective_match ( vector1 : torch . Tensor , vector2 : torch . Tensor , weight : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ] Calculate multi-perspective cosine matching between time-steps of vectors of the same length. Parameters vector1 : torch.Tensor A tensor of shape (batch, seq_len, hidden_size) vector2 : torch.Tensor A tensor of shape (batch, seq_len or 1, hidden_size) weight : torch.Tensor A tensor of shape (num_perspectives, hidden_size) Returns torch.Tensor : Shape (batch, seq_len, 1) . torch.Tensor : Shape (batch, seq_len, num_perspectives) . multi_perspective_match_pairwise # def multi_perspective_match_pairwise ( vector1 : torch . Tensor , vector2 : torch . Tensor , weight : torch . Tensor ) -> torch . Tensor Calculate multi-perspective cosine matching between each time step of one vector and each time step of another vector. Parameters vector1 : torch.Tensor A tensor of shape (batch, seq_len1, hidden_size) vector2 : torch.Tensor A tensor of shape (batch, seq_len2, hidden_size) weight : torch.Tensor A tensor of shape (num_perspectives, hidden_size) Returns torch.Tensor : A tensor of shape (batch, seq_len1, seq_len2, num_perspectives) consisting multi-perspective matching results BiMpmMatching # class BiMpmMatching ( nn . Module , FromParams ): | def __init__ ( | self , | hidden_dim : int = 100 , | num_perspectives : int = 20 , | share_weights_between_directions : bool = True , | is_forward : bool = None , | with_full_match : bool = True , | with_maxpool_match : bool = True , | with_attentive_match : bool = True , | with_max_attentive_match : bool = True | ) -> None This Module implements the matching layer of BiMPM model described in Bilateral Multi-Perspective Matching for Natural Language Sentences by Zhiguo Wang et al., 2017. Also please refer to the TensorFlow implementation and PyTorch implementation . Parameters hidden_dim : int , optional (default = 100 ) The hidden dimension of the representations num_perspectives : int , optional (default = 20 ) The number of perspectives for matching share_weights_between_directions : bool , optional (default = True ) If True, share weight between matching from sentence1 to sentence2 and from sentence2 to sentence1, useful for non-symmetric tasks is_forward : bool , optional (default = None ) Whether the matching is for forward sequence or backward sequence, useful in finding last token in full matching. It can not be None if with_full_match is True. with_full_match : bool , optional (default = True ) If True, include full match with_maxpool_match : bool , optional (default = True ) If True, include max pool match with_attentive_match : bool , optional (default = True ) If True, include attentive match with_max_attentive_match : bool , optional (default = True ) If True, include max attentive match get_output_dim # class BiMpmMatching ( nn . Module , FromParams ): | ... | def get_output_dim ( self ) -> int forward # class BiMpmMatching ( nn . Module , FromParams ): | ... | def forward ( | self , | context_1 : torch . Tensor , | mask_1 : torch . BoolTensor , | context_2 : torch . Tensor , | mask_2 : torch . BoolTensor | ) -> Tuple [ List [ torch . Tensor ], List [ torch . Tensor ]] Given the forward (or backward) representations of sentence1 and sentence2, apply four bilateral matching functions between them in one direction. Parameters context_1 : torch.Tensor Tensor of shape (batch_size, seq_len1, hidden_dim) representing the encoding of the first sentence. mask_1 : torch.BoolTensor Boolean Tensor of shape (batch_size, seq_len1), indicating which positions in the first sentence are padding (0) and which are not (1). context_2 : torch.Tensor Tensor of shape (batch_size, seq_len2, hidden_dim) representing the encoding of the second sentence. mask_2 : torch.BoolTensor Boolean Tensor of shape (batch_size, seq_len2), indicating which positions in the second sentence are padding (0) and which are not (1). Returns Tuple[List[torch.Tensor], List[torch.Tensor]] : A tuple of matching vectors for the two sentences. Each of which is a list of matching vectors of shape (batch, seq_len, num_perspectives or 1)","title":"bimpm_matching"},{"location":"api/modules/bimpm_matching/#multi_perspective_match","text":"def multi_perspective_match ( vector1 : torch . Tensor , vector2 : torch . Tensor , weight : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ] Calculate multi-perspective cosine matching between time-steps of vectors of the same length. Parameters vector1 : torch.Tensor A tensor of shape (batch, seq_len, hidden_size) vector2 : torch.Tensor A tensor of shape (batch, seq_len or 1, hidden_size) weight : torch.Tensor A tensor of shape (num_perspectives, hidden_size) Returns torch.Tensor : Shape (batch, seq_len, 1) . torch.Tensor : Shape (batch, seq_len, num_perspectives) .","title":"multi_perspective_match"},{"location":"api/modules/bimpm_matching/#multi_perspective_match_pairwise","text":"def multi_perspective_match_pairwise ( vector1 : torch . Tensor , vector2 : torch . Tensor , weight : torch . Tensor ) -> torch . Tensor Calculate multi-perspective cosine matching between each time step of one vector and each time step of another vector. Parameters vector1 : torch.Tensor A tensor of shape (batch, seq_len1, hidden_size) vector2 : torch.Tensor A tensor of shape (batch, seq_len2, hidden_size) weight : torch.Tensor A tensor of shape (num_perspectives, hidden_size) Returns torch.Tensor : A tensor of shape (batch, seq_len1, seq_len2, num_perspectives) consisting multi-perspective matching results","title":"multi_perspective_match_pairwise"},{"location":"api/modules/bimpm_matching/#bimpmmatching","text":"class BiMpmMatching ( nn . Module , FromParams ): | def __init__ ( | self , | hidden_dim : int = 100 , | num_perspectives : int = 20 , | share_weights_between_directions : bool = True , | is_forward : bool = None , | with_full_match : bool = True , | with_maxpool_match : bool = True , | with_attentive_match : bool = True , | with_max_attentive_match : bool = True | ) -> None This Module implements the matching layer of BiMPM model described in Bilateral Multi-Perspective Matching for Natural Language Sentences by Zhiguo Wang et al., 2017. Also please refer to the TensorFlow implementation and PyTorch implementation . Parameters hidden_dim : int , optional (default = 100 ) The hidden dimension of the representations num_perspectives : int , optional (default = 20 ) The number of perspectives for matching share_weights_between_directions : bool , optional (default = True ) If True, share weight between matching from sentence1 to sentence2 and from sentence2 to sentence1, useful for non-symmetric tasks is_forward : bool , optional (default = None ) Whether the matching is for forward sequence or backward sequence, useful in finding last token in full matching. It can not be None if with_full_match is True. with_full_match : bool , optional (default = True ) If True, include full match with_maxpool_match : bool , optional (default = True ) If True, include max pool match with_attentive_match : bool , optional (default = True ) If True, include attentive match with_max_attentive_match : bool , optional (default = True ) If True, include max attentive match","title":"BiMpmMatching"},{"location":"api/modules/bimpm_matching/#get_output_dim","text":"class BiMpmMatching ( nn . Module , FromParams ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/bimpm_matching/#forward","text":"class BiMpmMatching ( nn . Module , FromParams ): | ... | def forward ( | self , | context_1 : torch . Tensor , | mask_1 : torch . BoolTensor , | context_2 : torch . Tensor , | mask_2 : torch . BoolTensor | ) -> Tuple [ List [ torch . Tensor ], List [ torch . Tensor ]] Given the forward (or backward) representations of sentence1 and sentence2, apply four bilateral matching functions between them in one direction. Parameters context_1 : torch.Tensor Tensor of shape (batch_size, seq_len1, hidden_dim) representing the encoding of the first sentence. mask_1 : torch.BoolTensor Boolean Tensor of shape (batch_size, seq_len1), indicating which positions in the first sentence are padding (0) and which are not (1). context_2 : torch.Tensor Tensor of shape (batch_size, seq_len2, hidden_dim) representing the encoding of the second sentence. mask_2 : torch.BoolTensor Boolean Tensor of shape (batch_size, seq_len2), indicating which positions in the second sentence are padding (0) and which are not (1). Returns Tuple[List[torch.Tensor], List[torch.Tensor]] : A tuple of matching vectors for the two sentences. Each of which is a list of matching vectors of shape (batch, seq_len, num_perspectives or 1)","title":"forward"},{"location":"api/modules/conditional_random_field/","text":"[ allennlp .modules .conditional_random_field ] Conditional random field VITERBI_DECODING # VITERBI_DECODING = Tuple [ List [ int ], float ] allowed_transitions # def allowed_transitions ( constraint_type : str , labels : Dict [ int , str ] ) -> List [ Tuple [ int , int ]] Given labels and a constraint type, returns the allowed transitions. It will additionally include transitions for the start and end states, which are used by the conditional random field. Parameters constraint_type : str Indicates which constraint to apply. Current choices are \"BIO\", \"IOB1\", \"BIOUL\", and \"BMES\". labels : Dict[int, str] A mapping {label_id -> label}. Most commonly this would be the value from Vocabulary.get_index_to_token_vocabulary() Returns List[Tuple[int, int]] The allowed transitions (from_label_id, to_label_id). is_transition_allowed # def is_transition_allowed ( constraint_type : str , from_tag : str , from_entity : str , to_tag : str , to_entity : str ) Given a constraint type and strings from_tag and to_tag that represent the origin and destination of the transition, return whether the transition is allowed under the given constraint type. Parameters constraint_type : str Indicates which constraint to apply. Current choices are \"BIO\", \"IOB1\", \"BIOUL\", and \"BMES\". from_tag : str The tag that the transition originates from. For example, if the label is I-PER , the from_tag is I . from_entity : str The entity corresponding to the from_tag . For example, if the label is I-PER , the from_entity is PER . to_tag : str The tag that the transition leads to. For example, if the label is I-PER , the to_tag is I . to_entity : str The entity corresponding to the to_tag . For example, if the label is I-PER , the to_entity is PER . Returns bool Whether the transition is allowed under the given constraint_type . ConditionalRandomField # class ConditionalRandomField ( torch . nn . Module ): | def __init__ ( | self , | num_tags : int , | constraints : List [ Tuple [ int , int ]] = None , | include_start_end_transitions : bool = True | ) -> None This module uses the \"forward-backward\" algorithm to compute the log-likelihood of its inputs assuming a conditional random field model. See, e.g. http://www.cs.columbia.edu/~mcollins/fb.pdf Parameters num_tags : int The number of tags. constraints : List[Tuple[int, int]] , optional (default = None ) An optional list of allowed transitions (from_tag_id, to_tag_id). These are applied to viterbi_tags() but do not affect forward() . These should be derived from allowed_transitions so that the start and end transitions are handled correctly for your tag type. include_start_end_transitions : bool , optional (default = True ) Whether to include the start and end transition parameters. reset_parameters # class ConditionalRandomField ( torch . nn . Module ): | ... | def reset_parameters ( self ) forward # class ConditionalRandomField ( torch . nn . Module ): | ... | def forward ( | self , | inputs : torch . Tensor , | tags : torch . Tensor , | mask : torch . BoolTensor = None | ) -> torch . Tensor Computes the log likelihood. viterbi_tags # class ConditionalRandomField ( torch . nn . Module ): | ... | def viterbi_tags ( | self , | logits : torch . Tensor , | mask : torch . BoolTensor = None , | top_k : int = None | ) -> Union [ List [ VITERBI_DECODING ], List [ List [ VITERBI_DECODING ]]] Uses viterbi algorithm to find most likely tags for the given inputs. If constraints are applied, disallows all other transitions. Returns a list of results, of the same size as the batch (one result per batch member) Each result is a List of length top_k, containing the top K viterbi decodings Each decoding is a tuple (tag_sequence, viterbi_score) For backwards compatibility, if top_k is None, then instead returns a flat list of tag sequences (the top tag sequence for each batch item).","title":"conditional_random_field"},{"location":"api/modules/conditional_random_field/#viterbi_decoding","text":"VITERBI_DECODING = Tuple [ List [ int ], float ]","title":"VITERBI_DECODING"},{"location":"api/modules/conditional_random_field/#allowed_transitions","text":"def allowed_transitions ( constraint_type : str , labels : Dict [ int , str ] ) -> List [ Tuple [ int , int ]] Given labels and a constraint type, returns the allowed transitions. It will additionally include transitions for the start and end states, which are used by the conditional random field. Parameters constraint_type : str Indicates which constraint to apply. Current choices are \"BIO\", \"IOB1\", \"BIOUL\", and \"BMES\". labels : Dict[int, str] A mapping {label_id -> label}. Most commonly this would be the value from Vocabulary.get_index_to_token_vocabulary() Returns List[Tuple[int, int]] The allowed transitions (from_label_id, to_label_id).","title":"allowed_transitions"},{"location":"api/modules/conditional_random_field/#is_transition_allowed","text":"def is_transition_allowed ( constraint_type : str , from_tag : str , from_entity : str , to_tag : str , to_entity : str ) Given a constraint type and strings from_tag and to_tag that represent the origin and destination of the transition, return whether the transition is allowed under the given constraint type. Parameters constraint_type : str Indicates which constraint to apply. Current choices are \"BIO\", \"IOB1\", \"BIOUL\", and \"BMES\". from_tag : str The tag that the transition originates from. For example, if the label is I-PER , the from_tag is I . from_entity : str The entity corresponding to the from_tag . For example, if the label is I-PER , the from_entity is PER . to_tag : str The tag that the transition leads to. For example, if the label is I-PER , the to_tag is I . to_entity : str The entity corresponding to the to_tag . For example, if the label is I-PER , the to_entity is PER . Returns bool Whether the transition is allowed under the given constraint_type .","title":"is_transition_allowed"},{"location":"api/modules/conditional_random_field/#conditionalrandomfield","text":"class ConditionalRandomField ( torch . nn . Module ): | def __init__ ( | self , | num_tags : int , | constraints : List [ Tuple [ int , int ]] = None , | include_start_end_transitions : bool = True | ) -> None This module uses the \"forward-backward\" algorithm to compute the log-likelihood of its inputs assuming a conditional random field model. See, e.g. http://www.cs.columbia.edu/~mcollins/fb.pdf Parameters num_tags : int The number of tags. constraints : List[Tuple[int, int]] , optional (default = None ) An optional list of allowed transitions (from_tag_id, to_tag_id). These are applied to viterbi_tags() but do not affect forward() . These should be derived from allowed_transitions so that the start and end transitions are handled correctly for your tag type. include_start_end_transitions : bool , optional (default = True ) Whether to include the start and end transition parameters.","title":"ConditionalRandomField"},{"location":"api/modules/conditional_random_field/#reset_parameters","text":"class ConditionalRandomField ( torch . nn . Module ): | ... | def reset_parameters ( self )","title":"reset_parameters"},{"location":"api/modules/conditional_random_field/#forward","text":"class ConditionalRandomField ( torch . nn . Module ): | ... | def forward ( | self , | inputs : torch . Tensor , | tags : torch . Tensor , | mask : torch . BoolTensor = None | ) -> torch . Tensor Computes the log likelihood.","title":"forward"},{"location":"api/modules/conditional_random_field/#viterbi_tags","text":"class ConditionalRandomField ( torch . nn . Module ): | ... | def viterbi_tags ( | self , | logits : torch . Tensor , | mask : torch . BoolTensor = None , | top_k : int = None | ) -> Union [ List [ VITERBI_DECODING ], List [ List [ VITERBI_DECODING ]]] Uses viterbi algorithm to find most likely tags for the given inputs. If constraints are applied, disallows all other transitions. Returns a list of results, of the same size as the batch (one result per batch member) Each result is a List of length top_k, containing the top K viterbi decodings Each decoding is a tuple (tag_sequence, viterbi_score) For backwards compatibility, if top_k is None, then instead returns a flat list of tag sequences (the top tag sequence for each batch item).","title":"viterbi_tags"},{"location":"api/modules/elmo/","text":"[ allennlp .modules .elmo ] Elmo # class Elmo ( torch . nn . Module , FromParams ): | def __init__ ( | self , | options_file : str , | weight_file : str , | num_output_representations : int , | requires_grad : bool = False , | do_layer_norm : bool = False , | dropout : float = 0.5 , | vocab_to_cache : List [ str ] = None , | keep_sentence_boundaries : bool = False , | scalar_mix_parameters : List [ float ] = None , | module : torch . nn . Module = None | ) -> None Compute ELMo representations using a pre-trained bidirectional language model. See \"Deep contextualized word representations\", Peters et al. for details. This module takes character id input and computes num_output_representations different layers of ELMo representations. Typically num_output_representations is 1 or 2. For example, in the case of the SRL model in the above paper, num_output_representations=1 where ELMo was included at the input token representation layer. In the case of the SQuAD model, num_output_representations=2 as ELMo was also included at the GRU output layer. In the implementation below, we learn separate scalar weights for each output layer, but only run the biLM once on each input sequence for efficiency. Parameters options_file : str ELMo JSON options file weight_file : str ELMo hdf5 weight file num_output_representations : int The number of ELMo representation to output with different linear weighted combination of the 3 layers (i.e., character-convnet output, 1st lstm output, 2nd lstm output). requires_grad : bool , optional If True, compute gradient of ELMo parameters for fine tuning. do_layer_norm : bool , optional (default = False ) Should we apply layer normalization (passed to ScalarMix )? dropout : float , optional (default = 0.5 ) The dropout to be applied to the ELMo representations. vocab_to_cache : List[str] , optional (default = None ) A list of words to pre-compute and cache character convolutions for. If you use this option, Elmo expects that you pass word indices of shape (batch_size, timesteps) to forward, instead of character indices. If you use this option and pass a word which wasn't pre-cached, this will break. keep_sentence_boundaries : bool , optional (default = False ) If True, the representation of the sentence boundary tokens are not removed. scalar_mix_parameters : List[float] , optional (default = None ) If not None , use these scalar mix parameters to weight the representations produced by different layers. These mixing weights are not updated during training. The mixing weights here should be the unnormalized (i.e., pre-softmax) weights. So, if you wanted to use only the 1st layer of a 2-layer ELMo, you can set this to [-9e10, 1, -9e10 ]. module : torch.nn.Module , optional (default = None ) If provided, then use this module instead of the pre-trained ELMo biLM. If using this option, then pass None for both options_file and weight_file . The module must provide a public attribute num_layers with the number of internal layers and its forward method must return a dict with activations and mask keys (see _ElmoBilm for an example). Note that requires_grad is also ignored with this option. get_output_dim # class Elmo ( torch . nn . Module , FromParams ): | ... | def get_output_dim ( self ) forward # class Elmo ( torch . nn . Module , FromParams ): | ... | def forward ( | self , | inputs : torch . Tensor , | word_inputs : torch . Tensor = None | ) -> Dict [ str , Union [ torch . Tensor , List [ torch . Tensor ]]] Parameters inputs : torch.Tensor Shape (batch_size, timesteps, 50) of character ids representing the current batch. word_inputs : torch.Tensor If you passed a cached vocab, you can in addition pass a tensor of shape (batch_size, timesteps) , which represent word ids which have been pre-cached. Returns Dict[str, Union[torch.Tensor, List[torch.Tensor]]] A dict with the following keys: 'elmo_representations' ( List[torch.Tensor] ) : A num_output_representations list of ELMo representations for the input sequence. Each representation is shape (batch_size, timesteps, embedding_dim) 'mask' ( torch.BoolTensor ) : Shape (batch_size, timesteps) long tensor with sequence mask. batch_to_ids # def batch_to_ids ( batch : List [ List [ str ]]) -> torch . Tensor Converts a batch of tokenized sentences to a tensor representing the sentences with encoded characters (len(batch), max sentence length, max word length). Parameters batch : List[List[str]] A list of tokenized sentences. Returns A tensor of padded character ids.","title":"elmo"},{"location":"api/modules/elmo/#elmo","text":"class Elmo ( torch . nn . Module , FromParams ): | def __init__ ( | self , | options_file : str , | weight_file : str , | num_output_representations : int , | requires_grad : bool = False , | do_layer_norm : bool = False , | dropout : float = 0.5 , | vocab_to_cache : List [ str ] = None , | keep_sentence_boundaries : bool = False , | scalar_mix_parameters : List [ float ] = None , | module : torch . nn . Module = None | ) -> None Compute ELMo representations using a pre-trained bidirectional language model. See \"Deep contextualized word representations\", Peters et al. for details. This module takes character id input and computes num_output_representations different layers of ELMo representations. Typically num_output_representations is 1 or 2. For example, in the case of the SRL model in the above paper, num_output_representations=1 where ELMo was included at the input token representation layer. In the case of the SQuAD model, num_output_representations=2 as ELMo was also included at the GRU output layer. In the implementation below, we learn separate scalar weights for each output layer, but only run the biLM once on each input sequence for efficiency. Parameters options_file : str ELMo JSON options file weight_file : str ELMo hdf5 weight file num_output_representations : int The number of ELMo representation to output with different linear weighted combination of the 3 layers (i.e., character-convnet output, 1st lstm output, 2nd lstm output). requires_grad : bool , optional If True, compute gradient of ELMo parameters for fine tuning. do_layer_norm : bool , optional (default = False ) Should we apply layer normalization (passed to ScalarMix )? dropout : float , optional (default = 0.5 ) The dropout to be applied to the ELMo representations. vocab_to_cache : List[str] , optional (default = None ) A list of words to pre-compute and cache character convolutions for. If you use this option, Elmo expects that you pass word indices of shape (batch_size, timesteps) to forward, instead of character indices. If you use this option and pass a word which wasn't pre-cached, this will break. keep_sentence_boundaries : bool , optional (default = False ) If True, the representation of the sentence boundary tokens are not removed. scalar_mix_parameters : List[float] , optional (default = None ) If not None , use these scalar mix parameters to weight the representations produced by different layers. These mixing weights are not updated during training. The mixing weights here should be the unnormalized (i.e., pre-softmax) weights. So, if you wanted to use only the 1st layer of a 2-layer ELMo, you can set this to [-9e10, 1, -9e10 ]. module : torch.nn.Module , optional (default = None ) If provided, then use this module instead of the pre-trained ELMo biLM. If using this option, then pass None for both options_file and weight_file . The module must provide a public attribute num_layers with the number of internal layers and its forward method must return a dict with activations and mask keys (see _ElmoBilm for an example). Note that requires_grad is also ignored with this option.","title":"Elmo"},{"location":"api/modules/elmo/#get_output_dim","text":"class Elmo ( torch . nn . Module , FromParams ): | ... | def get_output_dim ( self )","title":"get_output_dim"},{"location":"api/modules/elmo/#forward","text":"class Elmo ( torch . nn . Module , FromParams ): | ... | def forward ( | self , | inputs : torch . Tensor , | word_inputs : torch . Tensor = None | ) -> Dict [ str , Union [ torch . Tensor , List [ torch . Tensor ]]] Parameters inputs : torch.Tensor Shape (batch_size, timesteps, 50) of character ids representing the current batch. word_inputs : torch.Tensor If you passed a cached vocab, you can in addition pass a tensor of shape (batch_size, timesteps) , which represent word ids which have been pre-cached. Returns Dict[str, Union[torch.Tensor, List[torch.Tensor]]] A dict with the following keys: 'elmo_representations' ( List[torch.Tensor] ) : A num_output_representations list of ELMo representations for the input sequence. Each representation is shape (batch_size, timesteps, embedding_dim) 'mask' ( torch.BoolTensor ) : Shape (batch_size, timesteps) long tensor with sequence mask.","title":"forward"},{"location":"api/modules/elmo/#batch_to_ids","text":"def batch_to_ids ( batch : List [ List [ str ]]) -> torch . Tensor Converts a batch of tokenized sentences to a tensor representing the sentences with encoded characters (len(batch), max sentence length, max word length). Parameters batch : List[List[str]] A list of tokenized sentences. Returns A tensor of padded character ids.","title":"batch_to_ids"},{"location":"api/modules/elmo_lstm/","text":"[ allennlp .modules .elmo_lstm ] A stacked bidirectional LSTM with skip connections between layers. ElmoLstm # class ElmoLstm ( _EncoderBase ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | cell_size : int , | num_layers : int , | requires_grad : bool = False , | recurrent_dropout_probability : float = 0.0 , | memory_cell_clip_value : Optional [ float ] = None , | state_projection_clip_value : Optional [ float ] = None | ) -> None A stacked, bidirectional LSTM which uses LstmCellWithProjection 's with highway layers between the inputs to layers. The inputs to the forward and backward directions are independent - forward and backward states are not concatenated between layers. Additionally, this LSTM maintains its own state, which is updated every time forward is called. It is dynamically resized for different batch sizes and is designed for use with non-continuous inputs (i.e inputs which aren't formatted as a stream, such as text used for a language modeling task, which is how stateful RNNs are typically used). This is non-standard, but can be thought of as having an \"end of sentence\" state, which is carried across different sentences. Parameters input_size : int The dimension of the inputs to the LSTM. hidden_size : int The dimension of the outputs of the LSTM. cell_size : int The dimension of the memory cell of the LstmCellWithProjection . num_layers : int The number of bidirectional LSTMs to use. requires_grad : bool , optional If True, compute gradient of ELMo parameters for fine tuning. recurrent_dropout_probability : float , optional (default = 0.0 ) The dropout probability to be used in a dropout scheme as stated in A Theoretically Grounded Application of Dropout in Recurrent Neural Networks . state_projection_clip_value : float , optional (default = None ) The magnitude with which to clip the hidden_state after projecting it. memory_cell_clip_value : float , optional (default = None ) The magnitude with which to clip the memory cell. forward # class ElmoLstm ( _EncoderBase ): | ... | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor | ) -> torch . Tensor Parameters inputs : torch.Tensor A Tensor of shape (batch_size, sequence_length, hidden_size) . mask : torch.BoolTensor A binary mask of shape (batch_size, sequence_length) representing the non-padded elements in each sequence in the batch. Returns torch.Tensor A torch.Tensor of shape (num_layers, batch_size, sequence_length, hidden_size), where the num_layers dimension represents the LSTM output from that layer. load_weights # class ElmoLstm ( _EncoderBase ): | ... | def load_weights ( self , weight_file : str ) -> None Load the pre-trained weights from the file.","title":"elmo_lstm"},{"location":"api/modules/elmo_lstm/#elmolstm","text":"class ElmoLstm ( _EncoderBase ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | cell_size : int , | num_layers : int , | requires_grad : bool = False , | recurrent_dropout_probability : float = 0.0 , | memory_cell_clip_value : Optional [ float ] = None , | state_projection_clip_value : Optional [ float ] = None | ) -> None A stacked, bidirectional LSTM which uses LstmCellWithProjection 's with highway layers between the inputs to layers. The inputs to the forward and backward directions are independent - forward and backward states are not concatenated between layers. Additionally, this LSTM maintains its own state, which is updated every time forward is called. It is dynamically resized for different batch sizes and is designed for use with non-continuous inputs (i.e inputs which aren't formatted as a stream, such as text used for a language modeling task, which is how stateful RNNs are typically used). This is non-standard, but can be thought of as having an \"end of sentence\" state, which is carried across different sentences. Parameters input_size : int The dimension of the inputs to the LSTM. hidden_size : int The dimension of the outputs of the LSTM. cell_size : int The dimension of the memory cell of the LstmCellWithProjection . num_layers : int The number of bidirectional LSTMs to use. requires_grad : bool , optional If True, compute gradient of ELMo parameters for fine tuning. recurrent_dropout_probability : float , optional (default = 0.0 ) The dropout probability to be used in a dropout scheme as stated in A Theoretically Grounded Application of Dropout in Recurrent Neural Networks . state_projection_clip_value : float , optional (default = None ) The magnitude with which to clip the hidden_state after projecting it. memory_cell_clip_value : float , optional (default = None ) The magnitude with which to clip the memory cell.","title":"ElmoLstm"},{"location":"api/modules/elmo_lstm/#forward","text":"class ElmoLstm ( _EncoderBase ): | ... | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor | ) -> torch . Tensor Parameters inputs : torch.Tensor A Tensor of shape (batch_size, sequence_length, hidden_size) . mask : torch.BoolTensor A binary mask of shape (batch_size, sequence_length) representing the non-padded elements in each sequence in the batch. Returns torch.Tensor A torch.Tensor of shape (num_layers, batch_size, sequence_length, hidden_size), where the num_layers dimension represents the LSTM output from that layer.","title":"forward"},{"location":"api/modules/elmo_lstm/#load_weights","text":"class ElmoLstm ( _EncoderBase ): | ... | def load_weights ( self , weight_file : str ) -> None Load the pre-trained weights from the file.","title":"load_weights"},{"location":"api/modules/encoder_base/","text":"[ allennlp .modules .encoder_base ] RnnState # RnnState = Union [ torch . Tensor , Tuple [ torch . Tensor , torch . Tensor ]] RnnStateStorage # RnnStateStorage = Tuple [ torch . Tensor , ... ]","title":"encoder_base"},{"location":"api/modules/encoder_base/#rnnstate","text":"RnnState = Union [ torch . Tensor , Tuple [ torch . Tensor , torch . Tensor ]]","title":"RnnState"},{"location":"api/modules/encoder_base/#rnnstatestorage","text":"RnnStateStorage = Tuple [ torch . Tensor , ... ]","title":"RnnStateStorage"},{"location":"api/modules/feedforward/","text":"[ allennlp .modules .feedforward ] A feed-forward neural network. FeedForward # class FeedForward ( torch . nn . Module , FromParams ): | def __init__ ( | self , | input_dim : int , | num_layers : int , | hidden_dims : Union [ int , List [ int ]], | activations : Union [ Activation , List [ Activation ]], | dropout : Union [ float , List [ float ]] = 0.0 | ) -> None This Module is a feed-forward neural network, just a sequence of Linear layers with activation functions in between. Parameters input_dim : int The dimensionality of the input. We assume the input has shape (batch_size, input_dim) . num_layers : int The number of Linear layers to apply to the input. hidden_dims : Union[int, List[int]] The output dimension of each of the Linear layers. If this is a single int , we use it for all Linear layers. If it is a List[int] , len(hidden_dims) must be num_layers . activations : Union[Activation, List[Activation]] The activation function to use after each Linear layer. If this is a single function, we use it after all Linear layers. If it is a List[Activation] , len(activations) must be num_layers . Activation must have torch.nn.Module type. dropout : Union[float, List[float]] , optional (default = 0.0 ) If given, we will apply this amount of dropout after each layer. Semantics of float versus List[float] is the same as with other parameters. Examples FeedForward ( 124 , 2 , [ 64 , 32 ], torch . nn . ReLU (), 0.2 ) #> FeedForward( #> (_activations): ModuleList( #> (0): ReLU() #> (1): ReLU() #> ) #> (_linear_layers): ModuleList( #> (0): Linear(in_features=124, out_features=64, bias=True) #> (1): Linear(in_features=64, out_features=32, bias=True) #> ) #> (_dropout): ModuleList( #> (0): Dropout(p=0.2, inplace=False) #> (1): Dropout(p=0.2, inplace=False) #> ) #> ) get_output_dim # class FeedForward ( torch . nn . Module , FromParams ): | ... | def get_output_dim ( self ) get_input_dim # class FeedForward ( torch . nn . Module , FromParams ): | ... | def get_input_dim ( self ) forward # class FeedForward ( torch . nn . Module , FromParams ): | ... | def forward ( self , inputs : torch . Tensor ) -> torch . Tensor","title":"feedforward"},{"location":"api/modules/feedforward/#feedforward","text":"class FeedForward ( torch . nn . Module , FromParams ): | def __init__ ( | self , | input_dim : int , | num_layers : int , | hidden_dims : Union [ int , List [ int ]], | activations : Union [ Activation , List [ Activation ]], | dropout : Union [ float , List [ float ]] = 0.0 | ) -> None This Module is a feed-forward neural network, just a sequence of Linear layers with activation functions in between. Parameters input_dim : int The dimensionality of the input. We assume the input has shape (batch_size, input_dim) . num_layers : int The number of Linear layers to apply to the input. hidden_dims : Union[int, List[int]] The output dimension of each of the Linear layers. If this is a single int , we use it for all Linear layers. If it is a List[int] , len(hidden_dims) must be num_layers . activations : Union[Activation, List[Activation]] The activation function to use after each Linear layer. If this is a single function, we use it after all Linear layers. If it is a List[Activation] , len(activations) must be num_layers . Activation must have torch.nn.Module type. dropout : Union[float, List[float]] , optional (default = 0.0 ) If given, we will apply this amount of dropout after each layer. Semantics of float versus List[float] is the same as with other parameters. Examples FeedForward ( 124 , 2 , [ 64 , 32 ], torch . nn . ReLU (), 0.2 ) #> FeedForward( #> (_activations): ModuleList( #> (0): ReLU() #> (1): ReLU() #> ) #> (_linear_layers): ModuleList( #> (0): Linear(in_features=124, out_features=64, bias=True) #> (1): Linear(in_features=64, out_features=32, bias=True) #> ) #> (_dropout): ModuleList( #> (0): Dropout(p=0.2, inplace=False) #> (1): Dropout(p=0.2, inplace=False) #> ) #> )","title":"FeedForward"},{"location":"api/modules/feedforward/#get_output_dim","text":"class FeedForward ( torch . nn . Module , FromParams ): | ... | def get_output_dim ( self )","title":"get_output_dim"},{"location":"api/modules/feedforward/#get_input_dim","text":"class FeedForward ( torch . nn . Module , FromParams ): | ... | def get_input_dim ( self )","title":"get_input_dim"},{"location":"api/modules/feedforward/#forward","text":"class FeedForward ( torch . nn . Module , FromParams ): | ... | def forward ( self , inputs : torch . Tensor ) -> torch . Tensor","title":"forward"},{"location":"api/modules/gated_sum/","text":"[ allennlp .modules .gated_sum ] GatedSum # class GatedSum ( torch . nn . Module ): | def __init__ ( | self , | input_dim : int , | activation : Activation = torch . nn . Sigmoid () | ) -> None This Module represents a gated sum of two tensors a and b . Specifically: f = activation(W [a; b]) out = f * a + (1 - f) * b Parameters input_dim : int The dimensionality of the input. We assume the input have shape (..., input_dim) . activation : Activation , optional (default = torch.nn.Sigmoid() ) The activation function to use. get_input_dim # class GatedSum ( torch . nn . Module ): | ... | def get_input_dim ( self ) get_output_dim # class GatedSum ( torch . nn . Module ): | ... | def get_output_dim ( self ) forward # class GatedSum ( torch . nn . Module ): | ... | def forward ( | self , | input_a : torch . Tensor , | input_b : torch . Tensor | ) -> torch . Tensor","title":"gated_sum"},{"location":"api/modules/gated_sum/#gatedsum","text":"class GatedSum ( torch . nn . Module ): | def __init__ ( | self , | input_dim : int , | activation : Activation = torch . nn . Sigmoid () | ) -> None This Module represents a gated sum of two tensors a and b . Specifically: f = activation(W [a; b]) out = f * a + (1 - f) * b Parameters input_dim : int The dimensionality of the input. We assume the input have shape (..., input_dim) . activation : Activation , optional (default = torch.nn.Sigmoid() ) The activation function to use.","title":"GatedSum"},{"location":"api/modules/gated_sum/#get_input_dim","text":"class GatedSum ( torch . nn . Module ): | ... | def get_input_dim ( self )","title":"get_input_dim"},{"location":"api/modules/gated_sum/#get_output_dim","text":"class GatedSum ( torch . nn . Module ): | ... | def get_output_dim ( self )","title":"get_output_dim"},{"location":"api/modules/gated_sum/#forward","text":"class GatedSum ( torch . nn . Module ): | ... | def forward ( | self , | input_a : torch . Tensor , | input_b : torch . Tensor | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/highway/","text":"[ allennlp .modules .highway ] A Highway layer that does a gated combination of a linear transformation and a non-linear transformation of its input. Highway # class Highway ( torch . nn . Module ): | def __init__ ( | self , | input_dim : int , | num_layers : int = 1 , | activation : Callable [[ torch . Tensor ], torch . Tensor ] = torch . nn . functional . relu | ) -> None A Highway layer does a gated combination of a linear transformation and a non-linear transformation of its input. :math: y = g * x + (1 - g) * f(A(x)) , where :math: A is a linear transformation, :math: f is an element-wise non-linearity, and :math: g is an element-wise gate, computed as :math: sigmoid(B(x)) . This module will apply a fixed number of highway layers to its input, returning the final result. Parameters input_dim : int The dimensionality of :math: x . We assume the input has shape (batch_size, ..., input_dim) . num_layers : int , optional (default = 1 ) The number of highway layers to apply to the input. activation : Callable[[torch.Tensor], torch.Tensor] , optional (default = torch.nn.functional.relu ) The non-linearity to use in the highway layers. forward # class Highway ( torch . nn . Module ): | ... | @overrides | def forward ( self , inputs : torch . Tensor ) -> torch . Tensor","title":"highway"},{"location":"api/modules/highway/#highway","text":"class Highway ( torch . nn . Module ): | def __init__ ( | self , | input_dim : int , | num_layers : int = 1 , | activation : Callable [[ torch . Tensor ], torch . Tensor ] = torch . nn . functional . relu | ) -> None A Highway layer does a gated combination of a linear transformation and a non-linear transformation of its input. :math: y = g * x + (1 - g) * f(A(x)) , where :math: A is a linear transformation, :math: f is an element-wise non-linearity, and :math: g is an element-wise gate, computed as :math: sigmoid(B(x)) . This module will apply a fixed number of highway layers to its input, returning the final result. Parameters input_dim : int The dimensionality of :math: x . We assume the input has shape (batch_size, ..., input_dim) . num_layers : int , optional (default = 1 ) The number of highway layers to apply to the input. activation : Callable[[torch.Tensor], torch.Tensor] , optional (default = torch.nn.functional.relu ) The non-linearity to use in the highway layers.","title":"Highway"},{"location":"api/modules/highway/#forward","text":"class Highway ( torch . nn . Module ): | ... | @overrides | def forward ( self , inputs : torch . Tensor ) -> torch . Tensor","title":"forward"},{"location":"api/modules/input_variational_dropout/","text":"[ allennlp .modules .input_variational_dropout ] InputVariationalDropout # class InputVariationalDropout ( torch . nn . Dropout ) Apply the dropout technique in Gal and Ghahramani, Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning to a 3D tensor. This module accepts a 3D tensor of shape (batch_size, num_timesteps, embedding_dim) and samples a single dropout mask of shape (batch_size, embedding_dim) and applies it to every time step. forward # class InputVariationalDropout ( torch . nn . Dropout ): | ... | def forward ( self , input_tensor ) Apply dropout to input tensor. Parameters input_tensor : torch.FloatTensor A tensor of shape (batch_size, num_timesteps, embedding_dim) Returns output : torch.FloatTensor A tensor of shape (batch_size, num_timesteps, embedding_dim) with dropout applied.","title":"input_variational_dropout"},{"location":"api/modules/input_variational_dropout/#inputvariationaldropout","text":"class InputVariationalDropout ( torch . nn . Dropout ) Apply the dropout technique in Gal and Ghahramani, Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning to a 3D tensor. This module accepts a 3D tensor of shape (batch_size, num_timesteps, embedding_dim) and samples a single dropout mask of shape (batch_size, embedding_dim) and applies it to every time step.","title":"InputVariationalDropout"},{"location":"api/modules/input_variational_dropout/#forward","text":"class InputVariationalDropout ( torch . nn . Dropout ): | ... | def forward ( self , input_tensor ) Apply dropout to input tensor. Parameters input_tensor : torch.FloatTensor A tensor of shape (batch_size, num_timesteps, embedding_dim) Returns output : torch.FloatTensor A tensor of shape (batch_size, num_timesteps, embedding_dim) with dropout applied.","title":"forward"},{"location":"api/modules/layer_norm/","text":"[ allennlp .modules .layer_norm ] LayerNorm # class LayerNorm ( torch . nn . Module ): | def __init__ ( self , dimension : int ) -> None An implementation of Layer Normalization . Layer Normalization stabilises the training of deep neural networks by normalising the outputs of neurons from a particular layer. It computes: output = (gamma * (tensor - mean) / (std + eps)) + beta Parameters dimension : int The dimension of the layer output to normalize. Returns The normalized layer output. forward # class LayerNorm ( torch . nn . Module ): | ... | def forward ( self , tensor : torch . Tensor )","title":"layer_norm"},{"location":"api/modules/layer_norm/#layernorm","text":"class LayerNorm ( torch . nn . Module ): | def __init__ ( self , dimension : int ) -> None An implementation of Layer Normalization . Layer Normalization stabilises the training of deep neural networks by normalising the outputs of neurons from a particular layer. It computes: output = (gamma * (tensor - mean) / (std + eps)) + beta Parameters dimension : int The dimension of the layer output to normalize. Returns The normalized layer output.","title":"LayerNorm"},{"location":"api/modules/layer_norm/#forward","text":"class LayerNorm ( torch . nn . Module ): | ... | def forward ( self , tensor : torch . Tensor )","title":"forward"},{"location":"api/modules/lstm_cell_with_projection/","text":"[ allennlp .modules .lstm_cell_with_projection ] An LSTM with Recurrent Dropout, a hidden_state which is projected and clipping on both the hidden state and the memory state of the LSTM. LstmCellWithProjection # class LstmCellWithProjection ( torch . nn . Module ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | cell_size : int , | go_forward : bool = True , | recurrent_dropout_probability : float = 0.0 , | memory_cell_clip_value : Optional [ float ] = None , | state_projection_clip_value : Optional [ float ] = None | ) -> None An LSTM with Recurrent Dropout and a projected and clipped hidden state and memory. Note: this implementation is slower than the native Pytorch LSTM because it cannot make use of CUDNN optimizations for stacked RNNs due to and variational dropout and the custom nature of the cell state. Parameters input_size : int The dimension of the inputs to the LSTM. hidden_size : int The dimension of the outputs of the LSTM. cell_size : int The dimension of the memory cell used for the LSTM. go_forward : bool , optional (default = True ) The direction in which the LSTM is applied to the sequence. Forwards by default, or backwards if False. recurrent_dropout_probability : float , optional (default = 0.0 ) The dropout probability to be used in a dropout scheme as stated in [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks] 0 . Implementation wise, this simply applies a fixed dropout mask per sequence to the recurrent connection of the LSTM. state_projection_clip_value : float , optional (default = None ) The magnitude with which to clip the hidden_state after projecting it. memory_cell_clip_value : float , optional (default = None ) The magnitude with which to clip the memory cell. Returns output_accumulator : torch.FloatTensor The outputs of the LSTM for each timestep. A tensor of shape (batch_size, max_timesteps, hidden_size) where for a given batch element, all outputs past the sequence length for that batch are zero tensors. final_state : Tuple[torch.FloatTensor, torch.FloatTensor] The final (state, memory) states of the LSTM, with shape (1, batch_size, hidden_size) and (1, batch_size, cell_size) respectively. The first dimension is 1 in order to match the Pytorch API for returning stacked LSTM states. reset_parameters # class LstmCellWithProjection ( torch . nn . Module ): | ... | def reset_parameters ( self ) Use sensible default initializations for parameters. forward # class LstmCellWithProjection ( torch . nn . Module ): | ... | def forward ( | self , | inputs : torch . FloatTensor , | batch_lengths : List [ int ], | initial_state : Optional [ Tuple [ torch . Tensor , torch . Tensor ]] = None | ) Parameters inputs : torch.FloatTensor A tensor of shape (batch_size, num_timesteps, input_size) to apply the LSTM over. batch_lengths : List[int] A list of length batch_size containing the lengths of the sequences in batch. initial_state : Tuple[torch.Tensor, torch.Tensor] , optional (default = None ) A tuple (state, memory) representing the initial hidden state and memory of the LSTM. The state has shape (1, batch_size, hidden_size) and the memory has shape (1, batch_size, cell_size). Returns output_accumulator : torch.FloatTensor The outputs of the LSTM for each timestep. A tensor of shape (batch_size, max_timesteps, hidden_size) where for a given batch element, all outputs past the sequence length for that batch are zero tensors. final_state : Tuple[torch.FloatTensor, torch.FloatTensor] A tuple (state, memory) representing the initial hidden state and memory of the LSTM. The state has shape (1, batch_size, hidden_size) and the memory has shape (1, batch_size, cell_size).","title":"lstm_cell_with_projection"},{"location":"api/modules/lstm_cell_with_projection/#lstmcellwithprojection","text":"class LstmCellWithProjection ( torch . nn . Module ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | cell_size : int , | go_forward : bool = True , | recurrent_dropout_probability : float = 0.0 , | memory_cell_clip_value : Optional [ float ] = None , | state_projection_clip_value : Optional [ float ] = None | ) -> None An LSTM with Recurrent Dropout and a projected and clipped hidden state and memory. Note: this implementation is slower than the native Pytorch LSTM because it cannot make use of CUDNN optimizations for stacked RNNs due to and variational dropout and the custom nature of the cell state. Parameters input_size : int The dimension of the inputs to the LSTM. hidden_size : int The dimension of the outputs of the LSTM. cell_size : int The dimension of the memory cell used for the LSTM. go_forward : bool , optional (default = True ) The direction in which the LSTM is applied to the sequence. Forwards by default, or backwards if False. recurrent_dropout_probability : float , optional (default = 0.0 ) The dropout probability to be used in a dropout scheme as stated in [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks] 0 . Implementation wise, this simply applies a fixed dropout mask per sequence to the recurrent connection of the LSTM. state_projection_clip_value : float , optional (default = None ) The magnitude with which to clip the hidden_state after projecting it. memory_cell_clip_value : float , optional (default = None ) The magnitude with which to clip the memory cell. Returns output_accumulator : torch.FloatTensor The outputs of the LSTM for each timestep. A tensor of shape (batch_size, max_timesteps, hidden_size) where for a given batch element, all outputs past the sequence length for that batch are zero tensors. final_state : Tuple[torch.FloatTensor, torch.FloatTensor] The final (state, memory) states of the LSTM, with shape (1, batch_size, hidden_size) and (1, batch_size, cell_size) respectively. The first dimension is 1 in order to match the Pytorch API for returning stacked LSTM states.","title":"LstmCellWithProjection"},{"location":"api/modules/lstm_cell_with_projection/#reset_parameters","text":"class LstmCellWithProjection ( torch . nn . Module ): | ... | def reset_parameters ( self ) Use sensible default initializations for parameters.","title":"reset_parameters"},{"location":"api/modules/lstm_cell_with_projection/#forward","text":"class LstmCellWithProjection ( torch . nn . Module ): | ... | def forward ( | self , | inputs : torch . FloatTensor , | batch_lengths : List [ int ], | initial_state : Optional [ Tuple [ torch . Tensor , torch . Tensor ]] = None | ) Parameters inputs : torch.FloatTensor A tensor of shape (batch_size, num_timesteps, input_size) to apply the LSTM over. batch_lengths : List[int] A list of length batch_size containing the lengths of the sequences in batch. initial_state : Tuple[torch.Tensor, torch.Tensor] , optional (default = None ) A tuple (state, memory) representing the initial hidden state and memory of the LSTM. The state has shape (1, batch_size, hidden_size) and the memory has shape (1, batch_size, cell_size). Returns output_accumulator : torch.FloatTensor The outputs of the LSTM for each timestep. A tensor of shape (batch_size, max_timesteps, hidden_size) where for a given batch element, all outputs past the sequence length for that batch are zero tensors. final_state : Tuple[torch.FloatTensor, torch.FloatTensor] A tuple (state, memory) representing the initial hidden state and memory of the LSTM. The state has shape (1, batch_size, hidden_size) and the memory has shape (1, batch_size, cell_size).","title":"forward"},{"location":"api/modules/masked_layer_norm/","text":"[ allennlp .modules .masked_layer_norm ] MaskedLayerNorm # class MaskedLayerNorm ( torch . nn . Module ): | def __init__ ( self , size : int , gamma0 : float = 0.1 ) -> None See LayerNorm for details. Note, however, that unlike LayerNorm this norm includes a batch component. forward # class MaskedLayerNorm ( torch . nn . Module ): | ... | def forward ( | self , | tensor : torch . Tensor , | mask : torch . BoolTensor | ) -> torch . Tensor","title":"masked_layer_norm"},{"location":"api/modules/masked_layer_norm/#maskedlayernorm","text":"class MaskedLayerNorm ( torch . nn . Module ): | def __init__ ( self , size : int , gamma0 : float = 0.1 ) -> None See LayerNorm for details. Note, however, that unlike LayerNorm this norm includes a batch component.","title":"MaskedLayerNorm"},{"location":"api/modules/masked_layer_norm/#forward","text":"class MaskedLayerNorm ( torch . nn . Module ): | ... | def forward ( | self , | tensor : torch . Tensor , | mask : torch . BoolTensor | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/maxout/","text":"[ allennlp .modules .maxout ] A maxout neural network. Maxout # class Maxout ( torch . nn . Module , FromParams ): | def __init__ ( | self , | input_dim : int , | num_layers : int , | output_dims : Union [ int , Sequence [ int ]], | pool_sizes : Union [ int , Sequence [ int ]], | dropout : Union [ float , Sequence [ float ]] = 0.0 | ) -> None This Module is a maxout neural network. Parameters input_dim : int The dimensionality of the input. We assume the input has shape (batch_size, input_dim) . num_layers : int The number of maxout layers to apply to the input. output_dims : Union[int, Sequence[int]] The output dimension of each of the maxout layers. If this is a single int , we use it for all maxout layers. If it is a Sequence[int] , len(output_dims) must be num_layers . pool_sizes : Union[int, Sequence[int]] The size of max-pools. If this is a single int , we use it for all maxout layers. If it is a Sequence[int] , len(pool_sizes) must be num_layers . dropout : Union[float, Sequence[float]] , optional (default = 0.0 ) If given, we will apply this amount of dropout after each layer. Semantics of float versus Sequence[float] is the same as with other parameters. get_output_dim # class Maxout ( torch . nn . Module , FromParams ): | ... | def get_output_dim ( self ) get_input_dim # class Maxout ( torch . nn . Module , FromParams ): | ... | def get_input_dim ( self ) forward # class Maxout ( torch . nn . Module , FromParams ): | ... | def forward ( self , inputs : torch . Tensor ) -> torch . Tensor","title":"maxout"},{"location":"api/modules/maxout/#maxout","text":"class Maxout ( torch . nn . Module , FromParams ): | def __init__ ( | self , | input_dim : int , | num_layers : int , | output_dims : Union [ int , Sequence [ int ]], | pool_sizes : Union [ int , Sequence [ int ]], | dropout : Union [ float , Sequence [ float ]] = 0.0 | ) -> None This Module is a maxout neural network. Parameters input_dim : int The dimensionality of the input. We assume the input has shape (batch_size, input_dim) . num_layers : int The number of maxout layers to apply to the input. output_dims : Union[int, Sequence[int]] The output dimension of each of the maxout layers. If this is a single int , we use it for all maxout layers. If it is a Sequence[int] , len(output_dims) must be num_layers . pool_sizes : Union[int, Sequence[int]] The size of max-pools. If this is a single int , we use it for all maxout layers. If it is a Sequence[int] , len(pool_sizes) must be num_layers . dropout : Union[float, Sequence[float]] , optional (default = 0.0 ) If given, we will apply this amount of dropout after each layer. Semantics of float versus Sequence[float] is the same as with other parameters.","title":"Maxout"},{"location":"api/modules/maxout/#get_output_dim","text":"class Maxout ( torch . nn . Module , FromParams ): | ... | def get_output_dim ( self )","title":"get_output_dim"},{"location":"api/modules/maxout/#get_input_dim","text":"class Maxout ( torch . nn . Module , FromParams ): | ... | def get_input_dim ( self )","title":"get_input_dim"},{"location":"api/modules/maxout/#forward","text":"class Maxout ( torch . nn . Module , FromParams ): | ... | def forward ( self , inputs : torch . Tensor ) -> torch . Tensor","title":"forward"},{"location":"api/modules/residual_with_layer_dropout/","text":"[ allennlp .modules .residual_with_layer_dropout ] ResidualWithLayerDropout # class ResidualWithLayerDropout ( torch . nn . Module ): | def __init__ ( self , undecayed_dropout_prob : float = 0.5 ) -> None A residual connection with the layer dropout technique Deep Networks with Stochastic Depth . This module accepts the input and output of a layer, decides whether this layer should be stochastically dropped, returns either the input or output + input. During testing, it will re-calibrate the outputs of this layer by the expected number of times it participates in training. forward # class ResidualWithLayerDropout ( torch . nn . Module ): | ... | def forward ( | self , | layer_input : torch . Tensor , | layer_output : torch . Tensor , | layer_index : int = None , | total_layers : int = None | ) -> torch . Tensor Apply dropout to this layer, for this whole mini-batch. dropout_prob = layer_index / total_layers * undecayed_dropout_prob if layer_idx and total_layers is specified, else it will use the undecayed_dropout_prob directly. Parameters layer_input torch.FloatTensor required The input tensor of this layer. layer_output torch.FloatTensor required The output tensor of this layer, with the same shape as the layer_input. layer_index int The layer index, starting from 1. This is used to calcuate the dropout prob together with the total_layers parameter. total_layers int The total number of layers. Returns output : torch.FloatTensor A tensor with the same shape as layer_input and layer_output .","title":"residual_with_layer_dropout"},{"location":"api/modules/residual_with_layer_dropout/#residualwithlayerdropout","text":"class ResidualWithLayerDropout ( torch . nn . Module ): | def __init__ ( self , undecayed_dropout_prob : float = 0.5 ) -> None A residual connection with the layer dropout technique Deep Networks with Stochastic Depth . This module accepts the input and output of a layer, decides whether this layer should be stochastically dropped, returns either the input or output + input. During testing, it will re-calibrate the outputs of this layer by the expected number of times it participates in training.","title":"ResidualWithLayerDropout"},{"location":"api/modules/residual_with_layer_dropout/#forward","text":"class ResidualWithLayerDropout ( torch . nn . Module ): | ... | def forward ( | self , | layer_input : torch . Tensor , | layer_output : torch . Tensor , | layer_index : int = None , | total_layers : int = None | ) -> torch . Tensor Apply dropout to this layer, for this whole mini-batch. dropout_prob = layer_index / total_layers * undecayed_dropout_prob if layer_idx and total_layers is specified, else it will use the undecayed_dropout_prob directly. Parameters layer_input torch.FloatTensor required The input tensor of this layer. layer_output torch.FloatTensor required The output tensor of this layer, with the same shape as the layer_input. layer_index int The layer index, starting from 1. This is used to calcuate the dropout prob together with the total_layers parameter. total_layers int The total number of layers. Returns output : torch.FloatTensor A tensor with the same shape as layer_input and layer_output .","title":"forward"},{"location":"api/modules/sampled_softmax_loss/","text":"[ allennlp .modules .sampled_softmax_loss ] https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/ops/nn_impl.py#L885 SampledSoftmaxLoss # class SampledSoftmaxLoss ( torch . nn . Module ): | def __init__ ( | self , | num_words : int , | embedding_dim : int , | num_samples : int , | sparse : bool = False , | unk_id : int = None , | use_character_inputs : bool = True , | use_fast_sampler : bool = False | ) -> None Based on the default log_uniform_candidate_sampler in tensorflow. Note num_words DOES NOT include padding id. Note In all cases except (tie_embeddings=True and use_character_inputs=False) the weights are dimensioned as num_words and do not include an entry for the padding (0) id. For the (tie_embeddings=True and use_character_inputs=False) case, then the embeddings DO include the extra 0 padding, to be consistent with the word embedding layer. Parameters num_words, int , required The number of words in the vocabulary embedding_dim, int , required The dimension to softmax over num_samples, int , required During training take this many samples. Must be less than num_words. sparse, bool , optional (default = False ) If this is true, we use a sparse embedding matrix. unk_id, int , optional (default = None ) If provided, the id that represents unknown characters. use_character_inputs, bool , optional (default = True ) Whether to use character inputs use_fast_sampler, bool , optional (default = False ) Whether to use the fast cython sampler. initialize_num_words # class SampledSoftmaxLoss ( torch . nn . Module ): | ... | def initialize_num_words ( self ) forward # class SampledSoftmaxLoss ( torch . nn . Module ): | ... | def forward ( | self , | embeddings : torch . Tensor , | targets : torch . Tensor , | target_token_embedding : torch . Tensor = None | ) -> torch . Tensor embeddings is size (n, embedding_dim) targets is (n_words, ) with the index of the actual target when tieing weights, target_token_embedding is required. it is size (n_words, embedding_dim) returns log likelihood loss (batch_size, ) Does not do any count normalization / divide by batch size log_uniform_candidate_sampler # class SampledSoftmaxLoss ( torch . nn . Module ): | ... | def log_uniform_candidate_sampler ( | self , | targets , | choice_func = _choice | ) algorithm: keep track of number of tries when doing sampling, then expected count is -expm1(num_tries * log1p(-p)) = (1 - (1-p)^num_tries) where p is self._probs[id]","title":"sampled_softmax_loss"},{"location":"api/modules/sampled_softmax_loss/#sampledsoftmaxloss","text":"class SampledSoftmaxLoss ( torch . nn . Module ): | def __init__ ( | self , | num_words : int , | embedding_dim : int , | num_samples : int , | sparse : bool = False , | unk_id : int = None , | use_character_inputs : bool = True , | use_fast_sampler : bool = False | ) -> None Based on the default log_uniform_candidate_sampler in tensorflow. Note num_words DOES NOT include padding id. Note In all cases except (tie_embeddings=True and use_character_inputs=False) the weights are dimensioned as num_words and do not include an entry for the padding (0) id. For the (tie_embeddings=True and use_character_inputs=False) case, then the embeddings DO include the extra 0 padding, to be consistent with the word embedding layer. Parameters num_words, int , required The number of words in the vocabulary embedding_dim, int , required The dimension to softmax over num_samples, int , required During training take this many samples. Must be less than num_words. sparse, bool , optional (default = False ) If this is true, we use a sparse embedding matrix. unk_id, int , optional (default = None ) If provided, the id that represents unknown characters. use_character_inputs, bool , optional (default = True ) Whether to use character inputs use_fast_sampler, bool , optional (default = False ) Whether to use the fast cython sampler.","title":"SampledSoftmaxLoss"},{"location":"api/modules/sampled_softmax_loss/#initialize_num_words","text":"class SampledSoftmaxLoss ( torch . nn . Module ): | ... | def initialize_num_words ( self )","title":"initialize_num_words"},{"location":"api/modules/sampled_softmax_loss/#forward","text":"class SampledSoftmaxLoss ( torch . nn . Module ): | ... | def forward ( | self , | embeddings : torch . Tensor , | targets : torch . Tensor , | target_token_embedding : torch . Tensor = None | ) -> torch . Tensor embeddings is size (n, embedding_dim) targets is (n_words, ) with the index of the actual target when tieing weights, target_token_embedding is required. it is size (n_words, embedding_dim) returns log likelihood loss (batch_size, ) Does not do any count normalization / divide by batch size","title":"forward"},{"location":"api/modules/sampled_softmax_loss/#log_uniform_candidate_sampler","text":"class SampledSoftmaxLoss ( torch . nn . Module ): | ... | def log_uniform_candidate_sampler ( | self , | targets , | choice_func = _choice | ) algorithm: keep track of number of tries when doing sampling, then expected count is -expm1(num_tries * log1p(-p)) = (1 - (1-p)^num_tries) where p is self._probs[id]","title":"log_uniform_candidate_sampler"},{"location":"api/modules/scalar_mix/","text":"[ allennlp .modules .scalar_mix ] ScalarMix # class ScalarMix ( torch . nn . Module ): | def __init__ ( | self , | mixture_size : int , | do_layer_norm : bool = False , | initial_scalar_parameters : List [ float ] = None , | trainable : bool = True | ) -> None Computes a parameterised scalar mixture of N tensors, mixture = gamma * sum(s_k * tensor_k) where s = softmax(w) , with w and gamma scalar parameters. In addition, if do_layer_norm=True then apply layer normalization to each tensor before weighting. forward # class ScalarMix ( torch . nn . Module ): | ... | def forward ( | self , | tensors : List [ torch . Tensor ], | mask : torch . BoolTensor = None | ) -> torch . Tensor Compute a weighted average of the tensors . The input tensors an be any shape with at least two dimensions, but must all be the same shape. When do_layer_norm=True , the mask is required input. If the tensors are dimensioned (dim_0, ..., dim_{n-1}, dim_n) , then the mask is dimensioned (dim_0, ..., dim_{n-1}) , as in the typical case with tensors of shape (batch_size, timesteps, dim) and mask of shape (batch_size, timesteps) . When do_layer_norm=False the mask is ignored.","title":"scalar_mix"},{"location":"api/modules/scalar_mix/#scalarmix","text":"class ScalarMix ( torch . nn . Module ): | def __init__ ( | self , | mixture_size : int , | do_layer_norm : bool = False , | initial_scalar_parameters : List [ float ] = None , | trainable : bool = True | ) -> None Computes a parameterised scalar mixture of N tensors, mixture = gamma * sum(s_k * tensor_k) where s = softmax(w) , with w and gamma scalar parameters. In addition, if do_layer_norm=True then apply layer normalization to each tensor before weighting.","title":"ScalarMix"},{"location":"api/modules/scalar_mix/#forward","text":"class ScalarMix ( torch . nn . Module ): | ... | def forward ( | self , | tensors : List [ torch . Tensor ], | mask : torch . BoolTensor = None | ) -> torch . Tensor Compute a weighted average of the tensors . The input tensors an be any shape with at least two dimensions, but must all be the same shape. When do_layer_norm=True , the mask is required input. If the tensors are dimensioned (dim_0, ..., dim_{n-1}, dim_n) , then the mask is dimensioned (dim_0, ..., dim_{n-1}) , as in the typical case with tensors of shape (batch_size, timesteps, dim) and mask of shape (batch_size, timesteps) . When do_layer_norm=False the mask is ignored.","title":"forward"},{"location":"api/modules/softmax_loss/","text":"[ allennlp .modules .softmax_loss ] SoftmaxLoss # class SoftmaxLoss ( torch . nn . Module ): | def __init__ ( self , num_words : int , embedding_dim : int ) -> None Given some embeddings and some targets, applies a linear layer to create logits over possible words and then returns the negative log likelihood. forward # class SoftmaxLoss ( torch . nn . Module ): | ... | def forward ( | self , | embeddings : torch . Tensor , | targets : torch . Tensor | ) -> torch . Tensor embeddings is size (n, embedding_dim) targets is (batch_size, ) with the correct class id Does not do any count normalization / divide by batch size","title":"softmax_loss"},{"location":"api/modules/softmax_loss/#softmaxloss","text":"class SoftmaxLoss ( torch . nn . Module ): | def __init__ ( self , num_words : int , embedding_dim : int ) -> None Given some embeddings and some targets, applies a linear layer to create logits over possible words and then returns the negative log likelihood.","title":"SoftmaxLoss"},{"location":"api/modules/softmax_loss/#forward","text":"class SoftmaxLoss ( torch . nn . Module ): | ... | def forward ( | self , | embeddings : torch . Tensor , | targets : torch . Tensor | ) -> torch . Tensor embeddings is size (n, embedding_dim) targets is (batch_size, ) with the correct class id Does not do any count normalization / divide by batch size","title":"forward"},{"location":"api/modules/stacked_alternating_lstm/","text":"[ allennlp .modules .stacked_alternating_lstm ] A stacked LSTM with LSTM layers which alternate between going forwards over the sequence and going backwards. TensorPair # TensorPair = Tuple [ torch . Tensor , torch . Tensor ] StackedAlternatingLstm # class StackedAlternatingLstm ( torch . nn . Module ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int , | recurrent_dropout_probability : float = 0.0 , | use_highway : bool = True , | use_input_projection_bias : bool = True | ) -> None A stacked LSTM with LSTM layers which alternate between going forwards over the sequence and going backwards. This implementation is based on the description in Deep Semantic Role Labelling - What works and what's next . Parameters input_size : int The dimension of the inputs to the LSTM. hidden_size : int The dimension of the outputs of the LSTM. num_layers : int The number of stacked LSTMs to use. recurrent_dropout_probability : float , optional (default = 0.0 ) The dropout probability to be used in a dropout scheme as stated in A Theoretically Grounded Application of Dropout in Recurrent Neural Networks . use_input_projection_bias : bool , optional (default = True ) Whether or not to use a bias on the input projection layer. This is mainly here for backwards compatibility reasons and will be removed (and set to False) in future releases. Returns output_accumulator : PackedSequence The outputs of the interleaved LSTMs per timestep. A tensor of shape (batch_size, max_timesteps, hidden_size) where for a given batch element, all outputs past the sequence length for that batch are zero tensors. forward # class StackedAlternatingLstm ( torch . nn . Module ): | ... | def forward ( | self , | inputs : PackedSequence , | initial_state : Optional [ TensorPair ] = None | ) -> Tuple [ Union [ torch . Tensor , PackedSequence ], TensorPair ] Parameters inputs : PackedSequence A batch first PackedSequence to run the stacked LSTM over. initial_state : Tuple[torch.Tensor, torch.Tensor] , optional (default = None ) A tuple (state, memory) representing the initial hidden state and memory of the LSTM. Each tensor has shape (1, batch_size, output_dimension). Returns output_sequence : PackedSequence The encoded sequence of shape (batch_size, sequence_length, hidden_size) final_states : Tuple[torch.Tensor, torch.Tensor] The per-layer final (state, memory) states of the LSTM, each with shape (num_layers, batch_size, hidden_size).","title":"stacked_alternating_lstm"},{"location":"api/modules/stacked_alternating_lstm/#tensorpair","text":"TensorPair = Tuple [ torch . Tensor , torch . Tensor ]","title":"TensorPair"},{"location":"api/modules/stacked_alternating_lstm/#stackedalternatinglstm","text":"class StackedAlternatingLstm ( torch . nn . Module ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int , | recurrent_dropout_probability : float = 0.0 , | use_highway : bool = True , | use_input_projection_bias : bool = True | ) -> None A stacked LSTM with LSTM layers which alternate between going forwards over the sequence and going backwards. This implementation is based on the description in Deep Semantic Role Labelling - What works and what's next . Parameters input_size : int The dimension of the inputs to the LSTM. hidden_size : int The dimension of the outputs of the LSTM. num_layers : int The number of stacked LSTMs to use. recurrent_dropout_probability : float , optional (default = 0.0 ) The dropout probability to be used in a dropout scheme as stated in A Theoretically Grounded Application of Dropout in Recurrent Neural Networks . use_input_projection_bias : bool , optional (default = True ) Whether or not to use a bias on the input projection layer. This is mainly here for backwards compatibility reasons and will be removed (and set to False) in future releases. Returns output_accumulator : PackedSequence The outputs of the interleaved LSTMs per timestep. A tensor of shape (batch_size, max_timesteps, hidden_size) where for a given batch element, all outputs past the sequence length for that batch are zero tensors.","title":"StackedAlternatingLstm"},{"location":"api/modules/stacked_alternating_lstm/#forward","text":"class StackedAlternatingLstm ( torch . nn . Module ): | ... | def forward ( | self , | inputs : PackedSequence , | initial_state : Optional [ TensorPair ] = None | ) -> Tuple [ Union [ torch . Tensor , PackedSequence ], TensorPair ] Parameters inputs : PackedSequence A batch first PackedSequence to run the stacked LSTM over. initial_state : Tuple[torch.Tensor, torch.Tensor] , optional (default = None ) A tuple (state, memory) representing the initial hidden state and memory of the LSTM. Each tensor has shape (1, batch_size, output_dimension). Returns output_sequence : PackedSequence The encoded sequence of shape (batch_size, sequence_length, hidden_size) final_states : Tuple[torch.Tensor, torch.Tensor] The per-layer final (state, memory) states of the LSTM, each with shape (num_layers, batch_size, hidden_size).","title":"forward"},{"location":"api/modules/stacked_bidirectional_lstm/","text":"[ allennlp .modules .stacked_bidirectional_lstm ] TensorPair # TensorPair = Tuple [ torch . Tensor , torch . Tensor ] StackedBidirectionalLstm # class StackedBidirectionalLstm ( torch . nn . Module ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int , | recurrent_dropout_probability : float = 0.0 , | layer_dropout_probability : float = 0.0 , | use_highway : bool = True | ) -> None A standard stacked Bidirectional LSTM where the LSTM layers are concatenated between each layer. The only difference between this and a regular bidirectional LSTM is the application of variational dropout to the hidden states and outputs of each layer apart from the last layer of the LSTM. Note that this will be slower, as it doesn't use CUDNN. Parameters input_size : int The dimension of the inputs to the LSTM. hidden_size : int The dimension of the outputs of the LSTM. num_layers : int The number of stacked Bidirectional LSTMs to use. recurrent_dropout_probability : float , optional (default = 0.0 ) The recurrent dropout probability to be used in a dropout scheme as stated in A Theoretically Grounded Application of Dropout in Recurrent Neural Networks . layer_dropout_probability : float , optional (default = 0.0 ) The layer wise dropout probability to be used in a dropout scheme as stated in A Theoretically Grounded Application of Dropout in Recurrent Neural Networks . use_highway : bool , optional (default = True ) Whether or not to use highway connections between layers. This effectively involves reparameterising the normal output of an LSTM as:: gate = sigmoid(W_x1 * x_t + W_h * h_t) output = gate * h_t + (1 - gate) * (W_x2 * x_t) forward # class StackedBidirectionalLstm ( torch . nn . Module ): | ... | def forward ( | self , | inputs : PackedSequence , | initial_state : Optional [ TensorPair ] = None | ) -> Tuple [ PackedSequence , TensorPair ] Parameters inputs : PackedSequence A batch first PackedSequence to run the stacked LSTM over. initial_state : Tuple[torch.Tensor, torch.Tensor] , optional (default = None ) A tuple (state, memory) representing the initial hidden state and memory of the LSTM. Each tensor has shape (num_layers, batch_size, output_dimension * 2). Returns output_sequence : PackedSequence The encoded sequence of shape (batch_size, sequence_length, hidden_size * 2) final_states : torch.Tensor The per-layer final (state, memory) states of the LSTM, each with shape (num_layers * 2, batch_size, hidden_size * 2).","title":"stacked_bidirectional_lstm"},{"location":"api/modules/stacked_bidirectional_lstm/#tensorpair","text":"TensorPair = Tuple [ torch . Tensor , torch . Tensor ]","title":"TensorPair"},{"location":"api/modules/stacked_bidirectional_lstm/#stackedbidirectionallstm","text":"class StackedBidirectionalLstm ( torch . nn . Module ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int , | recurrent_dropout_probability : float = 0.0 , | layer_dropout_probability : float = 0.0 , | use_highway : bool = True | ) -> None A standard stacked Bidirectional LSTM where the LSTM layers are concatenated between each layer. The only difference between this and a regular bidirectional LSTM is the application of variational dropout to the hidden states and outputs of each layer apart from the last layer of the LSTM. Note that this will be slower, as it doesn't use CUDNN. Parameters input_size : int The dimension of the inputs to the LSTM. hidden_size : int The dimension of the outputs of the LSTM. num_layers : int The number of stacked Bidirectional LSTMs to use. recurrent_dropout_probability : float , optional (default = 0.0 ) The recurrent dropout probability to be used in a dropout scheme as stated in A Theoretically Grounded Application of Dropout in Recurrent Neural Networks . layer_dropout_probability : float , optional (default = 0.0 ) The layer wise dropout probability to be used in a dropout scheme as stated in A Theoretically Grounded Application of Dropout in Recurrent Neural Networks . use_highway : bool , optional (default = True ) Whether or not to use highway connections between layers. This effectively involves reparameterising the normal output of an LSTM as:: gate = sigmoid(W_x1 * x_t + W_h * h_t) output = gate * h_t + (1 - gate) * (W_x2 * x_t)","title":"StackedBidirectionalLstm"},{"location":"api/modules/stacked_bidirectional_lstm/#forward","text":"class StackedBidirectionalLstm ( torch . nn . Module ): | ... | def forward ( | self , | inputs : PackedSequence , | initial_state : Optional [ TensorPair ] = None | ) -> Tuple [ PackedSequence , TensorPair ] Parameters inputs : PackedSequence A batch first PackedSequence to run the stacked LSTM over. initial_state : Tuple[torch.Tensor, torch.Tensor] , optional (default = None ) A tuple (state, memory) representing the initial hidden state and memory of the LSTM. Each tensor has shape (num_layers, batch_size, output_dimension * 2). Returns output_sequence : PackedSequence The encoded sequence of shape (batch_size, sequence_length, hidden_size * 2) final_states : torch.Tensor The per-layer final (state, memory) states of the LSTM, each with shape (num_layers * 2, batch_size, hidden_size * 2).","title":"forward"},{"location":"api/modules/time_distributed/","text":"[ allennlp .modules .time_distributed ] A wrapper that unrolls the second (time) dimension of a tensor into the first (batch) dimension, applies some other Module , and then rolls the time dimension back up. TimeDistributed # class TimeDistributed ( torch . nn . Module ): | def __init__ ( self , module ) Given an input shaped like (batch_size, time_steps, [rest]) and a Module that takes inputs like (batch_size, [rest]) , TimeDistributed reshapes the input to be (batch_size * time_steps, [rest]) , applies the contained Module , then reshapes it back. Note that while the above gives shapes with batch_size first, this Module also works if batch_size is second - we always just combine the first two dimensions, then split them. It also reshapes keyword arguments unless they are not tensors or their name is specified in the optional pass_through iterable. forward # class TimeDistributed ( torch . nn . Module ): | ... | @overrides | def forward ( | self , | * inputs , | pass_through : List [ str ] = None , | ** kwargs | )","title":"time_distributed"},{"location":"api/modules/time_distributed/#timedistributed","text":"class TimeDistributed ( torch . nn . Module ): | def __init__ ( self , module ) Given an input shaped like (batch_size, time_steps, [rest]) and a Module that takes inputs like (batch_size, [rest]) , TimeDistributed reshapes the input to be (batch_size * time_steps, [rest]) , applies the contained Module , then reshapes it back. Note that while the above gives shapes with batch_size first, this Module also works if batch_size is second - we always just combine the first two dimensions, then split them. It also reshapes keyword arguments unless they are not tensors or their name is specified in the optional pass_through iterable.","title":"TimeDistributed"},{"location":"api/modules/time_distributed/#forward","text":"class TimeDistributed ( torch . nn . Module ): | ... | @overrides | def forward ( | self , | * inputs , | pass_through : List [ str ] = None , | ** kwargs | )","title":"forward"},{"location":"api/modules/attention/additive_attention/","text":"[ allennlp .modules .attention .additive_attention ] AdditiveAttention # class AdditiveAttention ( Attention ): | def __init__ ( | self , | vector_dim : int , | matrix_dim : int , | normalize : bool = True | ) -> None Computes attention between a vector and a matrix using an additive attention function. This function has two matrices W , U and a vector V . The similarity between the vector x and the matrix y is computed as V tanh(Wx + Uy) . This attention is often referred as concat or additive attention. It was introduced in https://arxiv.org/abs/1409.0473 by Bahdanau et al. Registered as an Attention with name \"additive\". Parameters vector_dim : int The dimension of the vector, x , described above. This is x.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build the weight matrix correctly. matrix_dim : int The dimension of the matrix, y , described above. This is y.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build the weight matrix correctly. normalize : bool , optional (default = True ) If true, we normalize the computed similarities with a softmax, to return a probability distribution for your attention. If false, this is just computing a similarity score. reset_parameters # class AdditiveAttention ( Attention ): | ... | def reset_parameters ( self )","title":"additive_attention"},{"location":"api/modules/attention/additive_attention/#additiveattention","text":"class AdditiveAttention ( Attention ): | def __init__ ( | self , | vector_dim : int , | matrix_dim : int , | normalize : bool = True | ) -> None Computes attention between a vector and a matrix using an additive attention function. This function has two matrices W , U and a vector V . The similarity between the vector x and the matrix y is computed as V tanh(Wx + Uy) . This attention is often referred as concat or additive attention. It was introduced in https://arxiv.org/abs/1409.0473 by Bahdanau et al. Registered as an Attention with name \"additive\". Parameters vector_dim : int The dimension of the vector, x , described above. This is x.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build the weight matrix correctly. matrix_dim : int The dimension of the matrix, y , described above. This is y.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build the weight matrix correctly. normalize : bool , optional (default = True ) If true, we normalize the computed similarities with a softmax, to return a probability distribution for your attention. If false, this is just computing a similarity score.","title":"AdditiveAttention"},{"location":"api/modules/attention/additive_attention/#reset_parameters","text":"class AdditiveAttention ( Attention ): | ... | def reset_parameters ( self )","title":"reset_parameters"},{"location":"api/modules/attention/attention/","text":"[ allennlp .modules .attention .attention ] An attention module that computes the similarity between an input vector and the rows of a matrix. Attention # class Attention ( torch . nn . Module , Registrable ): | def __init__ ( self , normalize : bool = True ) -> None An Attention takes two inputs: a (batched) vector and a matrix, plus an optional mask on the rows of the matrix. We compute the similarity between the vector and each row in the matrix, and then (optionally) perform a softmax over rows using those computed similarities. Inputs: vector: shape (batch_size, embedding_dim) matrix: shape (batch_size, num_rows, embedding_dim) matrix_mask: shape (batch_size, num_rows) , specifying which rows are just padding. Output: attention: shape (batch_size, num_rows) . Parameters normalize : bool , optional (default = True ) If true, we normalize the computed similarities with a softmax, to return a probability distribution for your attention. If false, this is just computing a similarity score. forward # class Attention ( torch . nn . Module , Registrable ): | ... | @overrides | def forward ( | self , | vector : torch . Tensor , | matrix : torch . Tensor , | matrix_mask : torch . BoolTensor = None | ) -> torch . Tensor","title":"attention"},{"location":"api/modules/attention/attention/#attention","text":"class Attention ( torch . nn . Module , Registrable ): | def __init__ ( self , normalize : bool = True ) -> None An Attention takes two inputs: a (batched) vector and a matrix, plus an optional mask on the rows of the matrix. We compute the similarity between the vector and each row in the matrix, and then (optionally) perform a softmax over rows using those computed similarities. Inputs: vector: shape (batch_size, embedding_dim) matrix: shape (batch_size, num_rows, embedding_dim) matrix_mask: shape (batch_size, num_rows) , specifying which rows are just padding. Output: attention: shape (batch_size, num_rows) . Parameters normalize : bool , optional (default = True ) If true, we normalize the computed similarities with a softmax, to return a probability distribution for your attention. If false, this is just computing a similarity score.","title":"Attention"},{"location":"api/modules/attention/attention/#forward","text":"class Attention ( torch . nn . Module , Registrable ): | ... | @overrides | def forward ( | self , | vector : torch . Tensor , | matrix : torch . Tensor , | matrix_mask : torch . BoolTensor = None | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/attention/bilinear_attention/","text":"[ allennlp .modules .attention .bilinear_attention ] BilinearAttention # class BilinearAttention ( Attention ): | def __init__ ( | self , | vector_dim : int , | matrix_dim : int , | activation : Activation = None , | normalize : bool = True | ) -> None Computes attention between a vector and a matrix using a bilinear attention function. This function has a matrix of weights W and a bias b , and the similarity between the vector x and the matrix y is computed as x^T W y + b . Registered as an Attention with name \"bilinear\". Parameters vector_dim : int The dimension of the vector, x , described above. This is x.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build the weight matrix correctly. matrix_dim : int The dimension of the matrix, y , described above. This is y.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build the weight matrix correctly. activation : Activation , optional (default = linear ) An activation function applied after the x^T W y + b calculation. Default is linear, i.e. no activation. normalize : bool , optional (default = True ) If true, we normalize the computed similarities with a softmax, to return a probability distribution for your attention. If false, this is just computing a similarity score. reset_parameters # class BilinearAttention ( Attention ): | ... | def reset_parameters ( self )","title":"bilinear_attention"},{"location":"api/modules/attention/bilinear_attention/#bilinearattention","text":"class BilinearAttention ( Attention ): | def __init__ ( | self , | vector_dim : int , | matrix_dim : int , | activation : Activation = None , | normalize : bool = True | ) -> None Computes attention between a vector and a matrix using a bilinear attention function. This function has a matrix of weights W and a bias b , and the similarity between the vector x and the matrix y is computed as x^T W y + b . Registered as an Attention with name \"bilinear\". Parameters vector_dim : int The dimension of the vector, x , described above. This is x.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build the weight matrix correctly. matrix_dim : int The dimension of the matrix, y , described above. This is y.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build the weight matrix correctly. activation : Activation , optional (default = linear ) An activation function applied after the x^T W y + b calculation. Default is linear, i.e. no activation. normalize : bool , optional (default = True ) If true, we normalize the computed similarities with a softmax, to return a probability distribution for your attention. If false, this is just computing a similarity score.","title":"BilinearAttention"},{"location":"api/modules/attention/bilinear_attention/#reset_parameters","text":"class BilinearAttention ( Attention ): | ... | def reset_parameters ( self )","title":"reset_parameters"},{"location":"api/modules/attention/cosine_attention/","text":"[ allennlp .modules .attention .cosine_attention ] CosineAttention # class CosineAttention ( Attention ) Computes attention between a vector and a matrix using cosine similarity. Registered as an Attention with name \"cosine\".","title":"cosine_attention"},{"location":"api/modules/attention/cosine_attention/#cosineattention","text":"class CosineAttention ( Attention ) Computes attention between a vector and a matrix using cosine similarity. Registered as an Attention with name \"cosine\".","title":"CosineAttention"},{"location":"api/modules/attention/dot_product_attention/","text":"[ allennlp .modules .attention .dot_product_attention ] DotProductAttention # class DotProductAttention ( Attention ) Computes attention between a vector and a matrix using dot product. Registered as an Attention with name \"dot_product\".","title":"dot_product_attention"},{"location":"api/modules/attention/dot_product_attention/#dotproductattention","text":"class DotProductAttention ( Attention ) Computes attention between a vector and a matrix using dot product. Registered as an Attention with name \"dot_product\".","title":"DotProductAttention"},{"location":"api/modules/attention/linear_attention/","text":"[ allennlp .modules .attention .linear_attention ] LinearAttention # class LinearAttention ( Attention ): | def __init__ ( | self , | tensor_1_dim : int , | tensor_2_dim : int , | combination : str = \"x,y\" , | activation : Activation = None , | normalize : bool = True | ) -> None This Attention module performs a dot product between a vector of weights and some combination of the two input vectors, followed by an (optional) activation function. The combination used is configurable. If the two vectors are x and y , we allow the following kinds of combinations : x , y , x*y , x+y , x-y , x/y , where each of those binary operations is performed elementwise. You can list as many combinations as you want, comma separated. For example, you might give x,y,x*y as the combination parameter to this class. The computed similarity function would then be w^T [x; y; x*y] + b , where w is a vector of weights, b is a bias parameter, and [;] is vector concatenation. Note that if you want a bilinear similarity function with a diagonal weight matrix W, where the similarity function is computed as x * w * y + b (with w the diagonal of W ), you can accomplish that with this class by using \"x*y\" for combination . Registered as an Attention with name \"linear\". Parameters tensor_1_dim : int The dimension of the first tensor, x , described above. This is x.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build weight vectors correctly. tensor_2_dim : int The dimension of the second tensor, y , described above. This is y.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build weight vectors correctly. combination : str , optional (default = \"x,y\" ) Described above. activation : Activation , optional (default = linear ) An activation function applied after the w^T * [x;y] + b calculation. Default is linear, i.e. no activation. normalize : bool , optional (default = True ) reset_parameters # class LinearAttention ( Attention ): | ... | def reset_parameters ( self )","title":"linear_attention"},{"location":"api/modules/attention/linear_attention/#linearattention","text":"class LinearAttention ( Attention ): | def __init__ ( | self , | tensor_1_dim : int , | tensor_2_dim : int , | combination : str = \"x,y\" , | activation : Activation = None , | normalize : bool = True | ) -> None This Attention module performs a dot product between a vector of weights and some combination of the two input vectors, followed by an (optional) activation function. The combination used is configurable. If the two vectors are x and y , we allow the following kinds of combinations : x , y , x*y , x+y , x-y , x/y , where each of those binary operations is performed elementwise. You can list as many combinations as you want, comma separated. For example, you might give x,y,x*y as the combination parameter to this class. The computed similarity function would then be w^T [x; y; x*y] + b , where w is a vector of weights, b is a bias parameter, and [;] is vector concatenation. Note that if you want a bilinear similarity function with a diagonal weight matrix W, where the similarity function is computed as x * w * y + b (with w the diagonal of W ), you can accomplish that with this class by using \"x*y\" for combination . Registered as an Attention with name \"linear\". Parameters tensor_1_dim : int The dimension of the first tensor, x , described above. This is x.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build weight vectors correctly. tensor_2_dim : int The dimension of the second tensor, y , described above. This is y.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build weight vectors correctly. combination : str , optional (default = \"x,y\" ) Described above. activation : Activation , optional (default = linear ) An activation function applied after the w^T * [x;y] + b calculation. Default is linear, i.e. no activation. normalize : bool , optional (default = True )","title":"LinearAttention"},{"location":"api/modules/attention/linear_attention/#reset_parameters","text":"class LinearAttention ( Attention ): | ... | def reset_parameters ( self )","title":"reset_parameters"},{"location":"api/modules/matrix_attention/bilinear_matrix_attention/","text":"[ allennlp .modules .matrix_attention .bilinear_matrix_attention ] BilinearMatrixAttention # class BilinearMatrixAttention ( MatrixAttention ): | def __init__ ( | self , | matrix_1_dim : int , | matrix_2_dim : int , | activation : Activation = None , | use_input_biases : bool = False , | label_dim : int = 1 | ) -> None Computes attention between two matrices using a bilinear attention function. This function has a matrix of weights W and a bias b , and the similarity between the two matrices X and Y is computed as X W Y^T + b . Registered as a MatrixAttention with name \"bilinear\". Parameters matrix_1_dim : int The dimension of the matrix X , described above. This is X.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build the weight matrix correctly. matrix_2_dim : int The dimension of the matrix Y , described above. This is Y.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build the weight matrix correctly. activation : Activation , optional (default = linear ) An activation function applied after the X W Y^T + b calculation. Default is linear, i.e. no activation. use_input_biases : bool , optional (default = False ) If True, we add biases to the inputs such that the final computation is equivalent to the original bilinear matrix multiplication plus a projection of both inputs. label_dim : int , optional (default = 1 ) The number of output classes. Typically in an attention setting this will be one, but this parameter allows this class to function as an equivalent to torch.nn.Bilinear for matrices, rather than vectors. reset_parameters # class BilinearMatrixAttention ( MatrixAttention ): | ... | def reset_parameters ( self ) forward # class BilinearMatrixAttention ( MatrixAttention ): | ... | @overrides | def forward ( | self , | matrix_1 : torch . Tensor , | matrix_2 : torch . Tensor | ) -> torch . Tensor","title":"bilinear_matrix_attention"},{"location":"api/modules/matrix_attention/bilinear_matrix_attention/#bilinearmatrixattention","text":"class BilinearMatrixAttention ( MatrixAttention ): | def __init__ ( | self , | matrix_1_dim : int , | matrix_2_dim : int , | activation : Activation = None , | use_input_biases : bool = False , | label_dim : int = 1 | ) -> None Computes attention between two matrices using a bilinear attention function. This function has a matrix of weights W and a bias b , and the similarity between the two matrices X and Y is computed as X W Y^T + b . Registered as a MatrixAttention with name \"bilinear\". Parameters matrix_1_dim : int The dimension of the matrix X , described above. This is X.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build the weight matrix correctly. matrix_2_dim : int The dimension of the matrix Y , described above. This is Y.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build the weight matrix correctly. activation : Activation , optional (default = linear ) An activation function applied after the X W Y^T + b calculation. Default is linear, i.e. no activation. use_input_biases : bool , optional (default = False ) If True, we add biases to the inputs such that the final computation is equivalent to the original bilinear matrix multiplication plus a projection of both inputs. label_dim : int , optional (default = 1 ) The number of output classes. Typically in an attention setting this will be one, but this parameter allows this class to function as an equivalent to torch.nn.Bilinear for matrices, rather than vectors.","title":"BilinearMatrixAttention"},{"location":"api/modules/matrix_attention/bilinear_matrix_attention/#reset_parameters","text":"class BilinearMatrixAttention ( MatrixAttention ): | ... | def reset_parameters ( self )","title":"reset_parameters"},{"location":"api/modules/matrix_attention/bilinear_matrix_attention/#forward","text":"class BilinearMatrixAttention ( MatrixAttention ): | ... | @overrides | def forward ( | self , | matrix_1 : torch . Tensor , | matrix_2 : torch . Tensor | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/matrix_attention/cosine_matrix_attention/","text":"[ allennlp .modules .matrix_attention .cosine_matrix_attention ] CosineMatrixAttention # class CosineMatrixAttention ( MatrixAttention ) Computes attention between every entry in matrix_1 with every entry in matrix_2 using cosine similarity. Registered as a MatrixAttention with name \"cosine\". forward # class CosineMatrixAttention ( MatrixAttention ): | ... | @overrides | def forward ( | self , | matrix_1 : torch . Tensor , | matrix_2 : torch . Tensor | ) -> torch . Tensor","title":"cosine_matrix_attention"},{"location":"api/modules/matrix_attention/cosine_matrix_attention/#cosinematrixattention","text":"class CosineMatrixAttention ( MatrixAttention ) Computes attention between every entry in matrix_1 with every entry in matrix_2 using cosine similarity. Registered as a MatrixAttention with name \"cosine\".","title":"CosineMatrixAttention"},{"location":"api/modules/matrix_attention/cosine_matrix_attention/#forward","text":"class CosineMatrixAttention ( MatrixAttention ): | ... | @overrides | def forward ( | self , | matrix_1 : torch . Tensor , | matrix_2 : torch . Tensor | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/matrix_attention/dot_product_matrix_attention/","text":"[ allennlp .modules .matrix_attention .dot_product_matrix_attention ] DotProductMatrixAttention # class DotProductMatrixAttention ( MatrixAttention ) Computes attention between every entry in matrix_1 with every entry in matrix_2 using a dot product. Registered as a MatrixAttention with name \"dot_product\". forward # class DotProductMatrixAttention ( MatrixAttention ): | ... | @overrides | def forward ( | self , | matrix_1 : torch . Tensor , | matrix_2 : torch . Tensor | ) -> torch . Tensor","title":"dot_product_matrix_attention"},{"location":"api/modules/matrix_attention/dot_product_matrix_attention/#dotproductmatrixattention","text":"class DotProductMatrixAttention ( MatrixAttention ) Computes attention between every entry in matrix_1 with every entry in matrix_2 using a dot product. Registered as a MatrixAttention with name \"dot_product\".","title":"DotProductMatrixAttention"},{"location":"api/modules/matrix_attention/dot_product_matrix_attention/#forward","text":"class DotProductMatrixAttention ( MatrixAttention ): | ... | @overrides | def forward ( | self , | matrix_1 : torch . Tensor , | matrix_2 : torch . Tensor | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/matrix_attention/linear_matrix_attention/","text":"[ allennlp .modules .matrix_attention .linear_matrix_attention ] LinearMatrixAttention # class LinearMatrixAttention ( MatrixAttention ): | def __init__ ( | self , | tensor_1_dim : int , | tensor_2_dim : int , | combination : str = \"x,y\" , | activation : Activation = None | ) -> None This MatrixAttention takes two matrices as input and returns a matrix of attentions by performing a dot product between a vector of weights and some combination of the two input matrices, followed by an (optional) activation function. The combination used is configurable. If the two vectors are x and y , we allow the following kinds of combinations : x , y , x*y , x+y , x-y , x/y , where each of those binary operations is performed elementwise. You can list as many combinations as you want, comma separated. For example, you might give x,y,x*y as the combination parameter to this class. The computed similarity function would then be w^T [x; y; x*y] + b , where w is a vector of weights, b is a bias parameter, and [;] is vector concatenation. Note that if you want a bilinear similarity function with a diagonal weight matrix W, where the similarity function is computed as x * w * y + b (with w the diagonal of W ), you can accomplish that with this class by using \"x*y\" for combination . Registered as a MatrixAttention with name \"linear\". Parameters tensor_1_dim : int The dimension of the first tensor, x , described above. This is x.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build weight vectors correctly. tensor_2_dim : int The dimension of the second tensor, y , described above. This is y.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build weight vectors correctly. combination : str , optional (default = \"x,y\" ) Described above. activation : Activation , optional (default = linear ) An activation function applied after the w^T * [x;y] + b calculation. Default is linear, i.e. no activation. reset_parameters # class LinearMatrixAttention ( MatrixAttention ): | ... | def reset_parameters ( self ) forward # class LinearMatrixAttention ( MatrixAttention ): | ... | @overrides | def forward ( | self , | matrix_1 : torch . Tensor , | matrix_2 : torch . Tensor | ) -> torch . Tensor","title":"linear_matrix_attention"},{"location":"api/modules/matrix_attention/linear_matrix_attention/#linearmatrixattention","text":"class LinearMatrixAttention ( MatrixAttention ): | def __init__ ( | self , | tensor_1_dim : int , | tensor_2_dim : int , | combination : str = \"x,y\" , | activation : Activation = None | ) -> None This MatrixAttention takes two matrices as input and returns a matrix of attentions by performing a dot product between a vector of weights and some combination of the two input matrices, followed by an (optional) activation function. The combination used is configurable. If the two vectors are x and y , we allow the following kinds of combinations : x , y , x*y , x+y , x-y , x/y , where each of those binary operations is performed elementwise. You can list as many combinations as you want, comma separated. For example, you might give x,y,x*y as the combination parameter to this class. The computed similarity function would then be w^T [x; y; x*y] + b , where w is a vector of weights, b is a bias parameter, and [;] is vector concatenation. Note that if you want a bilinear similarity function with a diagonal weight matrix W, where the similarity function is computed as x * w * y + b (with w the diagonal of W ), you can accomplish that with this class by using \"x*y\" for combination . Registered as a MatrixAttention with name \"linear\". Parameters tensor_1_dim : int The dimension of the first tensor, x , described above. This is x.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build weight vectors correctly. tensor_2_dim : int The dimension of the second tensor, y , described above. This is y.size()[-1] - the length of the vector that will go into the similarity computation. We need this so we can build weight vectors correctly. combination : str , optional (default = \"x,y\" ) Described above. activation : Activation , optional (default = linear ) An activation function applied after the w^T * [x;y] + b calculation. Default is linear, i.e. no activation.","title":"LinearMatrixAttention"},{"location":"api/modules/matrix_attention/linear_matrix_attention/#reset_parameters","text":"class LinearMatrixAttention ( MatrixAttention ): | ... | def reset_parameters ( self )","title":"reset_parameters"},{"location":"api/modules/matrix_attention/linear_matrix_attention/#forward","text":"class LinearMatrixAttention ( MatrixAttention ): | ... | @overrides | def forward ( | self , | matrix_1 : torch . Tensor , | matrix_2 : torch . Tensor | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/matrix_attention/matrix_attention/","text":"[ allennlp .modules .matrix_attention .matrix_attention ] MatrixAttention # class MatrixAttention ( torch . nn . Module , Registrable ) MatrixAttention takes two matrices as input and returns a matrix of attentions. We compute the similarity between each row in each matrix and return unnormalized similarity scores. Because these scores are unnormalized, we don't take a mask as input; it's up to the caller to deal with masking properly when this output is used. Input: - matrix_1 : (batch_size, num_rows_1, embedding_dim_1) - matrix_2 : (batch_size, num_rows_2, embedding_dim_2) Output: - (batch_size, num_rows_1, num_rows_2) forward # class MatrixAttention ( torch . nn . Module , Registrable ): | ... | def forward ( | self , | matrix_1 : torch . Tensor , | matrix_2 : torch . Tensor | ) -> torch . Tensor","title":"matrix_attention"},{"location":"api/modules/matrix_attention/matrix_attention/#matrixattention","text":"class MatrixAttention ( torch . nn . Module , Registrable ) MatrixAttention takes two matrices as input and returns a matrix of attentions. We compute the similarity between each row in each matrix and return unnormalized similarity scores. Because these scores are unnormalized, we don't take a mask as input; it's up to the caller to deal with masking properly when this output is used. Input: - matrix_1 : (batch_size, num_rows_1, embedding_dim_1) - matrix_2 : (batch_size, num_rows_2, embedding_dim_2) Output: - (batch_size, num_rows_1, num_rows_2)","title":"MatrixAttention"},{"location":"api/modules/matrix_attention/matrix_attention/#forward","text":"class MatrixAttention ( torch . nn . Module , Registrable ): | ... | def forward ( | self , | matrix_1 : torch . Tensor , | matrix_2 : torch . Tensor | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/seq2seq_encoders/compose_encoder/","text":"[ allennlp .modules .seq2seq_encoders .compose_encoder ] ComposeEncoder # class ComposeEncoder ( Seq2SeqEncoder ): | def __init__ ( self , encoders : List [ Seq2SeqEncoder ]) This class can be used to compose several encoders in sequence. Among other things, this can be used to add a \"pre-contextualizer\" before a Seq2SeqEncoder. Registered as a Seq2SeqEncoder with name \"compose\". Parameters encoders : List[Seq2SeqEncoder] A non-empty list of encoders to compose. The encoders must match in bidirectionality. forward # class ComposeEncoder ( Seq2SeqEncoder ): | ... | @overrides | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor = None | ) -> torch . Tensor Parameters inputs : torch.Tensor A tensor of shape (batch_size, timesteps, input_dim) mask : torch.BoolTensor , optional (default = None ) A tensor of shape (batch_size, timesteps). Returns A tensor computed by composing the sequence of encoders. get_input_dim # class ComposeEncoder ( Seq2SeqEncoder ): | ... | @overrides | def get_input_dim ( self ) -> int get_output_dim # class ComposeEncoder ( Seq2SeqEncoder ): | ... | @overrides | def get_output_dim ( self ) -> int is_bidirectional # class ComposeEncoder ( Seq2SeqEncoder ): | ... | @overrides | def is_bidirectional ( self ) -> bool","title":"compose_encoder"},{"location":"api/modules/seq2seq_encoders/compose_encoder/#composeencoder","text":"class ComposeEncoder ( Seq2SeqEncoder ): | def __init__ ( self , encoders : List [ Seq2SeqEncoder ]) This class can be used to compose several encoders in sequence. Among other things, this can be used to add a \"pre-contextualizer\" before a Seq2SeqEncoder. Registered as a Seq2SeqEncoder with name \"compose\". Parameters encoders : List[Seq2SeqEncoder] A non-empty list of encoders to compose. The encoders must match in bidirectionality.","title":"ComposeEncoder"},{"location":"api/modules/seq2seq_encoders/compose_encoder/#forward","text":"class ComposeEncoder ( Seq2SeqEncoder ): | ... | @overrides | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor = None | ) -> torch . Tensor Parameters inputs : torch.Tensor A tensor of shape (batch_size, timesteps, input_dim) mask : torch.BoolTensor , optional (default = None ) A tensor of shape (batch_size, timesteps). Returns A tensor computed by composing the sequence of encoders.","title":"forward"},{"location":"api/modules/seq2seq_encoders/compose_encoder/#get_input_dim","text":"class ComposeEncoder ( Seq2SeqEncoder ): | ... | @overrides | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"api/modules/seq2seq_encoders/compose_encoder/#get_output_dim","text":"class ComposeEncoder ( Seq2SeqEncoder ): | ... | @overrides | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/seq2seq_encoders/compose_encoder/#is_bidirectional","text":"class ComposeEncoder ( Seq2SeqEncoder ): | ... | @overrides | def is_bidirectional ( self ) -> bool","title":"is_bidirectional"},{"location":"api/modules/seq2seq_encoders/feedforward_encoder/","text":"[ allennlp .modules .seq2seq_encoders .feedforward_encoder ] FeedForwardEncoder # class FeedForwardEncoder ( Seq2SeqEncoder ): | def __init__ ( self , feedforward : FeedForward ) -> None This class applies the FeedForward to each item in sequences. Registered as a Seq2SeqEncoder with name \"feedforward\". get_input_dim # class FeedForwardEncoder ( Seq2SeqEncoder ): | ... | @overrides | def get_input_dim ( self ) -> int get_output_dim # class FeedForwardEncoder ( Seq2SeqEncoder ): | ... | @overrides | def get_output_dim ( self ) -> int is_bidirectional # class FeedForwardEncoder ( Seq2SeqEncoder ): | ... | @overrides | def is_bidirectional ( self ) -> bool forward # class FeedForwardEncoder ( Seq2SeqEncoder ): | ... | @overrides | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor = None | ) -> torch . Tensor Parameters inputs : torch.Tensor A tensor of shape (batch_size, timesteps, input_dim) mask : torch.BoolTensor , optional (default = None ) A tensor of shape (batch_size, timesteps). Returns A tensor of shape (batch_size, timesteps, output_dim).","title":"feedforward_encoder"},{"location":"api/modules/seq2seq_encoders/feedforward_encoder/#feedforwardencoder","text":"class FeedForwardEncoder ( Seq2SeqEncoder ): | def __init__ ( self , feedforward : FeedForward ) -> None This class applies the FeedForward to each item in sequences. Registered as a Seq2SeqEncoder with name \"feedforward\".","title":"FeedForwardEncoder"},{"location":"api/modules/seq2seq_encoders/feedforward_encoder/#get_input_dim","text":"class FeedForwardEncoder ( Seq2SeqEncoder ): | ... | @overrides | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"api/modules/seq2seq_encoders/feedforward_encoder/#get_output_dim","text":"class FeedForwardEncoder ( Seq2SeqEncoder ): | ... | @overrides | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/seq2seq_encoders/feedforward_encoder/#is_bidirectional","text":"class FeedForwardEncoder ( Seq2SeqEncoder ): | ... | @overrides | def is_bidirectional ( self ) -> bool","title":"is_bidirectional"},{"location":"api/modules/seq2seq_encoders/feedforward_encoder/#forward","text":"class FeedForwardEncoder ( Seq2SeqEncoder ): | ... | @overrides | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor = None | ) -> torch . Tensor Parameters inputs : torch.Tensor A tensor of shape (batch_size, timesteps, input_dim) mask : torch.BoolTensor , optional (default = None ) A tensor of shape (batch_size, timesteps). Returns A tensor of shape (batch_size, timesteps, output_dim).","title":"forward"},{"location":"api/modules/seq2seq_encoders/gated_cnn_encoder/","text":"[ allennlp .modules .seq2seq_encoders .gated_cnn_encoder ] ResidualBlock # class ResidualBlock ( torch . nn . Module ): | def __init__ ( | self , | input_dim : int , | layers : Sequence [ Sequence [ int ]], | direction : str , | do_weight_norm : bool = True , | dropout : float = 0.0 | ) -> None forward # class ResidualBlock ( torch . nn . Module ): | ... | def forward ( self , x : torch . Tensor ) -> torch . Tensor x = (batch_size, dim, timesteps) outputs: (batch_size, dim, timesteps) = f(x) + x GatedCnnEncoder # class GatedCnnEncoder ( Seq2SeqEncoder ): | def __init__ ( | self , | input_dim : int , | layers : Sequence [ Sequence [ Sequence [ int ]]], | dropout : float = 0.0 , | return_all_layers : bool = False | ) -> None This is work-in-progress and has not been fully tested yet. Use at your own risk! A Seq2SeqEncoder that uses a Gated CNN. see Language Modeling with Gated Convolutional Networks, Yann N. Dauphin et al, ICML 2017 https://arxiv.org/abs/1612.08083 Convolutional Sequence to Sequence Learning, Jonas Gehring et al, ICML 2017 https://arxiv.org/abs/1705.03122 Some possibilities: Each element of the list is wrapped in a residual block: input_dim = 512 layers = [ [[4, 512]], [[4, 512], [4, 512]], [[4, 512], [4, 512]], [[4, 512], [4, 512]] dropout = 0.05 A \"bottleneck architecture\" input_dim = 512 layers = [ [[4, 512]], [[1, 128], [5, 128], [1, 512]], ... ] An architecture with dilated convolutions input_dim = 512 layers = [ [[2, 512, 1]], [[2, 512, 2]], [[2, 512, 4]], [[2, 512, 8]], # receptive field == 16 [[2, 512, 1]], [[2, 512, 2]], [[2, 512, 4]], [[2, 512, 8]], # receptive field == 31 [[2, 512, 1]], [[2, 512, 2]], [[2, 512, 4]], [[2, 512, 8]], # receptive field == 46 [[2, 512, 1]], [[2, 512, 2]], [[2, 512, 4]], [[2, 512, 8]], # receptive field == 57 ] Registered as a Seq2SeqEncoder with name \"gated-cnn-encoder\". Parameters input_dim : int The dimension of the inputs. layers : Sequence[Sequence[Sequence[int]]] The layer dimensions for each ResidualBlock . dropout : float , optional (default = 0.0 ) The dropout for each ResidualBlock . return_all_layers : bool , optional (default = False ) Whether to return all layers or just the last layer. forward # class GatedCnnEncoder ( Seq2SeqEncoder ): | ... | def forward ( | self , | token_embeddings : torch . Tensor , | mask : torch . BoolTensor | ) Convolutions need transposed input get_input_dim # class GatedCnnEncoder ( Seq2SeqEncoder ): | ... | def get_input_dim ( self ) -> int get_output_dim # class GatedCnnEncoder ( Seq2SeqEncoder ): | ... | def get_output_dim ( self ) -> int is_bidirectional # class GatedCnnEncoder ( Seq2SeqEncoder ): | ... | def is_bidirectional ( self ) -> bool","title":"gated_cnn_encoder"},{"location":"api/modules/seq2seq_encoders/gated_cnn_encoder/#residualblock","text":"class ResidualBlock ( torch . nn . Module ): | def __init__ ( | self , | input_dim : int , | layers : Sequence [ Sequence [ int ]], | direction : str , | do_weight_norm : bool = True , | dropout : float = 0.0 | ) -> None","title":"ResidualBlock"},{"location":"api/modules/seq2seq_encoders/gated_cnn_encoder/#forward","text":"class ResidualBlock ( torch . nn . Module ): | ... | def forward ( self , x : torch . Tensor ) -> torch . Tensor x = (batch_size, dim, timesteps) outputs: (batch_size, dim, timesteps) = f(x) + x","title":"forward"},{"location":"api/modules/seq2seq_encoders/gated_cnn_encoder/#gatedcnnencoder","text":"class GatedCnnEncoder ( Seq2SeqEncoder ): | def __init__ ( | self , | input_dim : int , | layers : Sequence [ Sequence [ Sequence [ int ]]], | dropout : float = 0.0 , | return_all_layers : bool = False | ) -> None This is work-in-progress and has not been fully tested yet. Use at your own risk! A Seq2SeqEncoder that uses a Gated CNN. see Language Modeling with Gated Convolutional Networks, Yann N. Dauphin et al, ICML 2017 https://arxiv.org/abs/1612.08083 Convolutional Sequence to Sequence Learning, Jonas Gehring et al, ICML 2017 https://arxiv.org/abs/1705.03122 Some possibilities: Each element of the list is wrapped in a residual block: input_dim = 512 layers = [ [[4, 512]], [[4, 512], [4, 512]], [[4, 512], [4, 512]], [[4, 512], [4, 512]] dropout = 0.05 A \"bottleneck architecture\" input_dim = 512 layers = [ [[4, 512]], [[1, 128], [5, 128], [1, 512]], ... ] An architecture with dilated convolutions input_dim = 512 layers = [ [[2, 512, 1]], [[2, 512, 2]], [[2, 512, 4]], [[2, 512, 8]], # receptive field == 16 [[2, 512, 1]], [[2, 512, 2]], [[2, 512, 4]], [[2, 512, 8]], # receptive field == 31 [[2, 512, 1]], [[2, 512, 2]], [[2, 512, 4]], [[2, 512, 8]], # receptive field == 46 [[2, 512, 1]], [[2, 512, 2]], [[2, 512, 4]], [[2, 512, 8]], # receptive field == 57 ] Registered as a Seq2SeqEncoder with name \"gated-cnn-encoder\". Parameters input_dim : int The dimension of the inputs. layers : Sequence[Sequence[Sequence[int]]] The layer dimensions for each ResidualBlock . dropout : float , optional (default = 0.0 ) The dropout for each ResidualBlock . return_all_layers : bool , optional (default = False ) Whether to return all layers or just the last layer.","title":"GatedCnnEncoder"},{"location":"api/modules/seq2seq_encoders/gated_cnn_encoder/#forward_1","text":"class GatedCnnEncoder ( Seq2SeqEncoder ): | ... | def forward ( | self , | token_embeddings : torch . Tensor , | mask : torch . BoolTensor | ) Convolutions need transposed input","title":"forward"},{"location":"api/modules/seq2seq_encoders/gated_cnn_encoder/#get_input_dim","text":"class GatedCnnEncoder ( Seq2SeqEncoder ): | ... | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"api/modules/seq2seq_encoders/gated_cnn_encoder/#get_output_dim","text":"class GatedCnnEncoder ( Seq2SeqEncoder ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/seq2seq_encoders/gated_cnn_encoder/#is_bidirectional","text":"class GatedCnnEncoder ( Seq2SeqEncoder ): | ... | def is_bidirectional ( self ) -> bool","title":"is_bidirectional"},{"location":"api/modules/seq2seq_encoders/pass_through_encoder/","text":"[ allennlp .modules .seq2seq_encoders .pass_through_encoder ] PassThroughEncoder # class PassThroughEncoder ( Seq2SeqEncoder ): | def __init__ ( self , input_dim : int ) -> None This class allows you to specify skipping a Seq2SeqEncoder just by changing a configuration file. This is useful for ablations and measuring the impact of different elements of your model. Registered as a Seq2SeqEncoder with name \"pass_through\". get_input_dim # class PassThroughEncoder ( Seq2SeqEncoder ): | ... | @overrides | def get_input_dim ( self ) -> int get_output_dim # class PassThroughEncoder ( Seq2SeqEncoder ): | ... | @overrides | def get_output_dim ( self ) -> int is_bidirectional # class PassThroughEncoder ( Seq2SeqEncoder ): | ... | @overrides | def is_bidirectional ( self ) forward # class PassThroughEncoder ( Seq2SeqEncoder ): | ... | @overrides | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor = None | ) -> torch . Tensor Parameters inputs : torch.Tensor A tensor of shape (batch_size, timesteps, input_dim) mask : torch.BoolTensor , optional (default = None ) A tensor of shape (batch_size, timesteps). Returns A tensor of shape (batch_size, timesteps, output_dim), where output_dim = input_dim.","title":"pass_through_encoder"},{"location":"api/modules/seq2seq_encoders/pass_through_encoder/#passthroughencoder","text":"class PassThroughEncoder ( Seq2SeqEncoder ): | def __init__ ( self , input_dim : int ) -> None This class allows you to specify skipping a Seq2SeqEncoder just by changing a configuration file. This is useful for ablations and measuring the impact of different elements of your model. Registered as a Seq2SeqEncoder with name \"pass_through\".","title":"PassThroughEncoder"},{"location":"api/modules/seq2seq_encoders/pass_through_encoder/#get_input_dim","text":"class PassThroughEncoder ( Seq2SeqEncoder ): | ... | @overrides | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"api/modules/seq2seq_encoders/pass_through_encoder/#get_output_dim","text":"class PassThroughEncoder ( Seq2SeqEncoder ): | ... | @overrides | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/seq2seq_encoders/pass_through_encoder/#is_bidirectional","text":"class PassThroughEncoder ( Seq2SeqEncoder ): | ... | @overrides | def is_bidirectional ( self )","title":"is_bidirectional"},{"location":"api/modules/seq2seq_encoders/pass_through_encoder/#forward","text":"class PassThroughEncoder ( Seq2SeqEncoder ): | ... | @overrides | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor = None | ) -> torch . Tensor Parameters inputs : torch.Tensor A tensor of shape (batch_size, timesteps, input_dim) mask : torch.BoolTensor , optional (default = None ) A tensor of shape (batch_size, timesteps). Returns A tensor of shape (batch_size, timesteps, output_dim), where output_dim = input_dim.","title":"forward"},{"location":"api/modules/seq2seq_encoders/pytorch_seq2seq_wrapper/","text":"[ allennlp .modules .seq2seq_encoders .pytorch_seq2seq_wrapper ] PytorchSeq2SeqWrapper # class PytorchSeq2SeqWrapper ( Seq2SeqEncoder ): | def __init__ ( | self , | module : torch . nn . Module , | stateful : bool = False | ) -> None Pytorch's RNNs have two outputs: the hidden state for every time step, and the hidden state at the last time step for every layer. We just want the first one as a single output. This wrapper pulls out that output, and adds a get_output_dim method, which is useful if you want to, e.g., define a linear + softmax layer on top of this to get some distribution over a set of labels. The linear layer needs to know its input dimension before it is called, and you can get that from get_output_dim . In order to be wrapped with this wrapper, a class must have the following members: - `self.input_size: int` - `self.hidden_size: int` - `def forward(inputs: PackedSequence, hidden_state: torch.Tensor) -> Tuple[PackedSequence, torch.Tensor]`. - `self.bidirectional: bool` (optional) This is what pytorch's RNN's look like - just make sure your class looks like those, and it should work. Note that we require you to pass a binary mask of shape (batch_size, sequence_length) when you call this module, to avoid subtle bugs around masking. If you already have a PackedSequence you can pass None as the second parameter. We support stateful RNNs where the final state from each batch is used as the initial state for the subsequent batch by passing stateful=True to the constructor. get_input_dim # class PytorchSeq2SeqWrapper ( Seq2SeqEncoder ): | ... | @overrides | def get_input_dim ( self ) -> int get_output_dim # class PytorchSeq2SeqWrapper ( Seq2SeqEncoder ): | ... | @overrides | def get_output_dim ( self ) -> int is_bidirectional # class PytorchSeq2SeqWrapper ( Seq2SeqEncoder ): | ... | @overrides | def is_bidirectional ( self ) -> bool forward # class PytorchSeq2SeqWrapper ( Seq2SeqEncoder ): | ... | @overrides | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor , | hidden_state : torch . Tensor = None | ) -> torch . Tensor GruSeq2SeqEncoder # class GruSeq2SeqEncoder ( PytorchSeq2SeqWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int = 1 , | bias : bool = True , | dropout : float = 0.0 , | bidirectional : bool = False , | stateful : bool = False | ) Registered as a Seq2SeqEncoder with name \"gru\". LstmSeq2SeqEncoder # class LstmSeq2SeqEncoder ( PytorchSeq2SeqWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int = 1 , | bias : bool = True , | dropout : float = 0.0 , | bidirectional : bool = False , | stateful : bool = False | ) Registered as a Seq2SeqEncoder with name \"lstm\". RnnSeq2SeqEncoder # class RnnSeq2SeqEncoder ( PytorchSeq2SeqWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int = 1 , | nonlinearity : str = \"tanh\" , | bias : bool = True , | dropout : float = 0.0 , | bidirectional : bool = False , | stateful : bool = False | ) Registered as a Seq2SeqEncoder with name \"rnn\". AugmentedLstmSeq2SeqEncoder # class AugmentedLstmSeq2SeqEncoder ( PytorchSeq2SeqWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | go_forward : bool = True , | recurrent_dropout_probability : float = 0.0 , | use_highway : bool = True , | use_input_projection_bias : bool = True , | stateful : bool = False | ) -> None Registered as a Seq2SeqEncoder with name \"augmented_lstm\". StackedAlternatingLstmSeq2SeqEncoder # class StackedAlternatingLstmSeq2SeqEncoder ( PytorchSeq2SeqWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int , | recurrent_dropout_probability : float = 0.0 , | use_highway : bool = True , | use_input_projection_bias : bool = True , | stateful : bool = False | ) -> None Registered as a Seq2SeqEncoder with name \"alternating_lstm\". StackedBidirectionalLstmSeq2SeqEncoder # class StackedBidirectionalLstmSeq2SeqEncoder ( PytorchSeq2SeqWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int , | recurrent_dropout_probability : float = 0.0 , | layer_dropout_probability : float = 0.0 , | use_highway : bool = True , | stateful : bool = False | ) -> None Registered as a Seq2SeqEncoder with name \"stacked_bidirectional_lstm\".","title":"pytorch_seq2seq_wrapper"},{"location":"api/modules/seq2seq_encoders/pytorch_seq2seq_wrapper/#pytorchseq2seqwrapper","text":"class PytorchSeq2SeqWrapper ( Seq2SeqEncoder ): | def __init__ ( | self , | module : torch . nn . Module , | stateful : bool = False | ) -> None Pytorch's RNNs have two outputs: the hidden state for every time step, and the hidden state at the last time step for every layer. We just want the first one as a single output. This wrapper pulls out that output, and adds a get_output_dim method, which is useful if you want to, e.g., define a linear + softmax layer on top of this to get some distribution over a set of labels. The linear layer needs to know its input dimension before it is called, and you can get that from get_output_dim . In order to be wrapped with this wrapper, a class must have the following members: - `self.input_size: int` - `self.hidden_size: int` - `def forward(inputs: PackedSequence, hidden_state: torch.Tensor) -> Tuple[PackedSequence, torch.Tensor]`. - `self.bidirectional: bool` (optional) This is what pytorch's RNN's look like - just make sure your class looks like those, and it should work. Note that we require you to pass a binary mask of shape (batch_size, sequence_length) when you call this module, to avoid subtle bugs around masking. If you already have a PackedSequence you can pass None as the second parameter. We support stateful RNNs where the final state from each batch is used as the initial state for the subsequent batch by passing stateful=True to the constructor.","title":"PytorchSeq2SeqWrapper"},{"location":"api/modules/seq2seq_encoders/pytorch_seq2seq_wrapper/#get_input_dim","text":"class PytorchSeq2SeqWrapper ( Seq2SeqEncoder ): | ... | @overrides | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"api/modules/seq2seq_encoders/pytorch_seq2seq_wrapper/#get_output_dim","text":"class PytorchSeq2SeqWrapper ( Seq2SeqEncoder ): | ... | @overrides | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/seq2seq_encoders/pytorch_seq2seq_wrapper/#is_bidirectional","text":"class PytorchSeq2SeqWrapper ( Seq2SeqEncoder ): | ... | @overrides | def is_bidirectional ( self ) -> bool","title":"is_bidirectional"},{"location":"api/modules/seq2seq_encoders/pytorch_seq2seq_wrapper/#forward","text":"class PytorchSeq2SeqWrapper ( Seq2SeqEncoder ): | ... | @overrides | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor , | hidden_state : torch . Tensor = None | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/seq2seq_encoders/pytorch_seq2seq_wrapper/#gruseq2seqencoder","text":"class GruSeq2SeqEncoder ( PytorchSeq2SeqWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int = 1 , | bias : bool = True , | dropout : float = 0.0 , | bidirectional : bool = False , | stateful : bool = False | ) Registered as a Seq2SeqEncoder with name \"gru\".","title":"GruSeq2SeqEncoder"},{"location":"api/modules/seq2seq_encoders/pytorch_seq2seq_wrapper/#lstmseq2seqencoder","text":"class LstmSeq2SeqEncoder ( PytorchSeq2SeqWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int = 1 , | bias : bool = True , | dropout : float = 0.0 , | bidirectional : bool = False , | stateful : bool = False | ) Registered as a Seq2SeqEncoder with name \"lstm\".","title":"LstmSeq2SeqEncoder"},{"location":"api/modules/seq2seq_encoders/pytorch_seq2seq_wrapper/#rnnseq2seqencoder","text":"class RnnSeq2SeqEncoder ( PytorchSeq2SeqWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int = 1 , | nonlinearity : str = \"tanh\" , | bias : bool = True , | dropout : float = 0.0 , | bidirectional : bool = False , | stateful : bool = False | ) Registered as a Seq2SeqEncoder with name \"rnn\".","title":"RnnSeq2SeqEncoder"},{"location":"api/modules/seq2seq_encoders/pytorch_seq2seq_wrapper/#augmentedlstmseq2seqencoder","text":"class AugmentedLstmSeq2SeqEncoder ( PytorchSeq2SeqWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | go_forward : bool = True , | recurrent_dropout_probability : float = 0.0 , | use_highway : bool = True , | use_input_projection_bias : bool = True , | stateful : bool = False | ) -> None Registered as a Seq2SeqEncoder with name \"augmented_lstm\".","title":"AugmentedLstmSeq2SeqEncoder"},{"location":"api/modules/seq2seq_encoders/pytorch_seq2seq_wrapper/#stackedalternatinglstmseq2seqencoder","text":"class StackedAlternatingLstmSeq2SeqEncoder ( PytorchSeq2SeqWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int , | recurrent_dropout_probability : float = 0.0 , | use_highway : bool = True , | use_input_projection_bias : bool = True , | stateful : bool = False | ) -> None Registered as a Seq2SeqEncoder with name \"alternating_lstm\".","title":"StackedAlternatingLstmSeq2SeqEncoder"},{"location":"api/modules/seq2seq_encoders/pytorch_seq2seq_wrapper/#stackedbidirectionallstmseq2seqencoder","text":"class StackedBidirectionalLstmSeq2SeqEncoder ( PytorchSeq2SeqWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int , | recurrent_dropout_probability : float = 0.0 , | layer_dropout_probability : float = 0.0 , | use_highway : bool = True , | stateful : bool = False | ) -> None Registered as a Seq2SeqEncoder with name \"stacked_bidirectional_lstm\".","title":"StackedBidirectionalLstmSeq2SeqEncoder"},{"location":"api/modules/seq2seq_encoders/pytorch_transformer_wrapper/","text":"[ allennlp .modules .seq2seq_encoders .pytorch_transformer_wrapper ] PytorchTransformer # class PytorchTransformer ( Seq2SeqEncoder ): | def __init__ ( | self , | input_dim : int , | num_layers : int , | feedforward_hidden_dim : int = 2048 , | num_attention_heads : int = 8 , | positional_encoding : Optional [ str ] = None , | positional_embedding_size : int = 512 , | dropout_prob : float = 0.1 , | activation : str = \"relu\" | ) -> None Implements a stacked self-attention encoder similar to the Transformer architecture in [Attention is all you Need] (https://www.semanticscholar.org/paper/Attention-Is-All-You-Need-Vaswani-Shazeer/0737da0767d77606169cbf4187b83e1ab62f6077). This class adapts the Transformer from torch.nn for use in AllenNLP. Optionally, it adds positional encodings. Registered as a Seq2SeqEncoder with name \"pytorch_transformer\". Parameters input_dim : int The input dimension of the encoder. feedforward_hidden_dim : int The middle dimension of the FeedForward network. The input and output dimensions are fixed to ensure sizes match up for the self attention layers. num_layers : int The number of stacked self attention -> feedforward -> layer normalisation blocks. num_attention_heads : int The number of attention heads to use per layer. use_positional_encoding : bool , optional (default = True ) Whether to add sinusoidal frequencies to the input tensor. This is strongly recommended, as without this feature, the self attention layers have no idea of absolute or relative position (as they are just computing pairwise similarity between vectors of elements), which can be important features for many tasks. dropout_prob : float , optional (default = 0.1 ) The dropout probability for the feedforward network. get_input_dim # class PytorchTransformer ( Seq2SeqEncoder ): | ... | @overrides | def get_input_dim ( self ) -> int get_output_dim # class PytorchTransformer ( Seq2SeqEncoder ): | ... | @overrides | def get_output_dim ( self ) -> int is_bidirectional # class PytorchTransformer ( Seq2SeqEncoder ): | ... | @overrides | def is_bidirectional ( self ) forward # class PytorchTransformer ( Seq2SeqEncoder ): | ... | @overrides | def forward ( self , inputs : torch . Tensor , mask : torch . BoolTensor )","title":"pytorch_transformer_wrapper"},{"location":"api/modules/seq2seq_encoders/pytorch_transformer_wrapper/#pytorchtransformer","text":"class PytorchTransformer ( Seq2SeqEncoder ): | def __init__ ( | self , | input_dim : int , | num_layers : int , | feedforward_hidden_dim : int = 2048 , | num_attention_heads : int = 8 , | positional_encoding : Optional [ str ] = None , | positional_embedding_size : int = 512 , | dropout_prob : float = 0.1 , | activation : str = \"relu\" | ) -> None Implements a stacked self-attention encoder similar to the Transformer architecture in [Attention is all you Need] (https://www.semanticscholar.org/paper/Attention-Is-All-You-Need-Vaswani-Shazeer/0737da0767d77606169cbf4187b83e1ab62f6077). This class adapts the Transformer from torch.nn for use in AllenNLP. Optionally, it adds positional encodings. Registered as a Seq2SeqEncoder with name \"pytorch_transformer\". Parameters input_dim : int The input dimension of the encoder. feedforward_hidden_dim : int The middle dimension of the FeedForward network. The input and output dimensions are fixed to ensure sizes match up for the self attention layers. num_layers : int The number of stacked self attention -> feedforward -> layer normalisation blocks. num_attention_heads : int The number of attention heads to use per layer. use_positional_encoding : bool , optional (default = True ) Whether to add sinusoidal frequencies to the input tensor. This is strongly recommended, as without this feature, the self attention layers have no idea of absolute or relative position (as they are just computing pairwise similarity between vectors of elements), which can be important features for many tasks. dropout_prob : float , optional (default = 0.1 ) The dropout probability for the feedforward network.","title":"PytorchTransformer"},{"location":"api/modules/seq2seq_encoders/pytorch_transformer_wrapper/#get_input_dim","text":"class PytorchTransformer ( Seq2SeqEncoder ): | ... | @overrides | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"api/modules/seq2seq_encoders/pytorch_transformer_wrapper/#get_output_dim","text":"class PytorchTransformer ( Seq2SeqEncoder ): | ... | @overrides | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/seq2seq_encoders/pytorch_transformer_wrapper/#is_bidirectional","text":"class PytorchTransformer ( Seq2SeqEncoder ): | ... | @overrides | def is_bidirectional ( self )","title":"is_bidirectional"},{"location":"api/modules/seq2seq_encoders/pytorch_transformer_wrapper/#forward","text":"class PytorchTransformer ( Seq2SeqEncoder ): | ... | @overrides | def forward ( self , inputs : torch . Tensor , mask : torch . BoolTensor )","title":"forward"},{"location":"api/modules/seq2seq_encoders/seq2seq_encoder/","text":"[ allennlp .modules .seq2seq_encoders .seq2seq_encoder ] Seq2SeqEncoder # class Seq2SeqEncoder ( _EncoderBase , Registrable ) A Seq2SeqEncoder is a Module that takes as input a sequence of vectors and returns a modified sequence of vectors. Input shape : (batch_size, sequence_length, input_dim) ; output shape : (batch_size, sequence_length, output_dim) . We add two methods to the basic Module API: get_input_dim() and get_output_dim() . You might need this if you want to construct a Linear layer using the output of this encoder, or to raise sensible errors for mis-matching input dimensions. get_input_dim # class Seq2SeqEncoder ( _EncoderBase , Registrable ): | ... | def get_input_dim ( self ) -> int Returns the dimension of the vector input for each element in the sequence input to a Seq2SeqEncoder . This is not the shape of the input tensor, but the last element of that shape. get_output_dim # class Seq2SeqEncoder ( _EncoderBase , Registrable ): | ... | def get_output_dim ( self ) -> int Returns the dimension of each vector in the sequence output by this Seq2SeqEncoder . This is not the shape of the returned tensor, but the last element of that shape. is_bidirectional # class Seq2SeqEncoder ( _EncoderBase , Registrable ): | ... | def is_bidirectional ( self ) -> bool Returns True if this encoder is bidirectional. If so, we assume the forward direction of the encoder is the first half of the final dimension, and the backward direction is the second half.","title":"seq2seq_encoder"},{"location":"api/modules/seq2seq_encoders/seq2seq_encoder/#seq2seqencoder","text":"class Seq2SeqEncoder ( _EncoderBase , Registrable ) A Seq2SeqEncoder is a Module that takes as input a sequence of vectors and returns a modified sequence of vectors. Input shape : (batch_size, sequence_length, input_dim) ; output shape : (batch_size, sequence_length, output_dim) . We add two methods to the basic Module API: get_input_dim() and get_output_dim() . You might need this if you want to construct a Linear layer using the output of this encoder, or to raise sensible errors for mis-matching input dimensions.","title":"Seq2SeqEncoder"},{"location":"api/modules/seq2seq_encoders/seq2seq_encoder/#get_input_dim","text":"class Seq2SeqEncoder ( _EncoderBase , Registrable ): | ... | def get_input_dim ( self ) -> int Returns the dimension of the vector input for each element in the sequence input to a Seq2SeqEncoder . This is not the shape of the input tensor, but the last element of that shape.","title":"get_input_dim"},{"location":"api/modules/seq2seq_encoders/seq2seq_encoder/#get_output_dim","text":"class Seq2SeqEncoder ( _EncoderBase , Registrable ): | ... | def get_output_dim ( self ) -> int Returns the dimension of each vector in the sequence output by this Seq2SeqEncoder . This is not the shape of the returned tensor, but the last element of that shape.","title":"get_output_dim"},{"location":"api/modules/seq2seq_encoders/seq2seq_encoder/#is_bidirectional","text":"class Seq2SeqEncoder ( _EncoderBase , Registrable ): | ... | def is_bidirectional ( self ) -> bool Returns True if this encoder is bidirectional. If so, we assume the forward direction of the encoder is the first half of the final dimension, and the backward direction is the second half.","title":"is_bidirectional"},{"location":"api/modules/seq2vec_encoders/bert_pooler/","text":"[ allennlp .modules .seq2vec_encoders .bert_pooler ] BertPooler # class BertPooler ( Seq2VecEncoder ): | def __init__ ( | self , | pretrained_model : str , | * , | override_weights_file : Optional [ str ] = None , | override_weights_strip_prefix : Optional [ str ] = None , | requires_grad : bool = True , | dropout : float = 0.0 | ) -> None The pooling layer at the end of the BERT model. This returns an embedding for the [CLS] token, after passing it through a non-linear tanh activation; the non-linear layer is also part of the BERT model. If you want to use the pretrained BERT model to build a classifier and you want to use the AllenNLP token-indexer -> token-embedder -> seq2vec encoder setup, this is the Seq2VecEncoder to use. (For example, if you want to experiment with other embedding / encoding combinations.) Registered as a Seq2VecEncoder with name \"bert_pooler\". Parameters pretrained_model : Union[str, BertModel] The pretrained BERT model to use. If this is a string, we will call transformers.AutoModel.from_pretrained(pretrained_model) and use that. requires_grad : bool , optional (default = True ) If True, the weights of the pooler will be updated during training. Otherwise they will not. dropout : float , optional (default = 0.0 ) Amount of dropout to apply after pooling get_input_dim # class BertPooler ( Seq2VecEncoder ): | ... | @overrides | def get_input_dim ( self ) -> int get_output_dim # class BertPooler ( Seq2VecEncoder ): | ... | @overrides | def get_output_dim ( self ) -> int forward # class BertPooler ( Seq2VecEncoder ): | ... | def forward ( | self , | tokens : torch . Tensor , | mask : torch . BoolTensor = None , | num_wrapping_dims : int = 0 | )","title":"bert_pooler"},{"location":"api/modules/seq2vec_encoders/bert_pooler/#bertpooler","text":"class BertPooler ( Seq2VecEncoder ): | def __init__ ( | self , | pretrained_model : str , | * , | override_weights_file : Optional [ str ] = None , | override_weights_strip_prefix : Optional [ str ] = None , | requires_grad : bool = True , | dropout : float = 0.0 | ) -> None The pooling layer at the end of the BERT model. This returns an embedding for the [CLS] token, after passing it through a non-linear tanh activation; the non-linear layer is also part of the BERT model. If you want to use the pretrained BERT model to build a classifier and you want to use the AllenNLP token-indexer -> token-embedder -> seq2vec encoder setup, this is the Seq2VecEncoder to use. (For example, if you want to experiment with other embedding / encoding combinations.) Registered as a Seq2VecEncoder with name \"bert_pooler\". Parameters pretrained_model : Union[str, BertModel] The pretrained BERT model to use. If this is a string, we will call transformers.AutoModel.from_pretrained(pretrained_model) and use that. requires_grad : bool , optional (default = True ) If True, the weights of the pooler will be updated during training. Otherwise they will not. dropout : float , optional (default = 0.0 ) Amount of dropout to apply after pooling","title":"BertPooler"},{"location":"api/modules/seq2vec_encoders/bert_pooler/#get_input_dim","text":"class BertPooler ( Seq2VecEncoder ): | ... | @overrides | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"api/modules/seq2vec_encoders/bert_pooler/#get_output_dim","text":"class BertPooler ( Seq2VecEncoder ): | ... | @overrides | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/seq2vec_encoders/bert_pooler/#forward","text":"class BertPooler ( Seq2VecEncoder ): | ... | def forward ( | self , | tokens : torch . Tensor , | mask : torch . BoolTensor = None , | num_wrapping_dims : int = 0 | )","title":"forward"},{"location":"api/modules/seq2vec_encoders/boe_encoder/","text":"[ allennlp .modules .seq2vec_encoders .boe_encoder ] BagOfEmbeddingsEncoder # class BagOfEmbeddingsEncoder ( Seq2VecEncoder ): | def __init__ ( self , embedding_dim : int , averaged : bool = False ) -> None A BagOfEmbeddingsEncoder is a simple Seq2VecEncoder which simply sums the embeddings of a sequence across the time dimension. The input to this module is of shape (batch_size, num_tokens, embedding_dim) , and the output is of shape (batch_size, embedding_dim) . Registered as a Seq2VecEncoder with name \"bag_of_embeddings\" and \"boe\". Parameters embedding_dim : int This is the input dimension to the encoder. averaged : bool , optional (default = False ) If True , this module will average the embeddings across time, rather than simply summing (ie. we will divide the summed embeddings by the length of the sentence). get_input_dim # class BagOfEmbeddingsEncoder ( Seq2VecEncoder ): | ... | @overrides | def get_input_dim ( self ) -> int get_output_dim # class BagOfEmbeddingsEncoder ( Seq2VecEncoder ): | ... | @overrides | def get_output_dim ( self ) -> int forward # class BagOfEmbeddingsEncoder ( Seq2VecEncoder ): | ... | def forward ( | self , | tokens : torch . Tensor , | mask : torch . BoolTensor = None | )","title":"boe_encoder"},{"location":"api/modules/seq2vec_encoders/boe_encoder/#bagofembeddingsencoder","text":"class BagOfEmbeddingsEncoder ( Seq2VecEncoder ): | def __init__ ( self , embedding_dim : int , averaged : bool = False ) -> None A BagOfEmbeddingsEncoder is a simple Seq2VecEncoder which simply sums the embeddings of a sequence across the time dimension. The input to this module is of shape (batch_size, num_tokens, embedding_dim) , and the output is of shape (batch_size, embedding_dim) . Registered as a Seq2VecEncoder with name \"bag_of_embeddings\" and \"boe\". Parameters embedding_dim : int This is the input dimension to the encoder. averaged : bool , optional (default = False ) If True , this module will average the embeddings across time, rather than simply summing (ie. we will divide the summed embeddings by the length of the sentence).","title":"BagOfEmbeddingsEncoder"},{"location":"api/modules/seq2vec_encoders/boe_encoder/#get_input_dim","text":"class BagOfEmbeddingsEncoder ( Seq2VecEncoder ): | ... | @overrides | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"api/modules/seq2vec_encoders/boe_encoder/#get_output_dim","text":"class BagOfEmbeddingsEncoder ( Seq2VecEncoder ): | ... | @overrides | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/seq2vec_encoders/boe_encoder/#forward","text":"class BagOfEmbeddingsEncoder ( Seq2VecEncoder ): | ... | def forward ( | self , | tokens : torch . Tensor , | mask : torch . BoolTensor = None | )","title":"forward"},{"location":"api/modules/seq2vec_encoders/cls_pooler/","text":"[ allennlp .modules .seq2vec_encoders .cls_pooler ] ClsPooler # class ClsPooler ( Seq2VecEncoder ): | def __init__ ( | self , | embedding_dim : int = None , | cls_is_last_token : bool = False | ) Just takes the first vector from a list of vectors (which in a transformer is typically the [CLS] token) and returns it. For BERT, it's recommended to use BertPooler instead. Registered as a Seq2VecEncoder with name \"cls_pooler\". Parameters embedding_dim : int , optional This isn't needed for any computation that we do, but we sometimes rely on get_input_dim and get_output_dim to check parameter settings, or to instantiate final linear layers. In order to give the right values there, we need to know the embedding dimension. If you're using this with a transformer from the transformers library, this can often be found with model.config.hidden_size , if you're not sure. cls_is_last_token : bool , optional The [CLS] token is the first token for most of the pretrained transformer models. For some models such as XLNet, however, it is the last token, and we therefore need to select at the end. get_input_dim # class ClsPooler ( Seq2VecEncoder ): | ... | @overrides | def get_input_dim ( self ) -> int get_output_dim # class ClsPooler ( Seq2VecEncoder ): | ... | @overrides | def get_output_dim ( self ) -> int forward # class ClsPooler ( Seq2VecEncoder ): | ... | @overrides | def forward ( | self , | tokens : torch . Tensor , | mask : torch . BoolTensor = None | ) tokens is assumed to have shape (batch_size, sequence_length, embedding_dim). mask is assumed to have shape (batch_size, sequence_length) with all 1s preceding all 0s.","title":"cls_pooler"},{"location":"api/modules/seq2vec_encoders/cls_pooler/#clspooler","text":"class ClsPooler ( Seq2VecEncoder ): | def __init__ ( | self , | embedding_dim : int = None , | cls_is_last_token : bool = False | ) Just takes the first vector from a list of vectors (which in a transformer is typically the [CLS] token) and returns it. For BERT, it's recommended to use BertPooler instead. Registered as a Seq2VecEncoder with name \"cls_pooler\". Parameters embedding_dim : int , optional This isn't needed for any computation that we do, but we sometimes rely on get_input_dim and get_output_dim to check parameter settings, or to instantiate final linear layers. In order to give the right values there, we need to know the embedding dimension. If you're using this with a transformer from the transformers library, this can often be found with model.config.hidden_size , if you're not sure. cls_is_last_token : bool , optional The [CLS] token is the first token for most of the pretrained transformer models. For some models such as XLNet, however, it is the last token, and we therefore need to select at the end.","title":"ClsPooler"},{"location":"api/modules/seq2vec_encoders/cls_pooler/#get_input_dim","text":"class ClsPooler ( Seq2VecEncoder ): | ... | @overrides | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"api/modules/seq2vec_encoders/cls_pooler/#get_output_dim","text":"class ClsPooler ( Seq2VecEncoder ): | ... | @overrides | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/seq2vec_encoders/cls_pooler/#forward","text":"class ClsPooler ( Seq2VecEncoder ): | ... | @overrides | def forward ( | self , | tokens : torch . Tensor , | mask : torch . BoolTensor = None | ) tokens is assumed to have shape (batch_size, sequence_length, embedding_dim). mask is assumed to have shape (batch_size, sequence_length) with all 1s preceding all 0s.","title":"forward"},{"location":"api/modules/seq2vec_encoders/cnn_encoder/","text":"[ allennlp .modules .seq2vec_encoders .cnn_encoder ] CnnEncoder # class CnnEncoder ( Seq2VecEncoder ): | def __init__ ( | self , | embedding_dim : int , | num_filters : int , | ngram_filter_sizes : Tuple [ int , ... ] = ( 2 , 3 , 4 , 5 ), | conv_layer_activation : Activation = None , | output_dim : Optional [ int ] = None | ) -> None A CnnEncoder is a combination of multiple convolution layers and max pooling layers. As a Seq2VecEncoder , the input to this module is of shape (batch_size, num_tokens, input_dim) , and the output is of shape (batch_size, output_dim) . The CNN has one convolution layer for each ngram filter size. Each convolution operation gives out a vector of size num_filters. The number of times a convolution layer will be used is num_tokens - ngram_size + 1 . The corresponding maxpooling layer aggregates all these outputs from the convolution layer and outputs the max. This operation is repeated for every ngram size passed, and consequently the dimensionality of the output after maxpooling is len(ngram_filter_sizes) * num_filters . This then gets (optionally) projected down to a lower dimensional output, specified by output_dim . We then use a fully connected layer to project in back to the desired output_dim. For more details, refer to \"A Sensitivity Analysis of (and Practitioners\u2019 Guide to) Convolutional Neural Networks for Sentence Classification\", Zhang and Wallace 2016, particularly Figure 1. Registered as a Seq2VecEncoder with name \"cnn\". Parameters embedding_dim : int This is the input dimension to the encoder. We need this because we can't do shape inference in pytorch, and we need to know what size filters to construct in the CNN. num_filters : int This is the output dim for each convolutional layer, which is the number of \"filters\" learned by that layer. ngram_filter_sizes : Tuple[int] , optional (default = (2, 3, 4, 5) ) This specifies both the number of convolutional layers we will create and their sizes. The default of (2, 3, 4, 5) will have four convolutional layers, corresponding to encoding ngrams of size 2 to 5 with some number of filters. conv_layer_activation : Activation , optional (default = torch.nn.ReLU ) Activation to use after the convolution layers. output_dim : Optional[int] , optional (default = None ) After doing convolutions and pooling, we'll project the collected features into a vector of this size. If this value is None , we will just return the result of the max pooling, giving an output of shape len(ngram_filter_sizes) * num_filters . get_input_dim # class CnnEncoder ( Seq2VecEncoder ): | ... | @overrides | def get_input_dim ( self ) -> int get_output_dim # class CnnEncoder ( Seq2VecEncoder ): | ... | @overrides | def get_output_dim ( self ) -> int forward # class CnnEncoder ( Seq2VecEncoder ): | ... | def forward ( self , tokens : torch . Tensor , mask : torch . BoolTensor )","title":"cnn_encoder"},{"location":"api/modules/seq2vec_encoders/cnn_encoder/#cnnencoder","text":"class CnnEncoder ( Seq2VecEncoder ): | def __init__ ( | self , | embedding_dim : int , | num_filters : int , | ngram_filter_sizes : Tuple [ int , ... ] = ( 2 , 3 , 4 , 5 ), | conv_layer_activation : Activation = None , | output_dim : Optional [ int ] = None | ) -> None A CnnEncoder is a combination of multiple convolution layers and max pooling layers. As a Seq2VecEncoder , the input to this module is of shape (batch_size, num_tokens, input_dim) , and the output is of shape (batch_size, output_dim) . The CNN has one convolution layer for each ngram filter size. Each convolution operation gives out a vector of size num_filters. The number of times a convolution layer will be used is num_tokens - ngram_size + 1 . The corresponding maxpooling layer aggregates all these outputs from the convolution layer and outputs the max. This operation is repeated for every ngram size passed, and consequently the dimensionality of the output after maxpooling is len(ngram_filter_sizes) * num_filters . This then gets (optionally) projected down to a lower dimensional output, specified by output_dim . We then use a fully connected layer to project in back to the desired output_dim. For more details, refer to \"A Sensitivity Analysis of (and Practitioners\u2019 Guide to) Convolutional Neural Networks for Sentence Classification\", Zhang and Wallace 2016, particularly Figure 1. Registered as a Seq2VecEncoder with name \"cnn\". Parameters embedding_dim : int This is the input dimension to the encoder. We need this because we can't do shape inference in pytorch, and we need to know what size filters to construct in the CNN. num_filters : int This is the output dim for each convolutional layer, which is the number of \"filters\" learned by that layer. ngram_filter_sizes : Tuple[int] , optional (default = (2, 3, 4, 5) ) This specifies both the number of convolutional layers we will create and their sizes. The default of (2, 3, 4, 5) will have four convolutional layers, corresponding to encoding ngrams of size 2 to 5 with some number of filters. conv_layer_activation : Activation , optional (default = torch.nn.ReLU ) Activation to use after the convolution layers. output_dim : Optional[int] , optional (default = None ) After doing convolutions and pooling, we'll project the collected features into a vector of this size. If this value is None , we will just return the result of the max pooling, giving an output of shape len(ngram_filter_sizes) * num_filters .","title":"CnnEncoder"},{"location":"api/modules/seq2vec_encoders/cnn_encoder/#get_input_dim","text":"class CnnEncoder ( Seq2VecEncoder ): | ... | @overrides | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"api/modules/seq2vec_encoders/cnn_encoder/#get_output_dim","text":"class CnnEncoder ( Seq2VecEncoder ): | ... | @overrides | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/seq2vec_encoders/cnn_encoder/#forward","text":"class CnnEncoder ( Seq2VecEncoder ): | ... | def forward ( self , tokens : torch . Tensor , mask : torch . BoolTensor )","title":"forward"},{"location":"api/modules/seq2vec_encoders/cnn_highway_encoder/","text":"[ allennlp .modules .seq2vec_encoders .cnn_highway_encoder ] CnnHighwayEncoder # class CnnHighwayEncoder ( Seq2VecEncoder ): | def __init__ ( | self , | embedding_dim : int , | filters : Sequence [ Sequence [ int ]], | num_highway : int , | projection_dim : int , | activation : str = \"relu\" , | projection_location : str = \"after_highway\" , | do_layer_norm : bool = False | ) -> None The character CNN + highway encoder from Kim et al \"Character aware neural language models\" with an optional projection. Registered as a Seq2VecEncoder with name \"cnn-highway\". Parameters embedding_dim : int The dimension of the initial character embedding. filters : Sequence[Sequence[int]] A sequence of pairs (filter_width, num_filters). num_highway : int The number of highway layers. projection_dim : int The output dimension of the projection layer. activation : str , optional (default = 'relu' ) The activation function for the convolutional layers. projection_location : str , optional (default = 'after_highway' ) Where to apply the projection layer. Valid values are 'after_highway', 'after_cnn', and None. forward # class CnnHighwayEncoder ( Seq2VecEncoder ): | ... | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor | ) -> Dict [ str , torch . Tensor ] Compute context insensitive token embeddings for ELMo representations. Parameters inputs : torch.Tensor Shape (batch_size, num_characters, embedding_dim) Character embeddings representing the current batch. mask : torch.BoolTensor Shape (batch_size, num_characters) Currently unused. The mask for characters is implicit. See TokenCharactersEncoder.forward. Returns encoding : Shape (batch_size, projection_dim) tensor with context-insensitive token representations. get_input_dim # class CnnHighwayEncoder ( Seq2VecEncoder ): | ... | def get_input_dim ( self ) -> int get_output_dim # class CnnHighwayEncoder ( Seq2VecEncoder ): | ... | def get_output_dim ( self ) -> int","title":"cnn_highway_encoder"},{"location":"api/modules/seq2vec_encoders/cnn_highway_encoder/#cnnhighwayencoder","text":"class CnnHighwayEncoder ( Seq2VecEncoder ): | def __init__ ( | self , | embedding_dim : int , | filters : Sequence [ Sequence [ int ]], | num_highway : int , | projection_dim : int , | activation : str = \"relu\" , | projection_location : str = \"after_highway\" , | do_layer_norm : bool = False | ) -> None The character CNN + highway encoder from Kim et al \"Character aware neural language models\" with an optional projection. Registered as a Seq2VecEncoder with name \"cnn-highway\". Parameters embedding_dim : int The dimension of the initial character embedding. filters : Sequence[Sequence[int]] A sequence of pairs (filter_width, num_filters). num_highway : int The number of highway layers. projection_dim : int The output dimension of the projection layer. activation : str , optional (default = 'relu' ) The activation function for the convolutional layers. projection_location : str , optional (default = 'after_highway' ) Where to apply the projection layer. Valid values are 'after_highway', 'after_cnn', and None.","title":"CnnHighwayEncoder"},{"location":"api/modules/seq2vec_encoders/cnn_highway_encoder/#forward","text":"class CnnHighwayEncoder ( Seq2VecEncoder ): | ... | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor | ) -> Dict [ str , torch . Tensor ] Compute context insensitive token embeddings for ELMo representations. Parameters inputs : torch.Tensor Shape (batch_size, num_characters, embedding_dim) Character embeddings representing the current batch. mask : torch.BoolTensor Shape (batch_size, num_characters) Currently unused. The mask for characters is implicit. See TokenCharactersEncoder.forward. Returns encoding : Shape (batch_size, projection_dim) tensor with context-insensitive token representations.","title":"forward"},{"location":"api/modules/seq2vec_encoders/cnn_highway_encoder/#get_input_dim","text":"class CnnHighwayEncoder ( Seq2VecEncoder ): | ... | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"api/modules/seq2vec_encoders/cnn_highway_encoder/#get_output_dim","text":"class CnnHighwayEncoder ( Seq2VecEncoder ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/seq2vec_encoders/pytorch_seq2vec_wrapper/","text":"[ allennlp .modules .seq2vec_encoders .pytorch_seq2vec_wrapper ] PytorchSeq2VecWrapper # class PytorchSeq2VecWrapper ( Seq2VecEncoder ): | def __init__ ( self , module : torch . nn . modules . RNNBase ) -> None Pytorch's RNNs have two outputs: the hidden state for every time step, and the hidden state at the last time step for every layer. We just want the second one as a single output. This wrapper pulls out that output, and adds a get_output_dim method, which is useful if you want to, e.g., define a linear + softmax layer on top of this to get some distribution over a set of labels. The linear layer needs to know its input dimension before it is called, and you can get that from get_output_dim . Also, there are lots of ways you could imagine going from an RNN hidden state at every timestep to a single vector - you could take the last vector at all layers in the stack, do some kind of pooling, take the last vector of the top layer in a stack, or many other options. We just take the final hidden state vector, or in the case of a bidirectional RNN cell, we concatenate the forward and backward final states together. TODO(mattg): allow for other ways of wrapping RNNs. In order to be wrapped with this wrapper, a class must have the following members: - `self.input_size: int` - `self.hidden_size: int` - `def forward(inputs: PackedSequence, hidden_state: torch.tensor) -> Tuple[PackedSequence, torch.Tensor]`. - `self.bidirectional: bool` (optional) This is what pytorch's RNN's look like - just make sure your class looks like those, and it should work. Note that we require you to pass sequence lengths when you call this module, to avoid subtle bugs around masking. If you already have a PackedSequence you can pass None as the second parameter. get_input_dim # class PytorchSeq2VecWrapper ( Seq2VecEncoder ): | ... | def get_input_dim ( self ) -> int get_output_dim # class PytorchSeq2VecWrapper ( Seq2VecEncoder ): | ... | def get_output_dim ( self ) -> int forward # class PytorchSeq2VecWrapper ( Seq2VecEncoder ): | ... | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor , | hidden_state : torch . Tensor = None | ) -> torch . Tensor GruSeq2VecEncoder # class GruSeq2VecEncoder ( PytorchSeq2VecWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int = 1 , | bias : bool = True , | dropout : float = 0.0 , | bidirectional : bool = False | ) Registered as a Seq2VecEncoder with name \"gru\". LstmSeq2VecEncoder # class LstmSeq2VecEncoder ( PytorchSeq2VecWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int = 1 , | bias : bool = True , | dropout : float = 0.0 , | bidirectional : bool = False | ) Registered as a Seq2VecEncoder with name \"lstm\". RnnSeq2VecEncoder # class RnnSeq2VecEncoder ( PytorchSeq2VecWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int = 1 , | nonlinearity : str = \"tanh\" , | bias : bool = True , | dropout : float = 0.0 , | bidirectional : bool = False | ) Registered as a Seq2VecEncoder with name \"rnn\". AugmentedLstmSeq2VecEncoder # class AugmentedLstmSeq2VecEncoder ( PytorchSeq2VecWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | go_forward : bool = True , | recurrent_dropout_probability : float = 0.0 , | use_highway : bool = True , | use_input_projection_bias : bool = True | ) -> None Registered as a Seq2VecEncoder with name \"augmented_lstm\". StackedAlternatingLstmSeq2VecEncoder # class StackedAlternatingLstmSeq2VecEncoder ( PytorchSeq2VecWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int , | recurrent_dropout_probability : float = 0.0 , | use_highway : bool = True , | use_input_projection_bias : bool = True | ) -> None Registered as a Seq2VecEncoder with name \"alternating_lstm\". StackedBidirectionalLstmSeq2VecEncoder # class StackedBidirectionalLstmSeq2VecEncoder ( PytorchSeq2VecWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int , | recurrent_dropout_probability : float = 0.0 , | layer_dropout_probability : float = 0.0 , | use_highway : bool = True | ) -> None Registered as a Seq2VecEncoder with name \"stacked_bidirectional_lstm\".","title":"pytorch_seq2vec_wrapper"},{"location":"api/modules/seq2vec_encoders/pytorch_seq2vec_wrapper/#pytorchseq2vecwrapper","text":"class PytorchSeq2VecWrapper ( Seq2VecEncoder ): | def __init__ ( self , module : torch . nn . modules . RNNBase ) -> None Pytorch's RNNs have two outputs: the hidden state for every time step, and the hidden state at the last time step for every layer. We just want the second one as a single output. This wrapper pulls out that output, and adds a get_output_dim method, which is useful if you want to, e.g., define a linear + softmax layer on top of this to get some distribution over a set of labels. The linear layer needs to know its input dimension before it is called, and you can get that from get_output_dim . Also, there are lots of ways you could imagine going from an RNN hidden state at every timestep to a single vector - you could take the last vector at all layers in the stack, do some kind of pooling, take the last vector of the top layer in a stack, or many other options. We just take the final hidden state vector, or in the case of a bidirectional RNN cell, we concatenate the forward and backward final states together. TODO(mattg): allow for other ways of wrapping RNNs. In order to be wrapped with this wrapper, a class must have the following members: - `self.input_size: int` - `self.hidden_size: int` - `def forward(inputs: PackedSequence, hidden_state: torch.tensor) -> Tuple[PackedSequence, torch.Tensor]`. - `self.bidirectional: bool` (optional) This is what pytorch's RNN's look like - just make sure your class looks like those, and it should work. Note that we require you to pass sequence lengths when you call this module, to avoid subtle bugs around masking. If you already have a PackedSequence you can pass None as the second parameter.","title":"PytorchSeq2VecWrapper"},{"location":"api/modules/seq2vec_encoders/pytorch_seq2vec_wrapper/#get_input_dim","text":"class PytorchSeq2VecWrapper ( Seq2VecEncoder ): | ... | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"api/modules/seq2vec_encoders/pytorch_seq2vec_wrapper/#get_output_dim","text":"class PytorchSeq2VecWrapper ( Seq2VecEncoder ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/seq2vec_encoders/pytorch_seq2vec_wrapper/#forward","text":"class PytorchSeq2VecWrapper ( Seq2VecEncoder ): | ... | def forward ( | self , | inputs : torch . Tensor , | mask : torch . BoolTensor , | hidden_state : torch . Tensor = None | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/seq2vec_encoders/pytorch_seq2vec_wrapper/#gruseq2vecencoder","text":"class GruSeq2VecEncoder ( PytorchSeq2VecWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int = 1 , | bias : bool = True , | dropout : float = 0.0 , | bidirectional : bool = False | ) Registered as a Seq2VecEncoder with name \"gru\".","title":"GruSeq2VecEncoder"},{"location":"api/modules/seq2vec_encoders/pytorch_seq2vec_wrapper/#lstmseq2vecencoder","text":"class LstmSeq2VecEncoder ( PytorchSeq2VecWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int = 1 , | bias : bool = True , | dropout : float = 0.0 , | bidirectional : bool = False | ) Registered as a Seq2VecEncoder with name \"lstm\".","title":"LstmSeq2VecEncoder"},{"location":"api/modules/seq2vec_encoders/pytorch_seq2vec_wrapper/#rnnseq2vecencoder","text":"class RnnSeq2VecEncoder ( PytorchSeq2VecWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int = 1 , | nonlinearity : str = \"tanh\" , | bias : bool = True , | dropout : float = 0.0 , | bidirectional : bool = False | ) Registered as a Seq2VecEncoder with name \"rnn\".","title":"RnnSeq2VecEncoder"},{"location":"api/modules/seq2vec_encoders/pytorch_seq2vec_wrapper/#augmentedlstmseq2vecencoder","text":"class AugmentedLstmSeq2VecEncoder ( PytorchSeq2VecWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | go_forward : bool = True , | recurrent_dropout_probability : float = 0.0 , | use_highway : bool = True , | use_input_projection_bias : bool = True | ) -> None Registered as a Seq2VecEncoder with name \"augmented_lstm\".","title":"AugmentedLstmSeq2VecEncoder"},{"location":"api/modules/seq2vec_encoders/pytorch_seq2vec_wrapper/#stackedalternatinglstmseq2vecencoder","text":"class StackedAlternatingLstmSeq2VecEncoder ( PytorchSeq2VecWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int , | recurrent_dropout_probability : float = 0.0 , | use_highway : bool = True , | use_input_projection_bias : bool = True | ) -> None Registered as a Seq2VecEncoder with name \"alternating_lstm\".","title":"StackedAlternatingLstmSeq2VecEncoder"},{"location":"api/modules/seq2vec_encoders/pytorch_seq2vec_wrapper/#stackedbidirectionallstmseq2vecencoder","text":"class StackedBidirectionalLstmSeq2VecEncoder ( PytorchSeq2VecWrapper ): | def __init__ ( | self , | input_size : int , | hidden_size : int , | num_layers : int , | recurrent_dropout_probability : float = 0.0 , | layer_dropout_probability : float = 0.0 , | use_highway : bool = True | ) -> None Registered as a Seq2VecEncoder with name \"stacked_bidirectional_lstm\".","title":"StackedBidirectionalLstmSeq2VecEncoder"},{"location":"api/modules/seq2vec_encoders/seq2vec_encoder/","text":"[ allennlp .modules .seq2vec_encoders .seq2vec_encoder ] Seq2VecEncoder # class Seq2VecEncoder ( _EncoderBase , Registrable ) A Seq2VecEncoder is a Module that takes as input a sequence of vectors and returns a single vector. Input shape : (batch_size, sequence_length, input_dim) ; output shape: (batch_size, output_dim) . We add two methods to the basic Module API: get_input_dim() and get_output_dim() . You might need this if you want to construct a Linear layer using the output of this encoder, or to raise sensible errors for mis-matching input dimensions. get_input_dim # class Seq2VecEncoder ( _EncoderBase , Registrable ): | ... | def get_input_dim ( self ) -> int Returns the dimension of the vector input for each element in the sequence input to a Seq2VecEncoder . This is not the shape of the input tensor, but the last element of that shape. get_output_dim # class Seq2VecEncoder ( _EncoderBase , Registrable ): | ... | def get_output_dim ( self ) -> int Returns the dimension of the final vector output by this Seq2VecEncoder . This is not the shape of the returned tensor, but the last element of that shape.","title":"seq2vec_encoder"},{"location":"api/modules/seq2vec_encoders/seq2vec_encoder/#seq2vecencoder","text":"class Seq2VecEncoder ( _EncoderBase , Registrable ) A Seq2VecEncoder is a Module that takes as input a sequence of vectors and returns a single vector. Input shape : (batch_size, sequence_length, input_dim) ; output shape: (batch_size, output_dim) . We add two methods to the basic Module API: get_input_dim() and get_output_dim() . You might need this if you want to construct a Linear layer using the output of this encoder, or to raise sensible errors for mis-matching input dimensions.","title":"Seq2VecEncoder"},{"location":"api/modules/seq2vec_encoders/seq2vec_encoder/#get_input_dim","text":"class Seq2VecEncoder ( _EncoderBase , Registrable ): | ... | def get_input_dim ( self ) -> int Returns the dimension of the vector input for each element in the sequence input to a Seq2VecEncoder . This is not the shape of the input tensor, but the last element of that shape.","title":"get_input_dim"},{"location":"api/modules/seq2vec_encoders/seq2vec_encoder/#get_output_dim","text":"class Seq2VecEncoder ( _EncoderBase , Registrable ): | ... | def get_output_dim ( self ) -> int Returns the dimension of the final vector output by this Seq2VecEncoder . This is not the shape of the returned tensor, but the last element of that shape.","title":"get_output_dim"},{"location":"api/modules/span_extractors/bidirectional_endpoint_span_extractor/","text":"[ allennlp .modules .span_extractors .bidirectional_endpoint_span_extractor ] BidirectionalEndpointSpanExtractor # class BidirectionalEndpointSpanExtractor ( SpanExtractor ): | def __init__ ( | self , | input_dim : int , | forward_combination : str = \"y-x\" , | backward_combination : str = \"x-y\" , | num_width_embeddings : int = None , | span_width_embedding_dim : int = None , | bucket_widths : bool = False , | use_sentinels : bool = True | ) -> None Represents spans from a bidirectional encoder as a concatenation of two different representations of the span endpoints, one for the forward direction of the encoder and one from the backward direction. This type of representation encodes some subtlety, because when you consider the forward and backward directions separately, the end index of the span for the backward direction's representation is actually the start index. By default, this SpanExtractor represents spans as sequence_tensor[inclusive_span_end] - sequence_tensor[exclusive_span_start] meaning that the representation is the difference between the the last word in the span and the word before the span started. Note that the start and end indices are with respect to the direction that the RNN is going in, so for the backward direction, the start/end indices are reversed. Additionally, the width of the spans can be embedded and concatenated on to the final combination. The following other types of representation are supported for both the forward and backward directions, assuming that x = span_start_embeddings and y = span_end_embeddings . x , y , x*y , x+y , x-y , x/y , where each of those binary operations is performed elementwise. You can list as many combinations as you want, comma separated. For example, you might give x,y,x*y as the combination parameter to this class. The computed similarity function would then be [x; y; x*y] , which can then be optionally concatenated with an embedded representation of the width of the span. Registered as a SpanExtractor with name \"bidirectional_endpoint\". Parameters input_dim : int The final dimension of the sequence_tensor . forward_combination : str , optional (default = \"y-x\" ) The method used to combine the forward_start_embeddings and forward_end_embeddings for the forward direction of the bidirectional representation. See above for a full description. backward_combination : str , optional (default = \"x-y\" ) The method used to combine the backward_start_embeddings and backward_end_embeddings for the backward direction of the bidirectional representation. See above for a full description. num_width_embeddings : int , optional (default = None ) Specifies the number of buckets to use when representing span width features. span_width_embedding_dim : int , optional (default = None ) The embedding size for the span_width features. bucket_widths : bool , optional (default = False ) Whether to bucket the span widths into log-space buckets. If False , the raw span widths are used. use_sentinels : bool , optional (default = True ) If True , sentinels are used to represent exclusive span indices for the elements in the first and last positions in the sequence (as the exclusive indices for these elements are outside of the the sequence boundary). This is not strictly necessary, as you may know that your exclusive start and end indices are always within your sequence representation, such as if you have appended/prepended and tokens to your sequence. get_input_dim # class BidirectionalEndpointSpanExtractor ( SpanExtractor ): | ... | def get_input_dim ( self ) -> int get_output_dim # class BidirectionalEndpointSpanExtractor ( SpanExtractor ): | ... | def get_output_dim ( self ) -> int forward # class BidirectionalEndpointSpanExtractor ( SpanExtractor ): | ... | @overrides | def forward ( | self , | sequence_tensor : torch . FloatTensor , | span_indices : torch . LongTensor , | sequence_mask : torch . BoolTensor = None , | span_indices_mask : torch . BoolTensor = None | ) -> torch . FloatTensor Both of shape (batch_size, sequence_length, embedding_size / 2)","title":"bidirectional_endpoint_span_extractor"},{"location":"api/modules/span_extractors/bidirectional_endpoint_span_extractor/#bidirectionalendpointspanextractor","text":"class BidirectionalEndpointSpanExtractor ( SpanExtractor ): | def __init__ ( | self , | input_dim : int , | forward_combination : str = \"y-x\" , | backward_combination : str = \"x-y\" , | num_width_embeddings : int = None , | span_width_embedding_dim : int = None , | bucket_widths : bool = False , | use_sentinels : bool = True | ) -> None Represents spans from a bidirectional encoder as a concatenation of two different representations of the span endpoints, one for the forward direction of the encoder and one from the backward direction. This type of representation encodes some subtlety, because when you consider the forward and backward directions separately, the end index of the span for the backward direction's representation is actually the start index. By default, this SpanExtractor represents spans as sequence_tensor[inclusive_span_end] - sequence_tensor[exclusive_span_start] meaning that the representation is the difference between the the last word in the span and the word before the span started. Note that the start and end indices are with respect to the direction that the RNN is going in, so for the backward direction, the start/end indices are reversed. Additionally, the width of the spans can be embedded and concatenated on to the final combination. The following other types of representation are supported for both the forward and backward directions, assuming that x = span_start_embeddings and y = span_end_embeddings . x , y , x*y , x+y , x-y , x/y , where each of those binary operations is performed elementwise. You can list as many combinations as you want, comma separated. For example, you might give x,y,x*y as the combination parameter to this class. The computed similarity function would then be [x; y; x*y] , which can then be optionally concatenated with an embedded representation of the width of the span. Registered as a SpanExtractor with name \"bidirectional_endpoint\". Parameters input_dim : int The final dimension of the sequence_tensor . forward_combination : str , optional (default = \"y-x\" ) The method used to combine the forward_start_embeddings and forward_end_embeddings for the forward direction of the bidirectional representation. See above for a full description. backward_combination : str , optional (default = \"x-y\" ) The method used to combine the backward_start_embeddings and backward_end_embeddings for the backward direction of the bidirectional representation. See above for a full description. num_width_embeddings : int , optional (default = None ) Specifies the number of buckets to use when representing span width features. span_width_embedding_dim : int , optional (default = None ) The embedding size for the span_width features. bucket_widths : bool , optional (default = False ) Whether to bucket the span widths into log-space buckets. If False , the raw span widths are used. use_sentinels : bool , optional (default = True ) If True , sentinels are used to represent exclusive span indices for the elements in the first and last positions in the sequence (as the exclusive indices for these elements are outside of the the sequence boundary). This is not strictly necessary, as you may know that your exclusive start and end indices are always within your sequence representation, such as if you have appended/prepended and tokens to your sequence.","title":"BidirectionalEndpointSpanExtractor"},{"location":"api/modules/span_extractors/bidirectional_endpoint_span_extractor/#get_input_dim","text":"class BidirectionalEndpointSpanExtractor ( SpanExtractor ): | ... | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"api/modules/span_extractors/bidirectional_endpoint_span_extractor/#get_output_dim","text":"class BidirectionalEndpointSpanExtractor ( SpanExtractor ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/span_extractors/bidirectional_endpoint_span_extractor/#forward","text":"class BidirectionalEndpointSpanExtractor ( SpanExtractor ): | ... | @overrides | def forward ( | self , | sequence_tensor : torch . FloatTensor , | span_indices : torch . LongTensor , | sequence_mask : torch . BoolTensor = None , | span_indices_mask : torch . BoolTensor = None | ) -> torch . FloatTensor Both of shape (batch_size, sequence_length, embedding_size / 2)","title":"forward"},{"location":"api/modules/span_extractors/endpoint_span_extractor/","text":"[ allennlp .modules .span_extractors .endpoint_span_extractor ] EndpointSpanExtractor # class EndpointSpanExtractor ( SpanExtractor ): | def __init__ ( | self , | input_dim : int , | combination : str = \"x,y\" , | num_width_embeddings : int = None , | span_width_embedding_dim : int = None , | bucket_widths : bool = False , | use_exclusive_start_indices : bool = False | ) -> None Represents spans as a combination of the embeddings of their endpoints. Additionally, the width of the spans can be embedded and concatenated on to the final combination. The following types of representation are supported, assuming that x = span_start_embeddings and y = span_end_embeddings . x , y , x*y , x+y , x-y , x/y , where each of those binary operations is performed elementwise. You can list as many combinations as you want, comma separated. For example, you might give x,y,x*y as the combination parameter to this class. The computed similarity function would then be [x; y; x*y] , which can then be optionally concatenated with an embedded representation of the width of the span. Registered as a SpanExtractor with name \"endpoint\". Parameters input_dim : int The final dimension of the sequence_tensor . combination : str , optional (default = \"x,y\" ) The method used to combine the start_embedding and end_embedding representations. See above for a full description. num_width_embeddings : int , optional (default = None ) Specifies the number of buckets to use when representing span width features. span_width_embedding_dim : int , optional (default = None ) The embedding size for the span_width features. bucket_widths : bool , optional (default = False ) Whether to bucket the span widths into log-space buckets. If False , the raw span widths are used. use_exclusive_start_indices : bool , optional (default = False ) If True , the start indices extracted are converted to exclusive indices. Sentinels are used to represent exclusive span indices for the elements in the first position in the sequence (as the exclusive indices for these elements are outside of the the sequence boundary) so that start indices can be exclusive. NOTE: This option can be helpful to avoid the pathological case in which you want span differences for length 1 spans - if you use inclusive indices, you will end up with an x - x operation for length 1 spans, which is not good. get_input_dim # class EndpointSpanExtractor ( SpanExtractor ): | ... | def get_input_dim ( self ) -> int get_output_dim # class EndpointSpanExtractor ( SpanExtractor ): | ... | def get_output_dim ( self ) -> int forward # class EndpointSpanExtractor ( SpanExtractor ): | ... | @overrides | def forward ( | self , | sequence_tensor : torch . FloatTensor , | span_indices : torch . LongTensor , | sequence_mask : torch . BoolTensor = None , | span_indices_mask : torch . BoolTensor = None | ) -> None shape (batch_size, num_spans)","title":"endpoint_span_extractor"},{"location":"api/modules/span_extractors/endpoint_span_extractor/#endpointspanextractor","text":"class EndpointSpanExtractor ( SpanExtractor ): | def __init__ ( | self , | input_dim : int , | combination : str = \"x,y\" , | num_width_embeddings : int = None , | span_width_embedding_dim : int = None , | bucket_widths : bool = False , | use_exclusive_start_indices : bool = False | ) -> None Represents spans as a combination of the embeddings of their endpoints. Additionally, the width of the spans can be embedded and concatenated on to the final combination. The following types of representation are supported, assuming that x = span_start_embeddings and y = span_end_embeddings . x , y , x*y , x+y , x-y , x/y , where each of those binary operations is performed elementwise. You can list as many combinations as you want, comma separated. For example, you might give x,y,x*y as the combination parameter to this class. The computed similarity function would then be [x; y; x*y] , which can then be optionally concatenated with an embedded representation of the width of the span. Registered as a SpanExtractor with name \"endpoint\". Parameters input_dim : int The final dimension of the sequence_tensor . combination : str , optional (default = \"x,y\" ) The method used to combine the start_embedding and end_embedding representations. See above for a full description. num_width_embeddings : int , optional (default = None ) Specifies the number of buckets to use when representing span width features. span_width_embedding_dim : int , optional (default = None ) The embedding size for the span_width features. bucket_widths : bool , optional (default = False ) Whether to bucket the span widths into log-space buckets. If False , the raw span widths are used. use_exclusive_start_indices : bool , optional (default = False ) If True , the start indices extracted are converted to exclusive indices. Sentinels are used to represent exclusive span indices for the elements in the first position in the sequence (as the exclusive indices for these elements are outside of the the sequence boundary) so that start indices can be exclusive. NOTE: This option can be helpful to avoid the pathological case in which you want span differences for length 1 spans - if you use inclusive indices, you will end up with an x - x operation for length 1 spans, which is not good.","title":"EndpointSpanExtractor"},{"location":"api/modules/span_extractors/endpoint_span_extractor/#get_input_dim","text":"class EndpointSpanExtractor ( SpanExtractor ): | ... | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"api/modules/span_extractors/endpoint_span_extractor/#get_output_dim","text":"class EndpointSpanExtractor ( SpanExtractor ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/span_extractors/endpoint_span_extractor/#forward","text":"class EndpointSpanExtractor ( SpanExtractor ): | ... | @overrides | def forward ( | self , | sequence_tensor : torch . FloatTensor , | span_indices : torch . LongTensor , | sequence_mask : torch . BoolTensor = None , | span_indices_mask : torch . BoolTensor = None | ) -> None shape (batch_size, num_spans)","title":"forward"},{"location":"api/modules/span_extractors/self_attentive_span_extractor/","text":"[ allennlp .modules .span_extractors .self_attentive_span_extractor ] SelfAttentiveSpanExtractor # class SelfAttentiveSpanExtractor ( SpanExtractor ): | def __init__ ( self , input_dim : int ) -> None Computes span representations by generating an unnormalized attention score for each word in the document. Spans representations are computed with respect to these scores by normalising the attention scores for words inside the span. Given these attention distributions over every span, this module weights the corresponding vector representations of the words in the span by this distribution, returning a weighted representation of each span. Registered as a SpanExtractor with name \"self_attentive\". Parameters input_dim : int The final dimension of the sequence_tensor . Returns attended_text_embeddings : torch.FloatTensor . A tensor of shape (batch_size, num_spans, input_dim), which each span representation is formed by locally normalising a global attention over the sequence. The only way in which the attention distribution differs over different spans is in the set of words over which they are normalized. get_input_dim # class SelfAttentiveSpanExtractor ( SpanExtractor ): | ... | def get_input_dim ( self ) -> int get_output_dim # class SelfAttentiveSpanExtractor ( SpanExtractor ): | ... | def get_output_dim ( self ) -> int forward # class SelfAttentiveSpanExtractor ( SpanExtractor ): | ... | @overrides | def forward ( | self , | sequence_tensor : torch . FloatTensor , | span_indices : torch . LongTensor , | span_indices_mask : torch . BoolTensor = None | ) -> torch . FloatTensor shape (batch_size, sequence_length, 1)","title":"self_attentive_span_extractor"},{"location":"api/modules/span_extractors/self_attentive_span_extractor/#selfattentivespanextractor","text":"class SelfAttentiveSpanExtractor ( SpanExtractor ): | def __init__ ( self , input_dim : int ) -> None Computes span representations by generating an unnormalized attention score for each word in the document. Spans representations are computed with respect to these scores by normalising the attention scores for words inside the span. Given these attention distributions over every span, this module weights the corresponding vector representations of the words in the span by this distribution, returning a weighted representation of each span. Registered as a SpanExtractor with name \"self_attentive\". Parameters input_dim : int The final dimension of the sequence_tensor . Returns attended_text_embeddings : torch.FloatTensor . A tensor of shape (batch_size, num_spans, input_dim), which each span representation is formed by locally normalising a global attention over the sequence. The only way in which the attention distribution differs over different spans is in the set of words over which they are normalized.","title":"SelfAttentiveSpanExtractor"},{"location":"api/modules/span_extractors/self_attentive_span_extractor/#get_input_dim","text":"class SelfAttentiveSpanExtractor ( SpanExtractor ): | ... | def get_input_dim ( self ) -> int","title":"get_input_dim"},{"location":"api/modules/span_extractors/self_attentive_span_extractor/#get_output_dim","text":"class SelfAttentiveSpanExtractor ( SpanExtractor ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/span_extractors/self_attentive_span_extractor/#forward","text":"class SelfAttentiveSpanExtractor ( SpanExtractor ): | ... | @overrides | def forward ( | self , | sequence_tensor : torch . FloatTensor , | span_indices : torch . LongTensor , | span_indices_mask : torch . BoolTensor = None | ) -> torch . FloatTensor shape (batch_size, sequence_length, 1)","title":"forward"},{"location":"api/modules/span_extractors/span_extractor/","text":"[ allennlp .modules .span_extractors .span_extractor ] SpanExtractor # class SpanExtractor ( torch . nn . Module , Registrable ) Many NLP models deal with representations of spans inside a sentence. SpanExtractors define methods for extracting and representing spans from a sentence. SpanExtractors take a sequence tensor of shape (batch_size, timesteps, embedding_dim) and indices of shape (batch_size, num_spans, 2) and return a tensor of shape (batch_size, num_spans, ...), forming some representation of the spans. forward # class SpanExtractor ( torch . nn . Module , Registrable ): | ... | @overrides | def forward ( | self , | sequence_tensor : torch . FloatTensor , | span_indices : torch . LongTensor , | sequence_mask : torch . BoolTensor = None , | span_indices_mask : torch . BoolTensor = None | ) Given a sequence tensor, extract spans and return representations of them. Span representation can be computed in many different ways, such as concatenation of the start and end spans, attention over the vectors contained inside the span, etc. Parameters sequence_tensor : torch.FloatTensor A tensor of shape (batch_size, sequence_length, embedding_size) representing an embedded sequence of words. span_indices : torch.LongTensor A tensor of shape (batch_size, num_spans, 2) , where the last dimension represents the inclusive start and end indices of the span to be extracted from the sequence_tensor . sequence_mask : torch.BoolTensor , optional (default = None ) A tensor of shape (batch_size, sequence_length) representing padded elements of the sequence. span_indices_mask : torch.BoolTensor , optional (default = None ) A tensor of shape (batch_size, num_spans) representing the valid spans in the indices tensor. This mask is optional because sometimes it's easier to worry about masking after calling this function, rather than passing a mask directly. Returns A tensor of shape (batch_size, num_spans, embedded_span_size) , where embedded_span_size depends on the way spans are represented. get_input_dim # class SpanExtractor ( torch . nn . Module , Registrable ): | ... | def get_input_dim ( self ) -> int Returns the expected final dimension of the sequence_tensor . get_output_dim # class SpanExtractor ( torch . nn . Module , Registrable ): | ... | def get_output_dim ( self ) -> int Returns the expected final dimension of the returned span representation.","title":"span_extractor"},{"location":"api/modules/span_extractors/span_extractor/#spanextractor","text":"class SpanExtractor ( torch . nn . Module , Registrable ) Many NLP models deal with representations of spans inside a sentence. SpanExtractors define methods for extracting and representing spans from a sentence. SpanExtractors take a sequence tensor of shape (batch_size, timesteps, embedding_dim) and indices of shape (batch_size, num_spans, 2) and return a tensor of shape (batch_size, num_spans, ...), forming some representation of the spans.","title":"SpanExtractor"},{"location":"api/modules/span_extractors/span_extractor/#forward","text":"class SpanExtractor ( torch . nn . Module , Registrable ): | ... | @overrides | def forward ( | self , | sequence_tensor : torch . FloatTensor , | span_indices : torch . LongTensor , | sequence_mask : torch . BoolTensor = None , | span_indices_mask : torch . BoolTensor = None | ) Given a sequence tensor, extract spans and return representations of them. Span representation can be computed in many different ways, such as concatenation of the start and end spans, attention over the vectors contained inside the span, etc. Parameters sequence_tensor : torch.FloatTensor A tensor of shape (batch_size, sequence_length, embedding_size) representing an embedded sequence of words. span_indices : torch.LongTensor A tensor of shape (batch_size, num_spans, 2) , where the last dimension represents the inclusive start and end indices of the span to be extracted from the sequence_tensor . sequence_mask : torch.BoolTensor , optional (default = None ) A tensor of shape (batch_size, sequence_length) representing padded elements of the sequence. span_indices_mask : torch.BoolTensor , optional (default = None ) A tensor of shape (batch_size, num_spans) representing the valid spans in the indices tensor. This mask is optional because sometimes it's easier to worry about masking after calling this function, rather than passing a mask directly. Returns A tensor of shape (batch_size, num_spans, embedded_span_size) , where embedded_span_size depends on the way spans are represented.","title":"forward"},{"location":"api/modules/span_extractors/span_extractor/#get_input_dim","text":"class SpanExtractor ( torch . nn . Module , Registrable ): | ... | def get_input_dim ( self ) -> int Returns the expected final dimension of the sequence_tensor .","title":"get_input_dim"},{"location":"api/modules/span_extractors/span_extractor/#get_output_dim","text":"class SpanExtractor ( torch . nn . Module , Registrable ): | ... | def get_output_dim ( self ) -> int Returns the expected final dimension of the returned span representation.","title":"get_output_dim"},{"location":"api/modules/text_field_embedders/basic_text_field_embedder/","text":"[ allennlp .modules .text_field_embedders .basic_text_field_embedder ] BasicTextFieldEmbedder # class BasicTextFieldEmbedder ( TextFieldEmbedder ): | def __init__ ( self , token_embedders : Dict [ str , TokenEmbedder ]) -> None This is a TextFieldEmbedder that wraps a collection of TokenEmbedder objects. Each TokenEmbedder embeds or encodes the representation output from one allennlp.data.TokenIndexer . As the data produced by a allennlp.data.fields.TextField is a dictionary mapping names to these representations, we take TokenEmbedders with corresponding names. Each TokenEmbedders embeds its input, and the result is concatenated in an arbitrary (but consistent) order. Registered as a TextFieldEmbedder with name \"basic\", which is also the default. Parameters token_embedders : Dict[str, TokenEmbedder] A dictionary mapping token embedder names to implementations. These names should match the corresponding indexer used to generate the tensor passed to the TokenEmbedder. get_output_dim # class BasicTextFieldEmbedder ( TextFieldEmbedder ): | ... | @overrides | def get_output_dim ( self ) -> int forward # class BasicTextFieldEmbedder ( TextFieldEmbedder ): | ... | def forward ( | self , | text_field_input : TextFieldTensors , | num_wrapping_dims : int = 0 , | ** kwargs | ) -> torch . Tensor","title":"basic_text_field_embedder"},{"location":"api/modules/text_field_embedders/basic_text_field_embedder/#basictextfieldembedder","text":"class BasicTextFieldEmbedder ( TextFieldEmbedder ): | def __init__ ( self , token_embedders : Dict [ str , TokenEmbedder ]) -> None This is a TextFieldEmbedder that wraps a collection of TokenEmbedder objects. Each TokenEmbedder embeds or encodes the representation output from one allennlp.data.TokenIndexer . As the data produced by a allennlp.data.fields.TextField is a dictionary mapping names to these representations, we take TokenEmbedders with corresponding names. Each TokenEmbedders embeds its input, and the result is concatenated in an arbitrary (but consistent) order. Registered as a TextFieldEmbedder with name \"basic\", which is also the default. Parameters token_embedders : Dict[str, TokenEmbedder] A dictionary mapping token embedder names to implementations. These names should match the corresponding indexer used to generate the tensor passed to the TokenEmbedder.","title":"BasicTextFieldEmbedder"},{"location":"api/modules/text_field_embedders/basic_text_field_embedder/#get_output_dim","text":"class BasicTextFieldEmbedder ( TextFieldEmbedder ): | ... | @overrides | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/text_field_embedders/basic_text_field_embedder/#forward","text":"class BasicTextFieldEmbedder ( TextFieldEmbedder ): | ... | def forward ( | self , | text_field_input : TextFieldTensors , | num_wrapping_dims : int = 0 , | ** kwargs | ) -> torch . Tensor","title":"forward"},{"location":"api/modules/text_field_embedders/text_field_embedder/","text":"[ allennlp .modules .text_field_embedders .text_field_embedder ] TextFieldEmbedder # class TextFieldEmbedder ( torch . nn . Module , Registrable ) A TextFieldEmbedder is a Module that takes as input the DataArray produced by a TextField and returns as output an embedded representation of the tokens in that field. The DataArrays produced by TextFields are dictionaries with named representations, like \"words\" and \"characters\". When you create a TextField , you pass in a dictionary of TokenIndexer objects, telling the field how exactly the tokens in the field should be represented. This class changes the type signature of Module.forward , restricting TextFieldEmbedders to take inputs corresponding to a single TextField , which is a dictionary of tensors with the same names as were passed to the TextField . We also add a method to the basic Module API: get_output_dim() . You might need this if you want to construct a Linear layer using the output of this embedder, for instance. default_implementation # class TextFieldEmbedder ( torch . nn . Module , Registrable ): | ... | default_implementation = \"basic\" forward # class TextFieldEmbedder ( torch . nn . Module , Registrable ): | ... | def forward ( | self , | text_field_input : TextFieldTensors , | num_wrapping_dims : int = 0 , | ** kwargs | ) -> torch . Tensor Parameters text_field_input : TextFieldTensors A dictionary that was the output of a call to TextField.as_tensor . Each tensor in here is assumed to have a shape roughly similar to (batch_size, sequence_length) (perhaps with an extra trailing dimension for the characters in each token). num_wrapping_dims : int , optional (default = 0 ) If you have a ListField[TextField] that created the text_field_input , you'll end up with tensors of shape (batch_size, wrapping_dim1, wrapping_dim2, ..., sequence_length) . This parameter tells us how many wrapping dimensions there are, so that we can correctly TimeDistribute the embedding of each named representation. get_output_dim # class TextFieldEmbedder ( torch . nn . Module , Registrable ): | ... | def get_output_dim ( self ) -> int Returns the dimension of the vector representing each token in the output of this TextFieldEmbedder . This is not the shape of the returned tensor, but the last element of that shape.","title":"text_field_embedder"},{"location":"api/modules/text_field_embedders/text_field_embedder/#textfieldembedder","text":"class TextFieldEmbedder ( torch . nn . Module , Registrable ) A TextFieldEmbedder is a Module that takes as input the DataArray produced by a TextField and returns as output an embedded representation of the tokens in that field. The DataArrays produced by TextFields are dictionaries with named representations, like \"words\" and \"characters\". When you create a TextField , you pass in a dictionary of TokenIndexer objects, telling the field how exactly the tokens in the field should be represented. This class changes the type signature of Module.forward , restricting TextFieldEmbedders to take inputs corresponding to a single TextField , which is a dictionary of tensors with the same names as were passed to the TextField . We also add a method to the basic Module API: get_output_dim() . You might need this if you want to construct a Linear layer using the output of this embedder, for instance.","title":"TextFieldEmbedder"},{"location":"api/modules/text_field_embedders/text_field_embedder/#default_implementation","text":"class TextFieldEmbedder ( torch . nn . Module , Registrable ): | ... | default_implementation = \"basic\"","title":"default_implementation"},{"location":"api/modules/text_field_embedders/text_field_embedder/#forward","text":"class TextFieldEmbedder ( torch . nn . Module , Registrable ): | ... | def forward ( | self , | text_field_input : TextFieldTensors , | num_wrapping_dims : int = 0 , | ** kwargs | ) -> torch . Tensor Parameters text_field_input : TextFieldTensors A dictionary that was the output of a call to TextField.as_tensor . Each tensor in here is assumed to have a shape roughly similar to (batch_size, sequence_length) (perhaps with an extra trailing dimension for the characters in each token). num_wrapping_dims : int , optional (default = 0 ) If you have a ListField[TextField] that created the text_field_input , you'll end up with tensors of shape (batch_size, wrapping_dim1, wrapping_dim2, ..., sequence_length) . This parameter tells us how many wrapping dimensions there are, so that we can correctly TimeDistribute the embedding of each named representation.","title":"forward"},{"location":"api/modules/text_field_embedders/text_field_embedder/#get_output_dim","text":"class TextFieldEmbedder ( torch . nn . Module , Registrable ): | ... | def get_output_dim ( self ) -> int Returns the dimension of the vector representing each token in the output of this TextFieldEmbedder . This is not the shape of the returned tensor, but the last element of that shape.","title":"get_output_dim"},{"location":"api/modules/token_embedders/bag_of_word_counts_token_embedder/","text":"[ allennlp .modules .token_embedders .bag_of_word_counts_token_embedder ] BagOfWordCountsTokenEmbedder # class BagOfWordCountsTokenEmbedder ( TokenEmbedder ): | def __init__ ( | self , | vocab : Vocabulary , | vocab_namespace : str = \"tokens\" , | projection_dim : int = None , | ignore_oov : bool = False | ) -> None Represents a sequence of tokens as a bag of (discrete) word ids, as it was done in the pre-neural days. Each sequence gets a vector of length vocabulary size, where the i'th entry in the vector corresponds to number of times the i'th token in the vocabulary appears in the sequence. By default, we ignore padding tokens. Registered as a TokenEmbedder with name \"bag_of_word_counts\". Parameters vocab : Vocabulary vocab_namespace : str , optional (default = \"tokens\" ) namespace of vocabulary to embed projection_dim : int , optional (default = None ) if specified, will project the resulting bag of words representation to specified dimension. ignore_oov : bool , optional (default = False ) If true, we ignore the OOV token. get_output_dim # class BagOfWordCountsTokenEmbedder ( TokenEmbedder ): | ... | def get_output_dim ( self ) forward # class BagOfWordCountsTokenEmbedder ( TokenEmbedder ): | ... | def forward ( self , inputs : torch . Tensor ) -> torch . Tensor Parameters inputs : torch.Tensor Shape (batch_size, timesteps, sequence_length) of word ids representing the current batch. Returns torch.Tensor The bag-of-words representations for the input sequence, shape (batch_size, vocab_size)","title":"bag_of_word_counts_token_embedder"},{"location":"api/modules/token_embedders/bag_of_word_counts_token_embedder/#bagofwordcountstokenembedder","text":"class BagOfWordCountsTokenEmbedder ( TokenEmbedder ): | def __init__ ( | self , | vocab : Vocabulary , | vocab_namespace : str = \"tokens\" , | projection_dim : int = None , | ignore_oov : bool = False | ) -> None Represents a sequence of tokens as a bag of (discrete) word ids, as it was done in the pre-neural days. Each sequence gets a vector of length vocabulary size, where the i'th entry in the vector corresponds to number of times the i'th token in the vocabulary appears in the sequence. By default, we ignore padding tokens. Registered as a TokenEmbedder with name \"bag_of_word_counts\". Parameters vocab : Vocabulary vocab_namespace : str , optional (default = \"tokens\" ) namespace of vocabulary to embed projection_dim : int , optional (default = None ) if specified, will project the resulting bag of words representation to specified dimension. ignore_oov : bool , optional (default = False ) If true, we ignore the OOV token.","title":"BagOfWordCountsTokenEmbedder"},{"location":"api/modules/token_embedders/bag_of_word_counts_token_embedder/#get_output_dim","text":"class BagOfWordCountsTokenEmbedder ( TokenEmbedder ): | ... | def get_output_dim ( self )","title":"get_output_dim"},{"location":"api/modules/token_embedders/bag_of_word_counts_token_embedder/#forward","text":"class BagOfWordCountsTokenEmbedder ( TokenEmbedder ): | ... | def forward ( self , inputs : torch . Tensor ) -> torch . Tensor Parameters inputs : torch.Tensor Shape (batch_size, timesteps, sequence_length) of word ids representing the current batch. Returns torch.Tensor The bag-of-words representations for the input sequence, shape (batch_size, vocab_size)","title":"forward"},{"location":"api/modules/token_embedders/elmo_token_embedder/","text":"[ allennlp .modules .token_embedders .elmo_token_embedder ] ElmoTokenEmbedder # class ElmoTokenEmbedder ( TokenEmbedder ): | def __init__ ( | self , | options_file : str = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/\" | + \"elmo_2x4096_512_2048cnn_2xhighway_options.json\" , | weight_file : str = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/\" | + \"elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\" , | do_layer_norm : bool = False , | dropout : float = 0.5 , | requires_grad : bool = False , | projection_dim : int = None , | vocab_to_cache : List [ str ] = None , | scalar_mix_parameters : List [ float ] = None | ) -> None Compute a single layer of ELMo representations. This class serves as a convenience when you only want to use one layer of ELMo representations at the input of your network. It's essentially a wrapper around Elmo(num_output_representations=1, ...) Registered as a TokenEmbedder with name \"elmo_token_embedder\". Parameters options_file : str An ELMo JSON options file. weight_file : str An ELMo hdf5 weight file. do_layer_norm : bool , optional Should we apply layer normalization (passed to ScalarMix )? dropout : float , optional (default = 0.5 ) The dropout value to be applied to the ELMo representations. requires_grad : bool , optional If True, compute gradient of ELMo parameters for fine tuning. projection_dim : int , optional If given, we will project the ELMo embedding down to this dimension. We recommend that you try using ELMo with a lot of dropout and no projection first, but we have found a few cases where projection helps (particularly where there is very limited training data). vocab_to_cache : List[str] , optional A list of words to pre-compute and cache character convolutions for. If you use this option, the ElmoTokenEmbedder expects that you pass word indices of shape (batch_size, timesteps) to forward, instead of character indices. If you use this option and pass a word which wasn't pre-cached, this will break. scalar_mix_parameters : List[int] , optional (default = None ) If not None , use these scalar mix parameters to weight the representations produced by different layers. These mixing weights are not updated during training. The mixing weights here should be the unnormalized (i.e., pre-softmax) weights. So, if you wanted to use only the 1st layer of a 2-layer ELMo, you can set this to [-9e10, 1, -9e10 ]. get_output_dim # class ElmoTokenEmbedder ( TokenEmbedder ): | ... | def get_output_dim ( self ) -> int forward # class ElmoTokenEmbedder ( TokenEmbedder ): | ... | def forward ( | self , | elmo_tokens : torch . Tensor , | word_inputs : torch . Tensor = None | ) -> torch . Tensor Parameters elmo_tokens : torch.Tensor Shape (batch_size, timesteps, 50) of character ids representing the current batch. word_inputs : torch.Tensor , optional If you passed a cached vocab, you can in addition pass a tensor of shape (batch_size, timesteps) , which represent word ids which have been pre-cached. Returns torch.Tensor The ELMo representations for the input sequence, shape (batch_size, timesteps, embedding_dim)","title":"elmo_token_embedder"},{"location":"api/modules/token_embedders/elmo_token_embedder/#elmotokenembedder","text":"class ElmoTokenEmbedder ( TokenEmbedder ): | def __init__ ( | self , | options_file : str = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/\" | + \"elmo_2x4096_512_2048cnn_2xhighway_options.json\" , | weight_file : str = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/\" | + \"elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\" , | do_layer_norm : bool = False , | dropout : float = 0.5 , | requires_grad : bool = False , | projection_dim : int = None , | vocab_to_cache : List [ str ] = None , | scalar_mix_parameters : List [ float ] = None | ) -> None Compute a single layer of ELMo representations. This class serves as a convenience when you only want to use one layer of ELMo representations at the input of your network. It's essentially a wrapper around Elmo(num_output_representations=1, ...) Registered as a TokenEmbedder with name \"elmo_token_embedder\". Parameters options_file : str An ELMo JSON options file. weight_file : str An ELMo hdf5 weight file. do_layer_norm : bool , optional Should we apply layer normalization (passed to ScalarMix )? dropout : float , optional (default = 0.5 ) The dropout value to be applied to the ELMo representations. requires_grad : bool , optional If True, compute gradient of ELMo parameters for fine tuning. projection_dim : int , optional If given, we will project the ELMo embedding down to this dimension. We recommend that you try using ELMo with a lot of dropout and no projection first, but we have found a few cases where projection helps (particularly where there is very limited training data). vocab_to_cache : List[str] , optional A list of words to pre-compute and cache character convolutions for. If you use this option, the ElmoTokenEmbedder expects that you pass word indices of shape (batch_size, timesteps) to forward, instead of character indices. If you use this option and pass a word which wasn't pre-cached, this will break. scalar_mix_parameters : List[int] , optional (default = None ) If not None , use these scalar mix parameters to weight the representations produced by different layers. These mixing weights are not updated during training. The mixing weights here should be the unnormalized (i.e., pre-softmax) weights. So, if you wanted to use only the 1st layer of a 2-layer ELMo, you can set this to [-9e10, 1, -9e10 ].","title":"ElmoTokenEmbedder"},{"location":"api/modules/token_embedders/elmo_token_embedder/#get_output_dim","text":"class ElmoTokenEmbedder ( TokenEmbedder ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/token_embedders/elmo_token_embedder/#forward","text":"class ElmoTokenEmbedder ( TokenEmbedder ): | ... | def forward ( | self , | elmo_tokens : torch . Tensor , | word_inputs : torch . Tensor = None | ) -> torch . Tensor Parameters elmo_tokens : torch.Tensor Shape (batch_size, timesteps, 50) of character ids representing the current batch. word_inputs : torch.Tensor , optional If you passed a cached vocab, you can in addition pass a tensor of shape (batch_size, timesteps) , which represent word ids which have been pre-cached. Returns torch.Tensor The ELMo representations for the input sequence, shape (batch_size, timesteps, embedding_dim)","title":"forward"},{"location":"api/modules/token_embedders/embedding/","text":"[ allennlp .modules .token_embedders .embedding ] Embedding # class Embedding ( TokenEmbedder ): | def __init__ ( | self , | embedding_dim : int , | num_embeddings : int = None , | projection_dim : int = None , | weight : torch . FloatTensor = None , | padding_index : int = None , | trainable : bool = True , | max_norm : float = None , | norm_type : float = 2.0 , | scale_grad_by_freq : bool = False , | sparse : bool = False , | vocab_namespace : str = \"tokens\" , | pretrained_file : str = None , | vocab : Vocabulary = None | ) -> None A more featureful embedding module than the default in Pytorch. Adds the ability to: 1. embed higher-order inputs 2. pre-specify the weight matrix 3. use a non-trainable embedding 4. project the resultant embeddings to some other dimension (which only makes sense with non-trainable embeddings). Note that if you are using our data API and are trying to embed a TextField , you should use a TextFieldEmbedder instead of using this directly. Registered as a TokenEmbedder with name \"embedding\". Parameters num_embeddings : int Size of the dictionary of embeddings (vocabulary size). embedding_dim : int The size of each embedding vector. projection_dim : int , optional (default = None ) If given, we add a projection layer after the embedding layer. This really only makes sense if trainable is False . weight : torch.FloatTensor , optional (default = None ) A pre-initialised weight matrix for the embedding lookup, allowing the use of pretrained vectors. padding_index : int , optional (default = None ) If given, pads the output with zeros whenever it encounters the index. trainable : bool , optional (default = True ) Whether or not to optimize the embedding parameters. max_norm : float , optional (default = None ) If given, will renormalize the embeddings to always have a norm lesser than this norm_type : float , optional (default = 2 ) The p of the p-norm to compute for the max_norm option scale_grad_by_freq : bool , optional (default = False ) If given, this will scale gradients by the frequency of the words in the mini-batch. sparse : bool , optional (default = False ) Whether or not the Pytorch backend should use a sparse representation of the embedding weight. vocab_namespace : str , optional (default = None ) In case of fine-tuning/transfer learning, the model's embedding matrix needs to be extended according to the size of extended-vocabulary. To be able to know how much to extend the embedding-matrix, it's necessary to know which vocab_namspace was used to construct it in the original training. We store vocab_namespace used during the original training as an attribute, so that it can be retrieved during fine-tuning. pretrained_file : str , optional (default = None ) Path to a file of word vectors to initialize the embedding matrix. It can be the path to a local file or a URL of a (cached) remote file. Two formats are supported: * hdf5 file - containing an embedding matrix in the form of a torch.Tensor; * text file - an utf-8 encoded text file with space separated fields. vocab : Vocabulary , optional (default = None ) Used to construct an embedding from a pretrained file. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"embedding\", it gets specified as a top-level parameter, then is passed in to this module separately. Returns An Embedding module. get_output_dim # class Embedding ( TokenEmbedder ): | ... | @overrides | def get_output_dim ( self ) -> int forward # class Embedding ( TokenEmbedder ): | ... | @overrides | def forward ( self , tokens : torch . Tensor ) -> torch . Tensor tokens may have extra dimensions (batch_size, d1, ..., dn, sequence_length), but embedding expects (batch_size, sequence_length), so pass tokens to util.combine_initial_dims (which is a no-op if there are no extra dimensions). Remember the original size. extend_vocab # class Embedding ( TokenEmbedder ): | ... | def extend_vocab ( | self , | extended_vocab : Vocabulary , | vocab_namespace : str = None , | extension_pretrained_file : str = None , | model_path : str = None | ) Extends the embedding matrix according to the extended vocabulary. If extension_pretrained_file is available, it will be used for initializing the new words embeddings in the extended vocabulary; otherwise we will check if _pretrained_file attribute is already available. If none is available, they will be initialized with xavier uniform. Parameters extended_vocab : Vocabulary Vocabulary extended from original vocabulary used to construct this Embedding . vocab_namespace : str , optional (default = None ) In case you know what vocab_namespace should be used for extension, you can pass it. If not passed, it will check if vocab_namespace used at the time of Embedding construction is available. If so, this namespace will be used or else extend_vocab will be a no-op. extension_pretrained_file : str , optional (default = None ) A file containing pretrained embeddings can be specified here. It can be the path to a local file or an URL of a (cached) remote file. Check format details in from_params of Embedding class. model_path : str , optional (default = None ) Path traversing the model attributes upto this embedding module. Eg. \"_text_field_embedder.token_embedder_tokens\". This is only useful to give a helpful error message when extend_vocab is implicitly called by train or any other command. format_embeddings_file_uri # def format_embeddings_file_uri ( main_file_path_or_url : str , path_inside_archive : Optional [ str ] = None ) -> str EmbeddingsFileURI # class EmbeddingsFileURI ( NamedTuple ) main_file_uri # class EmbeddingsFileURI ( NamedTuple ): | ... | main_file_uri : str = None path_inside_archive # class EmbeddingsFileURI ( NamedTuple ): | ... | path_inside_archive : Optional [ str ] = None parse_embeddings_file_uri # def parse_embeddings_file_uri ( uri : str ) -> \"EmbeddingsFileURI\" EmbeddingsTextFile # class EmbeddingsTextFile ( Iterator [ str ]): | def __init__ ( | self , | file_uri : str , | encoding : str = DEFAULT_ENCODING , | cache_dir : str = None | ) -> None Utility class for opening embeddings text files. Handles various compression formats, as well as context management. Parameters file_uri : str It can be: a file system path or a URL of an eventually compressed text file or a zip/tar archive containing a single file. URI of the type (archive_path_or_url)#file_path_inside_archive if the text file is contained in a multi-file archive. encoding : str cache_dir : str DEFAULT_ENCODING # class EmbeddingsTextFile ( Iterator [ str ]): | ... | DEFAULT_ENCODING = \"utf-8\" read # class EmbeddingsTextFile ( Iterator [ str ]): | ... | def read ( self ) -> str readline # class EmbeddingsTextFile ( Iterator [ str ]): | ... | def readline ( self ) -> str close # class EmbeddingsTextFile ( Iterator [ str ]): | ... | def close ( self ) -> None","title":"embedding"},{"location":"api/modules/token_embedders/embedding/#embedding","text":"class Embedding ( TokenEmbedder ): | def __init__ ( | self , | embedding_dim : int , | num_embeddings : int = None , | projection_dim : int = None , | weight : torch . FloatTensor = None , | padding_index : int = None , | trainable : bool = True , | max_norm : float = None , | norm_type : float = 2.0 , | scale_grad_by_freq : bool = False , | sparse : bool = False , | vocab_namespace : str = \"tokens\" , | pretrained_file : str = None , | vocab : Vocabulary = None | ) -> None A more featureful embedding module than the default in Pytorch. Adds the ability to: 1. embed higher-order inputs 2. pre-specify the weight matrix 3. use a non-trainable embedding 4. project the resultant embeddings to some other dimension (which only makes sense with non-trainable embeddings). Note that if you are using our data API and are trying to embed a TextField , you should use a TextFieldEmbedder instead of using this directly. Registered as a TokenEmbedder with name \"embedding\". Parameters num_embeddings : int Size of the dictionary of embeddings (vocabulary size). embedding_dim : int The size of each embedding vector. projection_dim : int , optional (default = None ) If given, we add a projection layer after the embedding layer. This really only makes sense if trainable is False . weight : torch.FloatTensor , optional (default = None ) A pre-initialised weight matrix for the embedding lookup, allowing the use of pretrained vectors. padding_index : int , optional (default = None ) If given, pads the output with zeros whenever it encounters the index. trainable : bool , optional (default = True ) Whether or not to optimize the embedding parameters. max_norm : float , optional (default = None ) If given, will renormalize the embeddings to always have a norm lesser than this norm_type : float , optional (default = 2 ) The p of the p-norm to compute for the max_norm option scale_grad_by_freq : bool , optional (default = False ) If given, this will scale gradients by the frequency of the words in the mini-batch. sparse : bool , optional (default = False ) Whether or not the Pytorch backend should use a sparse representation of the embedding weight. vocab_namespace : str , optional (default = None ) In case of fine-tuning/transfer learning, the model's embedding matrix needs to be extended according to the size of extended-vocabulary. To be able to know how much to extend the embedding-matrix, it's necessary to know which vocab_namspace was used to construct it in the original training. We store vocab_namespace used during the original training as an attribute, so that it can be retrieved during fine-tuning. pretrained_file : str , optional (default = None ) Path to a file of word vectors to initialize the embedding matrix. It can be the path to a local file or a URL of a (cached) remote file. Two formats are supported: * hdf5 file - containing an embedding matrix in the form of a torch.Tensor; * text file - an utf-8 encoded text file with space separated fields. vocab : Vocabulary , optional (default = None ) Used to construct an embedding from a pretrained file. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"embedding\", it gets specified as a top-level parameter, then is passed in to this module separately. Returns An Embedding module.","title":"Embedding"},{"location":"api/modules/token_embedders/embedding/#get_output_dim","text":"class Embedding ( TokenEmbedder ): | ... | @overrides | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/token_embedders/embedding/#forward","text":"class Embedding ( TokenEmbedder ): | ... | @overrides | def forward ( self , tokens : torch . Tensor ) -> torch . Tensor tokens may have extra dimensions (batch_size, d1, ..., dn, sequence_length), but embedding expects (batch_size, sequence_length), so pass tokens to util.combine_initial_dims (which is a no-op if there are no extra dimensions). Remember the original size.","title":"forward"},{"location":"api/modules/token_embedders/embedding/#extend_vocab","text":"class Embedding ( TokenEmbedder ): | ... | def extend_vocab ( | self , | extended_vocab : Vocabulary , | vocab_namespace : str = None , | extension_pretrained_file : str = None , | model_path : str = None | ) Extends the embedding matrix according to the extended vocabulary. If extension_pretrained_file is available, it will be used for initializing the new words embeddings in the extended vocabulary; otherwise we will check if _pretrained_file attribute is already available. If none is available, they will be initialized with xavier uniform. Parameters extended_vocab : Vocabulary Vocabulary extended from original vocabulary used to construct this Embedding . vocab_namespace : str , optional (default = None ) In case you know what vocab_namespace should be used for extension, you can pass it. If not passed, it will check if vocab_namespace used at the time of Embedding construction is available. If so, this namespace will be used or else extend_vocab will be a no-op. extension_pretrained_file : str , optional (default = None ) A file containing pretrained embeddings can be specified here. It can be the path to a local file or an URL of a (cached) remote file. Check format details in from_params of Embedding class. model_path : str , optional (default = None ) Path traversing the model attributes upto this embedding module. Eg. \"_text_field_embedder.token_embedder_tokens\". This is only useful to give a helpful error message when extend_vocab is implicitly called by train or any other command.","title":"extend_vocab"},{"location":"api/modules/token_embedders/embedding/#format_embeddings_file_uri","text":"def format_embeddings_file_uri ( main_file_path_or_url : str , path_inside_archive : Optional [ str ] = None ) -> str","title":"format_embeddings_file_uri"},{"location":"api/modules/token_embedders/embedding/#embeddingsfileuri","text":"class EmbeddingsFileURI ( NamedTuple )","title":"EmbeddingsFileURI"},{"location":"api/modules/token_embedders/embedding/#main_file_uri","text":"class EmbeddingsFileURI ( NamedTuple ): | ... | main_file_uri : str = None","title":"main_file_uri"},{"location":"api/modules/token_embedders/embedding/#path_inside_archive","text":"class EmbeddingsFileURI ( NamedTuple ): | ... | path_inside_archive : Optional [ str ] = None","title":"path_inside_archive"},{"location":"api/modules/token_embedders/embedding/#parse_embeddings_file_uri","text":"def parse_embeddings_file_uri ( uri : str ) -> \"EmbeddingsFileURI\"","title":"parse_embeddings_file_uri"},{"location":"api/modules/token_embedders/embedding/#embeddingstextfile","text":"class EmbeddingsTextFile ( Iterator [ str ]): | def __init__ ( | self , | file_uri : str , | encoding : str = DEFAULT_ENCODING , | cache_dir : str = None | ) -> None Utility class for opening embeddings text files. Handles various compression formats, as well as context management. Parameters file_uri : str It can be: a file system path or a URL of an eventually compressed text file or a zip/tar archive containing a single file. URI of the type (archive_path_or_url)#file_path_inside_archive if the text file is contained in a multi-file archive. encoding : str cache_dir : str","title":"EmbeddingsTextFile"},{"location":"api/modules/token_embedders/embedding/#default_encoding","text":"class EmbeddingsTextFile ( Iterator [ str ]): | ... | DEFAULT_ENCODING = \"utf-8\"","title":"DEFAULT_ENCODING"},{"location":"api/modules/token_embedders/embedding/#read","text":"class EmbeddingsTextFile ( Iterator [ str ]): | ... | def read ( self ) -> str","title":"read"},{"location":"api/modules/token_embedders/embedding/#readline","text":"class EmbeddingsTextFile ( Iterator [ str ]): | ... | def readline ( self ) -> str","title":"readline"},{"location":"api/modules/token_embedders/embedding/#close","text":"class EmbeddingsTextFile ( Iterator [ str ]): | ... | def close ( self ) -> None","title":"close"},{"location":"api/modules/token_embedders/empty_embedder/","text":"[ allennlp .modules .token_embedders .empty_embedder ] EmptyEmbedder # class EmptyEmbedder ( TokenEmbedder ): | def __init__ ( self ) -> None Assumes you want to completely ignore the output of a TokenIndexer for some reason, and does not return anything when asked to embed it. You should almost never need to use this; normally you would just not use a particular TokenIndexer . It's only in very rare cases, like simplicity in data processing for language modeling (where we use just one TextField to handle input embedding and computing target ids), where you might want to use this. Registered as a TokenEmbedder with name \"empty\". get_output_dim # class EmptyEmbedder ( TokenEmbedder ): | ... | def get_output_dim ( self ) forward # class EmptyEmbedder ( TokenEmbedder ): | ... | def forward ( self , * inputs , ** kwargs ) -> torch . Tensor","title":"empty_embedder"},{"location":"api/modules/token_embedders/empty_embedder/#emptyembedder","text":"class EmptyEmbedder ( TokenEmbedder ): | def __init__ ( self ) -> None Assumes you want to completely ignore the output of a TokenIndexer for some reason, and does not return anything when asked to embed it. You should almost never need to use this; normally you would just not use a particular TokenIndexer . It's only in very rare cases, like simplicity in data processing for language modeling (where we use just one TextField to handle input embedding and computing target ids), where you might want to use this. Registered as a TokenEmbedder with name \"empty\".","title":"EmptyEmbedder"},{"location":"api/modules/token_embedders/empty_embedder/#get_output_dim","text":"class EmptyEmbedder ( TokenEmbedder ): | ... | def get_output_dim ( self )","title":"get_output_dim"},{"location":"api/modules/token_embedders/empty_embedder/#forward","text":"class EmptyEmbedder ( TokenEmbedder ): | ... | def forward ( self , * inputs , ** kwargs ) -> torch . Tensor","title":"forward"},{"location":"api/modules/token_embedders/pass_through_token_embedder/","text":"[ allennlp .modules .token_embedders .pass_through_token_embedder ] PassThroughTokenEmbedder # class PassThroughTokenEmbedder ( TokenEmbedder ): | def __init__ ( self , hidden_dim : int ) -> None Assumes that the input is already vectorized in some way, and just returns it. Registered as a TokenEmbedder with name \"pass_through\". Parameters hidden_dim : int get_output_dim # class PassThroughTokenEmbedder ( TokenEmbedder ): | ... | def get_output_dim ( self ) forward # class PassThroughTokenEmbedder ( TokenEmbedder ): | ... | def forward ( self , tokens : torch . Tensor ) -> torch . Tensor","title":"pass_through_token_embedder"},{"location":"api/modules/token_embedders/pass_through_token_embedder/#passthroughtokenembedder","text":"class PassThroughTokenEmbedder ( TokenEmbedder ): | def __init__ ( self , hidden_dim : int ) -> None Assumes that the input is already vectorized in some way, and just returns it. Registered as a TokenEmbedder with name \"pass_through\". Parameters hidden_dim : int","title":"PassThroughTokenEmbedder"},{"location":"api/modules/token_embedders/pass_through_token_embedder/#get_output_dim","text":"class PassThroughTokenEmbedder ( TokenEmbedder ): | ... | def get_output_dim ( self )","title":"get_output_dim"},{"location":"api/modules/token_embedders/pass_through_token_embedder/#forward","text":"class PassThroughTokenEmbedder ( TokenEmbedder ): | ... | def forward ( self , tokens : torch . Tensor ) -> torch . Tensor","title":"forward"},{"location":"api/modules/token_embedders/pretrained_transformer_embedder/","text":"[ allennlp .modules .token_embedders .pretrained_transformer_embedder ] PretrainedTransformerEmbedder # class PretrainedTransformerEmbedder ( TokenEmbedder ): | def __init__ ( | self , | model_name : str , | * , | max_length : int = None , | sub_module : str = None , | train_parameters : bool = True , | last_layer_only : bool = True , | override_weights_file : Optional [ str ] = None , | override_weights_strip_prefix : Optional [ str ] = None , | gradient_checkpointing : Optional [ bool ] = None | ) -> None Uses a pretrained model from transformers as a TokenEmbedder . Registered as a TokenEmbedder with name \"pretrained_transformer\". Parameters model_name : str The name of the transformers model to use. Should be the same as the corresponding PretrainedTransformerIndexer . max_length : int , optional (default = None ) If positive, folds input token IDs into multiple segments of this length, pass them through the transformer model independently, and concatenate the final representations. Should be set to the same value as the max_length option on the PretrainedTransformerIndexer . sub_module : str , optional (default = None ) The name of a submodule of the transformer to be used as the embedder. Some transformers naturally act as embedders such as BERT. However, other models consist of encoder and decoder, in which case we just want to use the encoder. train_parameters : bool , optional (default = True ) If this is True , the transformer weights get updated during training. last_layer_only : bool , optional (default = True ) When True (the default), only the final layer of the pretrained transformer is taken for the embeddings. But if set to False , a scalar mix of all of the layers is used. gradient_checkpointing : bool , optional (default = None ) Enable or disable gradient checkpointing. get_output_dim # class PretrainedTransformerEmbedder ( TokenEmbedder ): | ... | @overrides | def get_output_dim ( self ) forward # class PretrainedTransformerEmbedder ( TokenEmbedder ): | ... | @overrides | def forward ( | self , | token_ids : torch . LongTensor , | mask : torch . BoolTensor , | type_ids : Optional [ torch . LongTensor ] = None , | segment_concat_mask : Optional [ torch . BoolTensor ] = None | ) -> torch . Tensor Parameters token_ids : torch.LongTensor Shape: [batch_size, num_wordpieces if max_length is None else num_segment_concat_wordpieces] . num_segment_concat_wordpieces is num_wordpieces plus special tokens inserted in the middle, e.g. the length of: \"[CLS] A B C [SEP] [CLS] D E F [SEP]\" (see indexer logic). mask : torch.BoolTensor Shape: [batch_size, num_wordpieces]. type_ids : Optional[torch.LongTensor] Shape: [batch_size, num_wordpieces if max_length is None else num_segment_concat_wordpieces] . segment_concat_mask : Optional[torch.BoolTensor] Shape: [batch_size, num_segment_concat_wordpieces] . Returns torch.Tensor Shape: [batch_size, num_wordpieces, embedding_size] .","title":"pretrained_transformer_embedder"},{"location":"api/modules/token_embedders/pretrained_transformer_embedder/#pretrainedtransformerembedder","text":"class PretrainedTransformerEmbedder ( TokenEmbedder ): | def __init__ ( | self , | model_name : str , | * , | max_length : int = None , | sub_module : str = None , | train_parameters : bool = True , | last_layer_only : bool = True , | override_weights_file : Optional [ str ] = None , | override_weights_strip_prefix : Optional [ str ] = None , | gradient_checkpointing : Optional [ bool ] = None | ) -> None Uses a pretrained model from transformers as a TokenEmbedder . Registered as a TokenEmbedder with name \"pretrained_transformer\". Parameters model_name : str The name of the transformers model to use. Should be the same as the corresponding PretrainedTransformerIndexer . max_length : int , optional (default = None ) If positive, folds input token IDs into multiple segments of this length, pass them through the transformer model independently, and concatenate the final representations. Should be set to the same value as the max_length option on the PretrainedTransformerIndexer . sub_module : str , optional (default = None ) The name of a submodule of the transformer to be used as the embedder. Some transformers naturally act as embedders such as BERT. However, other models consist of encoder and decoder, in which case we just want to use the encoder. train_parameters : bool , optional (default = True ) If this is True , the transformer weights get updated during training. last_layer_only : bool , optional (default = True ) When True (the default), only the final layer of the pretrained transformer is taken for the embeddings. But if set to False , a scalar mix of all of the layers is used. gradient_checkpointing : bool , optional (default = None ) Enable or disable gradient checkpointing.","title":"PretrainedTransformerEmbedder"},{"location":"api/modules/token_embedders/pretrained_transformer_embedder/#get_output_dim","text":"class PretrainedTransformerEmbedder ( TokenEmbedder ): | ... | @overrides | def get_output_dim ( self )","title":"get_output_dim"},{"location":"api/modules/token_embedders/pretrained_transformer_embedder/#forward","text":"class PretrainedTransformerEmbedder ( TokenEmbedder ): | ... | @overrides | def forward ( | self , | token_ids : torch . LongTensor , | mask : torch . BoolTensor , | type_ids : Optional [ torch . LongTensor ] = None , | segment_concat_mask : Optional [ torch . BoolTensor ] = None | ) -> torch . Tensor Parameters token_ids : torch.LongTensor Shape: [batch_size, num_wordpieces if max_length is None else num_segment_concat_wordpieces] . num_segment_concat_wordpieces is num_wordpieces plus special tokens inserted in the middle, e.g. the length of: \"[CLS] A B C [SEP] [CLS] D E F [SEP]\" (see indexer logic). mask : torch.BoolTensor Shape: [batch_size, num_wordpieces]. type_ids : Optional[torch.LongTensor] Shape: [batch_size, num_wordpieces if max_length is None else num_segment_concat_wordpieces] . segment_concat_mask : Optional[torch.BoolTensor] Shape: [batch_size, num_segment_concat_wordpieces] . Returns torch.Tensor Shape: [batch_size, num_wordpieces, embedding_size] .","title":"forward"},{"location":"api/modules/token_embedders/pretrained_transformer_mismatched_embedder/","text":"[ allennlp .modules .token_embedders .pretrained_transformer_mismatched_embedder ] PretrainedTransformerMismatchedEmbedder # class PretrainedTransformerMismatchedEmbedder ( TokenEmbedder ): | def __init__ ( | self , | model_name : str , | max_length : int = None , | train_parameters : bool = True , | last_layer_only : bool = True , | gradient_checkpointing : Optional [ bool ] = None | ) -> None Use this embedder to embed wordpieces given by PretrainedTransformerMismatchedIndexer and to pool the resulting vectors to get word-level representations. Registered as a TokenEmbedder with name \"pretrained_transformer_mismatchd\". Parameters model_name : str The name of the transformers model to use. Should be the same as the corresponding PretrainedTransformerMismatchedIndexer . max_length : int , optional (default = None ) If positive, folds input token IDs into multiple segments of this length, pass them through the transformer model independently, and concatenate the final representations. Should be set to the same value as the max_length option on the PretrainedTransformerMismatchedIndexer . train_parameters : bool , optional (default = True ) If this is True , the transformer weights get updated during training. last_layer_only : bool , optional (default = True ) When True (the default), only the final layer of the pretrained transformer is taken for the embeddings. But if set to False , a scalar mix of all of the layers is used. gradient_checkpointing : bool , optional (default = None ) Enable or disable gradient checkpointing. get_output_dim # class PretrainedTransformerMismatchedEmbedder ( TokenEmbedder ): | ... | @overrides | def get_output_dim ( self ) forward # class PretrainedTransformerMismatchedEmbedder ( TokenEmbedder ): | ... | @overrides | def forward ( | self , | token_ids : torch . LongTensor , | mask : torch . BoolTensor , | offsets : torch . LongTensor , | wordpiece_mask : torch . BoolTensor , | type_ids : Optional [ torch . LongTensor ] = None , | segment_concat_mask : Optional [ torch . BoolTensor ] = None | ) -> torch . Tensor Parameters token_ids : torch.LongTensor Shape: [batch_size, num_wordpieces] (for exception see PretrainedTransformerEmbedder ). mask : torch.BoolTensor Shape: [batch_size, num_orig_tokens]. offsets : torch.LongTensor Shape: [batch_size, num_orig_tokens, 2]. Maps indices for the original tokens, i.e. those given as input to the indexer, to a span in token_ids. token_ids[i][offsets[i][j][0]:offsets[i][j][1] + 1] corresponds to the original j-th token from the i-th batch. wordpiece_mask : torch.BoolTensor Shape: [batch_size, num_wordpieces]. type_ids : Optional[torch.LongTensor] Shape: [batch_size, num_wordpieces]. segment_concat_mask : Optional[torch.BoolTensor] See PretrainedTransformerEmbedder . Returns torch.Tensor Shape: [batch_size, num_orig_tokens, embedding_size].","title":"pretrained_transformer_mismatched_embedder"},{"location":"api/modules/token_embedders/pretrained_transformer_mismatched_embedder/#pretrainedtransformermismatchedembedder","text":"class PretrainedTransformerMismatchedEmbedder ( TokenEmbedder ): | def __init__ ( | self , | model_name : str , | max_length : int = None , | train_parameters : bool = True , | last_layer_only : bool = True , | gradient_checkpointing : Optional [ bool ] = None | ) -> None Use this embedder to embed wordpieces given by PretrainedTransformerMismatchedIndexer and to pool the resulting vectors to get word-level representations. Registered as a TokenEmbedder with name \"pretrained_transformer_mismatchd\". Parameters model_name : str The name of the transformers model to use. Should be the same as the corresponding PretrainedTransformerMismatchedIndexer . max_length : int , optional (default = None ) If positive, folds input token IDs into multiple segments of this length, pass them through the transformer model independently, and concatenate the final representations. Should be set to the same value as the max_length option on the PretrainedTransformerMismatchedIndexer . train_parameters : bool , optional (default = True ) If this is True , the transformer weights get updated during training. last_layer_only : bool , optional (default = True ) When True (the default), only the final layer of the pretrained transformer is taken for the embeddings. But if set to False , a scalar mix of all of the layers is used. gradient_checkpointing : bool , optional (default = None ) Enable or disable gradient checkpointing.","title":"PretrainedTransformerMismatchedEmbedder"},{"location":"api/modules/token_embedders/pretrained_transformer_mismatched_embedder/#get_output_dim","text":"class PretrainedTransformerMismatchedEmbedder ( TokenEmbedder ): | ... | @overrides | def get_output_dim ( self )","title":"get_output_dim"},{"location":"api/modules/token_embedders/pretrained_transformer_mismatched_embedder/#forward","text":"class PretrainedTransformerMismatchedEmbedder ( TokenEmbedder ): | ... | @overrides | def forward ( | self , | token_ids : torch . LongTensor , | mask : torch . BoolTensor , | offsets : torch . LongTensor , | wordpiece_mask : torch . BoolTensor , | type_ids : Optional [ torch . LongTensor ] = None , | segment_concat_mask : Optional [ torch . BoolTensor ] = None | ) -> torch . Tensor Parameters token_ids : torch.LongTensor Shape: [batch_size, num_wordpieces] (for exception see PretrainedTransformerEmbedder ). mask : torch.BoolTensor Shape: [batch_size, num_orig_tokens]. offsets : torch.LongTensor Shape: [batch_size, num_orig_tokens, 2]. Maps indices for the original tokens, i.e. those given as input to the indexer, to a span in token_ids. token_ids[i][offsets[i][j][0]:offsets[i][j][1] + 1] corresponds to the original j-th token from the i-th batch. wordpiece_mask : torch.BoolTensor Shape: [batch_size, num_wordpieces]. type_ids : Optional[torch.LongTensor] Shape: [batch_size, num_wordpieces]. segment_concat_mask : Optional[torch.BoolTensor] See PretrainedTransformerEmbedder . Returns torch.Tensor Shape: [batch_size, num_orig_tokens, embedding_size].","title":"forward"},{"location":"api/modules/token_embedders/token_characters_encoder/","text":"[ allennlp .modules .token_embedders .token_characters_encoder ] TokenCharactersEncoder # class TokenCharactersEncoder ( TokenEmbedder ): | def __init__ ( | self , | embedding : Embedding , | encoder : Seq2VecEncoder , | dropout : float = 0.0 | ) -> None A TokenCharactersEncoder takes the output of a TokenCharactersIndexer , which is a tensor of shape (batch_size, num_tokens, num_characters), embeds the characters, runs a token-level encoder, and returns the result, which is a tensor of shape (batch_size, num_tokens, encoding_dim). We also optionally apply dropout after the token-level encoder. We take the embedding and encoding modules as input, so this class is itself quite simple. Registered as a TokenEmbedder with name \"character_encoding\". get_output_dim # class TokenCharactersEncoder ( TokenEmbedder ): | ... | def get_output_dim ( self ) -> int forward # class TokenCharactersEncoder ( TokenEmbedder ): | ... | def forward ( self , token_characters : torch . Tensor ) -> torch . Tensor","title":"token_characters_encoder"},{"location":"api/modules/token_embedders/token_characters_encoder/#tokencharactersencoder","text":"class TokenCharactersEncoder ( TokenEmbedder ): | def __init__ ( | self , | embedding : Embedding , | encoder : Seq2VecEncoder , | dropout : float = 0.0 | ) -> None A TokenCharactersEncoder takes the output of a TokenCharactersIndexer , which is a tensor of shape (batch_size, num_tokens, num_characters), embeds the characters, runs a token-level encoder, and returns the result, which is a tensor of shape (batch_size, num_tokens, encoding_dim). We also optionally apply dropout after the token-level encoder. We take the embedding and encoding modules as input, so this class is itself quite simple. Registered as a TokenEmbedder with name \"character_encoding\".","title":"TokenCharactersEncoder"},{"location":"api/modules/token_embedders/token_characters_encoder/#get_output_dim","text":"class TokenCharactersEncoder ( TokenEmbedder ): | ... | def get_output_dim ( self ) -> int","title":"get_output_dim"},{"location":"api/modules/token_embedders/token_characters_encoder/#forward","text":"class TokenCharactersEncoder ( TokenEmbedder ): | ... | def forward ( self , token_characters : torch . Tensor ) -> torch . Tensor","title":"forward"},{"location":"api/modules/token_embedders/token_embedder/","text":"[ allennlp .modules .token_embedders .token_embedder ] TokenEmbedder # class TokenEmbedder ( torch . nn . Module , Registrable ) A TokenEmbedder is a Module that takes as input a tensor with integer ids that have been output from a TokenIndexer and outputs a vector per token in the input. The input typically has shape (batch_size, num_tokens) or (batch_size, num_tokens, num_characters) , and the output is of shape (batch_size, num_tokens, output_dim) . The simplest TokenEmbedder is just an embedding layer, but for character-level input, it could also be some kind of character encoder. We add a single method to the basic Module API: get_output_dim() . This lets us more easily compute output dimensions for the TextFieldEmbedder , which we might need when defining model parameters such as LSTMs or linear layers, which need to know their input dimension before the layers are called. default_implementation # class TokenEmbedder ( torch . nn . Module , Registrable ): | ... | default_implementation = \"embedding\" get_output_dim # class TokenEmbedder ( torch . nn . Module , Registrable ): | ... | def get_output_dim ( self ) -> int Returns the final output dimension that this TokenEmbedder uses to represent each token. This is not the shape of the returned tensor, but the last element of that shape.","title":"token_embedder"},{"location":"api/modules/token_embedders/token_embedder/#tokenembedder","text":"class TokenEmbedder ( torch . nn . Module , Registrable ) A TokenEmbedder is a Module that takes as input a tensor with integer ids that have been output from a TokenIndexer and outputs a vector per token in the input. The input typically has shape (batch_size, num_tokens) or (batch_size, num_tokens, num_characters) , and the output is of shape (batch_size, num_tokens, output_dim) . The simplest TokenEmbedder is just an embedding layer, but for character-level input, it could also be some kind of character encoder. We add a single method to the basic Module API: get_output_dim() . This lets us more easily compute output dimensions for the TextFieldEmbedder , which we might need when defining model parameters such as LSTMs or linear layers, which need to know their input dimension before the layers are called.","title":"TokenEmbedder"},{"location":"api/modules/token_embedders/token_embedder/#default_implementation","text":"class TokenEmbedder ( torch . nn . Module , Registrable ): | ... | default_implementation = \"embedding\"","title":"default_implementation"},{"location":"api/modules/token_embedders/token_embedder/#get_output_dim","text":"class TokenEmbedder ( torch . nn . Module , Registrable ): | ... | def get_output_dim ( self ) -> int Returns the final output dimension that this TokenEmbedder uses to represent each token. This is not the shape of the returned tensor, but the last element of that shape.","title":"get_output_dim"},{"location":"api/nn/activations/","text":"[ allennlp .nn .activations ] An Activation is just a function that takes some parameters and returns an element-wise activation function. For the most part we just use PyTorch activations . Here we provide a thin wrapper to allow registering them and instantiating them from_params . The available activation functions are \"linear\" \"mish\" \"swish\" \"relu\" \"relu6\" \"elu\" \"prelu\" \"leaky_relu\" \"threshold\" \"hardtanh\" \"sigmoid\" \"tanh\" \"log_sigmoid\" \"softplus\" \"softshrink\" \"softsign\" \"tanhshrink\" \"selu\" Activation # class Activation ( torch . nn . Module , Registrable ) Pytorch has a number of built-in activation functions. We group those here under a common type, just to make it easier to configure and instantiate them from_params using Registrable . Note that we're only including element-wise activation functions in this list. You really need to think about masking when you do a softmax or other similar activation function, so it requires a different API. __call__ # class Activation ( torch . nn . Module , Registrable ): | ... | def __call__ ( self , tensor : torch . Tensor ) -> torch . Tensor This function is here just to make mypy happy. We expect activation functions to follow this API; the builtin pytorch activation functions follow this just fine, even though they don't subclass Activation . We're just making it explicit here, so mypy knows that activations are callable like this. Registrable._registry[Activation] # Registrable . _registry [ Activation ] = { \"linear\" : ( lambda : _ActivationLambda ( lambda x : x , \"Linear\" ), None ), # type: ignore \"mish\" ...","title":"activations"},{"location":"api/nn/activations/#activation","text":"class Activation ( torch . nn . Module , Registrable ) Pytorch has a number of built-in activation functions. We group those here under a common type, just to make it easier to configure and instantiate them from_params using Registrable . Note that we're only including element-wise activation functions in this list. You really need to think about masking when you do a softmax or other similar activation function, so it requires a different API.","title":"Activation"},{"location":"api/nn/activations/#__call__","text":"class Activation ( torch . nn . Module , Registrable ): | ... | def __call__ ( self , tensor : torch . Tensor ) -> torch . Tensor This function is here just to make mypy happy. We expect activation functions to follow this API; the builtin pytorch activation functions follow this just fine, even though they don't subclass Activation . We're just making it explicit here, so mypy knows that activations are callable like this.","title":"__call__"},{"location":"api/nn/activations/#registrable_registryactivation","text":"Registrable . _registry [ Activation ] = { \"linear\" : ( lambda : _ActivationLambda ( lambda x : x , \"Linear\" ), None ), # type: ignore \"mish\" ...","title":"Registrable._registry[Activation]"},{"location":"api/nn/beam_search/","text":"[ allennlp .nn .beam_search ] StateType # StateType = Dict [ str , torch . Tensor ] StepFunctionType # StepFunctionType = Callable [[ torch . Tensor , StateType , int ], Tuple [ torch . Tensor , StateType ]] StepFunctionTypeNoTimestep # StepFunctionTypeNoTimestep = Callable [[ torch . Tensor , StateType ], Tuple [ torch . Tensor , StateType ]] BeamSearch # class BeamSearch : | def __init__ ( | self , | end_index : int , | max_steps : int = 50 , | beam_size : int = 10 , | per_node_beam_size : int = None | ) -> None Implements the beam search algorithm for decoding the most likely sequences. Parameters end_index : int The index of the \"stop\" or \"end\" token in the target vocabulary. max_steps : int , optional (default = 50 ) The maximum number of decoding steps to take, i.e. the maximum length of the predicted sequences. beam_size : int , optional (default = 10 ) The width of the beam used. per_node_beam_size : int , optional (default = beam_size ) The maximum number of candidates to consider per node, at each step in the search. If not given, this just defaults to beam_size . Setting this parameter to a number smaller than beam_size may give better results, as it can introduce more diversity into the search. See Beam Search Strategies for Neural Machine Translation. Freitag and Al-Onaizan, 2017 . reconstruct_sequences # class BeamSearch : | ... | @staticmethod | def reconstruct_sequences ( predictions , backpointers ) Reconstruct the sequences. shape: [(batch_size, beam_size, 1)] search # class BeamSearch : | ... | @torch . no_grad () | def search ( | self , | start_predictions : torch . Tensor , | start_state : StateType , | step : StepFunctionType | ) -> Tuple [ torch . Tensor , torch . Tensor ] Given a starting state and a step function, apply beam search to find the most likely target sequences. NotesIf your step function returns -inf for some log probabilities (like if you're using a masked log-softmax) then some of the \"best\" sequences returned may also have -inf log probability. Specifically this happens when the beam size is smaller than the number of actions with finite log probability (non-zero probability) returned by the step function. Therefore if you're using a mask you may want to check the results from search and potentially discard sequences with non-finite log probability. Parameters start_predictions : torch.Tensor A tensor containing the initial predictions with shape (batch_size,) . Usually the initial predictions are just the index of the \"start\" token in the target vocabulary. start_state : StateType The initial state passed to the step function. Each value of the state dict should be a tensor of shape (batch_size, *) , where * means any other number of dimensions. step : StepFunctionType A function that is responsible for computing the next most likely tokens, given the current state and the predictions from the last time step. The function should accept two arguments. The first being a tensor of shape (group_size,) , representing the index of the predicted tokens from the last time step, and the second being the current state. The group_size will be batch_size * beam_size , except in the initial step, for which it will just be batch_size . The function is expected to return a tuple, where the first element is a tensor of shape (group_size, target_vocab_size) containing the log probabilities of the tokens for the next step, and the second element is the updated state. The tensor in the state should have shape (group_size, *) , where * means any other number of dimensions. Returns Tuple[torch.Tensor, torch.Tensor] Tuple of (predictions, log_probabilities) , where predictions has shape (batch_size, beam_size, max_steps) and log_probabilities has shape (batch_size, beam_size) .","title":"beam_search"},{"location":"api/nn/beam_search/#statetype","text":"StateType = Dict [ str , torch . Tensor ]","title":"StateType"},{"location":"api/nn/beam_search/#stepfunctiontype","text":"StepFunctionType = Callable [[ torch . Tensor , StateType , int ], Tuple [ torch . Tensor , StateType ]]","title":"StepFunctionType"},{"location":"api/nn/beam_search/#stepfunctiontypenotimestep","text":"StepFunctionTypeNoTimestep = Callable [[ torch . Tensor , StateType ], Tuple [ torch . Tensor , StateType ]]","title":"StepFunctionTypeNoTimestep"},{"location":"api/nn/beam_search/#beamsearch","text":"class BeamSearch : | def __init__ ( | self , | end_index : int , | max_steps : int = 50 , | beam_size : int = 10 , | per_node_beam_size : int = None | ) -> None Implements the beam search algorithm for decoding the most likely sequences. Parameters end_index : int The index of the \"stop\" or \"end\" token in the target vocabulary. max_steps : int , optional (default = 50 ) The maximum number of decoding steps to take, i.e. the maximum length of the predicted sequences. beam_size : int , optional (default = 10 ) The width of the beam used. per_node_beam_size : int , optional (default = beam_size ) The maximum number of candidates to consider per node, at each step in the search. If not given, this just defaults to beam_size . Setting this parameter to a number smaller than beam_size may give better results, as it can introduce more diversity into the search. See Beam Search Strategies for Neural Machine Translation. Freitag and Al-Onaizan, 2017 .","title":"BeamSearch"},{"location":"api/nn/beam_search/#reconstruct_sequences","text":"class BeamSearch : | ... | @staticmethod | def reconstruct_sequences ( predictions , backpointers ) Reconstruct the sequences. shape: [(batch_size, beam_size, 1)]","title":"reconstruct_sequences"},{"location":"api/nn/beam_search/#search","text":"class BeamSearch : | ... | @torch . no_grad () | def search ( | self , | start_predictions : torch . Tensor , | start_state : StateType , | step : StepFunctionType | ) -> Tuple [ torch . Tensor , torch . Tensor ] Given a starting state and a step function, apply beam search to find the most likely target sequences. NotesIf your step function returns -inf for some log probabilities (like if you're using a masked log-softmax) then some of the \"best\" sequences returned may also have -inf log probability. Specifically this happens when the beam size is smaller than the number of actions with finite log probability (non-zero probability) returned by the step function. Therefore if you're using a mask you may want to check the results from search and potentially discard sequences with non-finite log probability. Parameters start_predictions : torch.Tensor A tensor containing the initial predictions with shape (batch_size,) . Usually the initial predictions are just the index of the \"start\" token in the target vocabulary. start_state : StateType The initial state passed to the step function. Each value of the state dict should be a tensor of shape (batch_size, *) , where * means any other number of dimensions. step : StepFunctionType A function that is responsible for computing the next most likely tokens, given the current state and the predictions from the last time step. The function should accept two arguments. The first being a tensor of shape (group_size,) , representing the index of the predicted tokens from the last time step, and the second being the current state. The group_size will be batch_size * beam_size , except in the initial step, for which it will just be batch_size . The function is expected to return a tuple, where the first element is a tensor of shape (group_size, target_vocab_size) containing the log probabilities of the tokens for the next step, and the second element is the updated state. The tensor in the state should have shape (group_size, *) , where * means any other number of dimensions. Returns Tuple[torch.Tensor, torch.Tensor] Tuple of (predictions, log_probabilities) , where predictions has shape (batch_size, beam_size, max_steps) and log_probabilities has shape (batch_size, beam_size) .","title":"search"},{"location":"api/nn/chu_liu_edmonds/","text":"[ allennlp .nn .chu_liu_edmonds ] decode_mst # def decode_mst ( energy : numpy . ndarray , length : int , has_labels : bool = True ) -> Tuple [ numpy . ndarray , numpy . ndarray ] Note: Counter to typical intuition, this function decodes the maximum spanning tree. Decode the optimal MST tree with the Chu-Liu-Edmonds algorithm for maximum spanning arborescences on graphs. Parameters energy : numpy.ndarray A tensor with shape (num_labels, timesteps, timesteps) containing the energy of each edge. If has_labels is False , the tensor should have shape (timesteps, timesteps) instead. length : int The length of this sequence, as the energy may have come from a padded batch. has_labels : bool , optional (default = True ) Whether the graph has labels or not. chu_liu_edmonds # def chu_liu_edmonds ( length : int , score_matrix : numpy . ndarray , current_nodes : List [ bool ], final_edges : Dict [ int , int ], old_input : numpy . ndarray , old_output : numpy . ndarray , representatives : List [ Set [ int ]] ) Applies the chu-liu-edmonds algorithm recursively to a graph with edge weights defined by score_matrix. Note that this function operates in place, so variables will be modified. Parameters length : int The number of nodes. score_matrix : numpy.ndarray The score matrix representing the scores for pairs of nodes. current_nodes : List[bool] The nodes which are representatives in the graph. A representative at it's most basic represents a node, but as the algorithm progresses, individual nodes will represent collapsed cycles in the graph. final_edges : Dict[int, int] An empty dictionary which will be populated with the nodes which are connected in the maximum spanning tree. old_input : numpy.ndarray old_output : numpy.ndarray representatives : List[Set[int]] A list containing the nodes that a particular node is representing at this iteration in the graph. Returns Nothing - all variables are modified in place.","title":"chu_liu_edmonds"},{"location":"api/nn/chu_liu_edmonds/#decode_mst","text":"def decode_mst ( energy : numpy . ndarray , length : int , has_labels : bool = True ) -> Tuple [ numpy . ndarray , numpy . ndarray ] Note: Counter to typical intuition, this function decodes the maximum spanning tree. Decode the optimal MST tree with the Chu-Liu-Edmonds algorithm for maximum spanning arborescences on graphs. Parameters energy : numpy.ndarray A tensor with shape (num_labels, timesteps, timesteps) containing the energy of each edge. If has_labels is False , the tensor should have shape (timesteps, timesteps) instead. length : int The length of this sequence, as the energy may have come from a padded batch. has_labels : bool , optional (default = True ) Whether the graph has labels or not.","title":"decode_mst"},{"location":"api/nn/chu_liu_edmonds/#chu_liu_edmonds","text":"def chu_liu_edmonds ( length : int , score_matrix : numpy . ndarray , current_nodes : List [ bool ], final_edges : Dict [ int , int ], old_input : numpy . ndarray , old_output : numpy . ndarray , representatives : List [ Set [ int ]] ) Applies the chu-liu-edmonds algorithm recursively to a graph with edge weights defined by score_matrix. Note that this function operates in place, so variables will be modified. Parameters length : int The number of nodes. score_matrix : numpy.ndarray The score matrix representing the scores for pairs of nodes. current_nodes : List[bool] The nodes which are representatives in the graph. A representative at it's most basic represents a node, but as the algorithm progresses, individual nodes will represent collapsed cycles in the graph. final_edges : Dict[int, int] An empty dictionary which will be populated with the nodes which are connected in the maximum spanning tree. old_input : numpy.ndarray old_output : numpy.ndarray representatives : List[Set[int]] A list containing the nodes that a particular node is representing at this iteration in the graph. Returns Nothing - all variables are modified in place.","title":"chu_liu_edmonds"},{"location":"api/nn/initializers/","text":"[ allennlp .nn .initializers ] An initializer is just a PyTorch function. Here we implement a proxy class that allows us to register them and supply any additional function arguments (for example, the mean and std of a normal initializer) as named arguments to the constructor. The available initialization functions are \"normal\" \"uniform\" \"constant\" \"eye\" \"dirac\" \"xavier_uniform\" \"xavier_normal\" \"kaiming_uniform\" \"kaiming_normal\" \"orthogonal\" \"sparse\" \"block_orthogonal\" \"uniform_unit_scaling\" \"pretrained\" Initializer # class Initializer ( Registrable ) An initializer is really just a bare pytorch function. This class is a proxy that allows us to implement Registrable for those functions. default_implementation # class Initializer ( Registrable ): | ... | default_implementation = \"normal\" __call__ # class Initializer ( Registrable ): | ... | def __call__ ( self , tensor : torch . Tensor , ** kwargs ) -> None This function is here just to make mypy happy. We expect initialization functions to follow this API; the builtin pytorch initialization functions follow this just fine, even though they don't subclass Initialization . We're just making it explicit here, so mypy knows that initializers are callable like this. uniform_unit_scaling # def uniform_unit_scaling ( tensor : torch . Tensor , nonlinearity : str = \"linear\" ) An initaliser which preserves output variance for approximately gaussian distributed inputs. This boils down to initialising layers using a uniform distribution in the range (-sqrt(3/dim[0]) * scale, sqrt(3 / dim[0]) * scale) , where dim[0] is equal to the input dimension of the parameter and the scale is a constant scaling factor which depends on the non-linearity used. See Random Walk Initialisation for Training Very Deep Feedforward Networks <https://www.semanticscholar.org/paper/Random-Walk-Initialization-for-Training-Very-Deep-Sussillo-Abbott/be9728a0728b6acf7a485225b1e41592176eda0b> _ for more information. Parameters tensor : torch.Tensor The tensor to initialise. nonlinearity : str , optional (default = \"linear\" ) The non-linearity which is performed after the projection that this tensor is involved in. This must be the name of a function contained in the torch.nn.functional package. Returns The initialised tensor. block_orthogonal # def block_orthogonal ( tensor : torch . Tensor , split_sizes : List [ int ], gain : float = 1.0 ) -> None An initializer which allows initializing model parameters in \"blocks\". This is helpful in the case of recurrent models which use multiple gates applied to linear projections, which can be computed efficiently if they are concatenated together. However, they are separate parameters which should be initialized independently. Parameters tensor : torch.Tensor A tensor to initialize. split_sizes : List[int] A list of length tensor.ndim() specifying the size of the blocks along that particular dimension. E.g. [10, 20] would result in the tensor being split into chunks of size 10 along the first dimension and 20 along the second. gain : float , optional (default = 1.0 ) The gain (scaling) applied to the orthogonal initialization. zero # def zero ( tensor : torch . Tensor ) -> None lstm_hidden_bias # def lstm_hidden_bias ( tensor : torch . Tensor ) -> None Initialize the biases of the forget gate to 1, and all other gates to 0, following Jozefowicz et al., An Empirical Exploration of Recurrent Network Architectures __call__ # class _InitializerWrapper ( Initializer ): | ... | def __call__ ( self , tensor : torch . Tensor , ** kwargs ) -> None NormalInitializer # class NormalInitializer ( _InitializerWrapper ): | def __init__ ( self , mean : float = 0.0 , std : float = 0.1 ) Registered as an Initializer with name \"normal\". OrthogonalInitializer # class OrthogonalInitializer ( _InitializerWrapper ): | def __init__ ( self , gain : float = 1.0 ) Registered as an Initializer with name \"orthogonal\". UniformInitializer # class UniformInitializer ( _InitializerWrapper ): | def __init__ ( self , a : float = 0.0 , b : float = 1.0 ) Registered as an Initializer with name \"uniform\". ConstantInitializer # class ConstantInitializer ( _InitializerWrapper ): | def __init__ ( self , val : float ) Registered as an Initializer with name \"constant\". DiracInitializer # class DiracInitializer ( _InitializerWrapper ): | def __init__ ( self ) Registered as an Initializer with name \"dirac\". XavierUniformInitializer # class XavierUniformInitializer ( _InitializerWrapper ): | def __init__ ( self , gain : float = 1.0 ) Registered as an Initializer with name \"xavir_uniform\". XavierNormalInitializer # class XavierNormalInitializer ( _InitializerWrapper ): | def __init__ ( self , gain : float = 1.0 ) Registered as an Initializer with name \"xavier_normal\". KaimingUniformInitializer # class KaimingUniformInitializer ( _InitializerWrapper ): | def __init__ ( | self , | a : float = 0.0 , | mode : str = \"fan_in\" , | nonlinearity : str = \"leaky_relu\" | ) Registered as an Initializer with name \"kaiming_uniform\". KaimingNormalInitializer # class KaimingNormalInitializer ( _InitializerWrapper ): | def __init__ ( | self , | a : float = 0.0 , | mode : str = \"fan_in\" , | nonlinearity : str = \"leaky_relu\" | ) Registered as an Initializer with name \"kaiming_normal\". SparseInitializer # class SparseInitializer ( _InitializerWrapper ): | def __init__ ( self , sparsity : float , std : float = 0.01 ) Registered as an Initializer with name \"sparse\". EyeInitializer # class EyeInitializer ( _InitializerWrapper ): | def __init__ ( self ) Registered as an Initializer with name \"eye\". BlockOrthogonalInitializer # class BlockOrthogonalInitializer ( _InitializerWrapper ): | def __init__ ( self , split_sizes : List [ int ], gain : float = 1.0 ) Registered as an Initializer with name \"block_orthogonal\". UniformUnitScalingInitializer # class UniformUnitScalingInitializer ( _InitializerWrapper ): | def __init__ ( self , nonlinearity : str = \"linear\" ) Registered as an Initializer with name \"uniform_unit_scaling\". ZeroInitializer # class ZeroInitializer ( _InitializerWrapper ): | def __init__ ( self ) Registered as an Initializer with name \"zero\". LstmHiddenBiasInitializer # class LstmHiddenBiasInitializer ( _InitializerWrapper ): | def __init__ ( self ) Registered as an Initializer with name \"lstm_hidden_bias\". PretrainedModelInitializer # class PretrainedModelInitializer ( Initializer ): | def __init__ ( | self , | weights_file_path : str , | parameter_name_overrides : Dict [ str , str ] = None | ) -> None An initializer which allows initializing parameters using a pretrained model. The initializer will load all of the weights from the weights_file_path and use the name of the new parameters to index into the pretrained parameters. Therefore, by default, the names of the new and pretrained parameters must be the same. However, this behavior can be overridden using the parameter_name_overrides , which remaps the name of the new parameter to the key which should be used to index into the pretrained parameters. The initializer will load all of the weights from the weights_file_path regardless of which parameters will actually be used to initialize the new model. So, if you need to initialize several parameters using a pretrained model, the most memory-efficient way to do this is to use one PretrainedModelInitializer per weights file and use a regex to match all of the new parameters which need to be initialized. If you are using a configuration file to instantiate this object, the below entry in the InitializerApplicator parameters will initialize linear_1.weight and linear_2.weight using a pretrained model. linear_1.weight will be initialized to the pretrained parameters called linear_1.weight , but linear_2.weight will be initialized to the pretrained parameters called linear_3.weight :: [\"linear_1.weight|linear_2.weight\", { \"type\": \"pretrained\", \"weights_file_path\": \"best.th\", \"parameter_name_overrides\": { \"linear_2.weight\": \"linear_3.weight\" } } ] To initialize weights for all the parameters from a pretrained model (assuming their names remain unchanged), use the following instead: [\".*\", { \"type\": \"pretrained\", \"weights_file_path\": \"best.th\", \"parameter_name_overrides\": {} } ] Registered as an Initializer with name \"pretrained\". Parameters weights_file_path : str The path to the weights file which has the pretrained model parameters. parameter_name_overrides : Dict[str, str] , optional (default = None ) The mapping from the new parameter name to the name which should be used to index into the pretrained model parameters. If a parameter name is not specified, the initializer will use the parameter's default name as the key. __call__ # class PretrainedModelInitializer ( Initializer ): | ... | @overrides | def __call__ ( | self , | tensor : torch . Tensor , | parameter_name : str , | ** kwargs | ) -> None Select the new parameter name if it's being overridden InitializerApplicator # class InitializerApplicator ( FromParams ): | def __init__ ( | self , | regexes : List [ Tuple [ str , Initializer ]] = None , | prevent_regexes : List [ str ] = None | ) -> None Applies initializers to the parameters of a Module based on regex matches. Any parameter not explicitly matching a regex will not be initialized, instead using whatever the default initialization was in the module's code. If you are instantiating this object from a config file, an example configuration is as follows: { \"regexes\" : [ [ \"parameter_regex_match1\" , { \"type\" : \"normal\" \"mean\" : 0.01 \"std\" : 0.1 } ], [ \"parameter_regex_match2\" , \"uniform\" ] ], \"prevent_regexes\" : [ \"prevent_init_regex\" ] } where the first item in each tuple under the regexes parameters is the regex that matches to parameters, and the second item specifies an Initializer. These values can either be strings, in which case they correspond to the names of initializers, or dictionaries, in which case they must contain the \"type\" key, corresponding to the name of an initializer. In addition, they may contain auxiliary named parameters which will be fed to the initializer itself. To determine valid auxiliary parameters, please refer to the torch.nn.init documentation. Parameters regexes : List[Tuple[str, Initializer]] , optional (default = [] ) A list mapping parameter regexes to initializers. We will check each parameter against each regex in turn, and apply the initializer paired with the first matching regex, if any. prevent_regexes : List[str] , optional (default = None ) Any parameter name matching one of these regexes will not be initialized, regardless of whether it matches one of the regexes passed in the regexes parameter. __call__ # class InitializerApplicator ( FromParams ): | ... | def __call__ ( self , module : torch . nn . Module ) -> None Applies an initializer to all parameters in a module that match one of the regexes we were given in this object's constructor. Does nothing to parameters that do not match. Parameters module : torch.nn.Module The Pytorch module to apply the initializers to.","title":"initializers"},{"location":"api/nn/initializers/#initializer","text":"class Initializer ( Registrable ) An initializer is really just a bare pytorch function. This class is a proxy that allows us to implement Registrable for those functions.","title":"Initializer"},{"location":"api/nn/initializers/#default_implementation","text":"class Initializer ( Registrable ): | ... | default_implementation = \"normal\"","title":"default_implementation"},{"location":"api/nn/initializers/#__call__","text":"class Initializer ( Registrable ): | ... | def __call__ ( self , tensor : torch . Tensor , ** kwargs ) -> None This function is here just to make mypy happy. We expect initialization functions to follow this API; the builtin pytorch initialization functions follow this just fine, even though they don't subclass Initialization . We're just making it explicit here, so mypy knows that initializers are callable like this.","title":"__call__"},{"location":"api/nn/initializers/#uniform_unit_scaling","text":"def uniform_unit_scaling ( tensor : torch . Tensor , nonlinearity : str = \"linear\" ) An initaliser which preserves output variance for approximately gaussian distributed inputs. This boils down to initialising layers using a uniform distribution in the range (-sqrt(3/dim[0]) * scale, sqrt(3 / dim[0]) * scale) , where dim[0] is equal to the input dimension of the parameter and the scale is a constant scaling factor which depends on the non-linearity used. See Random Walk Initialisation for Training Very Deep Feedforward Networks <https://www.semanticscholar.org/paper/Random-Walk-Initialization-for-Training-Very-Deep-Sussillo-Abbott/be9728a0728b6acf7a485225b1e41592176eda0b> _ for more information. Parameters tensor : torch.Tensor The tensor to initialise. nonlinearity : str , optional (default = \"linear\" ) The non-linearity which is performed after the projection that this tensor is involved in. This must be the name of a function contained in the torch.nn.functional package. Returns The initialised tensor.","title":"uniform_unit_scaling"},{"location":"api/nn/initializers/#block_orthogonal","text":"def block_orthogonal ( tensor : torch . Tensor , split_sizes : List [ int ], gain : float = 1.0 ) -> None An initializer which allows initializing model parameters in \"blocks\". This is helpful in the case of recurrent models which use multiple gates applied to linear projections, which can be computed efficiently if they are concatenated together. However, they are separate parameters which should be initialized independently. Parameters tensor : torch.Tensor A tensor to initialize. split_sizes : List[int] A list of length tensor.ndim() specifying the size of the blocks along that particular dimension. E.g. [10, 20] would result in the tensor being split into chunks of size 10 along the first dimension and 20 along the second. gain : float , optional (default = 1.0 ) The gain (scaling) applied to the orthogonal initialization.","title":"block_orthogonal"},{"location":"api/nn/initializers/#zero","text":"def zero ( tensor : torch . Tensor ) -> None","title":"zero"},{"location":"api/nn/initializers/#lstm_hidden_bias","text":"def lstm_hidden_bias ( tensor : torch . Tensor ) -> None Initialize the biases of the forget gate to 1, and all other gates to 0, following Jozefowicz et al., An Empirical Exploration of Recurrent Network Architectures","title":"lstm_hidden_bias"},{"location":"api/nn/initializers/#__call___1","text":"class _InitializerWrapper ( Initializer ): | ... | def __call__ ( self , tensor : torch . Tensor , ** kwargs ) -> None","title":"__call__"},{"location":"api/nn/initializers/#normalinitializer","text":"class NormalInitializer ( _InitializerWrapper ): | def __init__ ( self , mean : float = 0.0 , std : float = 0.1 ) Registered as an Initializer with name \"normal\".","title":"NormalInitializer"},{"location":"api/nn/initializers/#orthogonalinitializer","text":"class OrthogonalInitializer ( _InitializerWrapper ): | def __init__ ( self , gain : float = 1.0 ) Registered as an Initializer with name \"orthogonal\".","title":"OrthogonalInitializer"},{"location":"api/nn/initializers/#uniforminitializer","text":"class UniformInitializer ( _InitializerWrapper ): | def __init__ ( self , a : float = 0.0 , b : float = 1.0 ) Registered as an Initializer with name \"uniform\".","title":"UniformInitializer"},{"location":"api/nn/initializers/#constantinitializer","text":"class ConstantInitializer ( _InitializerWrapper ): | def __init__ ( self , val : float ) Registered as an Initializer with name \"constant\".","title":"ConstantInitializer"},{"location":"api/nn/initializers/#diracinitializer","text":"class DiracInitializer ( _InitializerWrapper ): | def __init__ ( self ) Registered as an Initializer with name \"dirac\".","title":"DiracInitializer"},{"location":"api/nn/initializers/#xavieruniforminitializer","text":"class XavierUniformInitializer ( _InitializerWrapper ): | def __init__ ( self , gain : float = 1.0 ) Registered as an Initializer with name \"xavir_uniform\".","title":"XavierUniformInitializer"},{"location":"api/nn/initializers/#xaviernormalinitializer","text":"class XavierNormalInitializer ( _InitializerWrapper ): | def __init__ ( self , gain : float = 1.0 ) Registered as an Initializer with name \"xavier_normal\".","title":"XavierNormalInitializer"},{"location":"api/nn/initializers/#kaiminguniforminitializer","text":"class KaimingUniformInitializer ( _InitializerWrapper ): | def __init__ ( | self , | a : float = 0.0 , | mode : str = \"fan_in\" , | nonlinearity : str = \"leaky_relu\" | ) Registered as an Initializer with name \"kaiming_uniform\".","title":"KaimingUniformInitializer"},{"location":"api/nn/initializers/#kaimingnormalinitializer","text":"class KaimingNormalInitializer ( _InitializerWrapper ): | def __init__ ( | self , | a : float = 0.0 , | mode : str = \"fan_in\" , | nonlinearity : str = \"leaky_relu\" | ) Registered as an Initializer with name \"kaiming_normal\".","title":"KaimingNormalInitializer"},{"location":"api/nn/initializers/#sparseinitializer","text":"class SparseInitializer ( _InitializerWrapper ): | def __init__ ( self , sparsity : float , std : float = 0.01 ) Registered as an Initializer with name \"sparse\".","title":"SparseInitializer"},{"location":"api/nn/initializers/#eyeinitializer","text":"class EyeInitializer ( _InitializerWrapper ): | def __init__ ( self ) Registered as an Initializer with name \"eye\".","title":"EyeInitializer"},{"location":"api/nn/initializers/#blockorthogonalinitializer","text":"class BlockOrthogonalInitializer ( _InitializerWrapper ): | def __init__ ( self , split_sizes : List [ int ], gain : float = 1.0 ) Registered as an Initializer with name \"block_orthogonal\".","title":"BlockOrthogonalInitializer"},{"location":"api/nn/initializers/#uniformunitscalinginitializer","text":"class UniformUnitScalingInitializer ( _InitializerWrapper ): | def __init__ ( self , nonlinearity : str = \"linear\" ) Registered as an Initializer with name \"uniform_unit_scaling\".","title":"UniformUnitScalingInitializer"},{"location":"api/nn/initializers/#zeroinitializer","text":"class ZeroInitializer ( _InitializerWrapper ): | def __init__ ( self ) Registered as an Initializer with name \"zero\".","title":"ZeroInitializer"},{"location":"api/nn/initializers/#lstmhiddenbiasinitializer","text":"class LstmHiddenBiasInitializer ( _InitializerWrapper ): | def __init__ ( self ) Registered as an Initializer with name \"lstm_hidden_bias\".","title":"LstmHiddenBiasInitializer"},{"location":"api/nn/initializers/#pretrainedmodelinitializer","text":"class PretrainedModelInitializer ( Initializer ): | def __init__ ( | self , | weights_file_path : str , | parameter_name_overrides : Dict [ str , str ] = None | ) -> None An initializer which allows initializing parameters using a pretrained model. The initializer will load all of the weights from the weights_file_path and use the name of the new parameters to index into the pretrained parameters. Therefore, by default, the names of the new and pretrained parameters must be the same. However, this behavior can be overridden using the parameter_name_overrides , which remaps the name of the new parameter to the key which should be used to index into the pretrained parameters. The initializer will load all of the weights from the weights_file_path regardless of which parameters will actually be used to initialize the new model. So, if you need to initialize several parameters using a pretrained model, the most memory-efficient way to do this is to use one PretrainedModelInitializer per weights file and use a regex to match all of the new parameters which need to be initialized. If you are using a configuration file to instantiate this object, the below entry in the InitializerApplicator parameters will initialize linear_1.weight and linear_2.weight using a pretrained model. linear_1.weight will be initialized to the pretrained parameters called linear_1.weight , but linear_2.weight will be initialized to the pretrained parameters called linear_3.weight :: [\"linear_1.weight|linear_2.weight\", { \"type\": \"pretrained\", \"weights_file_path\": \"best.th\", \"parameter_name_overrides\": { \"linear_2.weight\": \"linear_3.weight\" } } ] To initialize weights for all the parameters from a pretrained model (assuming their names remain unchanged), use the following instead: [\".*\", { \"type\": \"pretrained\", \"weights_file_path\": \"best.th\", \"parameter_name_overrides\": {} } ] Registered as an Initializer with name \"pretrained\". Parameters weights_file_path : str The path to the weights file which has the pretrained model parameters. parameter_name_overrides : Dict[str, str] , optional (default = None ) The mapping from the new parameter name to the name which should be used to index into the pretrained model parameters. If a parameter name is not specified, the initializer will use the parameter's default name as the key.","title":"PretrainedModelInitializer"},{"location":"api/nn/initializers/#__call___2","text":"class PretrainedModelInitializer ( Initializer ): | ... | @overrides | def __call__ ( | self , | tensor : torch . Tensor , | parameter_name : str , | ** kwargs | ) -> None Select the new parameter name if it's being overridden","title":"__call__"},{"location":"api/nn/initializers/#initializerapplicator","text":"class InitializerApplicator ( FromParams ): | def __init__ ( | self , | regexes : List [ Tuple [ str , Initializer ]] = None , | prevent_regexes : List [ str ] = None | ) -> None Applies initializers to the parameters of a Module based on regex matches. Any parameter not explicitly matching a regex will not be initialized, instead using whatever the default initialization was in the module's code. If you are instantiating this object from a config file, an example configuration is as follows: { \"regexes\" : [ [ \"parameter_regex_match1\" , { \"type\" : \"normal\" \"mean\" : 0.01 \"std\" : 0.1 } ], [ \"parameter_regex_match2\" , \"uniform\" ] ], \"prevent_regexes\" : [ \"prevent_init_regex\" ] } where the first item in each tuple under the regexes parameters is the regex that matches to parameters, and the second item specifies an Initializer. These values can either be strings, in which case they correspond to the names of initializers, or dictionaries, in which case they must contain the \"type\" key, corresponding to the name of an initializer. In addition, they may contain auxiliary named parameters which will be fed to the initializer itself. To determine valid auxiliary parameters, please refer to the torch.nn.init documentation. Parameters regexes : List[Tuple[str, Initializer]] , optional (default = [] ) A list mapping parameter regexes to initializers. We will check each parameter against each regex in turn, and apply the initializer paired with the first matching regex, if any. prevent_regexes : List[str] , optional (default = None ) Any parameter name matching one of these regexes will not be initialized, regardless of whether it matches one of the regexes passed in the regexes parameter.","title":"InitializerApplicator"},{"location":"api/nn/initializers/#__call___3","text":"class InitializerApplicator ( FromParams ): | ... | def __call__ ( self , module : torch . nn . Module ) -> None Applies an initializer to all parameters in a module that match one of the regexes we were given in this object's constructor. Does nothing to parameters that do not match. Parameters module : torch.nn.Module The Pytorch module to apply the initializers to.","title":"__call__"},{"location":"api/nn/util/","text":"[ allennlp .nn .util ] Assorted utilities for working with neural networks in AllenNLP. T # T = TypeVar ( \"T\" ) has_tensor # def has_tensor ( obj ) -> bool Given a possibly complex data structure, check if it has any torch.Tensors in it. move_to_device # def move_to_device ( obj , cuda_device : Union [ torch . device , int ]) Given a structure (possibly) containing Tensors on the CPU, move all the Tensors to the specified GPU (or do nothing, if they should be on the CPU). clamp_tensor # def clamp_tensor ( tensor , minimum , maximum ) Supports sparse and dense tensors. Returns a tensor with values clamped between the provided minimum and maximum, without modifying the original tensor. batch_tensor_dicts # def batch_tensor_dicts ( tensor_dicts : List [ Dict [ str , torch . Tensor ]], remove_trailing_dimension : bool = False ) -> Dict [ str , torch . Tensor ] Takes a list of tensor dictionaries, where each dictionary is assumed to have matching keys, and returns a single dictionary with all tensors with the same key batched together. Parameters tensor_dicts : List[Dict[str, torch.Tensor]] The list of tensor dictionaries to batch. remove_trailing_dimension : bool If True , we will check for a trailing dimension of size 1 on the tensors that are being batched, and remove it if we find it. get_lengths_from_binary_sequence_mask # def get_lengths_from_binary_sequence_mask ( mask : torch . BoolTensor ) -> torch . LongTensor Compute sequence lengths for each batch element in a tensor using a binary mask. Parameters mask : torch.BoolTensor A 2D binary mask of shape (batch_size, sequence_length) to calculate the per-batch sequence lengths from. Returns torch.LongTensor A torch.LongTensor of shape (batch_size,) representing the lengths of the sequences in the batch. get_mask_from_sequence_lengths # def get_mask_from_sequence_lengths ( sequence_lengths : torch . Tensor , max_length : int ) -> torch . BoolTensor Given a variable of shape (batch_size,) that represents the sequence lengths of each batch element, this function returns a (batch_size, max_length) mask variable. For example, if our input was [2, 2, 3] , with a max_length of 4, we'd return [[1, 1, 0, 0], [1, 1, 0, 0], [1, 1, 1, 0]] . We require max_length here instead of just computing it from the input sequence_lengths because it lets us avoid finding the max, then copying that value from the GPU to the CPU so that we can use it to construct a new tensor. sort_batch_by_length # def sort_batch_by_length ( tensor : torch . Tensor , sequence_lengths : torch . Tensor ) Sort a batch first tensor by some specified lengths. Parameters tensor : torch.FloatTensor A batch first Pytorch tensor. sequence_lengths : torch.LongTensor A tensor representing the lengths of some dimension of the tensor which we want to sort by. Returns sorted_tensor : torch.FloatTensor The original tensor sorted along the batch dimension with respect to sequence_lengths. sorted_sequence_lengths : torch.LongTensor The original sequence_lengths sorted by decreasing size. restoration_indices : torch.LongTensor Indices into the sorted_tensor such that sorted_tensor.index_select(0, restoration_indices) == original_tensor permutation_index : torch.LongTensor The indices used to sort the tensor. This is useful if you want to sort many tensors using the same ordering. get_final_encoder_states # def get_final_encoder_states ( encoder_outputs : torch . Tensor , mask : torch . BoolTensor , bidirectional : bool = False ) -> torch . Tensor Given the output from a Seq2SeqEncoder , with shape (batch_size, sequence_length, encoding_dim) , this method returns the final hidden state for each element of the batch, giving a tensor of shape (batch_size, encoding_dim) . This is not as simple as encoder_outputs[:, -1] , because the sequences could have different lengths. We use the mask (which has shape (batch_size, sequence_length) ) to find the final state for each batch instance. Additionally, if bidirectional is True , we will split the final dimension of the encoder_outputs into two and assume that the first half is for the forward direction of the encoder and the second half is for the backward direction. We will concatenate the last state for each encoder dimension, giving encoder_outputs[:, -1, :encoding_dim/2] concatenated with encoder_outputs[:, 0, encoding_dim/2:] . get_dropout_mask # def get_dropout_mask ( dropout_probability : float , tensor_for_masking : torch . Tensor ) Computes and returns an element-wise dropout mask for a given tensor, where each element in the mask is dropped out with probability dropout_probability. Note that the mask is NOT applied to the tensor - the tensor is passed to retain the correct CUDA tensor type for the mask. Parameters dropout_probability : float Probability of dropping a dimension of the input. tensor_for_masking : torch.Tensor Returns torch.FloatTensor A torch.FloatTensor consisting of the binary mask scaled by 1/ (1 - dropout_probability). This scaling ensures expected values and variances of the output of applying this mask and the original tensor are the same. masked_softmax # def masked_softmax ( vector : torch . Tensor , mask : torch . BoolTensor , dim : int = - 1 , memory_efficient : bool = False ) -> torch . Tensor torch.nn.functional.softmax(vector) does not work if some elements of vector should be masked. This performs a softmax on just the non-masked portions of vector . Passing None in for the mask is also acceptable; you'll just get a regular softmax. vector can have an arbitrary number of dimensions; the only requirement is that mask is broadcastable to vector's shape. If mask has fewer dimensions than vector , we will unsqueeze on dimension 1 until they match. If you need a different unsqueezing of your mask, do it yourself before passing the mask into this function. If memory_efficient is set to true, we will simply use a very large negative number for those masked positions so that the probabilities of those positions would be approximately 0. This is not accurate in math, but works for most cases and consumes less memory. In the case that the input vector is completely masked and memory_efficient is false, this function returns an array of 0.0 . This behavior may cause NaN if this is used as the last layer of a model that uses categorical cross-entropy loss. Instead, if memory_efficient is true, this function will treat every element as equal, and do softmax over equal numbers. masked_log_softmax # def masked_log_softmax ( vector : torch . Tensor , mask : torch . BoolTensor , dim : int = - 1 ) -> torch . Tensor torch.nn.functional.log_softmax(vector) does not work if some elements of vector should be masked. This performs a log_softmax on just the non-masked portions of vector . Passing None in for the mask is also acceptable; you'll just get a regular log_softmax. vector can have an arbitrary number of dimensions; the only requirement is that mask is broadcastable to vector's shape. If mask has fewer dimensions than vector , we will unsqueeze on dimension 1 until they match. If you need a different unsqueezing of your mask, do it yourself before passing the mask into this function. In the case that the input vector is completely masked, the return value of this function is arbitrary, but not nan . You should be masking the result of whatever computation comes out of this in that case, anyway, so the specific values returned shouldn't matter. Also, the way that we deal with this case relies on having single-precision floats; mixing half-precision floats with fully-masked vectors will likely give you nans . If your logits are all extremely negative (i.e., the max value in your logit vector is -50 or lower), the way we handle masking here could mess you up. But if you've got logit values that extreme, you've got bigger problems than this. masked_max # def masked_max ( vector : torch . Tensor , mask : torch . BoolTensor , dim : int , keepdim : bool = False ) -> torch . Tensor To calculate max along certain dimensions on masked values Parameters vector : torch.Tensor The vector to calculate max, assume unmasked parts are already zeros mask : torch.BoolTensor The mask of the vector. It must be broadcastable with vector. dim : int The dimension to calculate max keepdim : bool Whether to keep dimension Returns torch.Tensor A torch.Tensor of including the maximum values. masked_mean # def masked_mean ( vector : torch . Tensor , mask : torch . BoolTensor , dim : int , keepdim : bool = False ) -> torch . Tensor To calculate mean along certain dimensions on masked values Parameters vector : torch.Tensor The vector to calculate mean. mask : torch.BoolTensor The mask of the vector. It must be broadcastable with vector. dim : int The dimension to calculate mean keepdim : bool Whether to keep dimension Returns torch.Tensor A torch.Tensor of including the mean values. masked_flip # def masked_flip ( padded_sequence : torch . Tensor , sequence_lengths : List [ int ] ) -> torch . Tensor Flips a padded tensor along the time dimension without affecting masked entries. Parameters padded_sequence : torch.Tensor The tensor to flip along the time dimension. Assumed to be of dimensions (batch size, num timesteps, ...) sequence_lengths : torch.Tensor A list containing the lengths of each unpadded sequence in the batch. Returns torch.Tensor A torch.Tensor of the same shape as padded_sequence. viterbi_decode # def viterbi_decode ( tag_sequence : torch . Tensor , transition_matrix : torch . Tensor , tag_observations : Optional [ List [ int ]] = None , allowed_start_transitions : torch . Tensor = None , allowed_end_transitions : torch . Tensor = None , top_k : int = None ) Perform Viterbi decoding in log space over a sequence given a transition matrix specifying pairwise (transition) potentials between tags and a matrix of shape (sequence_length, num_tags) specifying unary potentials for possible tags per timestep. Parameters tag_sequence : torch.Tensor A tensor of shape (sequence_length, num_tags) representing scores for a set of tags over a given sequence. transition_matrix : torch.Tensor A tensor of shape (num_tags, num_tags) representing the binary potentials for transitioning between a given pair of tags. tag_observations : Optional[List[int]] , optional (default = None ) A list of length sequence_length containing the class ids of observed elements in the sequence, with unobserved elements being set to -1. Note that it is possible to provide evidence which results in degenerate labelings if the sequences of tags you provide as evidence cannot transition between each other, or those transitions are extremely unlikely. In this situation we log a warning, but the responsibility for providing self-consistent evidence ultimately lies with the user. allowed_start_transitions : torch.Tensor , optional (default = None ) An optional tensor of shape (num_tags,) describing which tags the START token may transition to . If provided, additional transition constraints will be used for determining the start element of the sequence. allowed_end_transitions : torch.Tensor , optional (default = None ) An optional tensor of shape (num_tags,) describing which tags may transition to the end tag. If provided, additional transition constraints will be used for determining the end element of the sequence. top_k : int , optional (default = None ) Optional integer specifying how many of the top paths to return. For top_k>=1, returns a tuple of two lists: top_k_paths, top_k_scores, For top_k==None, returns a flattened tuple with just the top path and its score (not in lists, for backwards compatibility). Returns viterbi_path : List[int] The tag indices of the maximum likelihood tag sequence. viterbi_score : torch.Tensor The score of the viterbi path. get_text_field_mask # def get_text_field_mask ( text_field_tensors : Dict [ str , Dict [ str , torch . Tensor ]], num_wrapping_dims : int = 0 , padding_id : int = 0 ) -> torch . BoolTensor Takes the dictionary of tensors produced by a TextField and returns a mask with 0 where the tokens are padding, and 1 otherwise. padding_id specifies the id of padding tokens. We also handle TextFields wrapped by an arbitrary number of ListFields , where the number of wrapping ListFields is given by num_wrapping_dims . If num_wrapping_dims == 0 , the returned mask has shape (batch_size, num_tokens) . If num_wrapping_dims > 0 then the returned mask has num_wrapping_dims extra dimensions, so the shape will be (batch_size, ..., num_tokens) . There could be several entries in the tensor dictionary with different shapes (e.g., one for word ids, one for character ids). In order to get a token mask, we use the tensor in the dictionary with the lowest number of dimensions. After subtracting num_wrapping_dims , if this tensor has two dimensions we assume it has shape (batch_size, ..., num_tokens) , and use it for the mask. If instead it has three dimensions, we assume it has shape (batch_size, ..., num_tokens, num_features) , and sum over the last dimension to produce the mask. Most frequently this will be a character id tensor, but it could also be a featurized representation of each token, etc. If the input text_field_tensors contains the \"mask\" key, this is returned instead of inferring the mask. get_token_ids_from_text_field_tensors # def get_token_ids_from_text_field_tensors ( text_field_tensors : Dict [ str , Dict [ str , torch . Tensor ]] ) -> torch . Tensor Our TextFieldTensors are complex output structures, because they try to handle a lot of potential variation. Sometimes, you just want to grab the token ids from this data structure, and that's not trivial without hard-coding assumptions about your data processing, which defeats the entire purpose of that generality. This method tries to let you get the token ids out of the data structure in your model without hard-coding any assumptions. weighted_sum # def weighted_sum ( matrix : torch . Tensor , attention : torch . Tensor ) -> torch . Tensor Takes a matrix of vectors and a set of weights over the rows in the matrix (which we call an \"attention\" vector), and returns a weighted sum of the rows in the matrix. This is the typical computation performed after an attention mechanism. Note that while we call this a \"matrix\" of vectors and an attention \"vector\", we also handle higher-order tensors. We always sum over the second-to-last dimension of the \"matrix\", and we assume that all dimensions in the \"matrix\" prior to the last dimension are matched in the \"vector\". Non-matched dimensions in the \"vector\" must be directly after the batch dimension . For example, say I have a \"matrix\" with dimensions (batch_size, num_queries, num_words, embedding_dim) . The attention \"vector\" then must have at least those dimensions, and could have more. Both: - `(batch_size, num_queries, num_words)` (distribution over words for each query) - `(batch_size, num_documents, num_queries, num_words)` (distribution over words in a query for each document) are valid input \"vectors\", producing tensors of shape: (batch_size, num_queries, embedding_dim) and (batch_size, num_documents, num_queries, embedding_dim) respectively. sequence_cross_entropy_with_logits # def sequence_cross_entropy_with_logits ( logits : torch . FloatTensor , targets : torch . LongTensor , weights : Union [ torch . FloatTensor , torch . BoolTensor ], average : str = \"batch\" , label_smoothing : float = None , gamma : float = None , alpha : Union [ float , List [ float ], torch . FloatTensor ] = None ) -> torch . FloatTensor Computes the cross entropy loss of a sequence, weighted with respect to some user provided weights. Note that the weighting here is not the same as in the torch.nn.CrossEntropyLoss() criterion, which is weighting classes; here we are weighting the loss contribution from particular elements in the sequence. This allows loss computations for models which use padding. Parameters logits : torch.FloatTensor A torch.FloatTensor of size (batch_size, sequence_length, num_classes) which contains the unnormalized probability for each class. targets : torch.LongTensor A torch.LongTensor of size (batch, sequence_length) which contains the index of the true class for each corresponding step. weights : Union[torch.FloatTensor, torch.BoolTensor] A torch.FloatTensor of size (batch, sequence_length) average : str , optional (default = \"batch\" ) If \"batch\", average the loss across the batches. If \"token\", average the loss across each item in the input. If None , return a vector of losses per batch element. label_smoothing : float , optional (default = None ) Whether or not to apply label smoothing to the cross-entropy loss. For example, with a label smoothing value of 0.2, a 4 class classification target would look like [0.05, 0.05, 0.85, 0.05] if the 3rd class was the correct label. gamma : float , optional (default = None ) Focal loss[*] focusing parameter gamma to reduces the relative loss for well-classified examples and put more focus on hard. The greater value gamma is, the more focus on hard examples. alpha : Union[float, List[float]] , optional (default = None ) Focal loss[ ] weighting factor alpha to balance between classes. Can be used independently with gamma . If a single float is provided, it is assumed binary case using alpha and 1 - alpha for positive and negative respectively. If a list of float is provided, with the same length as the number of classes, the weights will match the classes. [ ] T. Lin, P. Goyal, R. Girshick, K. He and P. Doll\u00e1r, \"Focal Loss for Dense Object Detection,\" 2017 IEEE International Conference on Computer Vision (ICCV), Venice, 2017, pp. 2999-3007. Returns torch.FloatTensor A torch.FloatTensor representing the cross entropy loss. If average==\"batch\" or average==\"token\" , the returned loss is a scalar. If average is None , the returned loss is a vector of shape (batch_size,). replace_masked_values # def replace_masked_values ( tensor : torch . Tensor , mask : torch . BoolTensor , replace_with : float ) -> torch . Tensor Replaces all masked values in tensor with replace_with . mask must be broadcastable to the same shape as tensor . We require that tensor.dim() == mask.dim() , as otherwise we won't know which dimensions of the mask to unsqueeze. This just does tensor.masked_fill() , except the pytorch method fills in things with a mask value of 1, where we want the opposite. You can do this in your own code with tensor.masked_fill(~mask, replace_with) . tensors_equal # def tensors_equal ( tensor1 : torch . Tensor , tensor2 : torch . Tensor , tolerance : float = 1e-12 ) -> bool A check for tensor equality (by value). We make sure that the tensors have the same shape, then check all of the entries in the tensor for equality. We additionally allow the input tensors to be lists or dictionaries, where we then do the above check on every position in the list / item in the dictionary. If we find objects that aren't tensors as we're doing that, we just defer to their equality check. This is kind of a catch-all method that's designed to make implementing __eq__ methods easier, in a way that's really only intended to be useful for tests. device_mapping # def device_mapping ( cuda_device : int ) In order to torch.load() a GPU-trained model onto a CPU (or specific GPU), you have to supply a map_location function. Call this with the desired cuda_device to get the function that torch.load() needs. combine_tensors # def combine_tensors ( combination : str , tensors : List [ torch . Tensor ] ) -> torch . Tensor Combines a list of tensors using element-wise operations and concatenation, specified by a combination string. The string refers to (1-indexed) positions in the input tensor list, and looks like \"1,2,1+2,3-1\" . We allow the following kinds of combinations : x , x*y , x+y , x-y , and x/y , where x and y are positive integers less than or equal to len(tensors) . Each of the binary operations is performed elementwise. You can give as many combinations as you want in the combination string. For example, for the input string \"1,2,1*2\" , the result would be [1;2;1*2] , as you would expect, where [;] is concatenation along the last dimension. If you have a fixed, known way to combine tensors that you use in a model, you should probably just use something like torch.cat([x_tensor, y_tensor, x_tensor * y_tensor]) . This function adds some complexity that is only necessary if you want the specific combination used to be configurable . If you want to do any element-wise operations, the tensors involved in each element-wise operation must have the same shape. This function also accepts x and y in place of 1 and 2 in the combination string. combine_tensors_and_multiply # def combine_tensors_and_multiply ( combination : str , tensors : List [ torch . Tensor ], weights : torch . nn . Parameter ) -> torch . Tensor Like combine_tensors , but does a weighted (linear) multiplication while combining. This is a separate function from combine_tensors because we try to avoid instantiating large intermediate tensors during the combination, which is possible because we know that we're going to be multiplying by a weight vector in the end. Parameters combination : str Same as in combine_tensors tensors : List[torch.Tensor] A list of tensors to combine, where the integers in the combination are (1-indexed) positions in this list of tensors. These tensors are all expected to have either three or four dimensions, with the final dimension being an embedding. If there are four dimensions, one of them must have length 1. weights : torch.nn.Parameter A vector of weights to use for the combinations. This should have shape (combined_dim,), as calculated by get_combined_dim . get_combined_dim # def get_combined_dim ( combination : str , tensor_dims : List [ int ]) -> int For use with combine_tensors . This function computes the resultant dimension when calling combine_tensors(combination, tensors) , when the tensor dimension is known. This is necessary for knowing the sizes of weight matrices when building models that use combine_tensors . Parameters combination : str A comma-separated list of combination pieces, like \"1,2,1*2\" , specified identically to combination in combine_tensors . tensor_dims : List[int] A list of tensor dimensions, where each dimension is from the last axis of the tensors that will be input to combine_tensors . logsumexp # def logsumexp ( tensor : torch . Tensor , dim : int = - 1 , keepdim : bool = False ) -> torch . Tensor A numerically stable computation of logsumexp. This is mathematically equivalent to tensor.exp().sum(dim, keep=keepdim).log() . This function is typically used for summing log probabilities. Parameters tensor : torch.FloatTensor A tensor of arbitrary size. dim : int , optional (default = -1 ) The dimension of the tensor to apply the logsumexp to. keepdim : bool , optional (default = False ) Whether to retain a dimension of size one at the dimension we reduce over. get_device_of # def get_device_of ( tensor : torch . Tensor ) -> int Returns the device of the tensor. flatten_and_batch_shift_indices # def flatten_and_batch_shift_indices ( indices : torch . Tensor , sequence_length : int ) -> torch . Tensor This is a subroutine for batched_index_select . The given indices of size (batch_size, d_1, ..., d_n) indexes into dimension 2 of a target tensor, which has size (batch_size, sequence_length, embedding_size) . This function returns a vector that correctly indexes into the flattened target. The sequence length of the target must be provided to compute the appropriate offsets. indices = torch . ones ([ 2 , 3 ], dtype = torch . long ) # Sequence length of the target tensor. sequence_length = 10 shifted_indices = flatten_and_batch_shift_indices ( indices , sequence_length ) # Indices into the second element in the batch are correctly shifted # to take into account that the target tensor will be flattened before # the indices are applied. assert shifted_indices == [ 1 , 1 , 1 , 11 , 11 , 11 ] Parameters indices : torch.LongTensor sequence_length : int The length of the sequence the indices index into. This must be the second dimension of the tensor. Returns offset_indices : torch.LongTensor batched_index_select # def batched_index_select ( target : torch . Tensor , indices : torch . LongTensor , flattened_indices : Optional [ torch . LongTensor ] = None ) -> torch . Tensor The given indices of size (batch_size, d_1, ..., d_n) indexes into the sequence dimension (dimension 2) of the target, which has size (batch_size, sequence_length, embedding_size) . This function returns selected values in the target with respect to the provided indices, which have size (batch_size, d_1, ..., d_n, embedding_size) . This can use the optionally precomputed flattened_indices with size (batch_size * d_1 * ... * d_n) if given. An example use case of this function is looking up the start and end indices of spans in a sequence tensor. This is used in the CoreferenceResolver . Model to select contextual word representations corresponding to the start and end indices of mentions. The key reason this can't be done with basic torch functions is that we want to be able to use look-up tensors with an arbitrary number of dimensions (for example, in the coref model, we don't know a-priori how many spans we are looking up). Parameters target : torch.Tensor A 3 dimensional tensor of shape (batch_size, sequence_length, embedding_size). This is the tensor to be indexed. indices : torch.LongTensor A tensor of shape (batch_size, ...), where each element is an index into the sequence_length dimension of the target tensor. flattened_indices : Optional[torch.Tensor] , optional (default = None ) An optional tensor representing the result of calling flatten_and_batch_shift_indices on indices . This is helpful in the case that the indices can be flattened once and cached for many batch lookups. Returns selected_targets : torch.Tensor A tensor with shape [indices.size(), target.size(-1)] representing the embedded indices extracted from the batch flattened target tensor. batched_span_select # def batched_span_select ( target : torch . Tensor , spans : torch . LongTensor ) -> torch . Tensor The given spans of size (batch_size, num_spans, 2) indexes into the sequence dimension (dimension 2) of the target, which has size (batch_size, sequence_length, embedding_size) . This function returns segmented spans in the target with respect to the provided span indices. It does not guarantee element order within each span. Parameters target : torch.Tensor A 3 dimensional tensor of shape (batch_size, sequence_length, embedding_size). This is the tensor to be indexed. indices : torch.LongTensor A 3 dimensional tensor of shape (batch_size, num_spans, 2) representing start and end indices (both inclusive) into the sequence_length dimension of the target tensor. Returns span_embeddings : torch.Tensor A tensor with shape (batch_size, num_spans, max_batch_span_width, embedding_size] representing the embedded spans extracted from the batch flattened target tensor. span_mask : torch.BoolTensor A tensor with shape (batch_size, num_spans, max_batch_span_width) representing the mask on the returned span embeddings. flattened_index_select # def flattened_index_select ( target : torch . Tensor , indices : torch . LongTensor ) -> torch . Tensor The given indices of size (set_size, subset_size) specifies subsets of the target that each of the set_size rows should select. The target has size (batch_size, sequence_length, embedding_size) , and the resulting selected tensor has size (batch_size, set_size, subset_size, embedding_size) . Parameters target : torch.Tensor A Tensor of shape (batch_size, sequence_length, embedding_size). indices : torch.LongTensor A LongTensor of shape (set_size, subset_size). All indices must be < sequence_length as this tensor is an index into the sequence_length dimension of the target. Returns selected : torch.Tensor , required. A Tensor of shape (batch_size, set_size, subset_size, embedding_size). get_range_vector # def get_range_vector ( size : int , device : int ) -> torch . Tensor Returns a range vector with the desired size, starting at 0. The CUDA implementation is meant to avoid copy data from CPU to GPU. bucket_values # def bucket_values ( distances : torch . Tensor , num_identity_buckets : int = 4 , num_total_buckets : int = 10 ) -> torch . Tensor Places the given values (designed for distances) into num_total_buckets semi-logscale buckets, with num_identity_buckets of these capturing single values. The default settings will bucket values into the following buckets: [0, 1, 2, 3, 4, 5-7, 8-15, 16-31, 32-63, 64+]. Parameters distances : torch.Tensor A Tensor of any size, to be bucketed. num_identity_buckets : int , optional (default = 4 ) The number of identity buckets (those only holding a single value). num_total_buckets : int , optional (default = 10 ) The total number of buckets to bucket values into. Returns torch.Tensor A tensor of the same shape as the input, containing the indices of the buckets the values were placed in. add_sentence_boundary_token_ids # def add_sentence_boundary_token_ids ( tensor : torch . Tensor , mask : torch . BoolTensor , sentence_begin_token : Any , sentence_end_token : Any ) -> Tuple [ torch . Tensor , torch . BoolTensor ] Add begin/end of sentence tokens to the batch of sentences. Given a batch of sentences with size (batch_size, timesteps) or (batch_size, timesteps, dim) this returns a tensor of shape (batch_size, timesteps + 2) or (batch_size, timesteps + 2, dim) respectively. Returns both the new tensor and updated mask. Parameters tensor : torch.Tensor A tensor of shape (batch_size, timesteps) or (batch_size, timesteps, dim) mask : torch.BoolTensor A tensor of shape (batch_size, timesteps) sentence_begin_token : Any Can be anything that can be broadcast in torch for assignment. For 2D input, a scalar with the <S> id. For 3D input, a tensor with length dim. sentence_end_token : Any Can be anything that can be broadcast in torch for assignment. For 2D input, a scalar with the </S> id. For 3D input, a tensor with length dim. Returns tensor_with_boundary_tokens : torch.Tensor The tensor with the appended and prepended boundary tokens. If the input was 2D, it has shape (batch_size, timesteps + 2) and if the input was 3D, it has shape (batch_size, timesteps + 2, dim). new_mask : torch.BoolTensor The new mask for the tensor, taking into account the appended tokens marking the beginning and end of the sentence. remove_sentence_boundaries # def remove_sentence_boundaries ( tensor : torch . Tensor , mask : torch . BoolTensor ) -> Tuple [ torch . Tensor , torch . Tensor ] Remove begin/end of sentence embeddings from the batch of sentences. Given a batch of sentences with size (batch_size, timesteps, dim) this returns a tensor of shape (batch_size, timesteps - 2, dim) after removing the beginning and end sentence markers. The sentences are assumed to be padded on the right, with the beginning of each sentence assumed to occur at index 0 (i.e., mask[:, 0] is assumed to be 1). Returns both the new tensor and updated mask. This function is the inverse of add_sentence_boundary_token_ids . Parameters tensor : torch.Tensor A tensor of shape (batch_size, timesteps, dim) mask : torch.BoolTensor A tensor of shape (batch_size, timesteps) Returns tensor_without_boundary_tokens : torch.Tensor The tensor after removing the boundary tokens of shape (batch_size, timesteps - 2, dim) new_mask : torch.BoolTensor The new mask for the tensor of shape (batch_size, timesteps - 2) . add_positional_features # def add_positional_features ( tensor : torch . Tensor , min_timescale : float = 1.0 , max_timescale : float = 1.0e4 ) Implements the frequency-based positional encoding described in Attention is All you Need . Adds sinusoids of different frequencies to a Tensor . A sinusoid of a different frequency and phase is added to each dimension of the input Tensor . This allows the attention heads to use absolute and relative positions. The number of timescales is equal to hidden_dim / 2 within the range (min_timescale, max_timescale). For each timescale, the two sinusoidal signals sin(timestep / timescale) and cos(timestep / timescale) are generated and concatenated along the hidden_dim dimension. Parameters tensor : torch.Tensor a Tensor with shape (batch_size, timesteps, hidden_dim). min_timescale : float , optional (default = 1.0 ) The smallest timescale to use. max_timescale : float , optional (default = 1.0e4 ) The largest timescale to use. Returns torch.Tensor The input tensor augmented with the sinusoidal frequencies. clone # def clone ( module : torch . nn . Module , num_copies : int ) -> torch . nn . ModuleList Produce N identical layers. combine_initial_dims # def combine_initial_dims ( tensor : torch . Tensor ) -> torch . Tensor Given a (possibly higher order) tensor of ids with shape (d1, ..., dn, sequence_length) Return a view that's (d1 * ... * dn, sequence_length). If original tensor is 1-d or 2-d, return it as is. uncombine_initial_dims # def uncombine_initial_dims ( tensor : torch . Tensor , original_size : torch . Size ) -> torch . Tensor Given a tensor of embeddings with shape (d1 * ... * dn, sequence_length, embedding_dim) and the original shape (d1, ..., dn, sequence_length), return the reshaped tensor of embeddings with shape (d1, ..., dn, sequence_length, embedding_dim). If original size is 1-d or 2-d, return it as is. inspect_parameters # def inspect_parameters ( module : torch . nn . Module , quiet : bool = False ) -> Dict [ str , Any ] Inspects the model/module parameters and their tunability. The output is structured in a nested dict so that parameters in same sub-modules are grouped together. This can be helpful to setup module path based regex, for example in initializer. It prints it by default (optional) and returns the inspection dict. Eg. output:: { \"_text_field_embedder\": { \"token_embedder_tokens\": { \"_projection\": { \"bias\": \"tunable\", \"weight\": \"tunable\" }, \"weight\": \"frozen\" } } } find_embedding_layer # def find_embedding_layer ( model : torch . nn . Module ) -> torch . nn . Module Takes a model (typically an AllenNLP Model , but this works for any torch.nn.Module ) and makes a best guess about which module is the embedding layer. For typical AllenNLP models, this often is the TextFieldEmbedder , but if you're using a pre-trained contextualizer, we really want layer 0 of that contextualizer, not the output. So there are a bunch of hacks in here for specific pre-trained contextualizers. extend_layer # def extend_layer ( layer : torch . nn . Module , new_dim : int ) -> None masked_topk # def masked_topk ( input_ : torch . FloatTensor , mask : torch . BoolTensor , k : Union [ int , torch . LongTensor ], dim : int = - 1 ) -> Tuple [ torch . LongTensor , torch . LongTensor , torch . FloatTensor ] Extracts the top-k items along a certain dimension. This is similar to torch.topk except: (1) we allow of a mask that makes the function not consider certain elements; (2) the returned top input, mask, and indices are sorted in their original order in the input; (3) May use the same k for all dimensions, or different k for each. Parameters input_ : torch.FloatTensor A tensor containing the items that we want to prune. mask : torch.BoolTensor A tensor with the same shape as input_ that makes the function not consider masked out (i.e. False) elements. k : Union[int, torch.LongTensor] If a tensor of shape as input_ except without dimension dim , specifies the number of items to keep for each dimension. If an int, keep the same number of items for all dimensions. Returns top_input : torch.FloatTensor The values of the top-k scoring items. Has the same shape as input_ except dimension dim has value k when it's an int or k.max() when it's a tensor. top_mask : torch.BoolTensor The corresponding mask for top_input . Has the shape as top_input . top_indices : torch.IntTensor The indices of the top-k scoring items into the original input_ tensor. This is returned because it can be useful to retain pointers to the original items, if each item is being scored by multiple distinct scorers, for instance. Has the shape as top_input . info_value_of_dtype # def info_value_of_dtype ( dtype : torch . dtype ) Returns the finfo or iinfo object of a given PyTorch data type. Does not allow torch.bool. min_value_of_dtype # def min_value_of_dtype ( dtype : torch . dtype ) Returns the minimum value of a given PyTorch data type. Does not allow torch.bool. max_value_of_dtype # def max_value_of_dtype ( dtype : torch . dtype ) Returns the maximum value of a given PyTorch data type. Does not allow torch.bool. tiny_value_of_dtype # def tiny_value_of_dtype ( dtype : torch . dtype ) Returns a moderately tiny value for a given PyTorch data type that is used to avoid numerical issues such as division by zero. This is different from info_value_of_dtype(dtype).tiny because it causes some NaN bugs. Only supports floating point dtypes.","title":"util"},{"location":"api/nn/util/#t","text":"T = TypeVar ( \"T\" )","title":"T"},{"location":"api/nn/util/#has_tensor","text":"def has_tensor ( obj ) -> bool Given a possibly complex data structure, check if it has any torch.Tensors in it.","title":"has_tensor"},{"location":"api/nn/util/#move_to_device","text":"def move_to_device ( obj , cuda_device : Union [ torch . device , int ]) Given a structure (possibly) containing Tensors on the CPU, move all the Tensors to the specified GPU (or do nothing, if they should be on the CPU).","title":"move_to_device"},{"location":"api/nn/util/#clamp_tensor","text":"def clamp_tensor ( tensor , minimum , maximum ) Supports sparse and dense tensors. Returns a tensor with values clamped between the provided minimum and maximum, without modifying the original tensor.","title":"clamp_tensor"},{"location":"api/nn/util/#batch_tensor_dicts","text":"def batch_tensor_dicts ( tensor_dicts : List [ Dict [ str , torch . Tensor ]], remove_trailing_dimension : bool = False ) -> Dict [ str , torch . Tensor ] Takes a list of tensor dictionaries, where each dictionary is assumed to have matching keys, and returns a single dictionary with all tensors with the same key batched together. Parameters tensor_dicts : List[Dict[str, torch.Tensor]] The list of tensor dictionaries to batch. remove_trailing_dimension : bool If True , we will check for a trailing dimension of size 1 on the tensors that are being batched, and remove it if we find it.","title":"batch_tensor_dicts"},{"location":"api/nn/util/#get_lengths_from_binary_sequence_mask","text":"def get_lengths_from_binary_sequence_mask ( mask : torch . BoolTensor ) -> torch . LongTensor Compute sequence lengths for each batch element in a tensor using a binary mask. Parameters mask : torch.BoolTensor A 2D binary mask of shape (batch_size, sequence_length) to calculate the per-batch sequence lengths from. Returns torch.LongTensor A torch.LongTensor of shape (batch_size,) representing the lengths of the sequences in the batch.","title":"get_lengths_from_binary_sequence_mask"},{"location":"api/nn/util/#get_mask_from_sequence_lengths","text":"def get_mask_from_sequence_lengths ( sequence_lengths : torch . Tensor , max_length : int ) -> torch . BoolTensor Given a variable of shape (batch_size,) that represents the sequence lengths of each batch element, this function returns a (batch_size, max_length) mask variable. For example, if our input was [2, 2, 3] , with a max_length of 4, we'd return [[1, 1, 0, 0], [1, 1, 0, 0], [1, 1, 1, 0]] . We require max_length here instead of just computing it from the input sequence_lengths because it lets us avoid finding the max, then copying that value from the GPU to the CPU so that we can use it to construct a new tensor.","title":"get_mask_from_sequence_lengths"},{"location":"api/nn/util/#sort_batch_by_length","text":"def sort_batch_by_length ( tensor : torch . Tensor , sequence_lengths : torch . Tensor ) Sort a batch first tensor by some specified lengths. Parameters tensor : torch.FloatTensor A batch first Pytorch tensor. sequence_lengths : torch.LongTensor A tensor representing the lengths of some dimension of the tensor which we want to sort by. Returns sorted_tensor : torch.FloatTensor The original tensor sorted along the batch dimension with respect to sequence_lengths. sorted_sequence_lengths : torch.LongTensor The original sequence_lengths sorted by decreasing size. restoration_indices : torch.LongTensor Indices into the sorted_tensor such that sorted_tensor.index_select(0, restoration_indices) == original_tensor permutation_index : torch.LongTensor The indices used to sort the tensor. This is useful if you want to sort many tensors using the same ordering.","title":"sort_batch_by_length"},{"location":"api/nn/util/#get_final_encoder_states","text":"def get_final_encoder_states ( encoder_outputs : torch . Tensor , mask : torch . BoolTensor , bidirectional : bool = False ) -> torch . Tensor Given the output from a Seq2SeqEncoder , with shape (batch_size, sequence_length, encoding_dim) , this method returns the final hidden state for each element of the batch, giving a tensor of shape (batch_size, encoding_dim) . This is not as simple as encoder_outputs[:, -1] , because the sequences could have different lengths. We use the mask (which has shape (batch_size, sequence_length) ) to find the final state for each batch instance. Additionally, if bidirectional is True , we will split the final dimension of the encoder_outputs into two and assume that the first half is for the forward direction of the encoder and the second half is for the backward direction. We will concatenate the last state for each encoder dimension, giving encoder_outputs[:, -1, :encoding_dim/2] concatenated with encoder_outputs[:, 0, encoding_dim/2:] .","title":"get_final_encoder_states"},{"location":"api/nn/util/#get_dropout_mask","text":"def get_dropout_mask ( dropout_probability : float , tensor_for_masking : torch . Tensor ) Computes and returns an element-wise dropout mask for a given tensor, where each element in the mask is dropped out with probability dropout_probability. Note that the mask is NOT applied to the tensor - the tensor is passed to retain the correct CUDA tensor type for the mask. Parameters dropout_probability : float Probability of dropping a dimension of the input. tensor_for_masking : torch.Tensor Returns torch.FloatTensor A torch.FloatTensor consisting of the binary mask scaled by 1/ (1 - dropout_probability). This scaling ensures expected values and variances of the output of applying this mask and the original tensor are the same.","title":"get_dropout_mask"},{"location":"api/nn/util/#masked_softmax","text":"def masked_softmax ( vector : torch . Tensor , mask : torch . BoolTensor , dim : int = - 1 , memory_efficient : bool = False ) -> torch . Tensor torch.nn.functional.softmax(vector) does not work if some elements of vector should be masked. This performs a softmax on just the non-masked portions of vector . Passing None in for the mask is also acceptable; you'll just get a regular softmax. vector can have an arbitrary number of dimensions; the only requirement is that mask is broadcastable to vector's shape. If mask has fewer dimensions than vector , we will unsqueeze on dimension 1 until they match. If you need a different unsqueezing of your mask, do it yourself before passing the mask into this function. If memory_efficient is set to true, we will simply use a very large negative number for those masked positions so that the probabilities of those positions would be approximately 0. This is not accurate in math, but works for most cases and consumes less memory. In the case that the input vector is completely masked and memory_efficient is false, this function returns an array of 0.0 . This behavior may cause NaN if this is used as the last layer of a model that uses categorical cross-entropy loss. Instead, if memory_efficient is true, this function will treat every element as equal, and do softmax over equal numbers.","title":"masked_softmax"},{"location":"api/nn/util/#masked_log_softmax","text":"def masked_log_softmax ( vector : torch . Tensor , mask : torch . BoolTensor , dim : int = - 1 ) -> torch . Tensor torch.nn.functional.log_softmax(vector) does not work if some elements of vector should be masked. This performs a log_softmax on just the non-masked portions of vector . Passing None in for the mask is also acceptable; you'll just get a regular log_softmax. vector can have an arbitrary number of dimensions; the only requirement is that mask is broadcastable to vector's shape. If mask has fewer dimensions than vector , we will unsqueeze on dimension 1 until they match. If you need a different unsqueezing of your mask, do it yourself before passing the mask into this function. In the case that the input vector is completely masked, the return value of this function is arbitrary, but not nan . You should be masking the result of whatever computation comes out of this in that case, anyway, so the specific values returned shouldn't matter. Also, the way that we deal with this case relies on having single-precision floats; mixing half-precision floats with fully-masked vectors will likely give you nans . If your logits are all extremely negative (i.e., the max value in your logit vector is -50 or lower), the way we handle masking here could mess you up. But if you've got logit values that extreme, you've got bigger problems than this.","title":"masked_log_softmax"},{"location":"api/nn/util/#masked_max","text":"def masked_max ( vector : torch . Tensor , mask : torch . BoolTensor , dim : int , keepdim : bool = False ) -> torch . Tensor To calculate max along certain dimensions on masked values Parameters vector : torch.Tensor The vector to calculate max, assume unmasked parts are already zeros mask : torch.BoolTensor The mask of the vector. It must be broadcastable with vector. dim : int The dimension to calculate max keepdim : bool Whether to keep dimension Returns torch.Tensor A torch.Tensor of including the maximum values.","title":"masked_max"},{"location":"api/nn/util/#masked_mean","text":"def masked_mean ( vector : torch . Tensor , mask : torch . BoolTensor , dim : int , keepdim : bool = False ) -> torch . Tensor To calculate mean along certain dimensions on masked values Parameters vector : torch.Tensor The vector to calculate mean. mask : torch.BoolTensor The mask of the vector. It must be broadcastable with vector. dim : int The dimension to calculate mean keepdim : bool Whether to keep dimension Returns torch.Tensor A torch.Tensor of including the mean values.","title":"masked_mean"},{"location":"api/nn/util/#masked_flip","text":"def masked_flip ( padded_sequence : torch . Tensor , sequence_lengths : List [ int ] ) -> torch . Tensor Flips a padded tensor along the time dimension without affecting masked entries. Parameters padded_sequence : torch.Tensor The tensor to flip along the time dimension. Assumed to be of dimensions (batch size, num timesteps, ...) sequence_lengths : torch.Tensor A list containing the lengths of each unpadded sequence in the batch. Returns torch.Tensor A torch.Tensor of the same shape as padded_sequence.","title":"masked_flip"},{"location":"api/nn/util/#viterbi_decode","text":"def viterbi_decode ( tag_sequence : torch . Tensor , transition_matrix : torch . Tensor , tag_observations : Optional [ List [ int ]] = None , allowed_start_transitions : torch . Tensor = None , allowed_end_transitions : torch . Tensor = None , top_k : int = None ) Perform Viterbi decoding in log space over a sequence given a transition matrix specifying pairwise (transition) potentials between tags and a matrix of shape (sequence_length, num_tags) specifying unary potentials for possible tags per timestep. Parameters tag_sequence : torch.Tensor A tensor of shape (sequence_length, num_tags) representing scores for a set of tags over a given sequence. transition_matrix : torch.Tensor A tensor of shape (num_tags, num_tags) representing the binary potentials for transitioning between a given pair of tags. tag_observations : Optional[List[int]] , optional (default = None ) A list of length sequence_length containing the class ids of observed elements in the sequence, with unobserved elements being set to -1. Note that it is possible to provide evidence which results in degenerate labelings if the sequences of tags you provide as evidence cannot transition between each other, or those transitions are extremely unlikely. In this situation we log a warning, but the responsibility for providing self-consistent evidence ultimately lies with the user. allowed_start_transitions : torch.Tensor , optional (default = None ) An optional tensor of shape (num_tags,) describing which tags the START token may transition to . If provided, additional transition constraints will be used for determining the start element of the sequence. allowed_end_transitions : torch.Tensor , optional (default = None ) An optional tensor of shape (num_tags,) describing which tags may transition to the end tag. If provided, additional transition constraints will be used for determining the end element of the sequence. top_k : int , optional (default = None ) Optional integer specifying how many of the top paths to return. For top_k>=1, returns a tuple of two lists: top_k_paths, top_k_scores, For top_k==None, returns a flattened tuple with just the top path and its score (not in lists, for backwards compatibility). Returns viterbi_path : List[int] The tag indices of the maximum likelihood tag sequence. viterbi_score : torch.Tensor The score of the viterbi path.","title":"viterbi_decode"},{"location":"api/nn/util/#get_text_field_mask","text":"def get_text_field_mask ( text_field_tensors : Dict [ str , Dict [ str , torch . Tensor ]], num_wrapping_dims : int = 0 , padding_id : int = 0 ) -> torch . BoolTensor Takes the dictionary of tensors produced by a TextField and returns a mask with 0 where the tokens are padding, and 1 otherwise. padding_id specifies the id of padding tokens. We also handle TextFields wrapped by an arbitrary number of ListFields , where the number of wrapping ListFields is given by num_wrapping_dims . If num_wrapping_dims == 0 , the returned mask has shape (batch_size, num_tokens) . If num_wrapping_dims > 0 then the returned mask has num_wrapping_dims extra dimensions, so the shape will be (batch_size, ..., num_tokens) . There could be several entries in the tensor dictionary with different shapes (e.g., one for word ids, one for character ids). In order to get a token mask, we use the tensor in the dictionary with the lowest number of dimensions. After subtracting num_wrapping_dims , if this tensor has two dimensions we assume it has shape (batch_size, ..., num_tokens) , and use it for the mask. If instead it has three dimensions, we assume it has shape (batch_size, ..., num_tokens, num_features) , and sum over the last dimension to produce the mask. Most frequently this will be a character id tensor, but it could also be a featurized representation of each token, etc. If the input text_field_tensors contains the \"mask\" key, this is returned instead of inferring the mask.","title":"get_text_field_mask"},{"location":"api/nn/util/#get_token_ids_from_text_field_tensors","text":"def get_token_ids_from_text_field_tensors ( text_field_tensors : Dict [ str , Dict [ str , torch . Tensor ]] ) -> torch . Tensor Our TextFieldTensors are complex output structures, because they try to handle a lot of potential variation. Sometimes, you just want to grab the token ids from this data structure, and that's not trivial without hard-coding assumptions about your data processing, which defeats the entire purpose of that generality. This method tries to let you get the token ids out of the data structure in your model without hard-coding any assumptions.","title":"get_token_ids_from_text_field_tensors"},{"location":"api/nn/util/#weighted_sum","text":"def weighted_sum ( matrix : torch . Tensor , attention : torch . Tensor ) -> torch . Tensor Takes a matrix of vectors and a set of weights over the rows in the matrix (which we call an \"attention\" vector), and returns a weighted sum of the rows in the matrix. This is the typical computation performed after an attention mechanism. Note that while we call this a \"matrix\" of vectors and an attention \"vector\", we also handle higher-order tensors. We always sum over the second-to-last dimension of the \"matrix\", and we assume that all dimensions in the \"matrix\" prior to the last dimension are matched in the \"vector\". Non-matched dimensions in the \"vector\" must be directly after the batch dimension . For example, say I have a \"matrix\" with dimensions (batch_size, num_queries, num_words, embedding_dim) . The attention \"vector\" then must have at least those dimensions, and could have more. Both: - `(batch_size, num_queries, num_words)` (distribution over words for each query) - `(batch_size, num_documents, num_queries, num_words)` (distribution over words in a query for each document) are valid input \"vectors\", producing tensors of shape: (batch_size, num_queries, embedding_dim) and (batch_size, num_documents, num_queries, embedding_dim) respectively.","title":"weighted_sum"},{"location":"api/nn/util/#sequence_cross_entropy_with_logits","text":"def sequence_cross_entropy_with_logits ( logits : torch . FloatTensor , targets : torch . LongTensor , weights : Union [ torch . FloatTensor , torch . BoolTensor ], average : str = \"batch\" , label_smoothing : float = None , gamma : float = None , alpha : Union [ float , List [ float ], torch . FloatTensor ] = None ) -> torch . FloatTensor Computes the cross entropy loss of a sequence, weighted with respect to some user provided weights. Note that the weighting here is not the same as in the torch.nn.CrossEntropyLoss() criterion, which is weighting classes; here we are weighting the loss contribution from particular elements in the sequence. This allows loss computations for models which use padding. Parameters logits : torch.FloatTensor A torch.FloatTensor of size (batch_size, sequence_length, num_classes) which contains the unnormalized probability for each class. targets : torch.LongTensor A torch.LongTensor of size (batch, sequence_length) which contains the index of the true class for each corresponding step. weights : Union[torch.FloatTensor, torch.BoolTensor] A torch.FloatTensor of size (batch, sequence_length) average : str , optional (default = \"batch\" ) If \"batch\", average the loss across the batches. If \"token\", average the loss across each item in the input. If None , return a vector of losses per batch element. label_smoothing : float , optional (default = None ) Whether or not to apply label smoothing to the cross-entropy loss. For example, with a label smoothing value of 0.2, a 4 class classification target would look like [0.05, 0.05, 0.85, 0.05] if the 3rd class was the correct label. gamma : float , optional (default = None ) Focal loss[*] focusing parameter gamma to reduces the relative loss for well-classified examples and put more focus on hard. The greater value gamma is, the more focus on hard examples. alpha : Union[float, List[float]] , optional (default = None ) Focal loss[ ] weighting factor alpha to balance between classes. Can be used independently with gamma . If a single float is provided, it is assumed binary case using alpha and 1 - alpha for positive and negative respectively. If a list of float is provided, with the same length as the number of classes, the weights will match the classes. [ ] T. Lin, P. Goyal, R. Girshick, K. He and P. Doll\u00e1r, \"Focal Loss for Dense Object Detection,\" 2017 IEEE International Conference on Computer Vision (ICCV), Venice, 2017, pp. 2999-3007. Returns torch.FloatTensor A torch.FloatTensor representing the cross entropy loss. If average==\"batch\" or average==\"token\" , the returned loss is a scalar. If average is None , the returned loss is a vector of shape (batch_size,).","title":"sequence_cross_entropy_with_logits"},{"location":"api/nn/util/#replace_masked_values","text":"def replace_masked_values ( tensor : torch . Tensor , mask : torch . BoolTensor , replace_with : float ) -> torch . Tensor Replaces all masked values in tensor with replace_with . mask must be broadcastable to the same shape as tensor . We require that tensor.dim() == mask.dim() , as otherwise we won't know which dimensions of the mask to unsqueeze. This just does tensor.masked_fill() , except the pytorch method fills in things with a mask value of 1, where we want the opposite. You can do this in your own code with tensor.masked_fill(~mask, replace_with) .","title":"replace_masked_values"},{"location":"api/nn/util/#tensors_equal","text":"def tensors_equal ( tensor1 : torch . Tensor , tensor2 : torch . Tensor , tolerance : float = 1e-12 ) -> bool A check for tensor equality (by value). We make sure that the tensors have the same shape, then check all of the entries in the tensor for equality. We additionally allow the input tensors to be lists or dictionaries, where we then do the above check on every position in the list / item in the dictionary. If we find objects that aren't tensors as we're doing that, we just defer to their equality check. This is kind of a catch-all method that's designed to make implementing __eq__ methods easier, in a way that's really only intended to be useful for tests.","title":"tensors_equal"},{"location":"api/nn/util/#device_mapping","text":"def device_mapping ( cuda_device : int ) In order to torch.load() a GPU-trained model onto a CPU (or specific GPU), you have to supply a map_location function. Call this with the desired cuda_device to get the function that torch.load() needs.","title":"device_mapping"},{"location":"api/nn/util/#combine_tensors","text":"def combine_tensors ( combination : str , tensors : List [ torch . Tensor ] ) -> torch . Tensor Combines a list of tensors using element-wise operations and concatenation, specified by a combination string. The string refers to (1-indexed) positions in the input tensor list, and looks like \"1,2,1+2,3-1\" . We allow the following kinds of combinations : x , x*y , x+y , x-y , and x/y , where x and y are positive integers less than or equal to len(tensors) . Each of the binary operations is performed elementwise. You can give as many combinations as you want in the combination string. For example, for the input string \"1,2,1*2\" , the result would be [1;2;1*2] , as you would expect, where [;] is concatenation along the last dimension. If you have a fixed, known way to combine tensors that you use in a model, you should probably just use something like torch.cat([x_tensor, y_tensor, x_tensor * y_tensor]) . This function adds some complexity that is only necessary if you want the specific combination used to be configurable . If you want to do any element-wise operations, the tensors involved in each element-wise operation must have the same shape. This function also accepts x and y in place of 1 and 2 in the combination string.","title":"combine_tensors"},{"location":"api/nn/util/#combine_tensors_and_multiply","text":"def combine_tensors_and_multiply ( combination : str , tensors : List [ torch . Tensor ], weights : torch . nn . Parameter ) -> torch . Tensor Like combine_tensors , but does a weighted (linear) multiplication while combining. This is a separate function from combine_tensors because we try to avoid instantiating large intermediate tensors during the combination, which is possible because we know that we're going to be multiplying by a weight vector in the end. Parameters combination : str Same as in combine_tensors tensors : List[torch.Tensor] A list of tensors to combine, where the integers in the combination are (1-indexed) positions in this list of tensors. These tensors are all expected to have either three or four dimensions, with the final dimension being an embedding. If there are four dimensions, one of them must have length 1. weights : torch.nn.Parameter A vector of weights to use for the combinations. This should have shape (combined_dim,), as calculated by get_combined_dim .","title":"combine_tensors_and_multiply"},{"location":"api/nn/util/#get_combined_dim","text":"def get_combined_dim ( combination : str , tensor_dims : List [ int ]) -> int For use with combine_tensors . This function computes the resultant dimension when calling combine_tensors(combination, tensors) , when the tensor dimension is known. This is necessary for knowing the sizes of weight matrices when building models that use combine_tensors . Parameters combination : str A comma-separated list of combination pieces, like \"1,2,1*2\" , specified identically to combination in combine_tensors . tensor_dims : List[int] A list of tensor dimensions, where each dimension is from the last axis of the tensors that will be input to combine_tensors .","title":"get_combined_dim"},{"location":"api/nn/util/#logsumexp","text":"def logsumexp ( tensor : torch . Tensor , dim : int = - 1 , keepdim : bool = False ) -> torch . Tensor A numerically stable computation of logsumexp. This is mathematically equivalent to tensor.exp().sum(dim, keep=keepdim).log() . This function is typically used for summing log probabilities. Parameters tensor : torch.FloatTensor A tensor of arbitrary size. dim : int , optional (default = -1 ) The dimension of the tensor to apply the logsumexp to. keepdim : bool , optional (default = False ) Whether to retain a dimension of size one at the dimension we reduce over.","title":"logsumexp"},{"location":"api/nn/util/#get_device_of","text":"def get_device_of ( tensor : torch . Tensor ) -> int Returns the device of the tensor.","title":"get_device_of"},{"location":"api/nn/util/#flatten_and_batch_shift_indices","text":"def flatten_and_batch_shift_indices ( indices : torch . Tensor , sequence_length : int ) -> torch . Tensor This is a subroutine for batched_index_select . The given indices of size (batch_size, d_1, ..., d_n) indexes into dimension 2 of a target tensor, which has size (batch_size, sequence_length, embedding_size) . This function returns a vector that correctly indexes into the flattened target. The sequence length of the target must be provided to compute the appropriate offsets. indices = torch . ones ([ 2 , 3 ], dtype = torch . long ) # Sequence length of the target tensor. sequence_length = 10 shifted_indices = flatten_and_batch_shift_indices ( indices , sequence_length ) # Indices into the second element in the batch are correctly shifted # to take into account that the target tensor will be flattened before # the indices are applied. assert shifted_indices == [ 1 , 1 , 1 , 11 , 11 , 11 ] Parameters indices : torch.LongTensor sequence_length : int The length of the sequence the indices index into. This must be the second dimension of the tensor. Returns offset_indices : torch.LongTensor","title":"flatten_and_batch_shift_indices"},{"location":"api/nn/util/#batched_index_select","text":"def batched_index_select ( target : torch . Tensor , indices : torch . LongTensor , flattened_indices : Optional [ torch . LongTensor ] = None ) -> torch . Tensor The given indices of size (batch_size, d_1, ..., d_n) indexes into the sequence dimension (dimension 2) of the target, which has size (batch_size, sequence_length, embedding_size) . This function returns selected values in the target with respect to the provided indices, which have size (batch_size, d_1, ..., d_n, embedding_size) . This can use the optionally precomputed flattened_indices with size (batch_size * d_1 * ... * d_n) if given. An example use case of this function is looking up the start and end indices of spans in a sequence tensor. This is used in the CoreferenceResolver . Model to select contextual word representations corresponding to the start and end indices of mentions. The key reason this can't be done with basic torch functions is that we want to be able to use look-up tensors with an arbitrary number of dimensions (for example, in the coref model, we don't know a-priori how many spans we are looking up). Parameters target : torch.Tensor A 3 dimensional tensor of shape (batch_size, sequence_length, embedding_size). This is the tensor to be indexed. indices : torch.LongTensor A tensor of shape (batch_size, ...), where each element is an index into the sequence_length dimension of the target tensor. flattened_indices : Optional[torch.Tensor] , optional (default = None ) An optional tensor representing the result of calling flatten_and_batch_shift_indices on indices . This is helpful in the case that the indices can be flattened once and cached for many batch lookups. Returns selected_targets : torch.Tensor A tensor with shape [indices.size(), target.size(-1)] representing the embedded indices extracted from the batch flattened target tensor.","title":"batched_index_select"},{"location":"api/nn/util/#batched_span_select","text":"def batched_span_select ( target : torch . Tensor , spans : torch . LongTensor ) -> torch . Tensor The given spans of size (batch_size, num_spans, 2) indexes into the sequence dimension (dimension 2) of the target, which has size (batch_size, sequence_length, embedding_size) . This function returns segmented spans in the target with respect to the provided span indices. It does not guarantee element order within each span. Parameters target : torch.Tensor A 3 dimensional tensor of shape (batch_size, sequence_length, embedding_size). This is the tensor to be indexed. indices : torch.LongTensor A 3 dimensional tensor of shape (batch_size, num_spans, 2) representing start and end indices (both inclusive) into the sequence_length dimension of the target tensor. Returns span_embeddings : torch.Tensor A tensor with shape (batch_size, num_spans, max_batch_span_width, embedding_size] representing the embedded spans extracted from the batch flattened target tensor. span_mask : torch.BoolTensor A tensor with shape (batch_size, num_spans, max_batch_span_width) representing the mask on the returned span embeddings.","title":"batched_span_select"},{"location":"api/nn/util/#flattened_index_select","text":"def flattened_index_select ( target : torch . Tensor , indices : torch . LongTensor ) -> torch . Tensor The given indices of size (set_size, subset_size) specifies subsets of the target that each of the set_size rows should select. The target has size (batch_size, sequence_length, embedding_size) , and the resulting selected tensor has size (batch_size, set_size, subset_size, embedding_size) . Parameters target : torch.Tensor A Tensor of shape (batch_size, sequence_length, embedding_size). indices : torch.LongTensor A LongTensor of shape (set_size, subset_size). All indices must be < sequence_length as this tensor is an index into the sequence_length dimension of the target. Returns selected : torch.Tensor , required. A Tensor of shape (batch_size, set_size, subset_size, embedding_size).","title":"flattened_index_select"},{"location":"api/nn/util/#get_range_vector","text":"def get_range_vector ( size : int , device : int ) -> torch . Tensor Returns a range vector with the desired size, starting at 0. The CUDA implementation is meant to avoid copy data from CPU to GPU.","title":"get_range_vector"},{"location":"api/nn/util/#bucket_values","text":"def bucket_values ( distances : torch . Tensor , num_identity_buckets : int = 4 , num_total_buckets : int = 10 ) -> torch . Tensor Places the given values (designed for distances) into num_total_buckets semi-logscale buckets, with num_identity_buckets of these capturing single values. The default settings will bucket values into the following buckets: [0, 1, 2, 3, 4, 5-7, 8-15, 16-31, 32-63, 64+]. Parameters distances : torch.Tensor A Tensor of any size, to be bucketed. num_identity_buckets : int , optional (default = 4 ) The number of identity buckets (those only holding a single value). num_total_buckets : int , optional (default = 10 ) The total number of buckets to bucket values into. Returns torch.Tensor A tensor of the same shape as the input, containing the indices of the buckets the values were placed in.","title":"bucket_values"},{"location":"api/nn/util/#add_sentence_boundary_token_ids","text":"def add_sentence_boundary_token_ids ( tensor : torch . Tensor , mask : torch . BoolTensor , sentence_begin_token : Any , sentence_end_token : Any ) -> Tuple [ torch . Tensor , torch . BoolTensor ] Add begin/end of sentence tokens to the batch of sentences. Given a batch of sentences with size (batch_size, timesteps) or (batch_size, timesteps, dim) this returns a tensor of shape (batch_size, timesteps + 2) or (batch_size, timesteps + 2, dim) respectively. Returns both the new tensor and updated mask. Parameters tensor : torch.Tensor A tensor of shape (batch_size, timesteps) or (batch_size, timesteps, dim) mask : torch.BoolTensor A tensor of shape (batch_size, timesteps) sentence_begin_token : Any Can be anything that can be broadcast in torch for assignment. For 2D input, a scalar with the <S> id. For 3D input, a tensor with length dim. sentence_end_token : Any Can be anything that can be broadcast in torch for assignment. For 2D input, a scalar with the </S> id. For 3D input, a tensor with length dim. Returns tensor_with_boundary_tokens : torch.Tensor The tensor with the appended and prepended boundary tokens. If the input was 2D, it has shape (batch_size, timesteps + 2) and if the input was 3D, it has shape (batch_size, timesteps + 2, dim). new_mask : torch.BoolTensor The new mask for the tensor, taking into account the appended tokens marking the beginning and end of the sentence.","title":"add_sentence_boundary_token_ids"},{"location":"api/nn/util/#remove_sentence_boundaries","text":"def remove_sentence_boundaries ( tensor : torch . Tensor , mask : torch . BoolTensor ) -> Tuple [ torch . Tensor , torch . Tensor ] Remove begin/end of sentence embeddings from the batch of sentences. Given a batch of sentences with size (batch_size, timesteps, dim) this returns a tensor of shape (batch_size, timesteps - 2, dim) after removing the beginning and end sentence markers. The sentences are assumed to be padded on the right, with the beginning of each sentence assumed to occur at index 0 (i.e., mask[:, 0] is assumed to be 1). Returns both the new tensor and updated mask. This function is the inverse of add_sentence_boundary_token_ids . Parameters tensor : torch.Tensor A tensor of shape (batch_size, timesteps, dim) mask : torch.BoolTensor A tensor of shape (batch_size, timesteps) Returns tensor_without_boundary_tokens : torch.Tensor The tensor after removing the boundary tokens of shape (batch_size, timesteps - 2, dim) new_mask : torch.BoolTensor The new mask for the tensor of shape (batch_size, timesteps - 2) .","title":"remove_sentence_boundaries"},{"location":"api/nn/util/#add_positional_features","text":"def add_positional_features ( tensor : torch . Tensor , min_timescale : float = 1.0 , max_timescale : float = 1.0e4 ) Implements the frequency-based positional encoding described in Attention is All you Need . Adds sinusoids of different frequencies to a Tensor . A sinusoid of a different frequency and phase is added to each dimension of the input Tensor . This allows the attention heads to use absolute and relative positions. The number of timescales is equal to hidden_dim / 2 within the range (min_timescale, max_timescale). For each timescale, the two sinusoidal signals sin(timestep / timescale) and cos(timestep / timescale) are generated and concatenated along the hidden_dim dimension. Parameters tensor : torch.Tensor a Tensor with shape (batch_size, timesteps, hidden_dim). min_timescale : float , optional (default = 1.0 ) The smallest timescale to use. max_timescale : float , optional (default = 1.0e4 ) The largest timescale to use. Returns torch.Tensor The input tensor augmented with the sinusoidal frequencies.","title":"add_positional_features"},{"location":"api/nn/util/#clone","text":"def clone ( module : torch . nn . Module , num_copies : int ) -> torch . nn . ModuleList Produce N identical layers.","title":"clone"},{"location":"api/nn/util/#combine_initial_dims","text":"def combine_initial_dims ( tensor : torch . Tensor ) -> torch . Tensor Given a (possibly higher order) tensor of ids with shape (d1, ..., dn, sequence_length) Return a view that's (d1 * ... * dn, sequence_length). If original tensor is 1-d or 2-d, return it as is.","title":"combine_initial_dims"},{"location":"api/nn/util/#uncombine_initial_dims","text":"def uncombine_initial_dims ( tensor : torch . Tensor , original_size : torch . Size ) -> torch . Tensor Given a tensor of embeddings with shape (d1 * ... * dn, sequence_length, embedding_dim) and the original shape (d1, ..., dn, sequence_length), return the reshaped tensor of embeddings with shape (d1, ..., dn, sequence_length, embedding_dim). If original size is 1-d or 2-d, return it as is.","title":"uncombine_initial_dims"},{"location":"api/nn/util/#inspect_parameters","text":"def inspect_parameters ( module : torch . nn . Module , quiet : bool = False ) -> Dict [ str , Any ] Inspects the model/module parameters and their tunability. The output is structured in a nested dict so that parameters in same sub-modules are grouped together. This can be helpful to setup module path based regex, for example in initializer. It prints it by default (optional) and returns the inspection dict. Eg. output:: { \"_text_field_embedder\": { \"token_embedder_tokens\": { \"_projection\": { \"bias\": \"tunable\", \"weight\": \"tunable\" }, \"weight\": \"frozen\" } } }","title":"inspect_parameters"},{"location":"api/nn/util/#find_embedding_layer","text":"def find_embedding_layer ( model : torch . nn . Module ) -> torch . nn . Module Takes a model (typically an AllenNLP Model , but this works for any torch.nn.Module ) and makes a best guess about which module is the embedding layer. For typical AllenNLP models, this often is the TextFieldEmbedder , but if you're using a pre-trained contextualizer, we really want layer 0 of that contextualizer, not the output. So there are a bunch of hacks in here for specific pre-trained contextualizers.","title":"find_embedding_layer"},{"location":"api/nn/util/#extend_layer","text":"def extend_layer ( layer : torch . nn . Module , new_dim : int ) -> None","title":"extend_layer"},{"location":"api/nn/util/#masked_topk","text":"def masked_topk ( input_ : torch . FloatTensor , mask : torch . BoolTensor , k : Union [ int , torch . LongTensor ], dim : int = - 1 ) -> Tuple [ torch . LongTensor , torch . LongTensor , torch . FloatTensor ] Extracts the top-k items along a certain dimension. This is similar to torch.topk except: (1) we allow of a mask that makes the function not consider certain elements; (2) the returned top input, mask, and indices are sorted in their original order in the input; (3) May use the same k for all dimensions, or different k for each. Parameters input_ : torch.FloatTensor A tensor containing the items that we want to prune. mask : torch.BoolTensor A tensor with the same shape as input_ that makes the function not consider masked out (i.e. False) elements. k : Union[int, torch.LongTensor] If a tensor of shape as input_ except without dimension dim , specifies the number of items to keep for each dimension. If an int, keep the same number of items for all dimensions. Returns top_input : torch.FloatTensor The values of the top-k scoring items. Has the same shape as input_ except dimension dim has value k when it's an int or k.max() when it's a tensor. top_mask : torch.BoolTensor The corresponding mask for top_input . Has the shape as top_input . top_indices : torch.IntTensor The indices of the top-k scoring items into the original input_ tensor. This is returned because it can be useful to retain pointers to the original items, if each item is being scored by multiple distinct scorers, for instance. Has the shape as top_input .","title":"masked_topk"},{"location":"api/nn/util/#info_value_of_dtype","text":"def info_value_of_dtype ( dtype : torch . dtype ) Returns the finfo or iinfo object of a given PyTorch data type. Does not allow torch.bool.","title":"info_value_of_dtype"},{"location":"api/nn/util/#min_value_of_dtype","text":"def min_value_of_dtype ( dtype : torch . dtype ) Returns the minimum value of a given PyTorch data type. Does not allow torch.bool.","title":"min_value_of_dtype"},{"location":"api/nn/util/#max_value_of_dtype","text":"def max_value_of_dtype ( dtype : torch . dtype ) Returns the maximum value of a given PyTorch data type. Does not allow torch.bool.","title":"max_value_of_dtype"},{"location":"api/nn/util/#tiny_value_of_dtype","text":"def tiny_value_of_dtype ( dtype : torch . dtype ) Returns a moderately tiny value for a given PyTorch data type that is used to avoid numerical issues such as division by zero. This is different from info_value_of_dtype(dtype).tiny because it causes some NaN bugs. Only supports floating point dtypes.","title":"tiny_value_of_dtype"},{"location":"api/nn/regularizers/regularizer/","text":"[ allennlp .nn .regularizers .regularizer ] Regularizer # class Regularizer ( Registrable ) An abstract class representing a regularizer. It must implement call, returning a scalar tensor. default_implementation # class Regularizer ( Registrable ): | ... | default_implementation = \"l2\" __call__ # class Regularizer ( Registrable ): | ... | def __call__ ( self , parameter : torch . Tensor ) -> torch . Tensor","title":"regularizer"},{"location":"api/nn/regularizers/regularizer/#regularizer","text":"class Regularizer ( Registrable ) An abstract class representing a regularizer. It must implement call, returning a scalar tensor.","title":"Regularizer"},{"location":"api/nn/regularizers/regularizer/#default_implementation","text":"class Regularizer ( Registrable ): | ... | default_implementation = \"l2\"","title":"default_implementation"},{"location":"api/nn/regularizers/regularizer/#__call__","text":"class Regularizer ( Registrable ): | ... | def __call__ ( self , parameter : torch . Tensor ) -> torch . Tensor","title":"__call__"},{"location":"api/nn/regularizers/regularizer_applicator/","text":"[ allennlp .nn .regularizers .regularizer_applicator ] RegularizerApplicator # class RegularizerApplicator ( FromParams ): | def __init__ ( | self , | regexes : List [ Tuple [ str , Regularizer ]] = None | ) -> None Applies regularizers to the parameters of a Module based on regex matches. __call__ # class RegularizerApplicator ( FromParams ): | ... | def __call__ ( self , module : torch . nn . Module ) -> torch . Tensor Parameters module : torch.nn.Module The module to regularize.","title":"regularizer_applicator"},{"location":"api/nn/regularizers/regularizer_applicator/#regularizerapplicator","text":"class RegularizerApplicator ( FromParams ): | def __init__ ( | self , | regexes : List [ Tuple [ str , Regularizer ]] = None | ) -> None Applies regularizers to the parameters of a Module based on regex matches.","title":"RegularizerApplicator"},{"location":"api/nn/regularizers/regularizer_applicator/#__call__","text":"class RegularizerApplicator ( FromParams ): | ... | def __call__ ( self , module : torch . nn . Module ) -> torch . Tensor Parameters module : torch.nn.Module The module to regularize.","title":"__call__"},{"location":"api/nn/regularizers/regularizers/","text":"[ allennlp .nn .regularizers .regularizers ] L1Regularizer # class L1Regularizer ( Regularizer ): | def __init__ ( self , alpha : float = 0.01 ) -> None Represents a penalty proportional to the sum of the absolute values of the parameters Registered as a Regularizer with name \"l1\". __call__ # class L1Regularizer ( Regularizer ): | ... | def __call__ ( self , parameter : torch . Tensor ) -> torch . Tensor L2Regularizer # class L2Regularizer ( Regularizer ): | def __init__ ( self , alpha : float = 0.01 ) -> None Represents a penalty proportional to the sum of squared values of the parameters Registered as a Regularizer with name \"l2\". __call__ # class L2Regularizer ( Regularizer ): | ... | def __call__ ( self , parameter : torch . Tensor ) -> torch . Tensor","title":"regularizers"},{"location":"api/nn/regularizers/regularizers/#l1regularizer","text":"class L1Regularizer ( Regularizer ): | def __init__ ( self , alpha : float = 0.01 ) -> None Represents a penalty proportional to the sum of the absolute values of the parameters Registered as a Regularizer with name \"l1\".","title":"L1Regularizer"},{"location":"api/nn/regularizers/regularizers/#__call__","text":"class L1Regularizer ( Regularizer ): | ... | def __call__ ( self , parameter : torch . Tensor ) -> torch . Tensor","title":"__call__"},{"location":"api/nn/regularizers/regularizers/#l2regularizer","text":"class L2Regularizer ( Regularizer ): | def __init__ ( self , alpha : float = 0.01 ) -> None Represents a penalty proportional to the sum of squared values of the parameters Registered as a Regularizer with name \"l2\".","title":"L2Regularizer"},{"location":"api/nn/regularizers/regularizers/#__call___1","text":"class L2Regularizer ( Regularizer ): | ... | def __call__ ( self , parameter : torch . Tensor ) -> torch . Tensor","title":"__call__"},{"location":"api/predictors/predictor/","text":"[ allennlp .predictors .predictor ] Predictor # class Predictor ( Registrable ): | def __init__ ( | self , | model : Model , | dataset_reader : DatasetReader , | frozen : bool = True | ) -> None a Predictor is a thin wrapper around an AllenNLP model that handles JSON -> JSON predictions that can be used for serving models through the web API or making predictions in bulk. load_line # class Predictor ( Registrable ): | ... | def load_line ( self , line : str ) -> JsonDict If your inputs are not in JSON-lines format (e.g. you have a CSV) you can override this function to parse them correctly. dump_line # class Predictor ( Registrable ): | ... | def dump_line ( self , outputs : JsonDict ) -> str If you don't want your outputs in JSON-lines format you can override this function to output them differently. predict_json # class Predictor ( Registrable ): | ... | def predict_json ( self , inputs : JsonDict ) -> JsonDict json_to_labeled_instances # class Predictor ( Registrable ): | ... | def json_to_labeled_instances ( | self , | inputs : JsonDict | ) -> List [ Instance ] Converts incoming json to a Instance , runs the model on the newly created instance, and adds labels to the Instance s given by the model's output. Returns List[instance] A list of Instance 's. get_gradients # class Predictor ( Registrable ): | ... | def get_gradients ( | self , | instances : List [ Instance ] | ) -> Tuple [ Dict [ str , Any ], Dict [ str , Any ]] Gets the gradients of the loss with respect to the model inputs. Parameters instances : List[Instance] Returns Tuple[Dict[str, Any], Dict[str, Any]] The first item is a Dict of gradient entries for each input. The keys have the form {grad_input_1: ..., grad_input_2: ... } up to the number of inputs given. The second item is the model's output. Notes Takes a JsonDict representing the inputs of the model and converts them to Instances ), sends these through the model forward function after registering hooks on the embedding layer of the model. Calls backward on the loss and then removes the hooks. capture_model_internals # class Predictor ( Registrable ): | ... | @contextmanager | def capture_model_internals ( self ) -> Iterator [ dict ] Context manager that captures the internal-module outputs of this predictor's model. The idea is that you could use it as follows: with predictor . capture_model_internals () as internals : outputs = predictor . predict_json ( inputs ) return { ** outputs , \"model_internals\" : internals } predict_instance # class Predictor ( Registrable ): | ... | def predict_instance ( self , instance : Instance ) -> JsonDict predictions_to_labeled_instances # class Predictor ( Registrable ): | ... | def predictions_to_labeled_instances ( | self , | instance : Instance , | outputs : Dict [ str , numpy . ndarray ] | ) -> List [ Instance ] This function takes a model's outputs for an Instance, and it labels that instance according to the output. For example, in classification this function labels the instance according to the class with the highest probability. This function is used to to compute gradients of what the model predicted. The return type is a list because in some tasks there are multiple predictions in the output (e.g., in NER a model predicts multiple spans). In this case, each instance in the returned list of Instances contains an individual entity prediction as the label. predict_batch_json # class Predictor ( Registrable ): | ... | def predict_batch_json ( self , inputs : List [ JsonDict ]) -> List [ JsonDict ] predict_batch_instance # class Predictor ( Registrable ): | ... | def predict_batch_instance ( | self , | instances : List [ Instance ] | ) -> List [ JsonDict ] from_path # class Predictor ( Registrable ): | ... | @classmethod | def from_path ( | cls , | archive_path : str , | predictor_name : str = None , | cuda_device : int = - 1 , | dataset_reader_to_load : str = \"validation\" , | frozen : bool = True , | import_plugins : bool = True | ) -> \"Predictor\" Instantiate a Predictor from an archive path. If you need more detailed configuration options, such as overrides, please use from_archive . Parameters archive_path : str The path to the archive. predictor_name : str , optional (default = None ) Name that the predictor is registered as, or None to use the predictor associated with the model. cuda_device : int , optional (default = -1 ) If cuda_device is >= 0, the model will be loaded onto the corresponding GPU. Otherwise it will be loaded onto the CPU. dataset_reader_to_load : str , optional (default = \"validation\" ) Which dataset reader to load from the archive, either \"train\" or \"validation\". frozen : bool , optional (default = True ) If we should call model.eval() when building the predictor. import_plugins : bool , optional (default = True ) If True , we attempt to import plugins before loading the predictor. This comes with additional overhead, but means you don't need to explicitly import the modules that your predictor depends on as long as those modules can be found by allennlp.common.plugins.import_plugins() . Returns Predictor A Predictor instance. from_archive # class Predictor ( Registrable ): | ... | @classmethod | def from_archive ( | cls , | archive : Archive , | predictor_name : str = None , | dataset_reader_to_load : str = \"validation\" , | frozen : bool = True | ) -> \"Predictor\" Instantiate a Predictor from an Archive ; that is, from the result of training a model. Optionally specify which Predictor subclass; otherwise, we try to find a corresponding predictor in DEFAULT_PREDICTORS , or if one is not found, the base class (i.e. Predictor ) will be used. Optionally specify which DatasetReader should be loaded; otherwise, the validation one will be used if it exists followed by the training dataset reader. Optionally specify if the loaded model should be frozen, meaning model.eval() will be called.","title":"predictor"},{"location":"api/predictors/predictor/#predictor","text":"class Predictor ( Registrable ): | def __init__ ( | self , | model : Model , | dataset_reader : DatasetReader , | frozen : bool = True | ) -> None a Predictor is a thin wrapper around an AllenNLP model that handles JSON -> JSON predictions that can be used for serving models through the web API or making predictions in bulk.","title":"Predictor"},{"location":"api/predictors/predictor/#load_line","text":"class Predictor ( Registrable ): | ... | def load_line ( self , line : str ) -> JsonDict If your inputs are not in JSON-lines format (e.g. you have a CSV) you can override this function to parse them correctly.","title":"load_line"},{"location":"api/predictors/predictor/#dump_line","text":"class Predictor ( Registrable ): | ... | def dump_line ( self , outputs : JsonDict ) -> str If you don't want your outputs in JSON-lines format you can override this function to output them differently.","title":"dump_line"},{"location":"api/predictors/predictor/#predict_json","text":"class Predictor ( Registrable ): | ... | def predict_json ( self , inputs : JsonDict ) -> JsonDict","title":"predict_json"},{"location":"api/predictors/predictor/#json_to_labeled_instances","text":"class Predictor ( Registrable ): | ... | def json_to_labeled_instances ( | self , | inputs : JsonDict | ) -> List [ Instance ] Converts incoming json to a Instance , runs the model on the newly created instance, and adds labels to the Instance s given by the model's output. Returns List[instance] A list of Instance 's.","title":"json_to_labeled_instances"},{"location":"api/predictors/predictor/#get_gradients","text":"class Predictor ( Registrable ): | ... | def get_gradients ( | self , | instances : List [ Instance ] | ) -> Tuple [ Dict [ str , Any ], Dict [ str , Any ]] Gets the gradients of the loss with respect to the model inputs. Parameters instances : List[Instance] Returns Tuple[Dict[str, Any], Dict[str, Any]] The first item is a Dict of gradient entries for each input. The keys have the form {grad_input_1: ..., grad_input_2: ... } up to the number of inputs given. The second item is the model's output. Notes Takes a JsonDict representing the inputs of the model and converts them to Instances ), sends these through the model forward function after registering hooks on the embedding layer of the model. Calls backward on the loss and then removes the hooks.","title":"get_gradients"},{"location":"api/predictors/predictor/#capture_model_internals","text":"class Predictor ( Registrable ): | ... | @contextmanager | def capture_model_internals ( self ) -> Iterator [ dict ] Context manager that captures the internal-module outputs of this predictor's model. The idea is that you could use it as follows: with predictor . capture_model_internals () as internals : outputs = predictor . predict_json ( inputs ) return { ** outputs , \"model_internals\" : internals }","title":"capture_model_internals"},{"location":"api/predictors/predictor/#predict_instance","text":"class Predictor ( Registrable ): | ... | def predict_instance ( self , instance : Instance ) -> JsonDict","title":"predict_instance"},{"location":"api/predictors/predictor/#predictions_to_labeled_instances","text":"class Predictor ( Registrable ): | ... | def predictions_to_labeled_instances ( | self , | instance : Instance , | outputs : Dict [ str , numpy . ndarray ] | ) -> List [ Instance ] This function takes a model's outputs for an Instance, and it labels that instance according to the output. For example, in classification this function labels the instance according to the class with the highest probability. This function is used to to compute gradients of what the model predicted. The return type is a list because in some tasks there are multiple predictions in the output (e.g., in NER a model predicts multiple spans). In this case, each instance in the returned list of Instances contains an individual entity prediction as the label.","title":"predictions_to_labeled_instances"},{"location":"api/predictors/predictor/#predict_batch_json","text":"class Predictor ( Registrable ): | ... | def predict_batch_json ( self , inputs : List [ JsonDict ]) -> List [ JsonDict ]","title":"predict_batch_json"},{"location":"api/predictors/predictor/#predict_batch_instance","text":"class Predictor ( Registrable ): | ... | def predict_batch_instance ( | self , | instances : List [ Instance ] | ) -> List [ JsonDict ]","title":"predict_batch_instance"},{"location":"api/predictors/predictor/#from_path","text":"class Predictor ( Registrable ): | ... | @classmethod | def from_path ( | cls , | archive_path : str , | predictor_name : str = None , | cuda_device : int = - 1 , | dataset_reader_to_load : str = \"validation\" , | frozen : bool = True , | import_plugins : bool = True | ) -> \"Predictor\" Instantiate a Predictor from an archive path. If you need more detailed configuration options, such as overrides, please use from_archive . Parameters archive_path : str The path to the archive. predictor_name : str , optional (default = None ) Name that the predictor is registered as, or None to use the predictor associated with the model. cuda_device : int , optional (default = -1 ) If cuda_device is >= 0, the model will be loaded onto the corresponding GPU. Otherwise it will be loaded onto the CPU. dataset_reader_to_load : str , optional (default = \"validation\" ) Which dataset reader to load from the archive, either \"train\" or \"validation\". frozen : bool , optional (default = True ) If we should call model.eval() when building the predictor. import_plugins : bool , optional (default = True ) If True , we attempt to import plugins before loading the predictor. This comes with additional overhead, but means you don't need to explicitly import the modules that your predictor depends on as long as those modules can be found by allennlp.common.plugins.import_plugins() . Returns Predictor A Predictor instance.","title":"from_path"},{"location":"api/predictors/predictor/#from_archive","text":"class Predictor ( Registrable ): | ... | @classmethod | def from_archive ( | cls , | archive : Archive , | predictor_name : str = None , | dataset_reader_to_load : str = \"validation\" , | frozen : bool = True | ) -> \"Predictor\" Instantiate a Predictor from an Archive ; that is, from the result of training a model. Optionally specify which Predictor subclass; otherwise, we try to find a corresponding predictor in DEFAULT_PREDICTORS , or if one is not found, the base class (i.e. Predictor ) will be used. Optionally specify which DatasetReader should be loaded; otherwise, the validation one will be used if it exists followed by the training dataset reader. Optionally specify if the loaded model should be frozen, meaning model.eval() will be called.","title":"from_archive"},{"location":"api/predictors/sentence_tagger/","text":"[ allennlp .predictors .sentence_tagger ] SentenceTaggerPredictor # class SentenceTaggerPredictor ( Predictor ): | def __init__ ( | self , | model : Model , | dataset_reader : DatasetReader , | language : str = \"en_core_web_sm\" | ) -> None Predictor for any model that takes in a sentence and returns a single set of tags for it. In particular, it can be used with the CrfTagger model and also the SimpleTagger model. Registered as a Predictor with name \"sentence_tagger\". predict # class SentenceTaggerPredictor ( Predictor ): | ... | def predict ( self , sentence : str ) -> JsonDict predictions_to_labeled_instances # class SentenceTaggerPredictor ( Predictor ): | ... | @overrides | def predictions_to_labeled_instances ( | self , | instance : Instance , | outputs : Dict [ str , numpy . ndarray ] | ) -> List [ Instance ] This function currently only handles BIOUL tags. Imagine an NER model predicts three named entities (each one with potentially multiple tokens). For each individual entity, we create a new Instance that has the label set to only that entity and the rest of the tokens are labeled as outside. We then return a list of those Instances. For example: Mary went to Seattle to visit Microsoft Research U-Per O O U-Loc O O B-Org L-Org We create three instances. Mary went to Seattle to visit Microsoft Research U-Per O O O O O O O Mary went to Seattle to visit Microsoft Research O O O U-LOC O O O O Mary went to Seattle to visit Microsoft Research O O O O O O B-Org L-Org We additionally add a flag to these instances to tell the model to only compute loss on non-O tags, so that we get gradients that are specific to the particular span prediction that each instance represents.","title":"sentence_tagger"},{"location":"api/predictors/sentence_tagger/#sentencetaggerpredictor","text":"class SentenceTaggerPredictor ( Predictor ): | def __init__ ( | self , | model : Model , | dataset_reader : DatasetReader , | language : str = \"en_core_web_sm\" | ) -> None Predictor for any model that takes in a sentence and returns a single set of tags for it. In particular, it can be used with the CrfTagger model and also the SimpleTagger model. Registered as a Predictor with name \"sentence_tagger\".","title":"SentenceTaggerPredictor"},{"location":"api/predictors/sentence_tagger/#predict","text":"class SentenceTaggerPredictor ( Predictor ): | ... | def predict ( self , sentence : str ) -> JsonDict","title":"predict"},{"location":"api/predictors/sentence_tagger/#predictions_to_labeled_instances","text":"class SentenceTaggerPredictor ( Predictor ): | ... | @overrides | def predictions_to_labeled_instances ( | self , | instance : Instance , | outputs : Dict [ str , numpy . ndarray ] | ) -> List [ Instance ] This function currently only handles BIOUL tags. Imagine an NER model predicts three named entities (each one with potentially multiple tokens). For each individual entity, we create a new Instance that has the label set to only that entity and the rest of the tokens are labeled as outside. We then return a list of those Instances. For example: Mary went to Seattle to visit Microsoft Research U-Per O O U-Loc O O B-Org L-Org We create three instances. Mary went to Seattle to visit Microsoft Research U-Per O O O O O O O Mary went to Seattle to visit Microsoft Research O O O U-LOC O O O O Mary went to Seattle to visit Microsoft Research O O O O O O B-Org L-Org We additionally add a flag to these instances to tell the model to only compute loss on non-O tags, so that we get gradients that are specific to the particular span prediction that each instance represents.","title":"predictions_to_labeled_instances"},{"location":"api/predictors/text_classifier/","text":"[ allennlp .predictors .text_classifier ] TextClassifierPredictor # class TextClassifierPredictor ( Predictor ) Predictor for any model that takes in a sentence and returns a single class for it. In particular, it can be used with the BasicClassifier model. Registered as a Predictor with name \"text_classifier\". predict # class TextClassifierPredictor ( Predictor ): | ... | def predict ( self , sentence : str ) -> JsonDict predictions_to_labeled_instances # class TextClassifierPredictor ( Predictor ): | ... | @overrides | def predictions_to_labeled_instances ( | self , | instance : Instance , | outputs : Dict [ str , numpy . ndarray ] | ) -> List [ Instance ]","title":"text_classifier"},{"location":"api/predictors/text_classifier/#textclassifierpredictor","text":"class TextClassifierPredictor ( Predictor ) Predictor for any model that takes in a sentence and returns a single class for it. In particular, it can be used with the BasicClassifier model. Registered as a Predictor with name \"text_classifier\".","title":"TextClassifierPredictor"},{"location":"api/predictors/text_classifier/#predict","text":"class TextClassifierPredictor ( Predictor ): | ... | def predict ( self , sentence : str ) -> JsonDict","title":"predict"},{"location":"api/predictors/text_classifier/#predictions_to_labeled_instances","text":"class TextClassifierPredictor ( Predictor ): | ... | @overrides | def predictions_to_labeled_instances ( | self , | instance : Instance , | outputs : Dict [ str , numpy . ndarray ] | ) -> List [ Instance ]","title":"predictions_to_labeled_instances"},{"location":"api/tools/archive_surgery/","text":"[ allennlp .tools .archive_surgery ] Helper script for modifying config.json files that are locked inside model.tar.gz archives. This is useful if you need to rename things or add or remove values, usually because of changes to the library. This script will untar the archive to a temp directory, launch an editor to modify the config.json, and then re-tar everything to a new archive. If your $EDITOR environment variable is not set, you'll have to explicitly specify which editor to use. main # def main ()","title":"archive_surgery"},{"location":"api/tools/archive_surgery/#main","text":"def main ()","title":"main"},{"location":"api/tools/create_elmo_embeddings_from_vocab/","text":"[ allennlp .tools .create_elmo_embeddings_from_vocab ] main # def main ( vocab_path : str , elmo_config_path : str , elmo_weights_path : str , output_dir : str , batch_size : int , device : int , use_custom_oov_token : bool = False ) Creates ELMo word representations from a vocabulary file. These word representations are independent - they are the result of running the CNN and Highway layers of the ELMo model, but not the Bidirectional LSTM. ELMo requires 2 additional tokens: and . The first token in this file is assumed to be an unknown token. This script produces two artifacts: A new vocabulary file with the and tokens inserted and a glove formatted embedding file containing word : vector pairs, one per line, with all values separated by a space.","title":"create_elmo_embeddings_from_vocab"},{"location":"api/tools/create_elmo_embeddings_from_vocab/#main","text":"def main ( vocab_path : str , elmo_config_path : str , elmo_weights_path : str , output_dir : str , batch_size : int , device : int , use_custom_oov_token : bool = False ) Creates ELMo word representations from a vocabulary file. These word representations are independent - they are the result of running the CNN and Highway layers of the ELMo model, but not the Bidirectional LSTM. ELMo requires 2 additional tokens: and . The first token in this file is assumed to be an unknown token. This script produces two artifacts: A new vocabulary file with the and tokens inserted and a glove formatted embedding file containing word : vector pairs, one per line, with all values separated by a space.","title":"main"},{"location":"api/tools/inspect_cache/","text":"[ allennlp .tools .inspect_cache ] main # def main ()","title":"inspect_cache"},{"location":"api/tools/inspect_cache/#main","text":"def main ()","title":"main"},{"location":"api/training/checkpointer/","text":"[ allennlp .training .checkpointer ] Checkpointer # class Checkpointer ( Registrable ): | def __init__ ( | self , | serialization_dir : str = None , | keep_serialized_model_every_num_seconds : int = None , | num_serialized_models_to_keep : int = 2 , | model_save_interval : float = None | ) -> None This class implements the functionality for checkpointing your model and trainer state during training. It is agnostic as to what those states look like (they are typed as Dict[str, Any]), but they will be fed to torch.save so they should be serializable in that sense. They will also be restored as Dict[str, Any], which means the calling code is responsible for knowing what to do with them. Parameters num_serialized_models_to_keep : int , optional (default = 2 ) Number of previous model checkpoints to retain. Default is to keep 2 checkpoints. A value of None or -1 means all checkpoints will be kept. In a typical AllenNLP configuration file, this argument does not get an entry under the \"checkpointer\", it gets passed in separately. - keep_serialized_model_every_num_seconds : int , optional (default = None ) If num_serialized_models_to_keep is not None, then occasionally it's useful to save models at a given interval in addition to the last num_serialized_models_to_keep. To do so, specify keep_serialized_model_every_num_seconds as the number of seconds between permanently saved checkpoints. Note that this option is only used if num_serialized_models_to_keep is not None, otherwise all checkpoints are kept. - model_save_interval : float , optional (default = None ) If provided, then serialize models every model_save_interval seconds within single epochs. In all cases, models are also saved at the end of every epoch if serialization_dir is provided. default_implementation # class Checkpointer ( Registrable ): | ... | default_implementation = \"default\" maybe_save_checkpoint # class Checkpointer ( Registrable ): | ... | def maybe_save_checkpoint ( | self , | trainer : \"allennlp.training.trainer.Trainer\" , | epoch : int , | batches_this_epoch : int | ) -> None Given amount of time lapsed between the last save and now (tracked internally), the current epoch, and the number of batches seen so far this epoch, this method decides whether to save a checkpoint or not. If we decide to save a checkpoint, we grab whatever state we need out of the Trainer and save it. This function is intended to be called at the end of each batch in an epoch (perhaps because your data is large enough that you don't really have \"epochs\"). The default implementation only looks at time, not batch or epoch number, though those parameters are available to you if you want to customize the behavior of this function. save_checkpoint # class Checkpointer ( Registrable ): | ... | def save_checkpoint ( | self , | epoch : Union [ int , str ], | trainer : \"allennlp.training.trainer.Trainer\" , | is_best_so_far : bool = False | ) -> None find_latest_checkpoint # class Checkpointer ( Registrable ): | ... | def find_latest_checkpoint ( self ) -> Tuple [ str , str ] Return the location of the latest model and training state files. If there isn't a valid checkpoint then return None. restore_checkpoint # class Checkpointer ( Registrable ): | ... | def restore_checkpoint ( self ) -> Tuple [ Dict [ str , Any ], Dict [ str , Any ]] Restores a model from a serialization_dir to the last saved checkpoint. This includes a training state (typically consisting of an epoch count and optimizer state), which is serialized separately from model parameters. This function should only be used to continue training - if you wish to load a model for inference/load parts of a model into a new computation graph, you should use the native Pytorch functions: model.load_state_dict(torch.load(\"/path/to/model/weights.th\")) If self._serialization_dir does not exist or does not contain any checkpointed weights, this function will do nothing and return empty dicts. Returns states : Tuple[Dict[str, Any], Dict[str, Any]] The model state and the training state. best_model_state # class Checkpointer ( Registrable ): | ... | def best_model_state ( self ) -> Dict [ str , Any ]","title":"checkpointer"},{"location":"api/training/checkpointer/#checkpointer","text":"class Checkpointer ( Registrable ): | def __init__ ( | self , | serialization_dir : str = None , | keep_serialized_model_every_num_seconds : int = None , | num_serialized_models_to_keep : int = 2 , | model_save_interval : float = None | ) -> None This class implements the functionality for checkpointing your model and trainer state during training. It is agnostic as to what those states look like (they are typed as Dict[str, Any]), but they will be fed to torch.save so they should be serializable in that sense. They will also be restored as Dict[str, Any], which means the calling code is responsible for knowing what to do with them. Parameters num_serialized_models_to_keep : int , optional (default = 2 ) Number of previous model checkpoints to retain. Default is to keep 2 checkpoints. A value of None or -1 means all checkpoints will be kept. In a typical AllenNLP configuration file, this argument does not get an entry under the \"checkpointer\", it gets passed in separately. - keep_serialized_model_every_num_seconds : int , optional (default = None ) If num_serialized_models_to_keep is not None, then occasionally it's useful to save models at a given interval in addition to the last num_serialized_models_to_keep. To do so, specify keep_serialized_model_every_num_seconds as the number of seconds between permanently saved checkpoints. Note that this option is only used if num_serialized_models_to_keep is not None, otherwise all checkpoints are kept. - model_save_interval : float , optional (default = None ) If provided, then serialize models every model_save_interval seconds within single epochs. In all cases, models are also saved at the end of every epoch if serialization_dir is provided.","title":"Checkpointer"},{"location":"api/training/checkpointer/#default_implementation","text":"class Checkpointer ( Registrable ): | ... | default_implementation = \"default\"","title":"default_implementation"},{"location":"api/training/checkpointer/#maybe_save_checkpoint","text":"class Checkpointer ( Registrable ): | ... | def maybe_save_checkpoint ( | self , | trainer : \"allennlp.training.trainer.Trainer\" , | epoch : int , | batches_this_epoch : int | ) -> None Given amount of time lapsed between the last save and now (tracked internally), the current epoch, and the number of batches seen so far this epoch, this method decides whether to save a checkpoint or not. If we decide to save a checkpoint, we grab whatever state we need out of the Trainer and save it. This function is intended to be called at the end of each batch in an epoch (perhaps because your data is large enough that you don't really have \"epochs\"). The default implementation only looks at time, not batch or epoch number, though those parameters are available to you if you want to customize the behavior of this function.","title":"maybe_save_checkpoint"},{"location":"api/training/checkpointer/#save_checkpoint","text":"class Checkpointer ( Registrable ): | ... | def save_checkpoint ( | self , | epoch : Union [ int , str ], | trainer : \"allennlp.training.trainer.Trainer\" , | is_best_so_far : bool = False | ) -> None","title":"save_checkpoint"},{"location":"api/training/checkpointer/#find_latest_checkpoint","text":"class Checkpointer ( Registrable ): | ... | def find_latest_checkpoint ( self ) -> Tuple [ str , str ] Return the location of the latest model and training state files. If there isn't a valid checkpoint then return None.","title":"find_latest_checkpoint"},{"location":"api/training/checkpointer/#restore_checkpoint","text":"class Checkpointer ( Registrable ): | ... | def restore_checkpoint ( self ) -> Tuple [ Dict [ str , Any ], Dict [ str , Any ]] Restores a model from a serialization_dir to the last saved checkpoint. This includes a training state (typically consisting of an epoch count and optimizer state), which is serialized separately from model parameters. This function should only be used to continue training - if you wish to load a model for inference/load parts of a model into a new computation graph, you should use the native Pytorch functions: model.load_state_dict(torch.load(\"/path/to/model/weights.th\")) If self._serialization_dir does not exist or does not contain any checkpointed weights, this function will do nothing and return empty dicts. Returns states : Tuple[Dict[str, Any], Dict[str, Any]] The model state and the training state.","title":"restore_checkpoint"},{"location":"api/training/checkpointer/#best_model_state","text":"class Checkpointer ( Registrable ): | ... | def best_model_state ( self ) -> Dict [ str , Any ]","title":"best_model_state"},{"location":"api/training/metric_tracker/","text":"[ allennlp .training .metric_tracker ] MetricTracker # class MetricTracker : | def __init__ ( | self , | patience : Optional [ int ] = None , | metric_name : str = None , | should_decrease : bool = None | ) -> None This class tracks a metric during training for the dual purposes of early stopping and for knowing whether the current value is the best so far. It mimics the PyTorch state_dict / load_state_dict interface, so that it can be checkpointed along with your model and optimizer. Some metrics improve by increasing; others by decreasing. Here you can either explicitly supply should_decrease , or you can provide a metric_name in which case \"should decrease\" is inferred from the first character, which must be \"+\" or \"-\". Parameters patience : int , optional (default = None ) If provided, then should_stop_early() returns True if we go this many epochs without seeing a new best value. metric_name : str , optional (default = None ) If provided, it's used to infer whether we expect the metric values to increase (if it starts with \"+\") or decrease (if it starts with \"-\"). It's an error if it doesn't start with one of those. If it's not provided, you should specify should_decrease instead. should_decrease : str , optional (default = None ) If metric_name isn't provided (in which case we can't infer should_decrease ), then you have to specify it here. clear # class MetricTracker : | ... | def clear ( self ) -> None Clears out the tracked metrics, but keeps the patience and should_decrease settings. state_dict # class MetricTracker : | ... | def state_dict ( self ) -> Dict [ str , Any ] A Trainer can use this to serialize the state of the metric tracker. load_state_dict # class MetricTracker : | ... | def load_state_dict ( self , state_dict : Dict [ str , Any ]) -> None A Trainer can use this to hydrate a metric tracker from a serialized state. add_metric # class MetricTracker : | ... | def add_metric ( self , metric : float ) -> None Record a new value of the metric and update the various things that depend on it. add_metrics # class MetricTracker : | ... | def add_metrics ( self , metrics : Iterable [ float ]) -> None Helper to add multiple metrics at once. is_best_so_far # class MetricTracker : | ... | def is_best_so_far ( self ) -> bool Returns true if the most recent value of the metric is the best so far. should_stop_early # class MetricTracker : | ... | def should_stop_early ( self ) -> bool Returns true if improvement has stopped for long enough.","title":"metric_tracker"},{"location":"api/training/metric_tracker/#metrictracker","text":"class MetricTracker : | def __init__ ( | self , | patience : Optional [ int ] = None , | metric_name : str = None , | should_decrease : bool = None | ) -> None This class tracks a metric during training for the dual purposes of early stopping and for knowing whether the current value is the best so far. It mimics the PyTorch state_dict / load_state_dict interface, so that it can be checkpointed along with your model and optimizer. Some metrics improve by increasing; others by decreasing. Here you can either explicitly supply should_decrease , or you can provide a metric_name in which case \"should decrease\" is inferred from the first character, which must be \"+\" or \"-\". Parameters patience : int , optional (default = None ) If provided, then should_stop_early() returns True if we go this many epochs without seeing a new best value. metric_name : str , optional (default = None ) If provided, it's used to infer whether we expect the metric values to increase (if it starts with \"+\") or decrease (if it starts with \"-\"). It's an error if it doesn't start with one of those. If it's not provided, you should specify should_decrease instead. should_decrease : str , optional (default = None ) If metric_name isn't provided (in which case we can't infer should_decrease ), then you have to specify it here.","title":"MetricTracker"},{"location":"api/training/metric_tracker/#clear","text":"class MetricTracker : | ... | def clear ( self ) -> None Clears out the tracked metrics, but keeps the patience and should_decrease settings.","title":"clear"},{"location":"api/training/metric_tracker/#state_dict","text":"class MetricTracker : | ... | def state_dict ( self ) -> Dict [ str , Any ] A Trainer can use this to serialize the state of the metric tracker.","title":"state_dict"},{"location":"api/training/metric_tracker/#load_state_dict","text":"class MetricTracker : | ... | def load_state_dict ( self , state_dict : Dict [ str , Any ]) -> None A Trainer can use this to hydrate a metric tracker from a serialized state.","title":"load_state_dict"},{"location":"api/training/metric_tracker/#add_metric","text":"class MetricTracker : | ... | def add_metric ( self , metric : float ) -> None Record a new value of the metric and update the various things that depend on it.","title":"add_metric"},{"location":"api/training/metric_tracker/#add_metrics","text":"class MetricTracker : | ... | def add_metrics ( self , metrics : Iterable [ float ]) -> None Helper to add multiple metrics at once.","title":"add_metrics"},{"location":"api/training/metric_tracker/#is_best_so_far","text":"class MetricTracker : | ... | def is_best_so_far ( self ) -> bool Returns true if the most recent value of the metric is the best so far.","title":"is_best_so_far"},{"location":"api/training/metric_tracker/#should_stop_early","text":"class MetricTracker : | ... | def should_stop_early ( self ) -> bool Returns true if improvement has stopped for long enough.","title":"should_stop_early"},{"location":"api/training/moving_average/","text":"[ allennlp .training .moving_average ] NamedParameter # NamedParameter = Tuple [ str , torch . Tensor ] MovingAverage # class MovingAverage ( Registrable ): | def __init__ ( self , parameters : Iterable [ NamedParameter ]) -> None Tracks a moving average of model parameters. default_implementation # class MovingAverage ( Registrable ): | ... | default_implementation = \"exponential\" apply # class MovingAverage ( Registrable ): | ... | def apply ( self , num_updates : Optional [ int ] = None ) Update the moving averages based on the latest values of the parameters. assign_average_value # class MovingAverage ( Registrable ): | ... | def assign_average_value ( self ) -> None Replace all the parameter values with the averages. Save the current parameter values to restore later. restore # class MovingAverage ( Registrable ): | ... | def restore ( self ) -> None Restore the backed-up (non-average) parameter values. ExponentialMovingAverage # class ExponentialMovingAverage ( MovingAverage ): | def __init__ ( | self , | parameters : Iterable [ NamedParameter ], | decay : float = 0.9999 , | numerator : float = 1.0 , | denominator : float = 10.0 | ) -> None Create shadow variables and maintain exponential moving average for model parameters. Registered as a MovingAverage with name \"exponential\". Parameters parameters : Iterable[Tuple[str, Parameter]] The parameters whose averages we'll be tracking. In a typical AllenNLP configuration file, this argument does not get an entry under the \"moving_average\", it gets passed in separately. - decay : float , optional (default = 0.9999 ) The decay rate that will be used if num_updates is not passed (and that will be used as an upper bound if num_updates is passed). - numerator : float , optional (default = 1.0 ) The numerator used to compute the decay rate if num_updates is passed. - denominator : float , optional (default = 10.0 ) The denominator used to compute the decay rate if num_updates is passed. apply # class ExponentialMovingAverage ( MovingAverage ): | ... | def apply ( self , num_updates : Optional [ int ] = None ) -> None Apply exponential moving average to named_parameters if specified, or we will apply this to all the trainable parameters of the model. The optional num_updates parameter allows one to tweak the decay rate dynamically. If passed, the actual decay rate used is: `min(decay, (numerator + num_updates) / (denominator + num_updates))` (This logic is based on the Tensorflow exponential moving average https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage )","title":"moving_average"},{"location":"api/training/moving_average/#namedparameter","text":"NamedParameter = Tuple [ str , torch . Tensor ]","title":"NamedParameter"},{"location":"api/training/moving_average/#movingaverage","text":"class MovingAverage ( Registrable ): | def __init__ ( self , parameters : Iterable [ NamedParameter ]) -> None Tracks a moving average of model parameters.","title":"MovingAverage"},{"location":"api/training/moving_average/#default_implementation","text":"class MovingAverage ( Registrable ): | ... | default_implementation = \"exponential\"","title":"default_implementation"},{"location":"api/training/moving_average/#apply","text":"class MovingAverage ( Registrable ): | ... | def apply ( self , num_updates : Optional [ int ] = None ) Update the moving averages based on the latest values of the parameters.","title":"apply"},{"location":"api/training/moving_average/#assign_average_value","text":"class MovingAverage ( Registrable ): | ... | def assign_average_value ( self ) -> None Replace all the parameter values with the averages. Save the current parameter values to restore later.","title":"assign_average_value"},{"location":"api/training/moving_average/#restore","text":"class MovingAverage ( Registrable ): | ... | def restore ( self ) -> None Restore the backed-up (non-average) parameter values.","title":"restore"},{"location":"api/training/moving_average/#exponentialmovingaverage","text":"class ExponentialMovingAverage ( MovingAverage ): | def __init__ ( | self , | parameters : Iterable [ NamedParameter ], | decay : float = 0.9999 , | numerator : float = 1.0 , | denominator : float = 10.0 | ) -> None Create shadow variables and maintain exponential moving average for model parameters. Registered as a MovingAverage with name \"exponential\". Parameters parameters : Iterable[Tuple[str, Parameter]] The parameters whose averages we'll be tracking. In a typical AllenNLP configuration file, this argument does not get an entry under the \"moving_average\", it gets passed in separately. - decay : float , optional (default = 0.9999 ) The decay rate that will be used if num_updates is not passed (and that will be used as an upper bound if num_updates is passed). - numerator : float , optional (default = 1.0 ) The numerator used to compute the decay rate if num_updates is passed. - denominator : float , optional (default = 10.0 ) The denominator used to compute the decay rate if num_updates is passed.","title":"ExponentialMovingAverage"},{"location":"api/training/moving_average/#apply_1","text":"class ExponentialMovingAverage ( MovingAverage ): | ... | def apply ( self , num_updates : Optional [ int ] = None ) -> None Apply exponential moving average to named_parameters if specified, or we will apply this to all the trainable parameters of the model. The optional num_updates parameter allows one to tweak the decay rate dynamically. If passed, the actual decay rate used is: `min(decay, (numerator + num_updates) / (denominator + num_updates))` (This logic is based on the Tensorflow exponential moving average https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage )","title":"apply"},{"location":"api/training/no_op_trainer/","text":"[ allennlp .training .no_op_trainer ] NoOpTrainer # class NoOpTrainer ( Trainer ): | def __init__ ( self , serialization_dir : str , model : Model ) -> None Registered as a Trainer with name \"no_op\". train # class NoOpTrainer ( Trainer ): | ... | def train ( self ) -> Dict [ str , Any ] get_checkpoint_state # class NoOpTrainer ( Trainer ): | ... | @contextmanager | def get_checkpoint_state ( | self | ) -> Iterator [ Tuple [ Dict [ str , Any ], Dict [ str , Any ]]]","title":"no_op_trainer"},{"location":"api/training/no_op_trainer/#nooptrainer","text":"class NoOpTrainer ( Trainer ): | def __init__ ( self , serialization_dir : str , model : Model ) -> None Registered as a Trainer with name \"no_op\".","title":"NoOpTrainer"},{"location":"api/training/no_op_trainer/#train","text":"class NoOpTrainer ( Trainer ): | ... | def train ( self ) -> Dict [ str , Any ]","title":"train"},{"location":"api/training/no_op_trainer/#get_checkpoint_state","text":"class NoOpTrainer ( Trainer ): | ... | @contextmanager | def get_checkpoint_state ( | self | ) -> Iterator [ Tuple [ Dict [ str , Any ], Dict [ str , Any ]]]","title":"get_checkpoint_state"},{"location":"api/training/optimizers/","text":"[ allennlp .training .optimizers ] AllenNLP just uses PyTorch optimizers , with a thin wrapper to allow registering them and instantiating them from_params . The available optimizers are adadelta adagrad adam adamw huggingface_adamw sparse_adam sgd rmsprop adamax averaged_sgd make_parameter_groups # def make_parameter_groups ( model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None ) -> Union [ List [ Dict [ str , Any ]], List [ torch . nn . Parameter ]] Takes a list of model parameters with associated names (typically coming from something like model.parameters ), along with a grouping (as specified below), and prepares them to be passed to the __init__ function of a torch.Optimizer . This means separating the parameters into groups with the given regexes, and prepping whatever keyword arguments are given for those regexes in groups . groups contains something like: [ ([\"regex1\", \"regex2\"], {\"lr\": 1e-3}), ([\"regex3\"], {\"lr\": 1e-4}) ] All of key-value pairs specified in each of these dictionaries will passed passed as-is to the optimizer, with the exception of a dictionaries that specify requires_grad to be False : [ ... ([\"regex\"], {\"requires_grad\": False}) ] When a parameter group has {\"requires_grad\": False} , the gradient on all matching parameters will be disabled and that group will be dropped so that it's not actually passed to the optimizer. Ultimately, the return value of this function is in the right format to be passed directly as the params argument to a pytorch Optimizer . If there are multiple groups specified, this is list of dictionaries, where each dict contains a \"parameter group\" and groups specific options, e.g., {'params': [list of parameters], 'lr': 1e-3, ...}. Any config option not specified in the additional options (e.g. for the default group) is inherited from the top level arguments given in the constructor. See: https://pytorch.org/docs/0.3.0/optim.html?#per-parameter-options . See also our test_optimizer_parameter_groups test for an example of how this works in this code. The dictionary's return type is labeled as Any , because it can be a List[torch.nn.Parameter] (for the \"params\" key), or anything else (typically a float) for the other keys. Optimizer # class Optimizer ( Registrable ) This class just allows us to implement Registrable for Pytorch Optimizers. We do something a little bit different with Optimizers , because they are implemented as classes in PyTorch, and we want to use those classes. To make things easy, we just inherit from those classes, using multiple inheritance to also inherit from Optimizer . The only reason we do this is to make type inference on parameters possible, so we can construct these objects using our configuration framework. If you are writing your own script, you can safely ignore these classes and just use the torch.optim classes directly. If you are implementing one of these classes, the model_parameters and parameter_groups arguments to __init__ are important, and should always be present. The trainer will pass the trainable parameters in the model to the optimizer using the name model_parameters , so if you use a different name, your code will crash. Nothing will technically crash if you use a name other than parameter_groups for your second argument, it will just be annoyingly inconsistent. Most subclasses of Optimizer take both a model_parameters and a parameter_groups constructor argument. The model_parameters argument does not get an entry in a typical AllenNLP configuration file, but the parameter_groups argument does (if you want a non-default value). See the documentation for the make_parameter_groups function for more information on how the parameter_groups argument should be specified. default_implementation # class Optimizer ( Registrable ): | ... | default_implementation = \"adam\" default # class Optimizer ( Registrable ): | ... | @staticmethod | def default ( model_parameters : List ) -> \"Optimizer\" AdamOptimizer # class AdamOptimizer ( Optimizer , torch . optim . Adam ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 0.001 , | betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), | eps : float = 1e-08 , | weight_decay : float = 0.0 , | amsgrad : bool = False | ) Registered as an Optimizer with name \"adam\". SparseAdamOptimizer # class SparseAdamOptimizer ( Optimizer , torch . optim . SparseAdam ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 0.001 , | betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), | eps : float = 1e-08 | ) Registered as an Optimizer with name \"sparse_adam\". AdamaxOptimizer # class AdamaxOptimizer ( Optimizer , torch . optim . Adamax ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 0.002 , | betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), | eps : float = 1e-08 , | weight_decay : float = 0.0 | ) Registered as an Optimizer with name \"adamax\". AdamWOptimizer # class AdamWOptimizer ( Optimizer , torch . optim . AdamW ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 0.001 , | betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), | eps : float = 1e-08 , | weight_decay : float = 0.01 , | amsgrad : bool = False | ) Registered as an Optimizer with name \"adamw\". HuggingfaceAdamWOptimizer # class HuggingfaceAdamWOptimizer ( Optimizer , transformers . AdamW ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 1e-5 , | betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), | eps : float = 1e-08 , | weight_decay : float = 0.0 , | correct_bias : bool = True | ) Registered as an Optimizer with name \"huggingface_adamw\". AdagradOptimizer # class AdagradOptimizer ( Optimizer , torch . optim . Adagrad ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 0.01 , | lr_decay : float = 0.0 , | weight_decay : float = 0.0 , | initial_accumulator_value : float = 0.0 , | eps : float = 1e-10 | ) Registered as an Optimizer with name \"adagrad\". AdadeltaOptimizer # class AdadeltaOptimizer ( Optimizer , torch . optim . Adadelta ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 1.0 , | rho : float = 0.9 , | eps : float = 1e-06 , | weight_decay : float = 0.0 | ) Registered as an Optimizer with name \"adadelta\". SgdOptimizer # class SgdOptimizer ( Optimizer , torch . optim . SGD ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | lr : float , | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | momentum : float = 0.0 , | dampening : float = 0 , | weight_decay : float = 0.0 , | nesterov : bool = False | ) Registered as an Optimizer with name \"sgd\". RmsPropOptimizer # class RmsPropOptimizer ( Optimizer , torch . optim . RMSprop ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 0.01 , | alpha : float = 0.99 , | eps : float = 1e-08 , | weight_decay : float = 0.0 , | momentum : float = 0.0 , | centered : bool = False | ) Registered as an Optimizer with name \"rmsprop\". AveragedSgdOptimizer # class AveragedSgdOptimizer ( Optimizer , torch . optim . ASGD ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 0.01 , | lambd : float = 0.0001 , | alpha : float = 0.75 , | t0 : float = 1000000.0 , | weight_decay : float = 0.0 | ) Registered as an Optimizer with name \"averaged_sgd\". DenseSparseAdam # class DenseSparseAdam ( Optimizer , torch . optim . Optimizer ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr = 1e-3 , | betas = ( 0.9 , 0.999 ), | eps = 1e-8 | ) NOTE: This class has been copied verbatim from the separate Dense and Sparse versions of Adam in Pytorch. Implements Adam algorithm with dense & sparse gradients. It has been proposed in Adam: A Method for Stochastic Optimization. Registered as an Optimizer with name \"dense_sparse_adam\". Parameters params : iterable iterable of parameters to optimize or dicts defining parameter groups lr : float , optional (default = 1e-3 ) The learning rate. betas : Tuple[float, float] , optional (default = (0.9, 0.999) ) coefficients used for computing running averages of gradient and its square. eps : float , optional (default = 1e-8 ) A term added to the denominator to improve numerical stability. step # class DenseSparseAdam ( Optimizer , torch . optim . Optimizer ): | ... | def step ( self , closure = None ) Performs a single optimization step. Parameters closure : callable , optional A closure that reevaluates the model and returns the loss.","title":"optimizers"},{"location":"api/training/optimizers/#make_parameter_groups","text":"def make_parameter_groups ( model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None ) -> Union [ List [ Dict [ str , Any ]], List [ torch . nn . Parameter ]] Takes a list of model parameters with associated names (typically coming from something like model.parameters ), along with a grouping (as specified below), and prepares them to be passed to the __init__ function of a torch.Optimizer . This means separating the parameters into groups with the given regexes, and prepping whatever keyword arguments are given for those regexes in groups . groups contains something like: [ ([\"regex1\", \"regex2\"], {\"lr\": 1e-3}), ([\"regex3\"], {\"lr\": 1e-4}) ] All of key-value pairs specified in each of these dictionaries will passed passed as-is to the optimizer, with the exception of a dictionaries that specify requires_grad to be False : [ ... ([\"regex\"], {\"requires_grad\": False}) ] When a parameter group has {\"requires_grad\": False} , the gradient on all matching parameters will be disabled and that group will be dropped so that it's not actually passed to the optimizer. Ultimately, the return value of this function is in the right format to be passed directly as the params argument to a pytorch Optimizer . If there are multiple groups specified, this is list of dictionaries, where each dict contains a \"parameter group\" and groups specific options, e.g., {'params': [list of parameters], 'lr': 1e-3, ...}. Any config option not specified in the additional options (e.g. for the default group) is inherited from the top level arguments given in the constructor. See: https://pytorch.org/docs/0.3.0/optim.html?#per-parameter-options . See also our test_optimizer_parameter_groups test for an example of how this works in this code. The dictionary's return type is labeled as Any , because it can be a List[torch.nn.Parameter] (for the \"params\" key), or anything else (typically a float) for the other keys.","title":"make_parameter_groups"},{"location":"api/training/optimizers/#optimizer","text":"class Optimizer ( Registrable ) This class just allows us to implement Registrable for Pytorch Optimizers. We do something a little bit different with Optimizers , because they are implemented as classes in PyTorch, and we want to use those classes. To make things easy, we just inherit from those classes, using multiple inheritance to also inherit from Optimizer . The only reason we do this is to make type inference on parameters possible, so we can construct these objects using our configuration framework. If you are writing your own script, you can safely ignore these classes and just use the torch.optim classes directly. If you are implementing one of these classes, the model_parameters and parameter_groups arguments to __init__ are important, and should always be present. The trainer will pass the trainable parameters in the model to the optimizer using the name model_parameters , so if you use a different name, your code will crash. Nothing will technically crash if you use a name other than parameter_groups for your second argument, it will just be annoyingly inconsistent. Most subclasses of Optimizer take both a model_parameters and a parameter_groups constructor argument. The model_parameters argument does not get an entry in a typical AllenNLP configuration file, but the parameter_groups argument does (if you want a non-default value). See the documentation for the make_parameter_groups function for more information on how the parameter_groups argument should be specified.","title":"Optimizer"},{"location":"api/training/optimizers/#default_implementation","text":"class Optimizer ( Registrable ): | ... | default_implementation = \"adam\"","title":"default_implementation"},{"location":"api/training/optimizers/#default","text":"class Optimizer ( Registrable ): | ... | @staticmethod | def default ( model_parameters : List ) -> \"Optimizer\"","title":"default"},{"location":"api/training/optimizers/#adamoptimizer","text":"class AdamOptimizer ( Optimizer , torch . optim . Adam ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 0.001 , | betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), | eps : float = 1e-08 , | weight_decay : float = 0.0 , | amsgrad : bool = False | ) Registered as an Optimizer with name \"adam\".","title":"AdamOptimizer"},{"location":"api/training/optimizers/#sparseadamoptimizer","text":"class SparseAdamOptimizer ( Optimizer , torch . optim . SparseAdam ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 0.001 , | betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), | eps : float = 1e-08 | ) Registered as an Optimizer with name \"sparse_adam\".","title":"SparseAdamOptimizer"},{"location":"api/training/optimizers/#adamaxoptimizer","text":"class AdamaxOptimizer ( Optimizer , torch . optim . Adamax ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 0.002 , | betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), | eps : float = 1e-08 , | weight_decay : float = 0.0 | ) Registered as an Optimizer with name \"adamax\".","title":"AdamaxOptimizer"},{"location":"api/training/optimizers/#adamwoptimizer","text":"class AdamWOptimizer ( Optimizer , torch . optim . AdamW ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 0.001 , | betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), | eps : float = 1e-08 , | weight_decay : float = 0.01 , | amsgrad : bool = False | ) Registered as an Optimizer with name \"adamw\".","title":"AdamWOptimizer"},{"location":"api/training/optimizers/#huggingfaceadamwoptimizer","text":"class HuggingfaceAdamWOptimizer ( Optimizer , transformers . AdamW ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 1e-5 , | betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), | eps : float = 1e-08 , | weight_decay : float = 0.0 , | correct_bias : bool = True | ) Registered as an Optimizer with name \"huggingface_adamw\".","title":"HuggingfaceAdamWOptimizer"},{"location":"api/training/optimizers/#adagradoptimizer","text":"class AdagradOptimizer ( Optimizer , torch . optim . Adagrad ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 0.01 , | lr_decay : float = 0.0 , | weight_decay : float = 0.0 , | initial_accumulator_value : float = 0.0 , | eps : float = 1e-10 | ) Registered as an Optimizer with name \"adagrad\".","title":"AdagradOptimizer"},{"location":"api/training/optimizers/#adadeltaoptimizer","text":"class AdadeltaOptimizer ( Optimizer , torch . optim . Adadelta ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 1.0 , | rho : float = 0.9 , | eps : float = 1e-06 , | weight_decay : float = 0.0 | ) Registered as an Optimizer with name \"adadelta\".","title":"AdadeltaOptimizer"},{"location":"api/training/optimizers/#sgdoptimizer","text":"class SgdOptimizer ( Optimizer , torch . optim . SGD ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | lr : float , | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | momentum : float = 0.0 , | dampening : float = 0 , | weight_decay : float = 0.0 , | nesterov : bool = False | ) Registered as an Optimizer with name \"sgd\".","title":"SgdOptimizer"},{"location":"api/training/optimizers/#rmspropoptimizer","text":"class RmsPropOptimizer ( Optimizer , torch . optim . RMSprop ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 0.01 , | alpha : float = 0.99 , | eps : float = 1e-08 , | weight_decay : float = 0.0 , | momentum : float = 0.0 , | centered : bool = False | ) Registered as an Optimizer with name \"rmsprop\".","title":"RmsPropOptimizer"},{"location":"api/training/optimizers/#averagedsgdoptimizer","text":"class AveragedSgdOptimizer ( Optimizer , torch . optim . ASGD ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr : float = 0.01 , | lambd : float = 0.0001 , | alpha : float = 0.75 , | t0 : float = 1000000.0 , | weight_decay : float = 0.0 | ) Registered as an Optimizer with name \"averaged_sgd\".","title":"AveragedSgdOptimizer"},{"location":"api/training/optimizers/#densesparseadam","text":"class DenseSparseAdam ( Optimizer , torch . optim . Optimizer ): | def __init__ ( | self , | model_parameters : List [ Tuple [ str , torch . nn . Parameter ]], | parameter_groups : List [ Tuple [ List [ str ], Dict [ str , Any ]]] = None , | lr = 1e-3 , | betas = ( 0.9 , 0.999 ), | eps = 1e-8 | ) NOTE: This class has been copied verbatim from the separate Dense and Sparse versions of Adam in Pytorch. Implements Adam algorithm with dense & sparse gradients. It has been proposed in Adam: A Method for Stochastic Optimization. Registered as an Optimizer with name \"dense_sparse_adam\". Parameters params : iterable iterable of parameters to optimize or dicts defining parameter groups lr : float , optional (default = 1e-3 ) The learning rate. betas : Tuple[float, float] , optional (default = (0.9, 0.999) ) coefficients used for computing running averages of gradient and its square. eps : float , optional (default = 1e-8 ) A term added to the denominator to improve numerical stability.","title":"DenseSparseAdam"},{"location":"api/training/optimizers/#step","text":"class DenseSparseAdam ( Optimizer , torch . optim . Optimizer ): | ... | def step ( self , closure = None ) Performs a single optimization step. Parameters closure : callable , optional A closure that reevaluates the model and returns the loss.","title":"step"},{"location":"api/training/scheduler/","text":"[ allennlp .training .scheduler ] Scheduler # class Scheduler : | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | param_group_field : str , | last_epoch : int = - 1 | ) -> None A Scheduler is a generalization of PyTorch learning rate schedulers. A scheduler can be used to update any field in an optimizer's parameter groups, not just the learning rate. During training using the AllenNLP Trainer , this is the API and calling sequence for step and step_batch :: scheduler = ... # creates scheduler, calls self.step(last_epoch=-1) in init batch_num_total = 0 for epoch in range(num_epochs): for batch in batchs_in_epoch: # compute loss, update parameters with current learning rates # call step_batch AFTER updating parameters batch_num_total += 1 scheduler.step_batch(batch_num_total) # call step() at the END of each epoch scheduler.step(validation_metrics, epoch) state_dict # class Scheduler : | ... | def state_dict ( self ) -> Dict [ str , Any ] Returns the state of the scheduler as a dict . load_state_dict # class Scheduler : | ... | def load_state_dict ( self , state_dict : Dict [ str , Any ]) -> None Load the schedulers state. Parameters state_dict : Dict[str, Any] Scheduler state. Should be an object returned from a call to state_dict . get_values # class Scheduler : | ... | def get_values ( self ) step # class Scheduler : | ... | def step ( self , metric : float = None ) -> None step_batch # class Scheduler : | ... | def step_batch ( self , batch_num_total : int = None ) -> None By default, a scheduler is assumed to only update every epoch, not every batch. So this does nothing unless it's overriden.","title":"scheduler"},{"location":"api/training/scheduler/#scheduler","text":"class Scheduler : | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | param_group_field : str , | last_epoch : int = - 1 | ) -> None A Scheduler is a generalization of PyTorch learning rate schedulers. A scheduler can be used to update any field in an optimizer's parameter groups, not just the learning rate. During training using the AllenNLP Trainer , this is the API and calling sequence for step and step_batch :: scheduler = ... # creates scheduler, calls self.step(last_epoch=-1) in init batch_num_total = 0 for epoch in range(num_epochs): for batch in batchs_in_epoch: # compute loss, update parameters with current learning rates # call step_batch AFTER updating parameters batch_num_total += 1 scheduler.step_batch(batch_num_total) # call step() at the END of each epoch scheduler.step(validation_metrics, epoch)","title":"Scheduler"},{"location":"api/training/scheduler/#state_dict","text":"class Scheduler : | ... | def state_dict ( self ) -> Dict [ str , Any ] Returns the state of the scheduler as a dict .","title":"state_dict"},{"location":"api/training/scheduler/#load_state_dict","text":"class Scheduler : | ... | def load_state_dict ( self , state_dict : Dict [ str , Any ]) -> None Load the schedulers state. Parameters state_dict : Dict[str, Any] Scheduler state. Should be an object returned from a call to state_dict .","title":"load_state_dict"},{"location":"api/training/scheduler/#get_values","text":"class Scheduler : | ... | def get_values ( self )","title":"get_values"},{"location":"api/training/scheduler/#step","text":"class Scheduler : | ... | def step ( self , metric : float = None ) -> None","title":"step"},{"location":"api/training/scheduler/#step_batch","text":"class Scheduler : | ... | def step_batch ( self , batch_num_total : int = None ) -> None By default, a scheduler is assumed to only update every epoch, not every batch. So this does nothing unless it's overriden.","title":"step_batch"},{"location":"api/training/tensorboard_writer/","text":"[ allennlp .training .tensorboard_writer ] TensorboardWriter # class TensorboardWriter ( FromParams ): | def __init__ ( | self , | serialization_dir : Optional [ str ] = None , | summary_interval : int = 100 , | histogram_interval : int = None , | batch_size_interval : Optional [ int ] = None , | should_log_parameter_statistics : bool = True , | should_log_learning_rate : bool = False , | get_batch_num_total : Callable [[], int ] = None | ) -> None Class that handles Tensorboard (and other) logging. Parameters serialization_dir : str , optional (default = None ) If provided, this is where the Tensorboard logs will be written. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"tensorboard_writer\", it gets passed in separately. - summary_interval : int , optional (default = 100 ) Most statistics will be written out only every this many batches. - histogram_interval : int , optional (default = None ) If provided, activation histograms will be written out every this many batches. If None, activation histograms will not be written out. When this parameter is specified, the following additional logging is enabled: * Histograms of model parameters * The ratio of parameter update norm to parameter norm * Histogram of layer activations We log histograms of the parameters returned by model.get_parameters_for_histogram_tensorboard_logging . The layer activations are logged for any modules in the Model that have the attribute should_log_activations set to True . Logging histograms requires a number of GPU-CPU copies during training and is typically slow, so we recommend logging histograms relatively infrequently. Note: only Modules that return tensors, tuples of tensors or dicts with tensors as values currently support activation logging. - batch_size_interval : int , optional (default = None ) If defined, how often to log the average batch size. - should_log_parameter_statistics : bool , optional (default = True ) Whether to log parameter statistics (mean and standard deviation of parameters and gradients). - should_log_learning_rate : bool , optional (default = False ) Whether to log (parameter-specific) learning rate. - get_batch_num_total : Callable[[], int] , optional (default = None ) A thunk that returns the number of batches so far. Most likely this will be a closure around an instance variable in your Trainer class. Because of circular dependencies in constructing this object and the Trainer , this is typically None when you construct the object, but it gets set inside the constructor of our Trainer . log_memory_usage # class TensorboardWriter ( FromParams ): | ... | def log_memory_usage ( | self , | cpu_memory_usage : Dict [ int , float ], | gpu_memory_usage : Dict [ int , int ] | ) log_batch # class TensorboardWriter ( FromParams ): | ... | def log_batch ( | self , | model : Model , | optimizer : Optimizer , | batch_grad_norm : Optional [ float ], | metrics : Dict [ str , float ], | batch_group : List [ List [ TensorDict ]], | param_updates : Optional [ Dict [ str , torch . Tensor ]] | ) -> None reset_epoch # class TensorboardWriter ( FromParams ): | ... | def reset_epoch ( self ) -> None should_log_this_batch # class TensorboardWriter ( FromParams ): | ... | def should_log_this_batch ( self ) -> bool should_log_histograms_this_batch # class TensorboardWriter ( FromParams ): | ... | def should_log_histograms_this_batch ( self ) -> bool add_train_scalar # class TensorboardWriter ( FromParams ): | ... | def add_train_scalar ( | self , | name : str , | value : float , | timestep : int = None | ) -> None add_train_histogram # class TensorboardWriter ( FromParams ): | ... | def add_train_histogram ( self , name : str , values : torch . Tensor ) -> None add_validation_scalar # class TensorboardWriter ( FromParams ): | ... | def add_validation_scalar ( | self , | name : str , | value : float , | timestep : int = None | ) -> None log_parameter_and_gradient_statistics # class TensorboardWriter ( FromParams ): | ... | def log_parameter_and_gradient_statistics ( | self , | model : Model , | batch_grad_norm : float | ) -> None Send the mean and std of all parameters and gradients to tensorboard, as well as logging the average gradient norm. log_learning_rates # class TensorboardWriter ( FromParams ): | ... | def log_learning_rates ( | self , | model : Model , | optimizer : torch . optim . Optimizer | ) Send current parameter specific learning rates to tensorboard log_histograms # class TensorboardWriter ( FromParams ): | ... | def log_histograms ( self , model : Model ) -> None Send histograms of parameters to tensorboard. log_gradient_updates # class TensorboardWriter ( FromParams ): | ... | def log_gradient_updates ( | self , | model : Model , | param_updates : Dict [ str , torch . Tensor ] | ) -> None log_metrics # class TensorboardWriter ( FromParams ): | ... | def log_metrics ( | self , | train_metrics : dict , | val_metrics : dict = None , | epoch : int = None , | log_to_console : bool = False | ) -> None Sends all of the train metrics (and validation metrics, if provided) to tensorboard. enable_activation_logging # class TensorboardWriter ( FromParams ): | ... | def enable_activation_logging ( self , model : Model ) -> None log_activation_histogram # class TensorboardWriter ( FromParams ): | ... | def log_activation_histogram ( self , outputs , log_prefix : str ) -> None close # class TensorboardWriter ( FromParams ): | ... | def close ( self ) -> None Calls the close method of the SummaryWriter s which makes sure that pending scalars are flushed to disk and the tensorboard event files are closed properly.","title":"tensorboard_writer"},{"location":"api/training/tensorboard_writer/#tensorboardwriter","text":"class TensorboardWriter ( FromParams ): | def __init__ ( | self , | serialization_dir : Optional [ str ] = None , | summary_interval : int = 100 , | histogram_interval : int = None , | batch_size_interval : Optional [ int ] = None , | should_log_parameter_statistics : bool = True , | should_log_learning_rate : bool = False , | get_batch_num_total : Callable [[], int ] = None | ) -> None Class that handles Tensorboard (and other) logging. Parameters serialization_dir : str , optional (default = None ) If provided, this is where the Tensorboard logs will be written. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"tensorboard_writer\", it gets passed in separately. - summary_interval : int , optional (default = 100 ) Most statistics will be written out only every this many batches. - histogram_interval : int , optional (default = None ) If provided, activation histograms will be written out every this many batches. If None, activation histograms will not be written out. When this parameter is specified, the following additional logging is enabled: * Histograms of model parameters * The ratio of parameter update norm to parameter norm * Histogram of layer activations We log histograms of the parameters returned by model.get_parameters_for_histogram_tensorboard_logging . The layer activations are logged for any modules in the Model that have the attribute should_log_activations set to True . Logging histograms requires a number of GPU-CPU copies during training and is typically slow, so we recommend logging histograms relatively infrequently. Note: only Modules that return tensors, tuples of tensors or dicts with tensors as values currently support activation logging. - batch_size_interval : int , optional (default = None ) If defined, how often to log the average batch size. - should_log_parameter_statistics : bool , optional (default = True ) Whether to log parameter statistics (mean and standard deviation of parameters and gradients). - should_log_learning_rate : bool , optional (default = False ) Whether to log (parameter-specific) learning rate. - get_batch_num_total : Callable[[], int] , optional (default = None ) A thunk that returns the number of batches so far. Most likely this will be a closure around an instance variable in your Trainer class. Because of circular dependencies in constructing this object and the Trainer , this is typically None when you construct the object, but it gets set inside the constructor of our Trainer .","title":"TensorboardWriter"},{"location":"api/training/tensorboard_writer/#log_memory_usage","text":"class TensorboardWriter ( FromParams ): | ... | def log_memory_usage ( | self , | cpu_memory_usage : Dict [ int , float ], | gpu_memory_usage : Dict [ int , int ] | )","title":"log_memory_usage"},{"location":"api/training/tensorboard_writer/#log_batch","text":"class TensorboardWriter ( FromParams ): | ... | def log_batch ( | self , | model : Model , | optimizer : Optimizer , | batch_grad_norm : Optional [ float ], | metrics : Dict [ str , float ], | batch_group : List [ List [ TensorDict ]], | param_updates : Optional [ Dict [ str , torch . Tensor ]] | ) -> None","title":"log_batch"},{"location":"api/training/tensorboard_writer/#reset_epoch","text":"class TensorboardWriter ( FromParams ): | ... | def reset_epoch ( self ) -> None","title":"reset_epoch"},{"location":"api/training/tensorboard_writer/#should_log_this_batch","text":"class TensorboardWriter ( FromParams ): | ... | def should_log_this_batch ( self ) -> bool","title":"should_log_this_batch"},{"location":"api/training/tensorboard_writer/#should_log_histograms_this_batch","text":"class TensorboardWriter ( FromParams ): | ... | def should_log_histograms_this_batch ( self ) -> bool","title":"should_log_histograms_this_batch"},{"location":"api/training/tensorboard_writer/#add_train_scalar","text":"class TensorboardWriter ( FromParams ): | ... | def add_train_scalar ( | self , | name : str , | value : float , | timestep : int = None | ) -> None","title":"add_train_scalar"},{"location":"api/training/tensorboard_writer/#add_train_histogram","text":"class TensorboardWriter ( FromParams ): | ... | def add_train_histogram ( self , name : str , values : torch . Tensor ) -> None","title":"add_train_histogram"},{"location":"api/training/tensorboard_writer/#add_validation_scalar","text":"class TensorboardWriter ( FromParams ): | ... | def add_validation_scalar ( | self , | name : str , | value : float , | timestep : int = None | ) -> None","title":"add_validation_scalar"},{"location":"api/training/tensorboard_writer/#log_parameter_and_gradient_statistics","text":"class TensorboardWriter ( FromParams ): | ... | def log_parameter_and_gradient_statistics ( | self , | model : Model , | batch_grad_norm : float | ) -> None Send the mean and std of all parameters and gradients to tensorboard, as well as logging the average gradient norm.","title":"log_parameter_and_gradient_statistics"},{"location":"api/training/tensorboard_writer/#log_learning_rates","text":"class TensorboardWriter ( FromParams ): | ... | def log_learning_rates ( | self , | model : Model , | optimizer : torch . optim . Optimizer | ) Send current parameter specific learning rates to tensorboard","title":"log_learning_rates"},{"location":"api/training/tensorboard_writer/#log_histograms","text":"class TensorboardWriter ( FromParams ): | ... | def log_histograms ( self , model : Model ) -> None Send histograms of parameters to tensorboard.","title":"log_histograms"},{"location":"api/training/tensorboard_writer/#log_gradient_updates","text":"class TensorboardWriter ( FromParams ): | ... | def log_gradient_updates ( | self , | model : Model , | param_updates : Dict [ str , torch . Tensor ] | ) -> None","title":"log_gradient_updates"},{"location":"api/training/tensorboard_writer/#log_metrics","text":"class TensorboardWriter ( FromParams ): | ... | def log_metrics ( | self , | train_metrics : dict , | val_metrics : dict = None , | epoch : int = None , | log_to_console : bool = False | ) -> None Sends all of the train metrics (and validation metrics, if provided) to tensorboard.","title":"log_metrics"},{"location":"api/training/tensorboard_writer/#enable_activation_logging","text":"class TensorboardWriter ( FromParams ): | ... | def enable_activation_logging ( self , model : Model ) -> None","title":"enable_activation_logging"},{"location":"api/training/tensorboard_writer/#log_activation_histogram","text":"class TensorboardWriter ( FromParams ): | ... | def log_activation_histogram ( self , outputs , log_prefix : str ) -> None","title":"log_activation_histogram"},{"location":"api/training/tensorboard_writer/#close","text":"class TensorboardWriter ( FromParams ): | ... | def close ( self ) -> None Calls the close method of the SummaryWriter s which makes sure that pending scalars are flushed to disk and the tensorboard event files are closed properly.","title":"close"},{"location":"api/training/trainer/","text":"[ allennlp .training .trainer ] Trainer # class Trainer ( Registrable ): | def __init__ ( | self , | serialization_dir : str , | cuda_device : Optional [ Union [ int , torch . device ]] = None , | distributed : bool = False , | local_rank : int = 0 , | world_size : int = 1 | ) -> None The base class for an AllenNLP trainer. It can do pretty much anything you want. Your subclass should implement train and also probably from_params . default_implementation # class Trainer ( Registrable ): | ... | default_implementation = \"gradient_descent\" train # class Trainer ( Registrable ): | ... | def train ( self ) -> Dict [ str , Any ] Train a model and return the results. get_checkpoint_state # class Trainer ( Registrable ): | ... | @contextmanager | def get_checkpoint_state ( | self | ) -> Iterator [ Tuple [ Dict [ str , Any ], Dict [ str , Any ]]] Returns a tuple of (model state, training state), where training state could have several internal components (e.g., for an, optimizer, learning rate scheduler, etc.). This is a context manager, and should be called as with trainer.get_checkpoint_state() as state: , so that the trainer has the opportunity to change and restore its internal state for checkpointing. This is used, e.g., for moving averages of model weights. BatchCallback # class BatchCallback ( Registrable ) An optional callback that you can pass to the GradientDescentTrainer that will be called at the end of every batch, during both training and validation. The default implementation does nothing. You can implement your own callback and do whatever you want, such as saving predictions to disk or extra logging. __call__ # class BatchCallback ( Registrable ): | ... | def __call__ ( | self , | trainer : \"GradientDescentTrainer\" , | batch_inputs : List [ List [ TensorDict ]], | batch_outputs : List [ Dict [ str , Any ]], | epoch : int , | batch_number : int , | is_training : bool , | is_master : bool | ) -> None TensoboardBatchMemoryUsage # class TensoboardBatchMemoryUsage ( BatchCallback ) Logs the CPU and GPU memory usage to tensorboard on every batch. This is mainly used for debugging as it can cause a significant slowdown in training. __call__ # class TensoboardBatchMemoryUsage ( BatchCallback ): | ... | def __call__ ( | self , | trainer : \"GradientDescentTrainer\" , | batch_inputs : List [ List [ TensorDict ]], | batch_outputs : List [ Dict [ str , Any ]], | epoch : int , | batch_number : int , | is_training : bool , | is_master : bool | ) -> None In the distributed case we need to call this from every worker, since every worker reports its own memory usage. EpochCallback # class EpochCallback ( Registrable ) An optional callback that you can pass to the GradientDescentTrainer that will be called at the end of every epoch (and before the start of training, with epoch=-1 ). The default implementation does nothing. You can implement your own callback and do whatever you want, such as additional modifications of the trainer's state in between epochs. __call__ # class EpochCallback ( Registrable ): | ... | def __call__ ( | self , | trainer : \"GradientDescentTrainer\" , | metrics : Dict [ str , Any ], | epoch : int , | is_master : bool | ) -> None TrackEpochCallback # class TrackEpochCallback : | def __init__ ( self ) A callback that you can pass to the GradientDescentTrainer to access the current epoch number in your model during training. This callback sets model.epoch , which can be read inside of model.forward() . Since the EpochCallback passes epoch=-1 at the start of the training, we set model.epoch = epoch + 1 which now denotes the number of completed epochs at a given training state. __call__ # class TrackEpochCallback : | ... | def __call__ ( | self , | trainer : \"GradientDescentTrainer\" , | metrics : Dict [ str , Any ], | epoch : int , | is_master : bool | ) -> None GradientDescentTrainer # class GradientDescentTrainer ( Trainer ): | def __init__ ( | self , | model : Model , | optimizer : torch . optim . Optimizer , | data_loader : DataLoader , | patience : Optional [ int ] = None , | validation_metric : str = \"-loss\" , | validation_data_loader : DataLoader = None , | num_epochs : int = 20 , | serialization_dir : Optional [ str ] = None , | checkpointer : Checkpointer = None , | cuda_device : Optional [ Union [ int , torch . device ]] = None , | grad_norm : Optional [ float ] = None , | grad_clipping : Optional [ float ] = None , | learning_rate_scheduler : Optional [ LearningRateScheduler ] = None , | momentum_scheduler : Optional [ MomentumScheduler ] = None , | tensorboard_writer : TensorboardWriter = None , | moving_average : Optional [ MovingAverage ] = None , | batch_callbacks : List [ BatchCallback ] = None , | epoch_callbacks : List [ EpochCallback ] = None , | distributed : bool = False , | local_rank : int = 0 , | world_size : int = 1 , | num_gradient_accumulation_steps : int = 1 , | use_amp : bool = False | ) -> None A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset and a DataLoader , and uses the supplied Optimizer to learn the weights for your model over some fixed number of epochs. You can also pass in a validation dataloader and enable early stopping. There are many other bells and whistles as well. Registered as a Trainer with the name \"gradient_descent\" (and is also the default Trainer ). The constructor that is registered is from_partial_objects - see the arguments to that function for the exact keys that should be used, if you are using a configuration file. They largely match the arguments to __init__ , and we don't repeat their docstrings in from_partial_objects . Parameters model : Model An AllenNLP model to be optimized. Pytorch Modules can also be optimized if their forward method returns a dictionary with a \"loss\" key, containing a scalar tensor representing the loss function to be optimized. If you are training your model using GPUs, your model should already be on the correct device. (If you are using our train command this will be handled for you.) In a typical AllenNLP configuration file, this parameter does not get an entry under the \"trainer\", it gets constructed separately. optimizer : torch.nn.Optimizer An instance of a Pytorch Optimizer, instantiated with the parameters of the model to be optimized. data_loader : DataLoader A DataLoader containing your Dataset , yielding padded indexed batches. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"trainer\", it gets constructed separately. patience : Optional[int] > 0 , optional (default = None ) Number of epochs to be patient before early stopping: the training is stopped after patience epochs with no improvement. If given, it must be > 0 . If None, early stopping is disabled. validation_metric : str , optional (default = \"-loss\" ) Validation metric to measure for whether to stop training using patience and whether to serialize an is_best model each epoch. The metric name must be prepended with either \"+\" or \"-\", which specifies whether the metric is an increasing or decreasing function. validation_data_loader : DataLoader , optional (default = None ) A DataLoader to use for the validation set. If None , then use the training DataLoader with the validation data. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"trainer\", it gets constructed separately. num_epochs : int , optional (default = 20 ) Number of training epochs. serialization_dir : str , optional (default = None ) Path to directory for saving and loading model files. Models will not be saved if this parameter is not passed. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"trainer\", it gets constructed separately. checkpointer : Checkpointer , optional (default = None ) A Checkpointer is responsible for periodically saving model weights. If none is given here, we will construct one with default parameters. cuda_device : int , optional (default = -1 ) An integer specifying the CUDA device(s) to use for this process. If -1, the CPU is used. Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU. grad_norm : float , optional (default = None ) If provided, gradient norms will be rescaled to have a maximum of this value. grad_clipping : float , optional (default = None ) If provided, gradients will be clipped during the backward pass to have an (absolute) maximum of this value. If you are getting NaNs in your gradients during training that are not solved by using grad_norm , you may need this. learning_rate_scheduler : LearningRateScheduler , optional (default = None ) If specified, the learning rate will be decayed with respect to this schedule at the end of each epoch (or batch, if the scheduler implements the step_batch method). If you use torch.optim.lr_scheduler.ReduceLROnPlateau , this will use the validation_metric provided to determine if learning has plateaued. To support updating the learning rate on every batch, this can optionally implement step_batch(batch_num_total) which updates the learning rate given the batch number. momentum_scheduler : MomentumScheduler , optional (default = None ) If specified, the momentum will be updated at the end of each batch or epoch according to the schedule. tensorboard_writer : TensorboardWriter , optional If this is not provided, we will construct a TensorboardWriter with default parameters and use that. moving_average : MovingAverage , optional (default = None ) If provided, we will maintain moving averages for all parameters. During training, we employ a shadow variable for each parameter, which maintains the moving average. During evaluation, we backup the original parameters and assign the moving averages to corresponding parameters. Be careful that when saving the checkpoint, we will save the moving averages of parameters. This is necessary because we want the saved model to perform as well as the validated model if we load it later. But this may cause problems if you restart the training from checkpoint. batch_callbacks : List[BatchCallback] , optional (default = None ) A list of callbacks that will be called at the end of every batch, during both train and validation. epoch_callbacks : List[EpochCallback] , optional (default = None ) A list of callbacks that will be called at the end of every epoch, and at the start of training (with epoch = -1). distributed : bool , optional (default = False ) If set, PyTorch's DistributedDataParallel is used to train the model in multiple GPUs. This also requires world_size to be greater than 1. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to the \"trainer\" entry, that specifies a list of \"cuda_devices\"). local_rank : int , optional (default = 0 ) This is the unique identifier of the Trainer in a distributed process group. The GPU device id is used as the rank. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"trainer\", it gets constructed separately. world_size : int , optional (default = 1 ) The number of Trainer workers participating in the distributed training. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"trainer\", it gets constructed separately. num_gradient_accumulation_steps : int , optional (default = 1 ) Gradients are accumulated for the given number of steps before doing an optimizer step. This can be useful to accommodate batches that are larger than the RAM size. Refer Thomas Wolf's post for details on Gradient Accumulation. use_amp : bool , optional (default = False ) If True , we'll train using Automatic Mixed Precision . rescale_gradients # class GradientDescentTrainer ( Trainer ): | ... | def rescale_gradients ( self ) -> float Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled. Returns the norm of the gradients. batch_outputs # class GradientDescentTrainer ( Trainer ): | ... | def batch_outputs ( | self , | batch : TensorDict , | for_training : bool | ) -> Dict [ str , torch . Tensor ] Does a forward pass on the given batch and returns the output dictionary that the model returns, after adding any specified regularization penalty to the loss (if training). train # class GradientDescentTrainer ( Trainer ): | ... | def train ( self ) -> Dict [ str , Any ] Trains the supplied model with the supplied parameters. get_checkpoint_state # class GradientDescentTrainer ( Trainer ): | ... | @contextmanager | def get_checkpoint_state ( | self | ) -> Iterator [ Tuple [ Dict [ str , Any ], Dict [ str , Any ]]] from_partial_objects # class GradientDescentTrainer ( Trainer ): | ... | @classmethod | def from_partial_objects ( | cls , | model : Model , | serialization_dir : str , | data_loader : DataLoader , | validation_data_loader : DataLoader = None , | local_rank : int = 0 , | patience : int = None , | validation_metric : str = \"-loss\" , | num_epochs : int = 20 , | cuda_device : Optional [ Union [ int , torch . device ]] = None , | grad_norm : float = None , | grad_clipping : float = None , | distributed : bool = None , | world_size : int = 1 , | num_gradient_accumulation_steps : int = 1 , | use_amp : bool = False , | no_grad : List [ str ] = None , | optimizer : Lazy [ Optimizer ] = None , | learning_rate_scheduler : Lazy [ LearningRateScheduler ] = None , | momentum_scheduler : Lazy [ MomentumScheduler ] = None , | tensorboard_writer : Lazy [ TensorboardWriter ] = None , | moving_average : Lazy [ MovingAverage ] = None , | checkpointer : Lazy [ Checkpointer ] = None , | batch_callbacks : List [ BatchCallback ] = None , | epoch_callbacks : List [ EpochCallback ] = None | ) -> \"Trainer\" This method exists so that we can have a documented method to construct this class using FromParams . If you are not using FromParams or config files, you can safely ignore this method. The reason we can't just use __init__ with FromParams here is because there are sequential dependencies to this class's arguments. Anything that has a Lazy[] type annotation needs something from one of the non- Lazy arguments. The Optimizer needs to have the parameters from the Model before it's constructed, and the Schedulers need to have the Optimizer . Because of this, the typical way we construct things FromParams doesn't work, so we use Lazy to allow for constructing the objects sequentially. If you're not using FromParams , you can just construct these arguments in the right order yourself in your code and call the constructor directly.","title":"trainer"},{"location":"api/training/trainer/#trainer","text":"class Trainer ( Registrable ): | def __init__ ( | self , | serialization_dir : str , | cuda_device : Optional [ Union [ int , torch . device ]] = None , | distributed : bool = False , | local_rank : int = 0 , | world_size : int = 1 | ) -> None The base class for an AllenNLP trainer. It can do pretty much anything you want. Your subclass should implement train and also probably from_params .","title":"Trainer"},{"location":"api/training/trainer/#default_implementation","text":"class Trainer ( Registrable ): | ... | default_implementation = \"gradient_descent\"","title":"default_implementation"},{"location":"api/training/trainer/#train","text":"class Trainer ( Registrable ): | ... | def train ( self ) -> Dict [ str , Any ] Train a model and return the results.","title":"train"},{"location":"api/training/trainer/#get_checkpoint_state","text":"class Trainer ( Registrable ): | ... | @contextmanager | def get_checkpoint_state ( | self | ) -> Iterator [ Tuple [ Dict [ str , Any ], Dict [ str , Any ]]] Returns a tuple of (model state, training state), where training state could have several internal components (e.g., for an, optimizer, learning rate scheduler, etc.). This is a context manager, and should be called as with trainer.get_checkpoint_state() as state: , so that the trainer has the opportunity to change and restore its internal state for checkpointing. This is used, e.g., for moving averages of model weights.","title":"get_checkpoint_state"},{"location":"api/training/trainer/#batchcallback","text":"class BatchCallback ( Registrable ) An optional callback that you can pass to the GradientDescentTrainer that will be called at the end of every batch, during both training and validation. The default implementation does nothing. You can implement your own callback and do whatever you want, such as saving predictions to disk or extra logging.","title":"BatchCallback"},{"location":"api/training/trainer/#__call__","text":"class BatchCallback ( Registrable ): | ... | def __call__ ( | self , | trainer : \"GradientDescentTrainer\" , | batch_inputs : List [ List [ TensorDict ]], | batch_outputs : List [ Dict [ str , Any ]], | epoch : int , | batch_number : int , | is_training : bool , | is_master : bool | ) -> None","title":"__call__"},{"location":"api/training/trainer/#tensoboardbatchmemoryusage","text":"class TensoboardBatchMemoryUsage ( BatchCallback ) Logs the CPU and GPU memory usage to tensorboard on every batch. This is mainly used for debugging as it can cause a significant slowdown in training.","title":"TensoboardBatchMemoryUsage"},{"location":"api/training/trainer/#__call___1","text":"class TensoboardBatchMemoryUsage ( BatchCallback ): | ... | def __call__ ( | self , | trainer : \"GradientDescentTrainer\" , | batch_inputs : List [ List [ TensorDict ]], | batch_outputs : List [ Dict [ str , Any ]], | epoch : int , | batch_number : int , | is_training : bool , | is_master : bool | ) -> None In the distributed case we need to call this from every worker, since every worker reports its own memory usage.","title":"__call__"},{"location":"api/training/trainer/#epochcallback","text":"class EpochCallback ( Registrable ) An optional callback that you can pass to the GradientDescentTrainer that will be called at the end of every epoch (and before the start of training, with epoch=-1 ). The default implementation does nothing. You can implement your own callback and do whatever you want, such as additional modifications of the trainer's state in between epochs.","title":"EpochCallback"},{"location":"api/training/trainer/#__call___2","text":"class EpochCallback ( Registrable ): | ... | def __call__ ( | self , | trainer : \"GradientDescentTrainer\" , | metrics : Dict [ str , Any ], | epoch : int , | is_master : bool | ) -> None","title":"__call__"},{"location":"api/training/trainer/#trackepochcallback","text":"class TrackEpochCallback : | def __init__ ( self ) A callback that you can pass to the GradientDescentTrainer to access the current epoch number in your model during training. This callback sets model.epoch , which can be read inside of model.forward() . Since the EpochCallback passes epoch=-1 at the start of the training, we set model.epoch = epoch + 1 which now denotes the number of completed epochs at a given training state.","title":"TrackEpochCallback"},{"location":"api/training/trainer/#__call___3","text":"class TrackEpochCallback : | ... | def __call__ ( | self , | trainer : \"GradientDescentTrainer\" , | metrics : Dict [ str , Any ], | epoch : int , | is_master : bool | ) -> None","title":"__call__"},{"location":"api/training/trainer/#gradientdescenttrainer","text":"class GradientDescentTrainer ( Trainer ): | def __init__ ( | self , | model : Model , | optimizer : torch . optim . Optimizer , | data_loader : DataLoader , | patience : Optional [ int ] = None , | validation_metric : str = \"-loss\" , | validation_data_loader : DataLoader = None , | num_epochs : int = 20 , | serialization_dir : Optional [ str ] = None , | checkpointer : Checkpointer = None , | cuda_device : Optional [ Union [ int , torch . device ]] = None , | grad_norm : Optional [ float ] = None , | grad_clipping : Optional [ float ] = None , | learning_rate_scheduler : Optional [ LearningRateScheduler ] = None , | momentum_scheduler : Optional [ MomentumScheduler ] = None , | tensorboard_writer : TensorboardWriter = None , | moving_average : Optional [ MovingAverage ] = None , | batch_callbacks : List [ BatchCallback ] = None , | epoch_callbacks : List [ EpochCallback ] = None , | distributed : bool = False , | local_rank : int = 0 , | world_size : int = 1 , | num_gradient_accumulation_steps : int = 1 , | use_amp : bool = False | ) -> None A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset and a DataLoader , and uses the supplied Optimizer to learn the weights for your model over some fixed number of epochs. You can also pass in a validation dataloader and enable early stopping. There are many other bells and whistles as well. Registered as a Trainer with the name \"gradient_descent\" (and is also the default Trainer ). The constructor that is registered is from_partial_objects - see the arguments to that function for the exact keys that should be used, if you are using a configuration file. They largely match the arguments to __init__ , and we don't repeat their docstrings in from_partial_objects . Parameters model : Model An AllenNLP model to be optimized. Pytorch Modules can also be optimized if their forward method returns a dictionary with a \"loss\" key, containing a scalar tensor representing the loss function to be optimized. If you are training your model using GPUs, your model should already be on the correct device. (If you are using our train command this will be handled for you.) In a typical AllenNLP configuration file, this parameter does not get an entry under the \"trainer\", it gets constructed separately. optimizer : torch.nn.Optimizer An instance of a Pytorch Optimizer, instantiated with the parameters of the model to be optimized. data_loader : DataLoader A DataLoader containing your Dataset , yielding padded indexed batches. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"trainer\", it gets constructed separately. patience : Optional[int] > 0 , optional (default = None ) Number of epochs to be patient before early stopping: the training is stopped after patience epochs with no improvement. If given, it must be > 0 . If None, early stopping is disabled. validation_metric : str , optional (default = \"-loss\" ) Validation metric to measure for whether to stop training using patience and whether to serialize an is_best model each epoch. The metric name must be prepended with either \"+\" or \"-\", which specifies whether the metric is an increasing or decreasing function. validation_data_loader : DataLoader , optional (default = None ) A DataLoader to use for the validation set. If None , then use the training DataLoader with the validation data. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"trainer\", it gets constructed separately. num_epochs : int , optional (default = 20 ) Number of training epochs. serialization_dir : str , optional (default = None ) Path to directory for saving and loading model files. Models will not be saved if this parameter is not passed. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"trainer\", it gets constructed separately. checkpointer : Checkpointer , optional (default = None ) A Checkpointer is responsible for periodically saving model weights. If none is given here, we will construct one with default parameters. cuda_device : int , optional (default = -1 ) An integer specifying the CUDA device(s) to use for this process. If -1, the CPU is used. Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU. grad_norm : float , optional (default = None ) If provided, gradient norms will be rescaled to have a maximum of this value. grad_clipping : float , optional (default = None ) If provided, gradients will be clipped during the backward pass to have an (absolute) maximum of this value. If you are getting NaNs in your gradients during training that are not solved by using grad_norm , you may need this. learning_rate_scheduler : LearningRateScheduler , optional (default = None ) If specified, the learning rate will be decayed with respect to this schedule at the end of each epoch (or batch, if the scheduler implements the step_batch method). If you use torch.optim.lr_scheduler.ReduceLROnPlateau , this will use the validation_metric provided to determine if learning has plateaued. To support updating the learning rate on every batch, this can optionally implement step_batch(batch_num_total) which updates the learning rate given the batch number. momentum_scheduler : MomentumScheduler , optional (default = None ) If specified, the momentum will be updated at the end of each batch or epoch according to the schedule. tensorboard_writer : TensorboardWriter , optional If this is not provided, we will construct a TensorboardWriter with default parameters and use that. moving_average : MovingAverage , optional (default = None ) If provided, we will maintain moving averages for all parameters. During training, we employ a shadow variable for each parameter, which maintains the moving average. During evaluation, we backup the original parameters and assign the moving averages to corresponding parameters. Be careful that when saving the checkpoint, we will save the moving averages of parameters. This is necessary because we want the saved model to perform as well as the validated model if we load it later. But this may cause problems if you restart the training from checkpoint. batch_callbacks : List[BatchCallback] , optional (default = None ) A list of callbacks that will be called at the end of every batch, during both train and validation. epoch_callbacks : List[EpochCallback] , optional (default = None ) A list of callbacks that will be called at the end of every epoch, and at the start of training (with epoch = -1). distributed : bool , optional (default = False ) If set, PyTorch's DistributedDataParallel is used to train the model in multiple GPUs. This also requires world_size to be greater than 1. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to the \"trainer\" entry, that specifies a list of \"cuda_devices\"). local_rank : int , optional (default = 0 ) This is the unique identifier of the Trainer in a distributed process group. The GPU device id is used as the rank. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"trainer\", it gets constructed separately. world_size : int , optional (default = 1 ) The number of Trainer workers participating in the distributed training. In a typical AllenNLP configuration file, this parameter does not get an entry under the \"trainer\", it gets constructed separately. num_gradient_accumulation_steps : int , optional (default = 1 ) Gradients are accumulated for the given number of steps before doing an optimizer step. This can be useful to accommodate batches that are larger than the RAM size. Refer Thomas Wolf's post for details on Gradient Accumulation. use_amp : bool , optional (default = False ) If True , we'll train using Automatic Mixed Precision .","title":"GradientDescentTrainer"},{"location":"api/training/trainer/#rescale_gradients","text":"class GradientDescentTrainer ( Trainer ): | ... | def rescale_gradients ( self ) -> float Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled. Returns the norm of the gradients.","title":"rescale_gradients"},{"location":"api/training/trainer/#batch_outputs","text":"class GradientDescentTrainer ( Trainer ): | ... | def batch_outputs ( | self , | batch : TensorDict , | for_training : bool | ) -> Dict [ str , torch . Tensor ] Does a forward pass on the given batch and returns the output dictionary that the model returns, after adding any specified regularization penalty to the loss (if training).","title":"batch_outputs"},{"location":"api/training/trainer/#train_1","text":"class GradientDescentTrainer ( Trainer ): | ... | def train ( self ) -> Dict [ str , Any ] Trains the supplied model with the supplied parameters.","title":"train"},{"location":"api/training/trainer/#get_checkpoint_state_1","text":"class GradientDescentTrainer ( Trainer ): | ... | @contextmanager | def get_checkpoint_state ( | self | ) -> Iterator [ Tuple [ Dict [ str , Any ], Dict [ str , Any ]]]","title":"get_checkpoint_state"},{"location":"api/training/trainer/#from_partial_objects","text":"class GradientDescentTrainer ( Trainer ): | ... | @classmethod | def from_partial_objects ( | cls , | model : Model , | serialization_dir : str , | data_loader : DataLoader , | validation_data_loader : DataLoader = None , | local_rank : int = 0 , | patience : int = None , | validation_metric : str = \"-loss\" , | num_epochs : int = 20 , | cuda_device : Optional [ Union [ int , torch . device ]] = None , | grad_norm : float = None , | grad_clipping : float = None , | distributed : bool = None , | world_size : int = 1 , | num_gradient_accumulation_steps : int = 1 , | use_amp : bool = False , | no_grad : List [ str ] = None , | optimizer : Lazy [ Optimizer ] = None , | learning_rate_scheduler : Lazy [ LearningRateScheduler ] = None , | momentum_scheduler : Lazy [ MomentumScheduler ] = None , | tensorboard_writer : Lazy [ TensorboardWriter ] = None , | moving_average : Lazy [ MovingAverage ] = None , | checkpointer : Lazy [ Checkpointer ] = None , | batch_callbacks : List [ BatchCallback ] = None , | epoch_callbacks : List [ EpochCallback ] = None | ) -> \"Trainer\" This method exists so that we can have a documented method to construct this class using FromParams . If you are not using FromParams or config files, you can safely ignore this method. The reason we can't just use __init__ with FromParams here is because there are sequential dependencies to this class's arguments. Anything that has a Lazy[] type annotation needs something from one of the non- Lazy arguments. The Optimizer needs to have the parameters from the Model before it's constructed, and the Schedulers need to have the Optimizer . Because of this, the typical way we construct things FromParams doesn't work, so we use Lazy to allow for constructing the objects sequentially. If you're not using FromParams , you can just construct these arguments in the right order yourself in your code and call the constructor directly.","title":"from_partial_objects"},{"location":"api/training/util/","text":"[ allennlp .training .util ] Helper functions for Trainers HasBeenWarned # class HasBeenWarned tqdm_ignores_underscores # class HasBeenWarned : | ... | tqdm_ignores_underscores = False move_optimizer_to_cuda # def move_optimizer_to_cuda ( optimizer ) Move the optimizer state to GPU, if necessary. After calling, any parameter specific state in the optimizer will be located on the same device as the parameter. get_batch_size # def get_batch_size ( batch : Union [ Dict , torch . Tensor ]) -> int Returns the size of the batch dimension. Assumes a well-formed batch, returns 0 otherwise. time_to_str # def time_to_str ( timestamp : int ) -> str Convert seconds past Epoch to human readable string. str_to_time # def str_to_time ( time_str : str ) -> datetime . datetime Convert human readable string to datetime.datetime. read_all_datasets # def read_all_datasets ( train_data_path : str , dataset_reader : DatasetReader , validation_dataset_reader : DatasetReader = None , validation_data_path : str = None , test_data_path : str = None ) -> Dict [ str , Dataset ] Reads all datasets (perhaps lazily, if the corresponding dataset readers are lazy) and returns a dictionary mapping dataset name (\"train\", \"validation\" or \"test\") to the iterable resulting from reader.read(filename) . datasets_from_params # def datasets_from_params ( params : Params , train : bool = True , validation : bool = True , test : bool = True ) -> Dict [ str , Dataset ] Load datasets specified by the config. create_serialization_dir # def create_serialization_dir ( params : Params , serialization_dir : Union [ str , PathLike ], recover : bool , force : bool ) -> None This function creates the serialization directory if it doesn't exist. If it already exists and is non-empty, then it verifies that we're recovering from a training with an identical configuration. Parameters params : Params A parameter object specifying an AllenNLP Experiment. serialization_dir : str The directory in which to save results and logs. recover : bool If True , we will try to recover from an existing serialization directory, and crash if the directory doesn't exist, or doesn't match the configuration we're given. force : bool If True , we will overwrite the serialization directory if it already exists. enable_gradient_clipping # def enable_gradient_clipping ( model : Model , grad_clipping : Optional [ float ] ) -> None rescale_gradients # def rescale_gradients ( model : Model , grad_norm : Optional [ float ] = None ) -> Optional [ float ] Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled. get_metrics # def get_metrics ( model : Model , total_loss : float , total_reg_loss : Optional [ float ], batch_loss : Optional [ float ], batch_reg_loss : Optional [ float ], num_batches : int , reset : bool = False , world_size : int = 1 , cuda_device : Union [ int , torch . device ] = torch . device ( \"cpu\" ) ) -> Dict [ str , float ] Gets the metrics but sets \"loss\" to the total loss divided by the num_batches so that the \"loss\" metric is \"average loss per batch\". Returns the \"batch_loss\" separately. evaluate # def evaluate ( model : Model , data_loader : DataLoader , cuda_device : int = - 1 , batch_weight_key : str = None ) -> Dict [ str , Any ] Parameters model : Model The model to evaluate data_loader : DataLoader The DataLoader that will iterate over the evaluation data (data loaders already contain their data). cuda_device : int , optional (default = -1 ) The cuda device to use for this evaluation. The model is assumed to already be using this device; this parameter is only used for moving the input data to the correct device. batch_weight_key : str , optional (default = None ) If given, this is a key in the output dictionary for each batch that specifies how to weight the loss for that batch. If this is not given, we use a weight of 1 for every batch. description_from_metrics # def description_from_metrics ( metrics : Dict [ str , float ]) -> str make_vocab_from_params # def make_vocab_from_params ( params : Params , serialization_dir : Union [ str , PathLike ], print_statistics : bool = False ) -> Vocabulary ngrams # def ngrams ( tensor : torch . LongTensor , ngram_size : int , exclude_indices : Set [ int ] ) -> Dict [ Tuple [ int , ... ], int ] get_valid_tokens_mask # def get_valid_tokens_mask ( tensor : torch . LongTensor , exclude_indices : Set [ int ] ) -> torch . ByteTensor","title":"util"},{"location":"api/training/util/#hasbeenwarned","text":"class HasBeenWarned","title":"HasBeenWarned"},{"location":"api/training/util/#tqdm_ignores_underscores","text":"class HasBeenWarned : | ... | tqdm_ignores_underscores = False","title":"tqdm_ignores_underscores"},{"location":"api/training/util/#move_optimizer_to_cuda","text":"def move_optimizer_to_cuda ( optimizer ) Move the optimizer state to GPU, if necessary. After calling, any parameter specific state in the optimizer will be located on the same device as the parameter.","title":"move_optimizer_to_cuda"},{"location":"api/training/util/#get_batch_size","text":"def get_batch_size ( batch : Union [ Dict , torch . Tensor ]) -> int Returns the size of the batch dimension. Assumes a well-formed batch, returns 0 otherwise.","title":"get_batch_size"},{"location":"api/training/util/#time_to_str","text":"def time_to_str ( timestamp : int ) -> str Convert seconds past Epoch to human readable string.","title":"time_to_str"},{"location":"api/training/util/#str_to_time","text":"def str_to_time ( time_str : str ) -> datetime . datetime Convert human readable string to datetime.datetime.","title":"str_to_time"},{"location":"api/training/util/#read_all_datasets","text":"def read_all_datasets ( train_data_path : str , dataset_reader : DatasetReader , validation_dataset_reader : DatasetReader = None , validation_data_path : str = None , test_data_path : str = None ) -> Dict [ str , Dataset ] Reads all datasets (perhaps lazily, if the corresponding dataset readers are lazy) and returns a dictionary mapping dataset name (\"train\", \"validation\" or \"test\") to the iterable resulting from reader.read(filename) .","title":"read_all_datasets"},{"location":"api/training/util/#datasets_from_params","text":"def datasets_from_params ( params : Params , train : bool = True , validation : bool = True , test : bool = True ) -> Dict [ str , Dataset ] Load datasets specified by the config.","title":"datasets_from_params"},{"location":"api/training/util/#create_serialization_dir","text":"def create_serialization_dir ( params : Params , serialization_dir : Union [ str , PathLike ], recover : bool , force : bool ) -> None This function creates the serialization directory if it doesn't exist. If it already exists and is non-empty, then it verifies that we're recovering from a training with an identical configuration. Parameters params : Params A parameter object specifying an AllenNLP Experiment. serialization_dir : str The directory in which to save results and logs. recover : bool If True , we will try to recover from an existing serialization directory, and crash if the directory doesn't exist, or doesn't match the configuration we're given. force : bool If True , we will overwrite the serialization directory if it already exists.","title":"create_serialization_dir"},{"location":"api/training/util/#enable_gradient_clipping","text":"def enable_gradient_clipping ( model : Model , grad_clipping : Optional [ float ] ) -> None","title":"enable_gradient_clipping"},{"location":"api/training/util/#rescale_gradients","text":"def rescale_gradients ( model : Model , grad_norm : Optional [ float ] = None ) -> Optional [ float ] Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.","title":"rescale_gradients"},{"location":"api/training/util/#get_metrics","text":"def get_metrics ( model : Model , total_loss : float , total_reg_loss : Optional [ float ], batch_loss : Optional [ float ], batch_reg_loss : Optional [ float ], num_batches : int , reset : bool = False , world_size : int = 1 , cuda_device : Union [ int , torch . device ] = torch . device ( \"cpu\" ) ) -> Dict [ str , float ] Gets the metrics but sets \"loss\" to the total loss divided by the num_batches so that the \"loss\" metric is \"average loss per batch\". Returns the \"batch_loss\" separately.","title":"get_metrics"},{"location":"api/training/util/#evaluate","text":"def evaluate ( model : Model , data_loader : DataLoader , cuda_device : int = - 1 , batch_weight_key : str = None ) -> Dict [ str , Any ] Parameters model : Model The model to evaluate data_loader : DataLoader The DataLoader that will iterate over the evaluation data (data loaders already contain their data). cuda_device : int , optional (default = -1 ) The cuda device to use for this evaluation. The model is assumed to already be using this device; this parameter is only used for moving the input data to the correct device. batch_weight_key : str , optional (default = None ) If given, this is a key in the output dictionary for each batch that specifies how to weight the loss for that batch. If this is not given, we use a weight of 1 for every batch.","title":"evaluate"},{"location":"api/training/util/#description_from_metrics","text":"def description_from_metrics ( metrics : Dict [ str , float ]) -> str","title":"description_from_metrics"},{"location":"api/training/util/#make_vocab_from_params","text":"def make_vocab_from_params ( params : Params , serialization_dir : Union [ str , PathLike ], print_statistics : bool = False ) -> Vocabulary","title":"make_vocab_from_params"},{"location":"api/training/util/#ngrams","text":"def ngrams ( tensor : torch . LongTensor , ngram_size : int , exclude_indices : Set [ int ] ) -> Dict [ Tuple [ int , ... ], int ]","title":"ngrams"},{"location":"api/training/util/#get_valid_tokens_mask","text":"def get_valid_tokens_mask ( tensor : torch . LongTensor , exclude_indices : Set [ int ] ) -> torch . ByteTensor","title":"get_valid_tokens_mask"},{"location":"api/training/learning_rate_schedulers/cosine/","text":"[ allennlp .training .learning_rate_schedulers .cosine ] CosineWithRestarts # class CosineWithRestarts ( LearningRateScheduler ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | t_initial : int , | t_mul : float = 1.0 , | eta_min : float = 0.0 , | eta_mul : float = 1.0 , | last_epoch : int = - 1 | ) -> None Cosine annealing with restarts. This is described in the paper https://arxiv.org/abs/1608.03983. Note that early stopping should typically be avoided when using this schedule. Registered as a LearningRateScheduler with name \"cosine\". Parameters optimizer : torch.optim.Optimizer This argument does not get an entry in a configuration file for the object. t_initial : int The number of iterations (epochs) within the first cycle. t_mul : float , optional (default = 1 ) Determines the number of iterations (epochs) in the i-th decay cycle, which is the length of the last cycle multiplied by t_mul . eta_min : float , optional (default = 0 ) The minimum learning rate. eta_mul : float , optional (default = 1 ) Determines the initial learning rate for the i-th decay cycle, which is the last initial learning rate multiplied by m_mul . last_epoch : int , optional (default = -1 ) The index of the last epoch. This is used when restarting. get_values # class CosineWithRestarts ( LearningRateScheduler ): | ... | @overrides | def get_values ( self ) Get updated learning rate.","title":"cosine"},{"location":"api/training/learning_rate_schedulers/cosine/#cosinewithrestarts","text":"class CosineWithRestarts ( LearningRateScheduler ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | t_initial : int , | t_mul : float = 1.0 , | eta_min : float = 0.0 , | eta_mul : float = 1.0 , | last_epoch : int = - 1 | ) -> None Cosine annealing with restarts. This is described in the paper https://arxiv.org/abs/1608.03983. Note that early stopping should typically be avoided when using this schedule. Registered as a LearningRateScheduler with name \"cosine\". Parameters optimizer : torch.optim.Optimizer This argument does not get an entry in a configuration file for the object. t_initial : int The number of iterations (epochs) within the first cycle. t_mul : float , optional (default = 1 ) Determines the number of iterations (epochs) in the i-th decay cycle, which is the length of the last cycle multiplied by t_mul . eta_min : float , optional (default = 0 ) The minimum learning rate. eta_mul : float , optional (default = 1 ) Determines the initial learning rate for the i-th decay cycle, which is the last initial learning rate multiplied by m_mul . last_epoch : int , optional (default = -1 ) The index of the last epoch. This is used when restarting.","title":"CosineWithRestarts"},{"location":"api/training/learning_rate_schedulers/cosine/#get_values","text":"class CosineWithRestarts ( LearningRateScheduler ): | ... | @overrides | def get_values ( self ) Get updated learning rate.","title":"get_values"},{"location":"api/training/learning_rate_schedulers/learning_rate_scheduler/","text":"[ allennlp .training .learning_rate_schedulers .learning_rate_scheduler ] LearningRateScheduler # class LearningRateScheduler ( Scheduler , Registrable ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | last_epoch : int = - 1 | ) -> None get_values # class LearningRateScheduler ( Scheduler , Registrable ): | ... | @overrides | def get_values ( self ) StepLearningRateScheduler # class StepLearningRateScheduler ( _PyTorchLearningRateSchedulerWrapper ): | def __init__ ( | self , | optimizer : Optimizer , | step_size : int , | gamma : float = 0.1 , | last_epoch : int = - 1 | ) -> None Registered as a LearningRateScheduler with name \"step\". The \"optimizer\" argument does not get an entry in a configuration file for the object. MultiStepLearningRateScheduler # class MultiStepLearningRateScheduler ( _PyTorchLearningRateSchedulerWrapper ): | def __init__ ( | self , | optimizer : Optimizer , | milestones : List [ int ], | gamma : float = 0.1 , | last_epoch : int = - 1 | ) -> None Registered as a LearningRateScheduler with name \"multi_step\". The \"optimizer\" argument does not get an entry in a configuration file for the object. ExponentialLearningRateScheduler # class ExponentialLearningRateScheduler ( _PyTorchLearningRateSchedulerWrapper ): | def __init__ ( | self , | optimizer : Optimizer , | gamma : float = 0.1 , | last_epoch : int = - 1 | ) -> None Registered as a LearningRateScheduler with name \"exponential\". The \"optimizer\" argument does not get an entry in a configuration file for the object. ReduceOnPlateauLearningRateScheduler # class ReduceOnPlateauLearningRateScheduler ( _PyTorchLearningRateSchedulerWithMetricsWrapper ): | def __init__ ( | self , | optimizer : Optimizer , | mode : str = \"min\" , | factor : float = 0.1 , | patience : int = 10 , | verbose : bool = False , | threshold_mode : str = \"rel\" , | threshold : float = 1e-4 , | cooldown : int = 0 , | min_lr : Union [ float , List [ float ]] = 0 , | eps : float = 1e-8 | ) -> None Registered as a LearningRateScheduler with name \"reduce_on_plateau\". The \"optimizer\" argument does not get an entry in a configuration file for the object.","title":"learning_rate_scheduler"},{"location":"api/training/learning_rate_schedulers/learning_rate_scheduler/#learningratescheduler","text":"class LearningRateScheduler ( Scheduler , Registrable ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | last_epoch : int = - 1 | ) -> None","title":"LearningRateScheduler"},{"location":"api/training/learning_rate_schedulers/learning_rate_scheduler/#get_values","text":"class LearningRateScheduler ( Scheduler , Registrable ): | ... | @overrides | def get_values ( self )","title":"get_values"},{"location":"api/training/learning_rate_schedulers/learning_rate_scheduler/#steplearningratescheduler","text":"class StepLearningRateScheduler ( _PyTorchLearningRateSchedulerWrapper ): | def __init__ ( | self , | optimizer : Optimizer , | step_size : int , | gamma : float = 0.1 , | last_epoch : int = - 1 | ) -> None Registered as a LearningRateScheduler with name \"step\". The \"optimizer\" argument does not get an entry in a configuration file for the object.","title":"StepLearningRateScheduler"},{"location":"api/training/learning_rate_schedulers/learning_rate_scheduler/#multisteplearningratescheduler","text":"class MultiStepLearningRateScheduler ( _PyTorchLearningRateSchedulerWrapper ): | def __init__ ( | self , | optimizer : Optimizer , | milestones : List [ int ], | gamma : float = 0.1 , | last_epoch : int = - 1 | ) -> None Registered as a LearningRateScheduler with name \"multi_step\". The \"optimizer\" argument does not get an entry in a configuration file for the object.","title":"MultiStepLearningRateScheduler"},{"location":"api/training/learning_rate_schedulers/learning_rate_scheduler/#exponentiallearningratescheduler","text":"class ExponentialLearningRateScheduler ( _PyTorchLearningRateSchedulerWrapper ): | def __init__ ( | self , | optimizer : Optimizer , | gamma : float = 0.1 , | last_epoch : int = - 1 | ) -> None Registered as a LearningRateScheduler with name \"exponential\". The \"optimizer\" argument does not get an entry in a configuration file for the object.","title":"ExponentialLearningRateScheduler"},{"location":"api/training/learning_rate_schedulers/learning_rate_scheduler/#reduceonplateaulearningratescheduler","text":"class ReduceOnPlateauLearningRateScheduler ( _PyTorchLearningRateSchedulerWithMetricsWrapper ): | def __init__ ( | self , | optimizer : Optimizer , | mode : str = \"min\" , | factor : float = 0.1 , | patience : int = 10 , | verbose : bool = False , | threshold_mode : str = \"rel\" , | threshold : float = 1e-4 , | cooldown : int = 0 , | min_lr : Union [ float , List [ float ]] = 0 , | eps : float = 1e-8 | ) -> None Registered as a LearningRateScheduler with name \"reduce_on_plateau\". The \"optimizer\" argument does not get an entry in a configuration file for the object.","title":"ReduceOnPlateauLearningRateScheduler"},{"location":"api/training/learning_rate_schedulers/linear_with_warmup/","text":"[ allennlp .training .learning_rate_schedulers .linear_with_warmup ] LinearWithWarmup # class LinearWithWarmup ( PolynomialDecay ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | num_epochs : int , | num_steps_per_epoch : int = None , | warmup_steps : int = 100 , | last_epoch : int = - 1 | ) -> None Implements a learning rate scheduler that increases the learning rate to lr during the first warmup_steps steps, and then decreases it to zero over the rest of the training steps.","title":"linear_with_warmup"},{"location":"api/training/learning_rate_schedulers/linear_with_warmup/#linearwithwarmup","text":"class LinearWithWarmup ( PolynomialDecay ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | num_epochs : int , | num_steps_per_epoch : int = None , | warmup_steps : int = 100 , | last_epoch : int = - 1 | ) -> None Implements a learning rate scheduler that increases the learning rate to lr during the first warmup_steps steps, and then decreases it to zero over the rest of the training steps.","title":"LinearWithWarmup"},{"location":"api/training/learning_rate_schedulers/noam/","text":"[ allennlp .training .learning_rate_schedulers .noam ] NoamLR # class NoamLR ( LearningRateScheduler ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | model_size : int , | warmup_steps : int , | factor : float = 1.0 , | last_epoch : int = - 1 | ) -> None Implements the Noam Learning rate schedule. This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number, scaled by the inverse square root of the dimensionality of the model. Time will tell if this is just madness or it's actually important. Registered as a LearningRateScheduler with name \"noam\". Parameters optimizer : torch.optim.Optimizer This argument does not get an entry in a configuration file for the object. model_size : int The hidden size parameter which dominates the number of parameters in your model. warmup_steps : int The number of steps to linearly increase the learning rate. factor : float , optional (default = 1.0 ) The overall scale factor for the learning rate decay. step # class NoamLR ( LearningRateScheduler ): | ... | @overrides | def step ( self , metric : float = None ) -> None step_batch # class NoamLR ( LearningRateScheduler ): | ... | def step_batch ( self , batch_num_total : int = None ) -> None get_values # class NoamLR ( LearningRateScheduler ): | ... | def get_values ( self )","title":"noam"},{"location":"api/training/learning_rate_schedulers/noam/#noamlr","text":"class NoamLR ( LearningRateScheduler ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | model_size : int , | warmup_steps : int , | factor : float = 1.0 , | last_epoch : int = - 1 | ) -> None Implements the Noam Learning rate schedule. This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number, scaled by the inverse square root of the dimensionality of the model. Time will tell if this is just madness or it's actually important. Registered as a LearningRateScheduler with name \"noam\". Parameters optimizer : torch.optim.Optimizer This argument does not get an entry in a configuration file for the object. model_size : int The hidden size parameter which dominates the number of parameters in your model. warmup_steps : int The number of steps to linearly increase the learning rate. factor : float , optional (default = 1.0 ) The overall scale factor for the learning rate decay.","title":"NoamLR"},{"location":"api/training/learning_rate_schedulers/noam/#step","text":"class NoamLR ( LearningRateScheduler ): | ... | @overrides | def step ( self , metric : float = None ) -> None","title":"step"},{"location":"api/training/learning_rate_schedulers/noam/#step_batch","text":"class NoamLR ( LearningRateScheduler ): | ... | def step_batch ( self , batch_num_total : int = None ) -> None","title":"step_batch"},{"location":"api/training/learning_rate_schedulers/noam/#get_values","text":"class NoamLR ( LearningRateScheduler ): | ... | def get_values ( self )","title":"get_values"},{"location":"api/training/learning_rate_schedulers/polynomial_decay/","text":"[ allennlp .training .learning_rate_schedulers .polynomial_decay ] PolynomialDecay # class PolynomialDecay ( LearningRateScheduler ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | num_epochs : int , | num_steps_per_epoch : int , | power = 1.0 , | warmup_steps = 0 , | end_learning_rate = 0.0 , | last_epoch : int = - 1 | ) Implements polynomial decay Learning rate scheduling. The learning rate is first linearly increased for the first warmup_steps training steps. Then it is decayed for total_steps - warmup_steps from the initial learning rate to end_learning_rate using a polynomial of degree power . Formally, lr = ( initial_lr - end_learning_rate ) * (( total_steps - steps )/( total_steps - warmup_steps )) ** power Parameters total_steps : int The total number of steps to adjust the learning rate for. warmup_steps : int The number of steps to linearly increase the learning rate. power : float , optional (default = 1.0 ) The power of the polynomial used for decaying. end_learning_rate : float , optional (default = 0.0 ) Final learning rate to decay towards. get_values # class PolynomialDecay ( LearningRateScheduler ): | ... | @overrides | def get_values ( self ) step # class PolynomialDecay ( LearningRateScheduler ): | ... | @overrides | def step ( self , metric : float = None ) -> None step_batch # class PolynomialDecay ( LearningRateScheduler ): | ... | @overrides | def step_batch ( self , batch_num_total : int = None ) -> None","title":"polynomial_decay"},{"location":"api/training/learning_rate_schedulers/polynomial_decay/#polynomialdecay","text":"class PolynomialDecay ( LearningRateScheduler ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | num_epochs : int , | num_steps_per_epoch : int , | power = 1.0 , | warmup_steps = 0 , | end_learning_rate = 0.0 , | last_epoch : int = - 1 | ) Implements polynomial decay Learning rate scheduling. The learning rate is first linearly increased for the first warmup_steps training steps. Then it is decayed for total_steps - warmup_steps from the initial learning rate to end_learning_rate using a polynomial of degree power . Formally, lr = ( initial_lr - end_learning_rate ) * (( total_steps - steps )/( total_steps - warmup_steps )) ** power Parameters total_steps : int The total number of steps to adjust the learning rate for. warmup_steps : int The number of steps to linearly increase the learning rate. power : float , optional (default = 1.0 ) The power of the polynomial used for decaying. end_learning_rate : float , optional (default = 0.0 ) Final learning rate to decay towards.","title":"PolynomialDecay"},{"location":"api/training/learning_rate_schedulers/polynomial_decay/#get_values","text":"class PolynomialDecay ( LearningRateScheduler ): | ... | @overrides | def get_values ( self )","title":"get_values"},{"location":"api/training/learning_rate_schedulers/polynomial_decay/#step","text":"class PolynomialDecay ( LearningRateScheduler ): | ... | @overrides | def step ( self , metric : float = None ) -> None","title":"step"},{"location":"api/training/learning_rate_schedulers/polynomial_decay/#step_batch","text":"class PolynomialDecay ( LearningRateScheduler ): | ... | @overrides | def step_batch ( self , batch_num_total : int = None ) -> None","title":"step_batch"},{"location":"api/training/learning_rate_schedulers/slanted_triangular/","text":"[ allennlp .training .learning_rate_schedulers .slanted_triangular ] SlantedTriangular # class SlantedTriangular ( LearningRateScheduler ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | num_epochs : int , | num_steps_per_epoch : Optional [ int ] = None , | cut_frac : float = 0.1 , | ratio : int = 32 , | last_epoch : int = - 1 , | gradual_unfreezing : bool = False , | discriminative_fine_tuning : bool = False , | decay_factor : float = 0.38 | ) -> None Implements the Slanted Triangular Learning Rate schedule with optional gradual unfreezing and discriminative fine-tuning. The schedule corresponds to first linearly increasing the learning rate over some number of epochs, and then linearly decreasing it over the remaining epochs. If we gradually unfreeze, then in the first epoch of training, only the top layer is trained; in the second epoch, the top two layers are trained, etc. During freezing, the learning rate is increased and annealed over one epoch. After freezing finished, the learning rate is increased and annealed over the remaining training iterations. Note that with this schedule, early stopping should typically be avoided. Registered as a LearningRateScheduler with name \"slanted_triangular\". Parameters optimizer : torch.optim.Optimizer This argument does not get an entry in a configuration file for the object. num_epochs : int The total number of epochs for which the model should be trained. num_steps_per_epoch : Optional[int] , optional (default = None ) The number of steps (updates, batches) per training epoch. cut_frac : float , optional (default = 0.1 ) The fraction of the steps to increase the learning rate. ratio : float , optional (default = 32 ) The ratio of the smallest to the (largest) base learning rate. gradual_unfreezing : bool , optional (default = False ) Whether gradual unfreezing should be used. discriminative_fine_tuning : bool , optional (default = False ) Whether discriminative fine-tuning (different learning rates per layer) are used. decay_factor : float , optional (default = 0.38 ) The decay factor by which the learning rate is reduced with discriminative fine-tuning when going a layer deeper. step # class SlantedTriangular ( LearningRateScheduler ): | ... | @overrides | def step ( self , metric : float = None ) -> None step_batch # class SlantedTriangular ( LearningRateScheduler ): | ... | def step_batch ( self , batch_num_total : int = None ) get_values # class SlantedTriangular ( LearningRateScheduler ): | ... | def get_values ( self ) get the actual number of batches per epoch seen in training","title":"slanted_triangular"},{"location":"api/training/learning_rate_schedulers/slanted_triangular/#slantedtriangular","text":"class SlantedTriangular ( LearningRateScheduler ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | num_epochs : int , | num_steps_per_epoch : Optional [ int ] = None , | cut_frac : float = 0.1 , | ratio : int = 32 , | last_epoch : int = - 1 , | gradual_unfreezing : bool = False , | discriminative_fine_tuning : bool = False , | decay_factor : float = 0.38 | ) -> None Implements the Slanted Triangular Learning Rate schedule with optional gradual unfreezing and discriminative fine-tuning. The schedule corresponds to first linearly increasing the learning rate over some number of epochs, and then linearly decreasing it over the remaining epochs. If we gradually unfreeze, then in the first epoch of training, only the top layer is trained; in the second epoch, the top two layers are trained, etc. During freezing, the learning rate is increased and annealed over one epoch. After freezing finished, the learning rate is increased and annealed over the remaining training iterations. Note that with this schedule, early stopping should typically be avoided. Registered as a LearningRateScheduler with name \"slanted_triangular\". Parameters optimizer : torch.optim.Optimizer This argument does not get an entry in a configuration file for the object. num_epochs : int The total number of epochs for which the model should be trained. num_steps_per_epoch : Optional[int] , optional (default = None ) The number of steps (updates, batches) per training epoch. cut_frac : float , optional (default = 0.1 ) The fraction of the steps to increase the learning rate. ratio : float , optional (default = 32 ) The ratio of the smallest to the (largest) base learning rate. gradual_unfreezing : bool , optional (default = False ) Whether gradual unfreezing should be used. discriminative_fine_tuning : bool , optional (default = False ) Whether discriminative fine-tuning (different learning rates per layer) are used. decay_factor : float , optional (default = 0.38 ) The decay factor by which the learning rate is reduced with discriminative fine-tuning when going a layer deeper.","title":"SlantedTriangular"},{"location":"api/training/learning_rate_schedulers/slanted_triangular/#step","text":"class SlantedTriangular ( LearningRateScheduler ): | ... | @overrides | def step ( self , metric : float = None ) -> None","title":"step"},{"location":"api/training/learning_rate_schedulers/slanted_triangular/#step_batch","text":"class SlantedTriangular ( LearningRateScheduler ): | ... | def step_batch ( self , batch_num_total : int = None )","title":"step_batch"},{"location":"api/training/learning_rate_schedulers/slanted_triangular/#get_values","text":"class SlantedTriangular ( LearningRateScheduler ): | ... | def get_values ( self ) get the actual number of batches per epoch seen in training","title":"get_values"},{"location":"api/training/metrics/attachment_scores/","text":"[ allennlp .training .metrics .attachment_scores ] AttachmentScores # class AttachmentScores ( Metric ): | def __init__ ( self , ignore_classes : List [ int ] = None ) -> None Computes labeled and unlabeled attachment scores for a dependency parse, as well as sentence level exact match for both labeled and unlabeled trees. Note that the input to this metric is the sampled predictions, not the distribution itself. Parameters ignore_classes : List[int] , optional (default = None ) A list of label ids to ignore when computing metrics. __call__ # class AttachmentScores ( Metric ): | ... | def __call__ ( | self , | predicted_indices : torch . Tensor , | predicted_labels : torch . Tensor , | gold_indices : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters predicted_indices : torch.Tensor A tensor of head index predictions of shape (batch_size, timesteps). predicted_labels : torch.Tensor A tensor of arc label predictions of shape (batch_size, timesteps). gold_indices : torch.Tensor A tensor of the same shape as predicted_indices . gold_labels : torch.Tensor A tensor of the same shape as predicted_labels . mask : torch.BoolTensor , optional (default = None ) A tensor of the same shape as predicted_indices . get_metric # class AttachmentScores ( Metric ): | ... | def get_metric ( | self , | reset : bool = False , | cuda_device : Union [ int , torch . device ] = torch . device ( \"cpu\" ) | ) Returns The accumulated metrics as a dictionary. reset # class AttachmentScores ( Metric ): | ... | @overrides | def reset ( self )","title":"attachment_scores"},{"location":"api/training/metrics/attachment_scores/#attachmentscores","text":"class AttachmentScores ( Metric ): | def __init__ ( self , ignore_classes : List [ int ] = None ) -> None Computes labeled and unlabeled attachment scores for a dependency parse, as well as sentence level exact match for both labeled and unlabeled trees. Note that the input to this metric is the sampled predictions, not the distribution itself. Parameters ignore_classes : List[int] , optional (default = None ) A list of label ids to ignore when computing metrics.","title":"AttachmentScores"},{"location":"api/training/metrics/attachment_scores/#__call__","text":"class AttachmentScores ( Metric ): | ... | def __call__ ( | self , | predicted_indices : torch . Tensor , | predicted_labels : torch . Tensor , | gold_indices : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters predicted_indices : torch.Tensor A tensor of head index predictions of shape (batch_size, timesteps). predicted_labels : torch.Tensor A tensor of arc label predictions of shape (batch_size, timesteps). gold_indices : torch.Tensor A tensor of the same shape as predicted_indices . gold_labels : torch.Tensor A tensor of the same shape as predicted_labels . mask : torch.BoolTensor , optional (default = None ) A tensor of the same shape as predicted_indices .","title":"__call__"},{"location":"api/training/metrics/attachment_scores/#get_metric","text":"class AttachmentScores ( Metric ): | ... | def get_metric ( | self , | reset : bool = False , | cuda_device : Union [ int , torch . device ] = torch . device ( \"cpu\" ) | ) Returns The accumulated metrics as a dictionary.","title":"get_metric"},{"location":"api/training/metrics/attachment_scores/#reset","text":"class AttachmentScores ( Metric ): | ... | @overrides | def reset ( self )","title":"reset"},{"location":"api/training/metrics/auc/","text":"[ allennlp .training .metrics .auc ] Auc # class Auc ( Metric ): | def __init__ ( self , positive_label = 1 ) The AUC Metric measures the area under the receiver-operating characteristic (ROC) curve for binary classification problems. __call__ # class Auc ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters predictions : torch.Tensor A one-dimensional tensor of prediction scores of shape (batch_size). gold_labels : torch.Tensor A one-dimensional label tensor of shape (batch_size), with {1, 0} entries for positive and negative class. If it's not binary, positive_label should be passed in the initialization. mask : torch.BoolTensor , optional (default = None ) A one-dimensional label tensor of shape (batch_size). get_metric # class Auc ( Metric ): | ... | def get_metric ( self , reset : bool = False ) reset # class Auc ( Metric ): | ... | @overrides | def reset ( self )","title":"auc"},{"location":"api/training/metrics/auc/#auc","text":"class Auc ( Metric ): | def __init__ ( self , positive_label = 1 ) The AUC Metric measures the area under the receiver-operating characteristic (ROC) curve for binary classification problems.","title":"Auc"},{"location":"api/training/metrics/auc/#__call__","text":"class Auc ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters predictions : torch.Tensor A one-dimensional tensor of prediction scores of shape (batch_size). gold_labels : torch.Tensor A one-dimensional label tensor of shape (batch_size), with {1, 0} entries for positive and negative class. If it's not binary, positive_label should be passed in the initialization. mask : torch.BoolTensor , optional (default = None ) A one-dimensional label tensor of shape (batch_size).","title":"__call__"},{"location":"api/training/metrics/auc/#get_metric","text":"class Auc ( Metric ): | ... | def get_metric ( self , reset : bool = False )","title":"get_metric"},{"location":"api/training/metrics/auc/#reset","text":"class Auc ( Metric ): | ... | @overrides | def reset ( self )","title":"reset"},{"location":"api/training/metrics/average/","text":"[ allennlp .training .metrics .average ] Average # class Average ( Metric ): | def __init__ ( self ) -> None This Metric breaks with the typical Metric API and just stores values that were computed in some fashion outside of a Metric . If you have some external code that computes the metric for you, for instance, you can use this to report the average result using our Metric API. __call__ # class Average ( Metric ): | ... | @overrides | def __call__ ( self , value ) Parameters value : float The value to average. get_metric # class Average ( Metric ): | ... | @overrides | def get_metric ( self , reset : bool = False ) Returns The average of all values that were passed to __call__ . reset # class Average ( Metric ): | ... | @overrides | def reset ( self )","title":"average"},{"location":"api/training/metrics/average/#average","text":"class Average ( Metric ): | def __init__ ( self ) -> None This Metric breaks with the typical Metric API and just stores values that were computed in some fashion outside of a Metric . If you have some external code that computes the metric for you, for instance, you can use this to report the average result using our Metric API.","title":"Average"},{"location":"api/training/metrics/average/#__call__","text":"class Average ( Metric ): | ... | @overrides | def __call__ ( self , value ) Parameters value : float The value to average.","title":"__call__"},{"location":"api/training/metrics/average/#get_metric","text":"class Average ( Metric ): | ... | @overrides | def get_metric ( self , reset : bool = False ) Returns The average of all values that were passed to __call__ .","title":"get_metric"},{"location":"api/training/metrics/average/#reset","text":"class Average ( Metric ): | ... | @overrides | def reset ( self )","title":"reset"},{"location":"api/training/metrics/bleu/","text":"[ allennlp .training .metrics .bleu ] BLEU # class BLEU ( Metric ): | def __init__ ( | self , | ngram_weights : Iterable [ float ] = ( 0.25 , 0.25 , 0.25 , 0.25 ), | exclude_indices : Set [ int ] = None | ) -> None Bilingual Evaluation Understudy (BLEU). BLEU is a common metric used for evaluating the quality of machine translations against a set of reference translations. See Papineni et. al., \"BLEU: a method for automatic evaluation of machine translation\", 2002 . Parameters ngram_weights : Iterable[float] , optional (default = (0.25, 0.25, 0.25, 0.25) ) Weights to assign to scores for each ngram size. exclude_indices : Set[int] , optional (default = None ) Indices to exclude when calculating ngrams. This should usually include the indices of the start, end, and pad tokens. Notes We chose to implement this from scratch instead of wrapping an existing implementation (such as nltk.translate.bleu_score ) for a two reasons. First, so that we could pass tensors directly to this metric instead of first converting the tensors to lists of strings. And second, because functions like nltk.translate.bleu_score.corpus_bleu() are meant to be called once over the entire corpus, whereas it is more efficient in our use case to update the running precision counts every batch. This implementation only considers a reference set of size 1, i.e. a single gold target sequence for each predicted sequence. reset # class BLEU ( Metric ): | ... | @overrides | def reset ( self ) -> None __call__ # class BLEU ( Metric ): | ... | @overrides | def __call__ ( | self , | predictions : torch . LongTensor , | gold_targets : torch . LongTensor | ) -> None Update precision counts. Parameters predictions : torch.LongTensor Batched predicted tokens of shape (batch_size, max_sequence_length) . references : torch.LongTensor Batched reference (gold) translations with shape (batch_size, max_gold_sequence_length) . Returns None get_metric # class BLEU ( Metric ): | ... | @overrides | def get_metric ( self , reset : bool = False ) -> Dict [ str , float ]","title":"bleu"},{"location":"api/training/metrics/bleu/#bleu","text":"class BLEU ( Metric ): | def __init__ ( | self , | ngram_weights : Iterable [ float ] = ( 0.25 , 0.25 , 0.25 , 0.25 ), | exclude_indices : Set [ int ] = None | ) -> None Bilingual Evaluation Understudy (BLEU). BLEU is a common metric used for evaluating the quality of machine translations against a set of reference translations. See Papineni et. al., \"BLEU: a method for automatic evaluation of machine translation\", 2002 . Parameters ngram_weights : Iterable[float] , optional (default = (0.25, 0.25, 0.25, 0.25) ) Weights to assign to scores for each ngram size. exclude_indices : Set[int] , optional (default = None ) Indices to exclude when calculating ngrams. This should usually include the indices of the start, end, and pad tokens. Notes We chose to implement this from scratch instead of wrapping an existing implementation (such as nltk.translate.bleu_score ) for a two reasons. First, so that we could pass tensors directly to this metric instead of first converting the tensors to lists of strings. And second, because functions like nltk.translate.bleu_score.corpus_bleu() are meant to be called once over the entire corpus, whereas it is more efficient in our use case to update the running precision counts every batch. This implementation only considers a reference set of size 1, i.e. a single gold target sequence for each predicted sequence.","title":"BLEU"},{"location":"api/training/metrics/bleu/#reset","text":"class BLEU ( Metric ): | ... | @overrides | def reset ( self ) -> None","title":"reset"},{"location":"api/training/metrics/bleu/#__call__","text":"class BLEU ( Metric ): | ... | @overrides | def __call__ ( | self , | predictions : torch . LongTensor , | gold_targets : torch . LongTensor | ) -> None Update precision counts. Parameters predictions : torch.LongTensor Batched predicted tokens of shape (batch_size, max_sequence_length) . references : torch.LongTensor Batched reference (gold) translations with shape (batch_size, max_gold_sequence_length) . Returns None","title":"__call__"},{"location":"api/training/metrics/bleu/#get_metric","text":"class BLEU ( Metric ): | ... | @overrides | def get_metric ( self , reset : bool = False ) -> Dict [ str , float ]","title":"get_metric"},{"location":"api/training/metrics/boolean_accuracy/","text":"[ allennlp .training .metrics .boolean_accuracy ] BooleanAccuracy # class BooleanAccuracy ( Metric ): | def __init__ ( self ) -> None Just checks batch-equality of two tensors and computes an accuracy metric based on that. That is, if your prediction has shape (batch_size, dim_1, ..., dim_n), this metric considers that as a set of batch_size predictions and checks that each is entirely correct across the remaining dims. This means the denominator in the accuracy computation is batch_size , with the caveat that predictions that are totally masked are ignored (in which case the denominator is the number of predictions that have at least one unmasked element). This is similar to CategoricalAccuracy , if you've already done a .max() on your predictions. If you have categorical output, though, you should typically just use CategoricalAccuracy . The reason you might want to use this instead is if you've done some kind of constrained inference and don't have a prediction tensor that matches the API of CategoricalAccuracy , which assumes a final dimension of size num_classes . __call__ # class BooleanAccuracy ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters predictions : torch.Tensor A tensor of predictions of shape (batch_size, ...). gold_labels : torch.Tensor A tensor of the same shape as predictions . mask : torch.BoolTensor , optional (default = None ) A tensor of the same shape as predictions . get_metric # class BooleanAccuracy ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns The accumulated accuracy. reset # class BooleanAccuracy ( Metric ): | ... | @overrides | def reset ( self )","title":"boolean_accuracy"},{"location":"api/training/metrics/boolean_accuracy/#booleanaccuracy","text":"class BooleanAccuracy ( Metric ): | def __init__ ( self ) -> None Just checks batch-equality of two tensors and computes an accuracy metric based on that. That is, if your prediction has shape (batch_size, dim_1, ..., dim_n), this metric considers that as a set of batch_size predictions and checks that each is entirely correct across the remaining dims. This means the denominator in the accuracy computation is batch_size , with the caveat that predictions that are totally masked are ignored (in which case the denominator is the number of predictions that have at least one unmasked element). This is similar to CategoricalAccuracy , if you've already done a .max() on your predictions. If you have categorical output, though, you should typically just use CategoricalAccuracy . The reason you might want to use this instead is if you've done some kind of constrained inference and don't have a prediction tensor that matches the API of CategoricalAccuracy , which assumes a final dimension of size num_classes .","title":"BooleanAccuracy"},{"location":"api/training/metrics/boolean_accuracy/#__call__","text":"class BooleanAccuracy ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters predictions : torch.Tensor A tensor of predictions of shape (batch_size, ...). gold_labels : torch.Tensor A tensor of the same shape as predictions . mask : torch.BoolTensor , optional (default = None ) A tensor of the same shape as predictions .","title":"__call__"},{"location":"api/training/metrics/boolean_accuracy/#get_metric","text":"class BooleanAccuracy ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns The accumulated accuracy.","title":"get_metric"},{"location":"api/training/metrics/boolean_accuracy/#reset","text":"class BooleanAccuracy ( Metric ): | ... | @overrides | def reset ( self )","title":"reset"},{"location":"api/training/metrics/categorical_accuracy/","text":"[ allennlp .training .metrics .categorical_accuracy ] CategoricalAccuracy # class CategoricalAccuracy ( Metric ): | def __init__ ( self , top_k : int = 1 , tie_break : bool = False ) -> None Categorical Top-K accuracy. Assumes integer labels, with each item to be classified having a single correct class. Tie break enables equal distribution of scores among the classes with same maximum predicted scores. supports_distributed # class CategoricalAccuracy ( Metric ): | ... | supports_distributed = True __call__ # class CategoricalAccuracy ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters predictions : torch.Tensor A tensor of predictions of shape (batch_size, ..., num_classes). gold_labels : torch.Tensor A tensor of integer class label of shape (batch_size, ...). It must be the same shape as the predictions tensor without the num_classes dimension. mask : torch.BoolTensor , optional (default = None ) A masking tensor the same size as gold_labels . get_metric # class CategoricalAccuracy ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns The accumulated accuracy. reset # class CategoricalAccuracy ( Metric ): | ... | @overrides | def reset ( self )","title":"categorical_accuracy"},{"location":"api/training/metrics/categorical_accuracy/#categoricalaccuracy","text":"class CategoricalAccuracy ( Metric ): | def __init__ ( self , top_k : int = 1 , tie_break : bool = False ) -> None Categorical Top-K accuracy. Assumes integer labels, with each item to be classified having a single correct class. Tie break enables equal distribution of scores among the classes with same maximum predicted scores.","title":"CategoricalAccuracy"},{"location":"api/training/metrics/categorical_accuracy/#supports_distributed","text":"class CategoricalAccuracy ( Metric ): | ... | supports_distributed = True","title":"supports_distributed"},{"location":"api/training/metrics/categorical_accuracy/#__call__","text":"class CategoricalAccuracy ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters predictions : torch.Tensor A tensor of predictions of shape (batch_size, ..., num_classes). gold_labels : torch.Tensor A tensor of integer class label of shape (batch_size, ...). It must be the same shape as the predictions tensor without the num_classes dimension. mask : torch.BoolTensor , optional (default = None ) A masking tensor the same size as gold_labels .","title":"__call__"},{"location":"api/training/metrics/categorical_accuracy/#get_metric","text":"class CategoricalAccuracy ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns The accumulated accuracy.","title":"get_metric"},{"location":"api/training/metrics/categorical_accuracy/#reset","text":"class CategoricalAccuracy ( Metric ): | ... | @overrides | def reset ( self )","title":"reset"},{"location":"api/training/metrics/covariance/","text":"[ allennlp .training .metrics .covariance ] Covariance # class Covariance ( Metric ): | def __init__ ( self ) -> None This Metric calculates the unbiased sample covariance between two tensors. Each element in the two tensors is assumed to be a different observation of the variable (i.e., the input tensors are implicitly flattened into vectors and the covariance is calculated between the vectors). This implementation is mostly modeled after the streaming_covariance function in Tensorflow. See: https://github.com/tensorflow/tensorflow/blob/v1.10.1/tensorflow/contrib/metrics/python/ops/metric_ops.py#L3127 The following is copied from the Tensorflow documentation: The algorithm used for this online computation is described in https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online . Specifically, the formula used to combine two sample comoments is C_AB = C_A + C_B + (E[x_A] - E[x_B]) * (E[y_A] - E[y_B]) * n_A * n_B / n_AB The comoment for a single batch of data is simply sum((x - E[x]) * (y - E[y])) , optionally masked. __call__ # class Covariance ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters predictions : torch.Tensor A tensor of predictions of shape (batch_size, ...). gold_labels : torch.Tensor A tensor of the same shape as predictions . mask : torch.BoolTensor , optional (default = None ) A tensor of the same shape as predictions . get_metric # class Covariance ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns The accumulated covariance. reset # class Covariance ( Metric ): | ... | @overrides | def reset ( self )","title":"covariance"},{"location":"api/training/metrics/covariance/#covariance","text":"class Covariance ( Metric ): | def __init__ ( self ) -> None This Metric calculates the unbiased sample covariance between two tensors. Each element in the two tensors is assumed to be a different observation of the variable (i.e., the input tensors are implicitly flattened into vectors and the covariance is calculated between the vectors). This implementation is mostly modeled after the streaming_covariance function in Tensorflow. See: https://github.com/tensorflow/tensorflow/blob/v1.10.1/tensorflow/contrib/metrics/python/ops/metric_ops.py#L3127 The following is copied from the Tensorflow documentation: The algorithm used for this online computation is described in https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online . Specifically, the formula used to combine two sample comoments is C_AB = C_A + C_B + (E[x_A] - E[x_B]) * (E[y_A] - E[y_B]) * n_A * n_B / n_AB The comoment for a single batch of data is simply sum((x - E[x]) * (y - E[y])) , optionally masked.","title":"Covariance"},{"location":"api/training/metrics/covariance/#__call__","text":"class Covariance ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters predictions : torch.Tensor A tensor of predictions of shape (batch_size, ...). gold_labels : torch.Tensor A tensor of the same shape as predictions . mask : torch.BoolTensor , optional (default = None ) A tensor of the same shape as predictions .","title":"__call__"},{"location":"api/training/metrics/covariance/#get_metric","text":"class Covariance ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns The accumulated covariance.","title":"get_metric"},{"location":"api/training/metrics/covariance/#reset","text":"class Covariance ( Metric ): | ... | @overrides | def reset ( self )","title":"reset"},{"location":"api/training/metrics/entropy/","text":"[ allennlp .training .metrics .entropy ] Entropy # class Entropy ( Metric ): | def __init__ ( self ) -> None __call__ # class Entropy ( Metric ): | ... | @overrides | def __call__ ( | self , | logits : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters logits : torch.Tensor A tensor of unnormalized log probabilities of shape (batch_size, ..., num_classes). mask : torch.BoolTensor , optional (default = None ) A masking tensor of shape (batch_size, ...). get_metric # class Entropy ( Metric ): | ... | @overrides | def get_metric ( self , reset : bool = False ) Returns The scalar average entropy. reset # class Entropy ( Metric ): | ... | @overrides | def reset ( self )","title":"entropy"},{"location":"api/training/metrics/entropy/#entropy","text":"class Entropy ( Metric ): | def __init__ ( self ) -> None","title":"Entropy"},{"location":"api/training/metrics/entropy/#__call__","text":"class Entropy ( Metric ): | ... | @overrides | def __call__ ( | self , | logits : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters logits : torch.Tensor A tensor of unnormalized log probabilities of shape (batch_size, ..., num_classes). mask : torch.BoolTensor , optional (default = None ) A masking tensor of shape (batch_size, ...).","title":"__call__"},{"location":"api/training/metrics/entropy/#get_metric","text":"class Entropy ( Metric ): | ... | @overrides | def get_metric ( self , reset : bool = False ) Returns The scalar average entropy.","title":"get_metric"},{"location":"api/training/metrics/entropy/#reset","text":"class Entropy ( Metric ): | ... | @overrides | def reset ( self )","title":"reset"},{"location":"api/training/metrics/evalb_bracketing_scorer/","text":"[ allennlp .training .metrics .evalb_bracketing_scorer ] DEFAULT_EVALB_DIR # DEFAULT_EVALB_DIR = os . path . abspath ( os . path . join ( os . path . dirname ( os . path . realpath ( __file__ )), os . pardir , o ... EvalbBracketingScorer # class EvalbBracketingScorer ( Metric ): | def __init__ ( | self , | evalb_directory_path : str = DEFAULT_EVALB_DIR , | evalb_param_filename : str = \"COLLINS.prm\" , | evalb_num_errors_to_kill : int = 10 | ) -> None This class uses the external EVALB software for computing a broad range of metrics on parse trees. Here, we use it to compute the Precision, Recall and F1 metrics. You can download the source for EVALB from here: https://nlp.cs.nyu.edu/evalb/ . Note that this software is 20 years old. In order to compile it on modern hardware, you may need to remove an include <malloc.h> statement in evalb.c before it will compile. AllenNLP contains the EVALB software, but you will need to compile it yourself before using it because the binary it generates is system dependent. To build it, run make inside the allennlp/tools/EVALB directory. Note that this metric reads and writes from disk quite a bit. You probably don't want to include it in your training loop; instead, you should calculate this on a validation set only. Parameters evalb_directory_path : str The directory containing the EVALB executable. evalb_param_filename : str , optional (default = \"COLLINS.prm\" ) The relative name of the EVALB configuration file used when scoring the trees. By default, this uses the COLLINS.prm configuration file which comes with EVALB. This configuration ignores POS tags and some punctuation labels. evalb_num_errors_to_kill : int , optional (default = \"10\" ) The number of errors to tolerate from EVALB before terminating evaluation. __call__ # class EvalbBracketingScorer ( Metric ): | ... | @overrides | def __call__ ( | self , | predicted_trees : List [ Tree ], | gold_trees : List [ Tree ] | ) -> None Parameters predicted_trees : List[Tree] A list of predicted NLTK Trees to compute score for. gold_trees : List[Tree] A list of gold NLTK Trees to use as a reference. get_metric # class EvalbBracketingScorer ( Metric ): | ... | @overrides | def get_metric ( self , reset : bool = False ) Returns The average precision, recall and f1. reset # class EvalbBracketingScorer ( Metric ): | ... | @overrides | def reset ( self ) compile_evalb # class EvalbBracketingScorer ( Metric ): | ... | @staticmethod | def compile_evalb ( evalb_directory_path : str = DEFAULT_EVALB_DIR ) clean_evalb # class EvalbBracketingScorer ( Metric ): | ... | @staticmethod | def clean_evalb ( evalb_directory_path : str = DEFAULT_EVALB_DIR )","title":"evalb_bracketing_scorer"},{"location":"api/training/metrics/evalb_bracketing_scorer/#default_evalb_dir","text":"DEFAULT_EVALB_DIR = os . path . abspath ( os . path . join ( os . path . dirname ( os . path . realpath ( __file__ )), os . pardir , o ...","title":"DEFAULT_EVALB_DIR"},{"location":"api/training/metrics/evalb_bracketing_scorer/#evalbbracketingscorer","text":"class EvalbBracketingScorer ( Metric ): | def __init__ ( | self , | evalb_directory_path : str = DEFAULT_EVALB_DIR , | evalb_param_filename : str = \"COLLINS.prm\" , | evalb_num_errors_to_kill : int = 10 | ) -> None This class uses the external EVALB software for computing a broad range of metrics on parse trees. Here, we use it to compute the Precision, Recall and F1 metrics. You can download the source for EVALB from here: https://nlp.cs.nyu.edu/evalb/ . Note that this software is 20 years old. In order to compile it on modern hardware, you may need to remove an include <malloc.h> statement in evalb.c before it will compile. AllenNLP contains the EVALB software, but you will need to compile it yourself before using it because the binary it generates is system dependent. To build it, run make inside the allennlp/tools/EVALB directory. Note that this metric reads and writes from disk quite a bit. You probably don't want to include it in your training loop; instead, you should calculate this on a validation set only. Parameters evalb_directory_path : str The directory containing the EVALB executable. evalb_param_filename : str , optional (default = \"COLLINS.prm\" ) The relative name of the EVALB configuration file used when scoring the trees. By default, this uses the COLLINS.prm configuration file which comes with EVALB. This configuration ignores POS tags and some punctuation labels. evalb_num_errors_to_kill : int , optional (default = \"10\" ) The number of errors to tolerate from EVALB before terminating evaluation.","title":"EvalbBracketingScorer"},{"location":"api/training/metrics/evalb_bracketing_scorer/#__call__","text":"class EvalbBracketingScorer ( Metric ): | ... | @overrides | def __call__ ( | self , | predicted_trees : List [ Tree ], | gold_trees : List [ Tree ] | ) -> None Parameters predicted_trees : List[Tree] A list of predicted NLTK Trees to compute score for. gold_trees : List[Tree] A list of gold NLTK Trees to use as a reference.","title":"__call__"},{"location":"api/training/metrics/evalb_bracketing_scorer/#get_metric","text":"class EvalbBracketingScorer ( Metric ): | ... | @overrides | def get_metric ( self , reset : bool = False ) Returns The average precision, recall and f1.","title":"get_metric"},{"location":"api/training/metrics/evalb_bracketing_scorer/#reset","text":"class EvalbBracketingScorer ( Metric ): | ... | @overrides | def reset ( self )","title":"reset"},{"location":"api/training/metrics/evalb_bracketing_scorer/#compile_evalb","text":"class EvalbBracketingScorer ( Metric ): | ... | @staticmethod | def compile_evalb ( evalb_directory_path : str = DEFAULT_EVALB_DIR )","title":"compile_evalb"},{"location":"api/training/metrics/evalb_bracketing_scorer/#clean_evalb","text":"class EvalbBracketingScorer ( Metric ): | ... | @staticmethod | def clean_evalb ( evalb_directory_path : str = DEFAULT_EVALB_DIR )","title":"clean_evalb"},{"location":"api/training/metrics/f1_measure/","text":"[ allennlp .training .metrics .f1_measure ] F1Measure # class F1Measure ( FBetaMeasure ): | def __init__ ( self , positive_label : int ) -> None Computes Precision, Recall and F1 with respect to a given positive_label . For example, for a BIO tagging scheme, you would pass the classification index of the tag you are interested in, resulting in the Precision, Recall and F1 score being calculated for this tag only. get_metric # class F1Measure ( FBetaMeasure ): | ... | def get_metric ( self , reset : bool = False ) -> Dict [ str , float ] Returns precision : float recall : float f1-measure : float","title":"f1_measure"},{"location":"api/training/metrics/f1_measure/#f1measure","text":"class F1Measure ( FBetaMeasure ): | def __init__ ( self , positive_label : int ) -> None Computes Precision, Recall and F1 with respect to a given positive_label . For example, for a BIO tagging scheme, you would pass the classification index of the tag you are interested in, resulting in the Precision, Recall and F1 score being calculated for this tag only.","title":"F1Measure"},{"location":"api/training/metrics/f1_measure/#get_metric","text":"class F1Measure ( FBetaMeasure ): | ... | def get_metric ( self , reset : bool = False ) -> Dict [ str , float ] Returns precision : float recall : float f1-measure : float","title":"get_metric"},{"location":"api/training/metrics/fbeta_measure/","text":"[ allennlp .training .metrics .fbeta_measure ] FBetaMeasure # class FBetaMeasure ( Metric ): | def __init__ ( | self , | beta : float = 1.0 , | average : str = None , | labels : List [ int ] = None | ) -> None Compute precision, recall, F-measure and support for each class. The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples. The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0. If we have precision and recall, the F-beta score is simply: F-beta = (1 + beta ** 2) * precision * recall / (beta ** 2 * precision + recall) The F-beta score weights recall more than precision by a factor of beta . beta == 1.0 means recall and precision are equally important. The support is the number of occurrences of each class in y_true . Parameters beta : float , optional (default = 1.0 ) The strength of recall versus precision in the F-score. average : str , optional (default = None ) If None , the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data: 'micro' : Calculate metrics globally by counting the total true positives, false negatives and false positives. 'macro' : Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account. 'weighted' : Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters 'macro' to account for label imbalance; it can result in an F-score that is not between precision and recall. labels : list , optional The set of labels to include and their order if average is None . Labels present in the data can be excluded, for example to calculate a multi-class average ignoring a majority negative class. Labels not present in the data will result in 0 components in a macro or weighted average. __call__ # class FBetaMeasure ( Metric ): | ... | @overrides | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters predictions : torch.Tensor A tensor of predictions of shape (batch_size, ..., num_classes). gold_labels : torch.Tensor A tensor of integer class label of shape (batch_size, ...). It must be the same shape as the predictions tensor without the num_classes dimension. mask : torch.BoolTensor , optional (default = None ) A masking tensor the same size as gold_labels . get_metric # class FBetaMeasure ( Metric ): | ... | @overrides | def get_metric ( self , reset : bool = False ) Returns precisions : List[float] recalls : List[float] f1-measures : List[float] Note If self.average is not None , you will get float instead of List[float] . reset # class FBetaMeasure ( Metric ): | ... | @overrides | def reset ( self ) -> None","title":"fbeta_measure"},{"location":"api/training/metrics/fbeta_measure/#fbetameasure","text":"class FBetaMeasure ( Metric ): | def __init__ ( | self , | beta : float = 1.0 , | average : str = None , | labels : List [ int ] = None | ) -> None Compute precision, recall, F-measure and support for each class. The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples. The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0. If we have precision and recall, the F-beta score is simply: F-beta = (1 + beta ** 2) * precision * recall / (beta ** 2 * precision + recall) The F-beta score weights recall more than precision by a factor of beta . beta == 1.0 means recall and precision are equally important. The support is the number of occurrences of each class in y_true . Parameters beta : float , optional (default = 1.0 ) The strength of recall versus precision in the F-score. average : str , optional (default = None ) If None , the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data: 'micro' : Calculate metrics globally by counting the total true positives, false negatives and false positives. 'macro' : Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account. 'weighted' : Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters 'macro' to account for label imbalance; it can result in an F-score that is not between precision and recall. labels : list , optional The set of labels to include and their order if average is None . Labels present in the data can be excluded, for example to calculate a multi-class average ignoring a majority negative class. Labels not present in the data will result in 0 components in a macro or weighted average.","title":"FBetaMeasure"},{"location":"api/training/metrics/fbeta_measure/#__call__","text":"class FBetaMeasure ( Metric ): | ... | @overrides | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters predictions : torch.Tensor A tensor of predictions of shape (batch_size, ..., num_classes). gold_labels : torch.Tensor A tensor of integer class label of shape (batch_size, ...). It must be the same shape as the predictions tensor without the num_classes dimension. mask : torch.BoolTensor , optional (default = None ) A masking tensor the same size as gold_labels .","title":"__call__"},{"location":"api/training/metrics/fbeta_measure/#get_metric","text":"class FBetaMeasure ( Metric ): | ... | @overrides | def get_metric ( self , reset : bool = False ) Returns precisions : List[float] recalls : List[float] f1-measures : List[float] Note If self.average is not None , you will get float instead of List[float] .","title":"get_metric"},{"location":"api/training/metrics/fbeta_measure/#reset","text":"class FBetaMeasure ( Metric ): | ... | @overrides | def reset ( self ) -> None","title":"reset"},{"location":"api/training/metrics/mean_absolute_error/","text":"[ allennlp .training .metrics .mean_absolute_error ] MeanAbsoluteError # class MeanAbsoluteError ( Metric ): | def __init__ ( self ) -> None This Metric calculates the mean absolute error (MAE) between two tensors. __call__ # class MeanAbsoluteError ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters predictions : torch.Tensor A tensor of predictions of shape (batch_size, ...). gold_labels : torch.Tensor A tensor of the same shape as predictions . mask : torch.BoolTensor , optional (default = None ) A tensor of the same shape as predictions . get_metric # class MeanAbsoluteError ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns The accumulated mean absolute error. reset # class MeanAbsoluteError ( Metric ): | ... | @overrides | def reset ( self )","title":"mean_absolute_error"},{"location":"api/training/metrics/mean_absolute_error/#meanabsoluteerror","text":"class MeanAbsoluteError ( Metric ): | def __init__ ( self ) -> None This Metric calculates the mean absolute error (MAE) between two tensors.","title":"MeanAbsoluteError"},{"location":"api/training/metrics/mean_absolute_error/#__call__","text":"class MeanAbsoluteError ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters predictions : torch.Tensor A tensor of predictions of shape (batch_size, ...). gold_labels : torch.Tensor A tensor of the same shape as predictions . mask : torch.BoolTensor , optional (default = None ) A tensor of the same shape as predictions .","title":"__call__"},{"location":"api/training/metrics/mean_absolute_error/#get_metric","text":"class MeanAbsoluteError ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns The accumulated mean absolute error.","title":"get_metric"},{"location":"api/training/metrics/mean_absolute_error/#reset","text":"class MeanAbsoluteError ( Metric ): | ... | @overrides | def reset ( self )","title":"reset"},{"location":"api/training/metrics/metric/","text":"[ allennlp .training .metrics .metric ] Metric # class Metric ( Registrable ) A very general abstract class representing a metric which can be accumulated. supports_distributed # class Metric ( Registrable ): | ... | supports_distributed = False __call__ # class Metric ( Registrable ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] | ) Parameters predictions : torch.Tensor A tensor of predictions. gold_labels : torch.Tensor A tensor corresponding to some gold label to evaluate against. mask : torch.BoolTensor , optional (default = None ) A mask can be passed, in order to deal with metrics which are computed over potentially padded elements, such as sequence labels. get_metric # class Metric ( Registrable ): | ... | def get_metric ( self , reset : bool ) -> Dict [ str , Any ] Compute and return the metric. Optionally also call self.reset . reset # class Metric ( Registrable ): | ... | def reset ( self ) -> None Reset any accumulators or internal state. detach_tensors # class Metric ( Registrable ): | ... | @staticmethod | def detach_tensors ( * tensors : torch . Tensor ) -> Iterable [ torch . Tensor ] If you actually passed gradient-tracking Tensors to a Metric, there will be a huge memory leak, because it will prevent garbage collection for the computation graph. This method ensures the tensors are detached.","title":"metric"},{"location":"api/training/metrics/metric/#metric","text":"class Metric ( Registrable ) A very general abstract class representing a metric which can be accumulated.","title":"Metric"},{"location":"api/training/metrics/metric/#supports_distributed","text":"class Metric ( Registrable ): | ... | supports_distributed = False","title":"supports_distributed"},{"location":"api/training/metrics/metric/#__call__","text":"class Metric ( Registrable ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] | ) Parameters predictions : torch.Tensor A tensor of predictions. gold_labels : torch.Tensor A tensor corresponding to some gold label to evaluate against. mask : torch.BoolTensor , optional (default = None ) A mask can be passed, in order to deal with metrics which are computed over potentially padded elements, such as sequence labels.","title":"__call__"},{"location":"api/training/metrics/metric/#get_metric","text":"class Metric ( Registrable ): | ... | def get_metric ( self , reset : bool ) -> Dict [ str , Any ] Compute and return the metric. Optionally also call self.reset .","title":"get_metric"},{"location":"api/training/metrics/metric/#reset","text":"class Metric ( Registrable ): | ... | def reset ( self ) -> None Reset any accumulators or internal state.","title":"reset"},{"location":"api/training/metrics/metric/#detach_tensors","text":"class Metric ( Registrable ): | ... | @staticmethod | def detach_tensors ( * tensors : torch . Tensor ) -> Iterable [ torch . Tensor ] If you actually passed gradient-tracking Tensors to a Metric, there will be a huge memory leak, because it will prevent garbage collection for the computation graph. This method ensures the tensors are detached.","title":"detach_tensors"},{"location":"api/training/metrics/pearson_correlation/","text":"[ allennlp .training .metrics .pearson_correlation ] PearsonCorrelation # class PearsonCorrelation ( Metric ): | def __init__ ( self ) -> None This Metric calculates the sample Pearson correlation coefficient (r) between two tensors. Each element in the two tensors is assumed to be a different observation of the variable (i.e., the input tensors are implicitly flattened into vectors and the correlation is calculated between the vectors). This implementation is mostly modeled after the streaming_pearson_correlation function in Tensorflow. See https://github.com/tensorflow/tensorflow/blob/v1.10.1/tensorflow/contrib/metrics/python/ops/metric_ops.py#L3267 . This metric delegates to the Covariance metric the tracking of three [co]variances: covariance(predictions, labels) , i.e. covariance covariance(predictions, predictions) , i.e. variance of predictions covariance(labels, labels) , i.e. variance of labels If we have these values, the sample Pearson correlation coefficient is simply: r = covariance / (sqrt(predictions_variance) * sqrt(labels_variance)) if predictions_variance or labels_variance is 0, r is 0 __call__ # class PearsonCorrelation ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters predictions : torch.Tensor A tensor of predictions of shape (batch_size, ...). gold_labels : torch.Tensor A tensor of the same shape as predictions . mask : torch.BoolTensor , optional (default = None ) A tensor of the same shape as predictions . get_metric # class PearsonCorrelation ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns The accumulated sample Pearson correlation. reset # class PearsonCorrelation ( Metric ): | ... | @overrides | def reset ( self )","title":"pearson_correlation"},{"location":"api/training/metrics/pearson_correlation/#pearsoncorrelation","text":"class PearsonCorrelation ( Metric ): | def __init__ ( self ) -> None This Metric calculates the sample Pearson correlation coefficient (r) between two tensors. Each element in the two tensors is assumed to be a different observation of the variable (i.e., the input tensors are implicitly flattened into vectors and the correlation is calculated between the vectors). This implementation is mostly modeled after the streaming_pearson_correlation function in Tensorflow. See https://github.com/tensorflow/tensorflow/blob/v1.10.1/tensorflow/contrib/metrics/python/ops/metric_ops.py#L3267 . This metric delegates to the Covariance metric the tracking of three [co]variances: covariance(predictions, labels) , i.e. covariance covariance(predictions, predictions) , i.e. variance of predictions covariance(labels, labels) , i.e. variance of labels If we have these values, the sample Pearson correlation coefficient is simply: r = covariance / (sqrt(predictions_variance) * sqrt(labels_variance)) if predictions_variance or labels_variance is 0, r is 0","title":"PearsonCorrelation"},{"location":"api/training/metrics/pearson_correlation/#__call__","text":"class PearsonCorrelation ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters predictions : torch.Tensor A tensor of predictions of shape (batch_size, ...). gold_labels : torch.Tensor A tensor of the same shape as predictions . mask : torch.BoolTensor , optional (default = None ) A tensor of the same shape as predictions .","title":"__call__"},{"location":"api/training/metrics/pearson_correlation/#get_metric","text":"class PearsonCorrelation ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns The accumulated sample Pearson correlation.","title":"get_metric"},{"location":"api/training/metrics/pearson_correlation/#reset","text":"class PearsonCorrelation ( Metric ): | ... | @overrides | def reset ( self )","title":"reset"},{"location":"api/training/metrics/perplexity/","text":"[ allennlp .training .metrics .perplexity ] Perplexity # class Perplexity ( Average ) Perplexity is a common metric used for evaluating how well a language model predicts a sample. NotesAssumes negative log likelihood loss of each batch (base e). Provides the average perplexity of the batches. get_metric # class Perplexity ( Average ): | ... | @overrides | def get_metric ( self , reset : bool = False ) Returns The accumulated perplexity.","title":"perplexity"},{"location":"api/training/metrics/perplexity/#perplexity","text":"class Perplexity ( Average ) Perplexity is a common metric used for evaluating how well a language model predicts a sample. NotesAssumes negative log likelihood loss of each batch (base e). Provides the average perplexity of the batches.","title":"Perplexity"},{"location":"api/training/metrics/perplexity/#get_metric","text":"class Perplexity ( Average ): | ... | @overrides | def get_metric ( self , reset : bool = False ) Returns The accumulated perplexity.","title":"get_metric"},{"location":"api/training/metrics/rouge/","text":"[ allennlp .training .metrics .rouge ] ROUGE # class ROUGE ( Metric ): | def __init__ ( | self , | ngram_size : int = 2 , | exclude_indices : Set [ int ] = None | ) -> None Recall-Oriented Understudy for Gisting Evaluation (ROUGE) ROUGE is a metric for measuring the quality of summaries. It is based on calculating the recall between ngrams in the predicted summary and a set of reference summaries. See [Lin, \"ROUGE: A Package For Automatic Evaluation Of Summaries\", 2004] (https://api.semanticscholar.org/CorpusID:964287). Parameters ngram_size : int , optional (default = 2 ) ROUGE scores are calculate for ROUGE-1 .. ROUGE- ngram_size exclude_indices : Set[int] , optional (default = None ) Indices to exclude when calculating ngrams. This should usually include the indices of the start, end, and pad tokens. reset # class ROUGE ( Metric ): | ... | @overrides | def reset ( self ) -> None __call__ # class ROUGE ( Metric ): | ... | @overrides | def __call__ ( | self , | predictions : torch . LongTensor , | gold_targets : torch . LongTensor | ) -> None Update recall counts. Parameters predictions : torch.LongTensor Batched predicted tokens of shape (batch_size, max_sequence_length) . references : torch.LongTensor Batched reference (gold) sequences with shape (batch_size, max_gold_sequence_length) . Returns None get_metric # class ROUGE ( Metric ): | ... | @overrides | def get_metric ( self , reset : bool = False ) -> Dict [ str , float ] Parameters reset : bool , optional (default = False ) Reset any accumulators or internal state. Returns Dict[str, float]: A dictionary containing ROUGE-1 .. ROUGE-ngram_size scores.","title":"rouge"},{"location":"api/training/metrics/rouge/#rouge","text":"class ROUGE ( Metric ): | def __init__ ( | self , | ngram_size : int = 2 , | exclude_indices : Set [ int ] = None | ) -> None Recall-Oriented Understudy for Gisting Evaluation (ROUGE) ROUGE is a metric for measuring the quality of summaries. It is based on calculating the recall between ngrams in the predicted summary and a set of reference summaries. See [Lin, \"ROUGE: A Package For Automatic Evaluation Of Summaries\", 2004] (https://api.semanticscholar.org/CorpusID:964287). Parameters ngram_size : int , optional (default = 2 ) ROUGE scores are calculate for ROUGE-1 .. ROUGE- ngram_size exclude_indices : Set[int] , optional (default = None ) Indices to exclude when calculating ngrams. This should usually include the indices of the start, end, and pad tokens.","title":"ROUGE"},{"location":"api/training/metrics/rouge/#reset","text":"class ROUGE ( Metric ): | ... | @overrides | def reset ( self ) -> None","title":"reset"},{"location":"api/training/metrics/rouge/#__call__","text":"class ROUGE ( Metric ): | ... | @overrides | def __call__ ( | self , | predictions : torch . LongTensor , | gold_targets : torch . LongTensor | ) -> None Update recall counts. Parameters predictions : torch.LongTensor Batched predicted tokens of shape (batch_size, max_sequence_length) . references : torch.LongTensor Batched reference (gold) sequences with shape (batch_size, max_gold_sequence_length) . Returns None","title":"__call__"},{"location":"api/training/metrics/rouge/#get_metric","text":"class ROUGE ( Metric ): | ... | @overrides | def get_metric ( self , reset : bool = False ) -> Dict [ str , float ] Parameters reset : bool , optional (default = False ) Reset any accumulators or internal state. Returns Dict[str, float]: A dictionary containing ROUGE-1 .. ROUGE-ngram_size scores.","title":"get_metric"},{"location":"api/training/metrics/sequence_accuracy/","text":"[ allennlp .training .metrics .sequence_accuracy ] SequenceAccuracy # class SequenceAccuracy ( Metric ): | def __init__ ( self ) -> None Sequence Top-K accuracy. Assumes integer labels, with each item to be classified having a single correct class. __call__ # class SequenceAccuracy ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters predictions : torch.Tensor A tensor of predictions of shape (batch_size, k, sequence_length). gold_labels : torch.Tensor A tensor of integer class label of shape (batch_size, sequence_length). mask : torch.BoolTensor , optional (default = None ) A masking tensor the same size as gold_labels . get_metric # class SequenceAccuracy ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns The accumulated accuracy. reset # class SequenceAccuracy ( Metric ): | ... | @overrides | def reset ( self )","title":"sequence_accuracy"},{"location":"api/training/metrics/sequence_accuracy/#sequenceaccuracy","text":"class SequenceAccuracy ( Metric ): | def __init__ ( self ) -> None Sequence Top-K accuracy. Assumes integer labels, with each item to be classified having a single correct class.","title":"SequenceAccuracy"},{"location":"api/training/metrics/sequence_accuracy/#__call__","text":"class SequenceAccuracy ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters predictions : torch.Tensor A tensor of predictions of shape (batch_size, k, sequence_length). gold_labels : torch.Tensor A tensor of integer class label of shape (batch_size, sequence_length). mask : torch.BoolTensor , optional (default = None ) A masking tensor the same size as gold_labels .","title":"__call__"},{"location":"api/training/metrics/sequence_accuracy/#get_metric","text":"class SequenceAccuracy ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns The accumulated accuracy.","title":"get_metric"},{"location":"api/training/metrics/sequence_accuracy/#reset","text":"class SequenceAccuracy ( Metric ): | ... | @overrides | def reset ( self )","title":"reset"},{"location":"api/training/metrics/span_based_f1_measure/","text":"[ allennlp .training .metrics .span_based_f1_measure ] TAGS_TO_SPANS_FUNCTION_TYPE # TAGS_TO_SPANS_FUNCTION_TYPE = Callable [[ List [ str ], Optional [ List [ str ]]], List [ TypedStringSpan ]] SpanBasedF1Measure # class SpanBasedF1Measure ( Metric ): | def __init__ ( | self , | vocabulary : Vocabulary , | tag_namespace : str = \"tags\" , | ignore_classes : List [ str ] = None , | label_encoding : Optional [ str ] = \"BIO\" , | tags_to_spans_function : Optional [ TAGS_TO_SPANS_FUNCTION_TYPE ] = None | ) -> None The Conll SRL metrics are based on exact span matching. This metric implements span-based precision and recall metrics for a BIO tagging scheme. It will produce precision, recall and F1 measures per tag, as well as overall statistics. Note that the implementation of this metric is not exactly the same as the perl script used to evaluate the CONLL 2005 data - particularly, it does not consider continuations or reference spans as constituents of the original span. However, it is a close proxy, which can be helpful for judging model performance during training. This metric works properly when the spans are unlabeled (i.e., your labels are simply \"B\", \"I\", \"O\" if using the \"BIO\" label encoding). __call__ # class SpanBasedF1Measure ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None , | prediction_map : Optional [ torch . Tensor ] = None | ) Parameters predictions : torch.Tensor A tensor of predictions of shape (batch_size, sequence_length, num_classes). gold_labels : torch.Tensor A tensor of integer class label of shape (batch_size, sequence_length). It must be the same shape as the predictions tensor without the num_classes dimension. mask : torch.BoolTensor , optional (default = None ) A masking tensor the same size as gold_labels . prediction_map : torch.Tensor , optional (default = None ) A tensor of size (batch_size, num_classes) which provides a mapping from the index of predictions to the indices of the label vocabulary. If provided, the output label at each timestep will be vocabulary.get_index_to_token_vocabulary(prediction_map[batch, argmax(predictions[batch, t])) , rather than simply vocabulary.get_index_to_token_vocabulary(argmax(predictions[batch, t])) . This is useful in cases where each Instance in the dataset is associated with a different possible subset of labels from a large label-space (IE FrameNet, where each frame has a different set of possible roles associated with it). get_metric # class SpanBasedF1Measure ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns Dict[str, float] A Dict per label containing following the span based metrics: precision : float recall : float f1-measure : float Additionally, an overall key is included, which provides the precision, recall and f1-measure for all spans. reset # class SpanBasedF1Measure ( Metric ): | ... | def reset ( self )","title":"span_based_f1_measure"},{"location":"api/training/metrics/span_based_f1_measure/#tags_to_spans_function_type","text":"TAGS_TO_SPANS_FUNCTION_TYPE = Callable [[ List [ str ], Optional [ List [ str ]]], List [ TypedStringSpan ]]","title":"TAGS_TO_SPANS_FUNCTION_TYPE"},{"location":"api/training/metrics/span_based_f1_measure/#spanbasedf1measure","text":"class SpanBasedF1Measure ( Metric ): | def __init__ ( | self , | vocabulary : Vocabulary , | tag_namespace : str = \"tags\" , | ignore_classes : List [ str ] = None , | label_encoding : Optional [ str ] = \"BIO\" , | tags_to_spans_function : Optional [ TAGS_TO_SPANS_FUNCTION_TYPE ] = None | ) -> None The Conll SRL metrics are based on exact span matching. This metric implements span-based precision and recall metrics for a BIO tagging scheme. It will produce precision, recall and F1 measures per tag, as well as overall statistics. Note that the implementation of this metric is not exactly the same as the perl script used to evaluate the CONLL 2005 data - particularly, it does not consider continuations or reference spans as constituents of the original span. However, it is a close proxy, which can be helpful for judging model performance during training. This metric works properly when the spans are unlabeled (i.e., your labels are simply \"B\", \"I\", \"O\" if using the \"BIO\" label encoding).","title":"SpanBasedF1Measure"},{"location":"api/training/metrics/span_based_f1_measure/#__call__","text":"class SpanBasedF1Measure ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None , | prediction_map : Optional [ torch . Tensor ] = None | ) Parameters predictions : torch.Tensor A tensor of predictions of shape (batch_size, sequence_length, num_classes). gold_labels : torch.Tensor A tensor of integer class label of shape (batch_size, sequence_length). It must be the same shape as the predictions tensor without the num_classes dimension. mask : torch.BoolTensor , optional (default = None ) A masking tensor the same size as gold_labels . prediction_map : torch.Tensor , optional (default = None ) A tensor of size (batch_size, num_classes) which provides a mapping from the index of predictions to the indices of the label vocabulary. If provided, the output label at each timestep will be vocabulary.get_index_to_token_vocabulary(prediction_map[batch, argmax(predictions[batch, t])) , rather than simply vocabulary.get_index_to_token_vocabulary(argmax(predictions[batch, t])) . This is useful in cases where each Instance in the dataset is associated with a different possible subset of labels from a large label-space (IE FrameNet, where each frame has a different set of possible roles associated with it).","title":"__call__"},{"location":"api/training/metrics/span_based_f1_measure/#get_metric","text":"class SpanBasedF1Measure ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns Dict[str, float] A Dict per label containing following the span based metrics: precision : float recall : float f1-measure : float Additionally, an overall key is included, which provides the precision, recall and f1-measure for all spans.","title":"get_metric"},{"location":"api/training/metrics/span_based_f1_measure/#reset","text":"class SpanBasedF1Measure ( Metric ): | ... | def reset ( self )","title":"reset"},{"location":"api/training/metrics/spearman_correlation/","text":"[ allennlp .training .metrics .spearman_correlation ] SpearmanCorrelation # class SpearmanCorrelation ( Metric ): | def __init__ ( self ) -> None This Metric calculates the sample Spearman correlation coefficient (r) between two tensors. Each element in the two tensors is assumed to be a different observation of the variable (i.e., the input tensors are implicitly flattened into vectors and the correlation is calculated between the vectors). https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient __call__ # class SpearmanCorrelation ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters predictions : torch.Tensor A tensor of predictions of shape (batch_size, ...). gold_labels : torch.Tensor A tensor of the same shape as predictions . mask : torch.BoolTensor , optional (default = None ) A tensor of the same shape as predictions . get_metric # class SpearmanCorrelation ( Metric ): | ... | @overrides | def get_metric ( self , reset : bool = False ) Returns The accumulated sample Spearman correlation. reset # class SpearmanCorrelation ( Metric ): | ... | @overrides | def reset ( self )","title":"spearman_correlation"},{"location":"api/training/metrics/spearman_correlation/#spearmancorrelation","text":"class SpearmanCorrelation ( Metric ): | def __init__ ( self ) -> None This Metric calculates the sample Spearman correlation coefficient (r) between two tensors. Each element in the two tensors is assumed to be a different observation of the variable (i.e., the input tensors are implicitly flattened into vectors and the correlation is calculated between the vectors). https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient","title":"SpearmanCorrelation"},{"location":"api/training/metrics/spearman_correlation/#__call__","text":"class SpearmanCorrelation ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None | ) Parameters predictions : torch.Tensor A tensor of predictions of shape (batch_size, ...). gold_labels : torch.Tensor A tensor of the same shape as predictions . mask : torch.BoolTensor , optional (default = None ) A tensor of the same shape as predictions .","title":"__call__"},{"location":"api/training/metrics/spearman_correlation/#get_metric","text":"class SpearmanCorrelation ( Metric ): | ... | @overrides | def get_metric ( self , reset : bool = False ) Returns The accumulated sample Spearman correlation.","title":"get_metric"},{"location":"api/training/metrics/spearman_correlation/#reset","text":"class SpearmanCorrelation ( Metric ): | ... | @overrides | def reset ( self )","title":"reset"},{"location":"api/training/metrics/unigram_recall/","text":"[ allennlp .training .metrics .unigram_recall ] UnigramRecall # class UnigramRecall ( Metric ): | def __init__ ( self ) -> None Unigram top-K recall. This does not take word order into account. Assumes integer labels, with each item to be classified having a single correct class. __call__ # class UnigramRecall ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None , | end_index : int = sys . maxsize | ) Parameters predictions : torch.Tensor A tensor of predictions of shape (batch_size, k, sequence_length). gold_labels : torch.Tensor A tensor of integer class label of shape (batch_size, sequence_length). mask : torch.BoolTensor , optional (default = None ) A masking tensor the same size as gold_labels . get_metric # class UnigramRecall ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns The accumulated recall. reset # class UnigramRecall ( Metric ): | ... | @overrides | def reset ( self )","title":"unigram_recall"},{"location":"api/training/metrics/unigram_recall/#unigramrecall","text":"class UnigramRecall ( Metric ): | def __init__ ( self ) -> None Unigram top-K recall. This does not take word order into account. Assumes integer labels, with each item to be classified having a single correct class.","title":"UnigramRecall"},{"location":"api/training/metrics/unigram_recall/#__call__","text":"class UnigramRecall ( Metric ): | ... | def __call__ ( | self , | predictions : torch . Tensor , | gold_labels : torch . Tensor , | mask : Optional [ torch . BoolTensor ] = None , | end_index : int = sys . maxsize | ) Parameters predictions : torch.Tensor A tensor of predictions of shape (batch_size, k, sequence_length). gold_labels : torch.Tensor A tensor of integer class label of shape (batch_size, sequence_length). mask : torch.BoolTensor , optional (default = None ) A masking tensor the same size as gold_labels .","title":"__call__"},{"location":"api/training/metrics/unigram_recall/#get_metric","text":"class UnigramRecall ( Metric ): | ... | def get_metric ( self , reset : bool = False ) Returns The accumulated recall.","title":"get_metric"},{"location":"api/training/metrics/unigram_recall/#reset","text":"class UnigramRecall ( Metric ): | ... | @overrides | def reset ( self )","title":"reset"},{"location":"api/training/momentum_schedulers/inverted_triangular/","text":"[ allennlp .training .momentum_schedulers .inverted_triangular ] InvertedTriangular # class InvertedTriangular ( MomentumScheduler ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | cool_down : int , | warm_up : int , | ratio : int = 10 , | last_epoch : int = - 1 | ) -> None Adjust momentum during training according to an inverted triangle-like schedule. The momentum starts off high, then decreases linearly for cool_down epochs, until reaching 1 / ratio th of the original value. Then the momentum increases linearly for warm_up epochs until reaching its original value again. If there are still more epochs left over to train, the momentum will stay flat at the original value. Registered as a MomentumScheduler with name \"inverted_triangular\". The \"optimizer\" argument does not get an entry in a configuration file for the object. get_values # class InvertedTriangular ( MomentumScheduler ): | ... | def get_values ( self )","title":"inverted_triangular"},{"location":"api/training/momentum_schedulers/inverted_triangular/#invertedtriangular","text":"class InvertedTriangular ( MomentumScheduler ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | cool_down : int , | warm_up : int , | ratio : int = 10 , | last_epoch : int = - 1 | ) -> None Adjust momentum during training according to an inverted triangle-like schedule. The momentum starts off high, then decreases linearly for cool_down epochs, until reaching 1 / ratio th of the original value. Then the momentum increases linearly for warm_up epochs until reaching its original value again. If there are still more epochs left over to train, the momentum will stay flat at the original value. Registered as a MomentumScheduler with name \"inverted_triangular\". The \"optimizer\" argument does not get an entry in a configuration file for the object.","title":"InvertedTriangular"},{"location":"api/training/momentum_schedulers/inverted_triangular/#get_values","text":"class InvertedTriangular ( MomentumScheduler ): | ... | def get_values ( self )","title":"get_values"},{"location":"api/training/momentum_schedulers/momentum_scheduler/","text":"[ allennlp .training .momentum_schedulers .momentum_scheduler ] MomentumScheduler # class MomentumScheduler ( Scheduler , Registrable ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | last_epoch : int = - 1 | ) -> None get_values # class MomentumScheduler ( Scheduler , Registrable ): | ... | def get_values ( self ) -> None","title":"momentum_scheduler"},{"location":"api/training/momentum_schedulers/momentum_scheduler/#momentumscheduler","text":"class MomentumScheduler ( Scheduler , Registrable ): | def __init__ ( | self , | optimizer : torch . optim . Optimizer , | last_epoch : int = - 1 | ) -> None","title":"MomentumScheduler"},{"location":"api/training/momentum_schedulers/momentum_scheduler/#get_values","text":"class MomentumScheduler ( Scheduler , Registrable ): | ... | def get_values ( self ) -> None","title":"get_values"}]}